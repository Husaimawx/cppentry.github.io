---
layout:     post
title:      Hadoop简介
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为小黄鸭原创文章，未经博主允许不得转载。					https://blog.csdn.net/qq_29721419/article/details/82388152				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h3 id="hadoop的架构">Hadoop的架构</h3>

<p>在其核心，Hadoop主要有两个层次，即：</p>

<ul>
<li>加工/计算层(MapReduce)</li>
<li>存储层(Hadoop分布式文件系统)</li>
</ul>

<p>除了上面提到的两个核心组件，Hadoop的框架还包括以下两个模块：</p>

<ul>
<li>Hadoop通用：这是Java库和其他Hadoop组件所需的实用工具</li>
<li>Hadoop YARN ：这是作业调度和集群资源管理的框架</li>
</ul>

<p>Hadoop Streaming 是一个实用程序，它允许用户使用任何可执行文件（例如shell实用程序）作为映射器和/或reducer创建和运行作业。</p>



<h3 id="hdfs">HDFS:</h3>

<p>HDFS遵循主从架构，它具有以下元素。</p>

<p>1、名称节点 -Namenode</p>

<p>名称节点是包含GNU/Linux操作系统和软件名称节点的普通硬件。它是一个可以在商品硬件上运行的软件。具有名称节点系统作为主服务器，它执行以下任务： <br>
- 管理文件系统命名空间。 <br>
- 规范客户端对文件的访问。 <br>
- 它也执行文件系统操作，如重命名，关闭和打开的文件和目录。</p>

<p>2、数据节点 - Datanode</p>

<p>Datanode具有GNU/Linux操作系统和软件Datanode的普通硬件。对于集群中的每个节点(普通硬件/系统)，有一个数据节点。这些节点管理数据存储在它们的系统。 <br>
- 数据节点上的文件系统执行的读写操作，根据客户的请求。 <br>
- 还根据名称节点的指令执行操作，如块的创建，删除和复制。</p>

<p>3、块 -block</p>

<p>一般用户数据存储在HDFS文件。在一个文件系统中的文件将被划分为一个或多个段和/或存储在个人数据的节点。这些文件段被称为块。换句话说，数据的HDFS可以读取或写入的最小量被称为一个块。缺省的块大小为64MB，但它可以增加按需要在HDFS配置来改变</p>



<h3 id="hdfs常用命令">HDFS常用命令</h3>

<ul>
<li>运行jar包：hadoop jar /Users/kexin/work/projects/Hadoop/target/hadoop-0.0.1-SNAPSHOT.jar com.kexin.hadoop.units.WordCount /test/test.txt /project/wordcount/output</li>
<li>文件系统操作：hadoop fs -cat|ls|mkdir</li>
<li>上传文件：hadoop dfs -put ./testdata.txt /testdata</li>
<li>递归删除目录及文件：hadoop fs -rmr /testdata</li>
<li>删除文件：hadoop fs -rm /testdata.txt</li>
</ul>



<h3 id="mapreduce">MapReduce</h3>

<p>教程： <br>
- <a href="https://www.cnblogs.com/huxinga/p/6939896.html" rel="nofollow">https://www.cnblogs.com/huxinga/p/6939896.html</a> <br>
- <a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0" rel="nofollow">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0</a></p>

<p>MapReduce计划分三个阶段执行，即映射阶段，shuffle阶段，并减少阶段。</p>

<p>涉及到的角色：</p>

<p>1、客户端（client）：编写mapreduce程序，配置作业，提交作业，这就是程序员完成的工作；</p>

<p>2、JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业的执行；</p>

<p>3、TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务，TaskTracker和JobTracker的不同有个很重要的方面，就是在执行任务时候TaskTracker可以有n多个，JobTracker则只会有一个（JobTracker只能有一个就和hdfs里namenode一样存在单点故障，我会在后面的mapreduce的相关问题里讲到这个问题的）</p>

<p>4、Hdfs：保存作业的数据、配置信息等等，最后的结果也是保存在hdfs上面</p>

<ul>
<li><p>map阶段：映射或映射器的工作是处理输入数据。一般输入数据以存储在HDFS的文件或目录的形式，输入文件被传递到映射器功能线路，映射器处理该数据，并创建数据的若干小块。</p></li>
<li><p>reduce阶段：这个阶段是Shuffle阶段和Reduce阶段的组合。减速器的工作是处理该来自映射器中的数据。处理之后，它产生一组新的输出，这将被存储在HDFS。</p></li>
</ul>

<p><img src="https://images2015.cnblogs.com/blog/1106357/201706/1106357-20170604110818743-1832485363.png" alt="image" title=""></p>

<p>在一个MapReduce工作过程中：</p>

<p>1、由Hadoop发送Map和Reduce任务到集群的相应服务器</p>

<p>2、框架管理数据传递，例如发出任务的所有节点之间的集群周围的详细信息，验证任务完成，和复制数据</p>

<p>3、大部分的计算发生在与在本地磁盘上，可以减少网络通信量数据的节点</p>

<p>4、给定的任务完成后，将收集并减少了数据，以一个合适的结果发送回Hadoop服务器</p>

<p>应用程序通常实现Mapper和Reducer接口以提供map和reduce方法：</p>



<pre class="prettyprint"><code class=" hljs axapta"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TokenizerMapper</span> <span class="hljs-inheritance"><span class="hljs-keyword">extends</span></span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">Object</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; {</span>

        <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">static</span> IntWritable one = <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>);
        <span class="hljs-keyword">private</span> Text word = <span class="hljs-keyword">new</span> Text();

        <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> map(Object key, Text value, Context context) throws IOException, InterruptedException {
            StringTokenizer itr = <span class="hljs-keyword">new</span> StringTokenizer(value.toString());
            <span class="hljs-keyword">while</span> (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">IntSumReducer</span> <span class="hljs-inheritance"><span class="hljs-keyword">extends</span></span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt; {</span>
        <span class="hljs-keyword">private</span> IntWritable result = <span class="hljs-keyword">new</span> IntWritable();

        <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            <span class="hljs-keyword">int</span> <span class="hljs-keyword">sum</span> = <span class="hljs-number">0</span>;
            <span class="hljs-keyword">for</span> (IntWritable val : values) {
                <span class="hljs-keyword">sum</span> += val.get();
            }
            result.set(<span class="hljs-keyword">sum</span>);
            context.write(key, result);
        }
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> main(String[] args) throws Exception {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        Job job = Job.getInstance(conf, <span class="hljs-string">"word count"</span>);
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">0</span>]));
        FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(args[<span class="hljs-number">1</span>]));
        System.exit(job.waitForCompletion(<span class="hljs-keyword">true</span>) ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);
    }</code></pre>

<p>在idea中远程调试程序</p>



<pre class="prettyprint"><code class=" hljs avrasm">  System<span class="hljs-preprocessor">.setProperty</span>(<span class="hljs-string">"hadoop.home.dir"</span>, <span class="hljs-string">"/Users/kexin/work/app/hadoop/hadoop-2.6.5"</span>)<span class="hljs-comment">;</span>

    Configuration conf = new Configuration()<span class="hljs-comment">;</span>
    String uri = <span class="hljs-string">"hdfs://localhost:9000"</span><span class="hljs-comment">;</span>
    Job job = null<span class="hljs-comment">;</span>
    try {
        job = Job<span class="hljs-preprocessor">.getInstance</span>(conf)<span class="hljs-comment">;</span>
    } catch (IOException e) {
        e<span class="hljs-preprocessor">.printStackTrace</span>()<span class="hljs-comment">;</span>
    }
    job<span class="hljs-preprocessor">.setJarByClass</span>(WordCount<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
    job<span class="hljs-preprocessor">.setMapperClass</span>(TokenizerMapper<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
    job<span class="hljs-preprocessor">.setReducerClass</span>(IntSumReducer<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
    job<span class="hljs-preprocessor">.setOutputKeyClass</span>(Text<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
    job<span class="hljs-preprocessor">.setOutputValueClass</span>(IntWritable<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

    FileSystem fs = FileSystem<span class="hljs-preprocessor">.get</span>(URI<span class="hljs-preprocessor">.create</span>(uri), conf)<span class="hljs-comment">;</span>

    try {
        FileInputFormat<span class="hljs-preprocessor">.addInputPath</span>(job, new Path(<span class="hljs-string">"hdfs://localhost:9000/test/test.txt"</span>))<span class="hljs-comment">;</span>
        Path outpath = new Path(<span class="hljs-string">"hdfs://localhost:9000/project/wordcount/output"</span>)<span class="hljs-comment">;</span>
        if (fs<span class="hljs-preprocessor">.exists</span>(outpath)) {
            fs<span class="hljs-preprocessor">.delete</span>(outpath, true)<span class="hljs-comment">;</span>
        }
        FileOutputFormat<span class="hljs-preprocessor">.setOutputPath</span>(job, outpath)<span class="hljs-comment">;</span>
    } catch (IllegalArgumentException | IOException e) {
        e<span class="hljs-preprocessor">.printStackTrace</span>()<span class="hljs-comment">;</span>
    }

    try {
        job<span class="hljs-preprocessor">.submit</span>()<span class="hljs-comment">;</span>
    } catch (ClassNotFoundException | IOException | InterruptedException e) {
        e<span class="hljs-preprocessor">.printStackTrace</span>()<span class="hljs-comment">;</span>
    }</code></pre>

<p>在idea中本地调试程序</p>



<pre class="prettyprint"><code class=" hljs avrasm">System<span class="hljs-preprocessor">.setProperty</span>(<span class="hljs-string">"hadoop.home.dir"</span>, <span class="hljs-string">"/Users/kexin/work/app/hadoop/hadoop-2.6.5"</span>)<span class="hljs-comment">;</span>

    Configuration config = new Configuration()<span class="hljs-comment">;</span>

    try {
        FileSystem fs = FileSystem<span class="hljs-preprocessor">.get</span>(config)<span class="hljs-comment">;</span>

        Job job = Job<span class="hljs-preprocessor">.getInstance</span>(config)<span class="hljs-comment">;</span>
        job<span class="hljs-preprocessor">.setJarByClass</span>(WordCount<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        job<span class="hljs-preprocessor">.setJobName</span>(<span class="hljs-string">"word count"</span>)<span class="hljs-comment">;</span>

        job<span class="hljs-preprocessor">.setMapperClass</span>(TokenizerMapper<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
        job<span class="hljs-preprocessor">.setReducerClass</span>(IntSumReducer<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        job<span class="hljs-preprocessor">.setMapOutputKeyClass</span>(Text<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
        job<span class="hljs-preprocessor">.setMapOutputValueClass</span>(IntWritable<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>

        FileInputFormat<span class="hljs-preprocessor">.addInputPath</span>(job, new Path(<span class="hljs-string">"/Users/kexin/work/projects/Hadoop/src/main/resources/input"</span>))<span class="hljs-comment">;</span>
        Path outpath = new Path(<span class="hljs-string">"/Users/kexin/work/projects/Hadoop/src/main/resources/output"</span>)<span class="hljs-comment">;</span>
        if (fs<span class="hljs-preprocessor">.exists</span>(outpath)) {
            fs<span class="hljs-preprocessor">.delete</span>(outpath, true)<span class="hljs-comment">;</span>
        }
        FileOutputFormat<span class="hljs-preprocessor">.setOutputPath</span>(job, outpath)<span class="hljs-comment">;</span>

        boolean f = job<span class="hljs-preprocessor">.waitForCompletion</span>(true)<span class="hljs-comment">;</span>
        if (f) {
            System<span class="hljs-preprocessor">.out</span><span class="hljs-preprocessor">.println</span>(<span class="hljs-string">"job任务执行成功"</span>)<span class="hljs-comment">;</span>
        }
    } catch (Exception e) {
        e<span class="hljs-preprocessor">.printStackTrace</span>()<span class="hljs-comment">;</span>
    }</code></pre>



<h4 id="1映射器">1、映射器</h4>

<p>映射器将输入k/v对映射到一组中间k/v对。转换后的中间记录不需要与输入记录的类型相同。给定的输入对可以映射到零个或多个输出对。通过调用context.write（WritableComparable，Writable）来收集输出对。</p>

<p>Hadoop MapReduce框架为作业的InputFormat生成的每个InputSplit生成一个map任务。</p>

<p>总的来说，映射器实现通过Job.setMapperClass（Class）方法传递给作业。然后，框架为InputSplit中的每个k/v对调用该任务的map。</p>

<p>映射的数量通常由输入的总大小驱动，即输入文件的块总数。也可以使用Configuration.set（MRJobConfig.NUM_MAPS，int）来设置映射数量。</p>

<p>随后将与给定输出键关联的所有中间值按框架分组，并传递给Reducer以确定最终输出。用户可以通过Job.setGroupingComparatorClass（Class）指定Comparator来控制分组。</p>

<p>对Mapper输出进行排序，然后根据Reducer进行分区。分区总数与作业的reduce任务数相同。用户可以通过实现自定义分区程序来控制哪些键（以及记录）转到哪个Reducer。</p>

<p>用户可以选择通过Job.setCombinerClass（Class）指定组合器来执行中间输出的本地聚合，比如合并重复的key，这有助于减少从Mapper传输到Reducer的数据量。</p>



<h4 id="2reducer">2、Reducer</h4>

<p>Reducer有3个主要阶段：shuffle，sort和reduce</p>

<p>1、shuffle</p>

<p>Reducer的输入是映射器的排序输出。在此阶段，框架通过HTTP获取所有映射器的输出的相关分区</p>

<p>2、sort</p>

<p>框架在此阶段按键（因为不同的映射器可能输出相同的键）对Reducer输入进行分组。在获取map输出结果时，shuffle和sort阶段同时进行。</p>

<p>如果要求对中间密钥进行分组的等价规则与在减少之前对密钥进行分组的等价规则不同，则可以通过Job.setSortComparatorClass（Class）指定比较器。由于Job.setGroupingComparatorClass（Class）可用于控制中间键的分组方式，因此可以结合使用这些键来模拟值的二级排序。</p>

<p>3、reduce</p>

<p>在此阶段，为分组输入中的每个</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>