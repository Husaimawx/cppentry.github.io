---
layout:     post
title:      Spark的学习笔记
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p align="center"><strong>Spark <span style="font-family:'宋体';">的笔记</span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<p><strong><span style="color:rgb(51,51,51);">## S</span><span style="color:rgb(51,51,51);">park<span style="font-family:'宋体';">是什么</span></span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<p><strong><span style="color:rgb(51,51,51);">Apache Spark</span></strong><span style="color:rgb(51,51,51);"><span style="font-family:'宋体';">：一种快速，通用引擎用于大规模数据处理，</span>Spark<span style="font-family:'宋体';">是一个数据并行通用批量处理引擎。工作流中在一个类似的和怀旧风格的</span><span style="font-family:Arial;">MapReduce</span><span style="font-family:'宋体';">中定义，但是，比传统</span><span style="font-family:Arial;">Hadoop
 MapReduce</span><span style="font-family:'宋体';">的更能干。</span><span style="font-family:Arial;">Apache Spark</span><span style="font-family:'宋体';">有其流</span><span style="font-family:Arial;">API</span><span style="font-family:'宋体';">项目，该项目通过短间隔批次允许连续处理。</span><span style="font-family:Arial;">Apache
 Spark</span><span style="font-family:'宋体';">本身并不需要</span><span style="font-family:Arial;">Hadoop</span><span style="font-family:'宋体';">操作。但是，它的数据并行模式，需要稳定的数据优化使用共享文件系统。该稳定源的范围可以从</span><span style="font-family:Arial;">S3</span><span style="font-family:'宋体';">，</span><span style="font-family:Arial;">NFS</span><span style="font-family:'宋体';">或更典型地，</span><span style="font-family:Arial;">HDFS</span><span style="font-family:'宋体';">。执行</span><span style="font-family:Arial;">Spark</span><span style="font-family:'宋体';">应用程序并不需要</span><span style="font-family:Arial;">Hadoop
 YARN</span><span style="font-family:'宋体';">。</span><span style="font-family:Arial;">Spark</span><span style="font-family:'宋体';">有自己独立的主</span><span style="font-family:Arial;">/</span><span style="font-family:'宋体';">服务器进程。然而，这是共同的运行使用</span><span style="font-family:Arial;">YARN</span><span style="font-family:'宋体';">容器</span><span style="font-family:Arial;">Spark</span><span style="font-family:'宋体';">的应用程序。此外，</span><span style="font-family:Arial;">Spark</span><span style="font-family:'宋体';">还可以在</span><span style="font-family:Arial;">Mesos</span><span style="font-family:'宋体';">集群上运行。</span></span><span style="color:rgb(51,51,51);"><span style="font-family:'宋体';">运行在内存中。</span></span></p>
<p> </p>
<p><strong><span style="color:rgb(51,51,51);"><span style="color:rgb(51,51,51);"><strong>## </strong></span>Spark<span style="font-family:'宋体';">有什么</span></span></strong></p>
<p> </p>
<p> <img src="https://img-blog.csdn.net/20171024153620562?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p> </p>
<p><strong><span style="color:rgb(51,51,51);"><span style="color:rgb(51,51,51);"><strong>## </strong></span>Spark RDD<span style="font-family:'宋体';">是什么</span></span></strong></p>
<p> </p>
<p><span style="color:rgb(51,51,51);">RDD(Resilient Distributed Datasets,<span style="font-family:'宋体';">弹性分布式数据集</span><span style="font-family:Arial;">)</span><span style="font-family:'宋体';">是一个分区的只读记录的集合。</span><span style="font-family:Arial;">RDD</span><span style="font-family:'宋体';">只能通过在稳定的存储器或其他</span><span style="font-family:Arial;">RDD</span><span style="font-family:'宋体';">的数据上的确定性操作来创建。我们把这些操作称作变换以区别其他类型的操作。例如</span><span style="font-family:Arial;">map,filter</span><span style="font-family:'宋体';">和</span><span style="font-family:Arial;">join</span><span style="font-family:'宋体';">。</span></span><span style="color:rgb(51,51,51);"> </span></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<blockquote style="border:none;">
<p><strong><span style="color:rgb(51,51,51);">RDD <span style="font-family:'宋体';">五大特性</span></span></strong></p>
<p> </p>
</blockquote>
<blockquote style="border:none;">
<blockquote style="border:none;">
<p>1、<span style="font-family:'宋体';">一个</span>RDD<span style="font-family:'宋体';">由多个分区组成（</span><span style="font-family:Calibri;">partitinons</span><span style="font-family:'宋体';">）</span></p>
</blockquote>
<blockquote style="border:none;">
<p>2、并行计算</p>
</blockquote>
<blockquote style="border:none;">
<p>3、向上依赖，容错</p>
</blockquote>
<blockquote style="border:none;">
<p>4、<span style="font-family:'宋体';">重新分区</span>  <span style="font-family:'宋体';">（可选）</span></p>
</blockquote>
<blockquote style="border:none;">
<p>5、<span style="font-family:'宋体';">最优的计算，找到最优的位置</span> <span style="font-family:'宋体';">
（可选）</span></p>
</blockquote>
</blockquote>
<p><strong><span style="color:rgb(51,51,51);"><span style="color:rgb(51,51,51);"><strong>## </strong></span>Spark<span style="font-family:'宋体';">的几种运行模式</span></span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<blockquote style="border:none;">
<p>1、Spark On Local</p>
</blockquote>
<blockquote style="border:none;">
<blockquote style="border:none;">
<p><span style="color:rgb(85,85,85);"> </span> <span style="font-family:'微软雅黑';">此种模式下，我们只需要在安装</span>Spark时不进行hadoop和Yarn的环境配置，只要将Spark包解压即可使用，运行时Spark目录下的bin目录执行bin/spark-shell即可</p>
</blockquote>
<blockquote style="border:none;">
<p>    具体可参考这篇博客：<a href="http://blog.csdn.net/happyanger6/article/details/47070223" rel="nofollow"><span style="color:rgb(0,0,0);">http://blog.csdn.net/happyanger6/article/details/47070223</span></a></p>
</blockquote>
</blockquote>
<blockquote style="border:none;">
<p>2、Spark Standalone</p>
</blockquote>
<blockquote style="border:none;">
<blockquote style="border:none;">
<p>tandalone<span style="font-family:'微软雅黑';">模式是</span>Spark<span style="font-family:'微软雅黑';">实现的资源调度框架，其主要的节点有</span>Client<span style="font-family:'微软雅黑';">节点、</span>Master<span style="font-family:'微软雅黑';">节点和</span>Worker<span style="font-family:'微软雅黑';">节点。其中</span>Driver<span style="font-family:'微软雅黑';">既可以运行在</span>Master<span style="font-family:'微软雅黑';">节点上中，也可以运行在本地</span>Client<span style="font-family:'微软雅黑';">端。当用</span>spark-shell<span style="font-family:'微软雅黑';">交互式工具提交</span>Spark<span style="font-family:'微软雅黑';">的</span>Job<span style="font-family:'微软雅黑';">时，</span>Driver<span style="font-family:'微软雅黑';">在</span>Master<span style="font-family:'微软雅黑';">节点上运行；当使用</span>spark-submit<span style="font-family:'微软雅黑';">工具提交</span>Job<span style="font-family:'微软雅黑';">或者在</span>Eclips<span style="font-family:'微软雅黑';">、</span>IDEA<span style="font-family:'微软雅黑';">等开发平台上使用</span>”new
 SparkConf.setManager(“spark://master:7077”)”方式运行Spark<span style="font-family:'微软雅黑';">任务时，</span>Driver<span style="font-family:'微软雅黑';">是运行在本地</span>Client<span style="font-family:'微软雅黑';">端上的。</span></p>
</blockquote>
<blockquote style="border:none;">
<p><span style="color:rgb(85,85,85);">       <img src="https://img-blog.csdn.net/20171024153749423?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></span></p>
</blockquote>
<blockquote style="border:none;">
<p>    其运行过程如下：</p>
</blockquote>
<blockquote style="border:none;">
<p>    1.SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）；</p>
</blockquote>
<blockquote style="border:none;">
<p>    2.Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；</p>
</blockquote>
<blockquote style="border:none;">
<p>    3.StandaloneExecutorBackend向SparkContext注册；</p>
</blockquote>
<blockquote style="border:none;">
<p>    4.SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后以Stage（或者称为TaskSet）提交给Task Scheduler，Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；</p>
</blockquote>
<blockquote style="border:none;">
<p>    5.StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成。</p>
</blockquote>
<blockquote style="border:none;">
<p>    6.所有Task完成后，SparkContext向Master注销，释放资源。</p>
</blockquote>
</blockquote>
<blockquote style="border:none;">
<p> </p>
<p>   3、Spark On Yarn</p>
<p> </p>
</blockquote>
<blockquote style="border:none;">
<blockquote style="border:none;">
<p align="justify">  任何框架与YARN<span style="font-family:'微软雅黑';">的结合，都必须遵循</span>YARN<span style="font-family:'微软雅黑';">的开发模式。在分析</span>Spark on YARN<span style="font-family:'微软雅黑';">的实现细节之前，有必要先分析一下</span>YARN<span style="font-family:'微软雅黑';">框架的一些基本原理。</span>Yarn框架的基本流程如下：</p>
</blockquote>
<blockquote style="border:none;">
<p align="justify"><span style="color:rgb(85,85,85);">       <img src="https://img-blog.csdn.net/20171024153843862?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="">  </span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify">        其中，ResourceManager<span style="font-family:'微软雅黑';">负责将集群的资源分配给各个应用使用，而资源分配和调度的基本单位是</span>Container<span style="font-family:'微软雅黑';">，其中封装了机器资源，如内存、</span>CPU<span style="font-family:'微软雅黑';">、磁盘和网络等，每个任务会被分配一个</span>Container<span style="font-family:'微软雅黑';">，该任务只能在该</span>Container<span style="font-family:'微软雅黑';">中执行，并使用该</span>Container<span style="font-family:'微软雅黑';">封装的资源。</span>NodeManager<span style="font-family:'微软雅黑';">是一个个的计算节点，主要负责启动</span>Application<span style="font-family:'微软雅黑';">所需的</span>Container<span style="font-family:'微软雅黑';">，监控资源（内存、</span>CPU<span style="font-family:'微软雅黑';">、磁盘和网络等）的使用情况并将之汇报给</span>ResourceManager<span style="font-family:'微软雅黑';">。</span>ResourceManager<span style="font-family:'微软雅黑';">与</span>NodeManagers<span style="font-family:'微软雅黑';">共同组成整个数据计算框架，</span>ApplicationMaster<span style="font-family:'微软雅黑';">与具体的</span>Application<span style="font-family:'微软雅黑';">相关，主要负责同</span>ResourceManager<span style="font-family:'微软雅黑';">协商以获取合适的</span>Container<span style="font-family:'微软雅黑';">，并跟踪这些</span>Container<span style="font-family:'微软雅黑';">的状态和监控其进度。</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify"><br></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify"><strong><span style="font-size:14px;"><span style="font-family:'微软雅黑';">1、</span>Yarn Client模式 </span></strong></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify">         Yarn-Client<span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">模式中，</span>Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问。</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify">         YARN-client<span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">的工作流程分为以下几个步骤：</span></span></p>
</blockquote>
<blockquote style="border:none;">
<p><span style="color:rgb(85,85,85);">         <img src="https://img-blog.csdn.net/20171024153921731?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">       (1).</span><span style="color:rgb(85,85,85);">Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是</span>Yarn-Client<span style="color:rgb(51,51,51);"><span style="font-family:'微软雅黑';">模式，程序会选择</span>YarnClientClusterScheduler</span><span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">和</span>YarnClientSchedulerBackend；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">         (2).</span><span style="color:rgb(85,85,85);">ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">       (3).</span><span style="color:rgb(85,85,85);">Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">          (4).</span><span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">一旦</span>ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">        (5).</span><span style="color:rgb(85,85,85);">Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</span></p>
</blockquote>
<blockquote style="border:none;">
<p><span style="color:rgb(51,51,51);">          (6).</span><span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">应用程序运行完成后，</span>Client的SparkContext向ResourceManager申请注销并关闭自己</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify"><span style="color:rgb(85,85,85);"><span style="font-size:14px;"><strong>2、Spark Cluster模式</strong></span></span></p>
</blockquote>
<blockquote style="border:none;">
<p>          在YARN-Cluster<span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">模式中，当用户向</span>YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。</span></p>
</blockquote>
<blockquote style="border:none;">
<p>          YARN-cluster<span style="color:rgb(85,85,85);"><span style="font-family:'微软雅黑';">的工作流程分为以下几个步骤：</span></span></p>
</blockquote>
<blockquote style="border:none;">
<p><span style="color:rgb(85,85,85);">        <img src="https://img-blog.csdn.net/20171024153953572?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">         (1).</span><span style="color:rgb(85,85,85);">   Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">        (2).</span><span style="color:rgb(85,85,85);">   ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">         (3).</span><span style="color:rgb(85,85,85);">   ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">         (4).</span><span style="color:rgb(85,85,85);">   一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark
 Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等；</span></p>
</blockquote>
<blockquote style="border:none;">
<p style="background:rgb(254,254,254);"><span style="color:rgb(51,51,51);">         (5).</span><span style="color:rgb(85,85,85);">   ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify"><span style="color:rgb(51,51,51);">         (6).</span><span style="color:rgb(85,85,85);">   应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。</span></p>
<p align="justify"><strong><span style="font-size:14px;"><span style="color:#555555;">3、</span>Spark Client 和 Spark Cluster的区别</span></strong></p>
</blockquote>
</blockquote>
<blockquote style="border:none;">
<blockquote style="border:none;">
<p align="justify">        理解YARN-Client<span style="font-family:'微软雅黑';">和</span>YARN-Cluster<span style="font-family:'微软雅黑';">深层次的区别之前先清楚一个概念：</span>Application Master<span style="font-family:'微软雅黑';">。在</span>YARN<span style="font-family:'微软雅黑';">中，每个</span>Application<span style="font-family:'微软雅黑';">实例都有一个</span>ApplicationMaster<span style="font-family:'微软雅黑';">进程，它是</span>Application<span style="font-family:'微软雅黑';">启动的第一个容器。它负责和</span>ResourceManager<span style="font-family:'微软雅黑';">打交道并请求资源，获取资源之后告诉</span>NodeManager<span style="font-family:'微软雅黑';">为其启动</span>Container<span style="font-family:'微软雅黑';">。从深层次的含义讲</span>YARN-Cluster<span style="font-family:'微软雅黑';">和</span>YARN-Client<span style="font-family:'微软雅黑';">模式的区别其实就是</span>ApplicationMaster<span style="font-family:'微软雅黑';">进程的区别。</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify">               l<span style="color:rgb(85,85,85);">  YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业；</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify">              l<span style="color:rgb(85,85,85);">  YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。</span></p>
</blockquote>
<blockquote style="border:none;">
<p align="justify"><span style="color:rgb(85,85,85);">                <img src="https://img-blog.csdn.net/20171024154020383?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></span></p>
</blockquote>
</blockquote>
<p><strong><span style="color:rgb(51,51,51);"><br></span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"><span style="color:rgb(51,51,51);"><strong>##  </strong></span>Spark<span style="font-family:'宋体';">的</span><span style="font-family:Arial;">Transform</span><span style="font-family:'宋体';">与</span><span style="font-family:Arial;">Action</span><span style="font-family:'宋体';">操作</span></span></strong></p>
<p> </p>
<blockquote style="border:none;"><span style="font-family:'宋体';">注意：在没有执行</span>action<span style="font-family:'宋体';">之前在其中一个算了失败</span><span style="font-family:Calibri;">,RDD</span><span style="font-family:'宋体';">的向上依赖找到再重新计算。如果执行了</span><span style="font-family:Calibri;">actiion</span><span style="font-family:'宋体';">之后，在在其中一个算了失败要重头开始计算。<br></span> <br>
RDD<span style="font-family:'宋体';">两个操作</span><span style="font-family:Calibri;">:<br></span>Each RDD has 2 sets of parallel operations: transformation and action.<br>
(1)Transformation:Return a MappedRDD[U] by applying function f to each element<br>
(2)Action:return T by reducing the elements using specified commutative and associative binary operator<br>
 <br>
transformation:
<table><tbody><tr><td valign="top">
<p>Transformation</p>
</td>
<td valign="top">
<p>Meaning</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>map(func)</p>
</td>
<td valign="top">
<p>Return a new distributed dataset formed by passing each element of the source through a function func.</p>
<p>对调用<span style="font-family:Calibri;">map</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">RDD</span><span style="font-family:'宋体';">数据集中的每个</span><span style="font-family:Calibri;">element</span><span style="font-family:'宋体';">都使用</span><span style="font-family:Calibri;">func</span><span style="font-family:'宋体';">，然后返回一个新的</span><span style="font-family:Calibri;">RDD,</span><span style="font-family:'宋体';">这个返回的数据集是分布式的数据集</span><span style="font-family:Calibri;"> </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>filter(func)</p>
</td>
<td valign="top">
<p>Return a new dataset formed by selecting those elements of the source on which funcreturns true.</p>
<p>对调用<span style="font-family:Calibri;">filter</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">RDD</span><span style="font-family:'宋体';">数据集中的每个元素都使用</span><span style="font-family:Calibri;">func</span><span style="font-family:'宋体';">，然后返回一个包含使</span><span style="font-family:Calibri;">func</span><span style="font-family:'宋体';">为</span><span style="font-family:Calibri;">true</span><span style="font-family:'宋体';">的元素构成的</span><span style="font-family:Calibri;">RDD  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>flatMap(func)</p>
</td>
<td valign="top">
<p>Similar to map, but each input item can be mapped to 0 or more output items (so funcshould return a Seq rather than a single item).</p>
<p>和<span style="font-family:Calibri;">map</span><span style="font-family:'宋体';">差不多，但是</span><span style="font-family:Calibri;">flatMap</span><span style="font-family:'宋体';">生成的是多个结果</span><span style="font-family:Calibri;">,</span><span style="font-family:'宋体';">返回值是一个</span><span style="font-family:Calibri;">Seq</span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>mapPartitions(func)</p>
</td>
<td valign="top">
<p>Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T.</p>
<p>和<span style="font-family:Calibri;">map</span><span style="font-family:'宋体';">很像，但是</span><span style="font-family:Calibri;">map</span><span style="font-family:'宋体';">是每个</span><span style="font-family:Calibri;">element</span><span style="font-family:'宋体';">，而</span><span style="font-family:Calibri;">mapPartitions</span><span style="font-family:'宋体';">是每个</span><span style="font-family:Calibri;">partition</span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>mapPartitionsWithIndex(func)</p>
</td>
<td valign="top">
<p>Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.</p>
<p>和<span style="font-family:Calibri;">mapPartitions</span><span style="font-family:'宋体';">很像，但是</span><span style="font-family:Calibri;">func</span><span style="font-family:'宋体';">作用的是其中一个</span><span style="font-family:Calibri;">split</span><span style="font-family:'宋体';">上，所以</span><span style="font-family:Calibri;">func</span><span style="font-family:'宋体';">中应该有</span><span style="font-family:Calibri;">index  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>sample(withReplacement, fraction, seed)</p>
</td>
<td valign="top">
<p>Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.</p>
<p>抽样</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>union(otherDataset)</p>
</td>
<td valign="top">
<p>Return a new dataset that contains the union of the elements in the source dataset and the argument.</p>
<p>返回一个新的<span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">，包含源</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">和给定</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">的元素的集合</span><span style="font-family:Calibri;"> </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>intersection(otherDataset)</p>
</td>
<td valign="top">
<p>Return a new RDD that contains the intersection of elements in the source dataset and the argument.</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>distinct([numTasks]))</p>
</td>
<td valign="top">
<p>Return a new dataset that contains the distinct elements of the source dataset.</p>
<p>返回一个新的<span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">，这个</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">含有的是源</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">中的</span><span style="font-family:Calibri;">distinct</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">element  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>groupByKey([numTasks])</p>
</td>
<td valign="top">
<p>When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>
Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance. <br>
Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.</p>
<p>返回<span style="font-family:Calibri;">(K,Seq[V])</span><span style="font-family:'宋体';">，也就是</span><span style="font-family:Calibri;">hadoop</span><span style="font-family:'宋体';">中</span><span style="font-family:Calibri;">reduce</span><span style="font-family:'宋体';">函数接受的</span><span style="font-family:Calibri;">key-valuelist  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>reduceByKey(func, [numTasks])</p>
</td>
<td valign="top">
<p>When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) =&gt; V. Like in groupByKey, the number of reduce tasks is configurable
 through an optional second argument.</p>
<p>就是用一个给定的<span style="font-family:Calibri;">reduce func</span><span style="font-family:'宋体';">再作用在</span><span style="font-family:Calibri;">groupByKey</span><span style="font-family:'宋体';">产生的</span><span style="font-family:Calibri;">(K,Seq[V]),</span><span style="font-family:'宋体';">比如求和，求平均数</span><span style="font-family:Calibri;">  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</p>
</td>
<td valign="top">
<p>When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value
 type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>sortByKey([ascending], [numTasks])</p>
</td>
<td valign="top">
<p>When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.</p>
<p>按照<span style="font-family:Calibri;">key</span><span style="font-family:'宋体';">来进行排序，是升序还是降序，</span><span style="font-family:Calibri;">ascending</span><span style="font-family:'宋体';">是</span><span style="font-family:Calibri;">boolean</span><span style="font-family:'宋体';">类型</span><span style="font-family:Calibri;">  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>join(otherDataset, [numTasks])</p>
</td>
<td valign="top">
<p>When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin,rightOuterJoin, and fullOuterJoin.</p>
<p>当有两个<span style="font-family:Calibri;">KV</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">dataset(K,V)</span><span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">(K,W)</span><span style="font-family:'宋体';">，返回的是</span><span style="font-family:Calibri;">(K,(V,W))</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">dataset,numTasks</span><span style="font-family:'宋体';">为并发的任务数</span><span style="font-family:Calibri;">  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>cogroup(otherDataset, [numTasks])</p>
</td>
<td valign="top">
<p>When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called groupWith.</p>
<p>当有两个<span style="font-family:Calibri;">KV</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">dataset(K,V)</span><span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">(K,W)</span><span style="font-family:'宋体';">，返回的是</span><span style="font-family:Calibri;">(K,Seq[V],Seq[W])</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">dataset,numTasks</span><span style="font-family:'宋体';">为并发的任务数</span><span style="font-family:Calibri;">  </span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>cartesian(otherDataset)</p>
</td>
<td valign="top">
<p>When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).</p>
<p>笛卡尔积就是<span style="font-family:Calibri;">m*n</span></p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>pipe(command, [envVars])</p>
</td>
<td valign="top">
<p>Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>coalesce(numPartitions)</p>
</td>
<td valign="top">
<p>Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>repartition(numPartitions)</p>
</td>
<td valign="top">
<p>Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr><tr><td valign="top">
<p>repartitionAndSortWithinPartitions(partitioner)</p>
</td>
<td valign="top">
<p>Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the
 shuffle machinery.</p>
</td>
<td valign="top">
<p> </p>
</td>
</tr></tbody></table>
 
<p>Action:</p>
<table><tbody><tr><td valign="top">
<p>Action</p>
</td>
<td valign="top">
<p>Meaning</p>
</td>
</tr><tr><td valign="top">
<p>reduce(func)</p>
</td>
<td valign="top">
<p>Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</p>
<p>说白了就是聚集，但是传入的函数是两个参数输入返回一个值，这个函数必须是满足交换律和结合律的<span style="font-family:Calibri;">  </span></p>
</td>
</tr><tr><td valign="top">
<p>collect()</p>
</td>
<td valign="top">
<p>Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</p>
<p>一般在<span style="font-family:Calibri;">filter</span><span style="font-family:'宋体';">或者足够小的结果的时候，再用</span><span style="font-family:Calibri;">collect</span><span style="font-family:'宋体';">封装返回一个数组</span><span style="font-family:Calibri;"> </span></p>
</td>
</tr><tr><td valign="top">
<p>count()</p>
</td>
<td valign="top">
<p>Return the number of elements in the dataset. <span style="font-family:'宋体';">返回的是</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">中的</span><span style="font-family:Calibri;">element</span><span style="font-family:'宋体';">的个数</span><span style="font-family:Calibri;">  </span></p>
</td>
</tr><tr><td valign="top">
<p>first()</p>
</td>
<td valign="top">
<p>Return the first element of the dataset (similar to take(1)). <span style="font-family:'宋体';">返回的是</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">中的第一个元素</span><span style="font-family:Calibri;"> </span></p>
</td>
</tr><tr><td valign="top">
<p>take(n)</p>
</td>
<td valign="top">
<p>Return an array with the first n elements of the dataset. <span style="font-family:'宋体';">返回前</span><span style="font-family:Calibri;">n</span><span style="font-family:'宋体';">个</span><span style="font-family:Calibri;">elements</span><span style="font-family:'宋体';">，这个士</span><span style="font-family:Calibri;">driver program</span><span style="font-family:'宋体';">返回的</span><span style="font-family:Calibri;"> </span></p>
</td>
</tr><tr><td valign="top">
<p>takeSample(withReplacement,num, [seed])</p>
</td>
<td valign="top">
<p>Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. <span style="font-family:'宋体';">抽样返回一个</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">中的</span><span style="font-family:Calibri;">num</span><span style="font-family:'宋体';">个元素，随机种子</span><span style="font-family:Calibri;">seed  </span></p>
</td>
</tr><tr><td valign="top">
<p>takeOrdered(n, [ordering])</p>
</td>
<td valign="top">
<p>Return the first n elements of the RDD using either their natural order or a custom comparator.</p>
</td>
</tr><tr><td valign="top">
<p>saveAsTextFile(path)</p>
</td>
<td valign="top">
<p>Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. <span style="font-family:'宋体';">把</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">写到一个</span><span style="font-family:Calibri;">text file</span><span style="font-family:'宋体';">中，或者</span><span style="font-family:Calibri;">hdfs</span><span style="font-family:'宋体';">，或者</span><span style="font-family:Calibri;">hdfs</span><span style="font-family:'宋体';">支持的文件系统中，</span><span style="font-family:Calibri;">spark</span><span style="font-family:'宋体';">把每条记录都转换为一行记录，然后写到</span><span style="font-family:Calibri;">file</span><span style="font-family:'宋体';">中</span><span style="font-family:Calibri;">  </span></p>
</td>
</tr><tr><td valign="top">
<p>saveAsSequenceFile(path) <br>
(Java and Scala)</p>
</td>
<td valign="top">
<p>Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also
 available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). <span style="font-family:'宋体';">只能用在</span><span style="font-family:Calibri;">key-value</span><span style="font-family:'宋体';">对上，然后生成</span><span style="font-family:Calibri;">SequenceFile</span><span style="font-family:'宋体';">写到本地或者</span><span style="font-family:Calibri;">hadoop</span><span style="font-family:'宋体';">文件系统</span><span style="font-family:Calibri;">  </span></p>
</td>
</tr><tr><td valign="top">
<p>saveAsObjectFile(path) <br>
(Java and Scala)</p>
</td>
<td valign="top">
<p>Write the elements of the dataset in a simple format using Java serialization, which can then be loaded usingSparkContext.objectFile().</p>
</td>
</tr><tr><td valign="top">
<p>countByKey()</p>
</td>
<td valign="top">
<p>Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. <span style="font-family:'宋体';">返回的是</span><span style="font-family:Calibri;">key</span><span style="font-family:'宋体';">对应的个数的一个</span><span style="font-family:Calibri;">map</span><span style="font-family:'宋体';">，作用于一个</span><span style="font-family:Calibri;">RDD  </span></p>
</td>
</tr><tr><td valign="top">
<p>foreach(func)</p>
</td>
<td valign="top">
<p>Run a function func on each element of the dataset. This is usually done for side effects such as updating an<a href="#AccumLink" rel="nofollow">Accumulator</a> or interacting with external storage systems. <br>
Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See <a href="#ClosuresLink" rel="nofollow">Understanding closures </a>for more details. :<span style="font-family:'宋体';">对</span><span style="font-family:Calibri;">dataset</span><span style="font-family:'宋体';">中的每个元素都使用</span><span style="font-family:Calibri;">func</span></p>
</td>
</tr></tbody></table><p> </p>
</blockquote>
<p><strong><span style="color:rgb(51,51,51);"><span style="color:rgb(51,51,51);"><strong>##  </strong></span>RDD<span style="font-family:'宋体';">的宽依赖与窄依赖</span></span></strong></p>
<p> <img src="https://img-blog.csdn.net/20171024154119140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<blockquote style="border:none;">
<p>1、<span style="font-family:'宋体';">上面图是一个</span>job的运行。</p>
<p>2、<span style="font-family:'宋体';">三个宽依赖分别三个</span>stage,窄依赖不产生stage。</p>
<p>3、<span style="font-family:'宋体';">每个管道（</span>Pipeline）是一个task</p>
<p>4、根据算子来判断宽窄依赖。</p>
</blockquote>
<p><span style="font-family:'宋体';">好处：</span> </p>
<blockquote style="border:none;">
<p>1、减少IO，与网络</p>
<p>2、DAG的优化</p>
<p>3、<span style="font-family:'宋体';">为什么</span>A要先产生一个B,而不是直接与F一起join。</p>
</blockquote>
<blockquote style="border:none;">
<blockquote style="border:none;">
<p> <span style="font-family:'宋体';">一）先产生一个</span>B,可以做缓存。</p>
</blockquote>
<blockquote style="border:none;">
<p>二）<span style="font-family:'宋体';">如果没有的话。</span>7X3的shuffle.这样的话如查有一个失败，就会从重计算。</p>
</blockquote>
<blockquote style="border:none;">
<p>三）<span style="font-family:'宋体';">产生一个</span>B的话。先3X3，4X3的两个shuffle，如果有一个失败，只要从其他一个shuffle计算。</p>
</blockquote>
</blockquote>
<p> </p>
<p><strong><span style="color:rgb(51,51,51);"><span style="color:rgb(51,51,51);"><strong>##  </strong></span>Cluster Overview</span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<p><strong><span style="color:rgb(51,51,51);">Spark : <span style="font-family:'宋体';">
先申请资源再运行。    </span><span style="font-family:Arial;">MR:</span><span style="font-family:'宋体';">边运行边申请资源。</span></span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"><span style="font-family:'宋体';"><br></span></span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"> <img src="https://img-blog.csdn.net/20171024154149374?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1eXVxaTg0MDEyMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></span></strong></p>
<p><br></p>
<p><span style="font-family:'宋体';">步骤：</span>1<span style="font-family:'宋体';">、</span><span style="font-family:Calibri;">Drive</span><span style="font-family:'宋体';">先向</span><span style="font-family:Calibri;">Master</span><span style="font-family:'宋体';">申请是否有资源可用。</span></p>
<p>           2<span style="font-family:'宋体';">、</span><span style="font-family:Calibri;">Master</span><span style="font-family:'宋体';">再向所有</span><span style="font-family:Calibri;">WorkerNode</span><span style="font-family:'宋体';">询问后，拿到空闲的资源列表返回给</span><span style="font-family:Calibri;">Drive</span><span style="font-family:'宋体';">。</span></p>
<p>           3、Drive<span style="font-family:'宋体';">拿到后。就向</span><span style="font-family:Calibri;">WorkeNode</span><span style="font-family:'宋体';">申请资源。启动一个进程</span><span style="font-family:Calibri;">Exeutor,Exeutor</span><span style="font-family:'宋体';">返回告诉</span><span style="font-family:Calibri;">SparkContext</span>
  <span style="font-family:'宋体';">我是</span>WorkeNode<span style="font-family:'宋体';">启动的应用，可以与我交付了。</span></p>
<p>           4、<span style="font-family:'宋体';">接下就</span>SparkContext<span style="font-family:'宋体';">执行对应的业务逻辑了。这里就会在</span><span style="font-family:Calibri;">Exeutor</span><span style="font-family:'宋体';">中产生多个</span><span style="font-family:Calibri;">Task</span><span style="font-family:'宋体';">的线程运行。</span></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<p><strong><span style="color:rgb(51,51,51);"> </span></strong></p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
            </div>
                </div>