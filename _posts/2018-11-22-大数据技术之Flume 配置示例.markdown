---
layout:     post
title:      大数据技术之Flume 配置示例
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/haohaixingyun/article/details/51308518				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>大数据技术之Flume 配置示例   1 &amp; 3 <br></p>
<p><br></p>
<p>[root@bigdatacloud conf]# cat test1<br>
a1.sources = r1<br>
a1.sinks = k1<br>
a1.channels = c1<br><br>
# Describe/configure the source<br>
a1.sources.r1.type = netcat<br>
a1.sources.r1.bind =0.0.0.0<br>
a1.sources.r1.port = 44444<br><br>
# Describe the sink<br>
a1.sinks.k1.type = logger<br><br>
# Use a channel which buffers events in memory<br>
a1.channels.c1.type = memory<br>
a1.channels.c1.capacity = 1000<br>
a1.channels.c1.transactionCapacity = 100<br><br>
# Bind the source and sink to the channel<br>
a1.sources.r1.channels = c1<br>
a1.sinks.k1.channel = c1<br><br></p>
<p><br></p>
<p>====================</p>
<p>[root@bigdatacloud conf]# cat test2<br>
a1.sources=r1<br>
a1.sinks=k1<br>
a1.channels=c1<br><br>
# Describe/configure the source<br>
a1.sources.r1.type=spooldir<br>
a1.sources.r1.spoolDir=/opt/sqooldir<br><br>
# Describe the sink<br>
a1.sinks.k1.type=avro<br>
a1.sinks.k1.hostname=bigdatastorm<br>
a1.sinks.k1.port=44444<br><br>
# Use a channel which buffers events in memory<br>
a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br><br>
# Bind the source and sink to the channel<br>
a1.sources.r1.channels=c1<br>
a1.sinks.k1.channel=c1<br><br></p>
<p>=========================</p>
<p>[root@bigdatacloud conf]# cat test3<br>
a1.sources=r1<br>
a1.sinks=k1<br>
a1.channels=c1<br><br>
# Describe/configure the source<br>
a1.sources.r1.type=avro<br>
a1.sources.r1.bind=0.0.0.0<br>
a1.sources.r1.port=44444<br><br>
# Describe the sink<br>
a1.sinks.k1.type=hdfs<br>
a1.sinks.k1.hdfs.path=hdfs://mycluster/flume/data/%y-%m-%d<br>
a1.sinks.k1.hdfs.rollInterval=0<br>
a1.sinks.k1.hdfs.rollCount=0<br>
a1.sinks.k1.hdfs.rollSize=10240000<br>
a1.sinks.k1.hdfs.fileType=DataStream<br>
a1.sinks.k1.hdfs.idleTimeout=5<br>
a1.sinks.k1.hdfs.useLocalTimeStamp=true<br>
a1.sinks.k1.hdfs.callTimeout=10000<br><br>
#a1.sinks.k1.type=hdfs<br>
#a1.sinks.k1.type=hdfs<br>
#a1.sinks.k1.type=hdfs<br><br>
# Use a channel which buffers events in memory<br>
a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br><br>
# Bind the source and sink to the channel<br>
a1.sources.r1.channels=c1<br>
a1.sinks.k1.channel=c1<br><br>
====================</p>
<p>[root@bigdatacloud conf]# cat getnginxlog<br>
a1.sources=r1<br>
a1.sinks=k1<br>
a1.channels=c1<br><br>
# Describe/configure the source<br>
#a1.sources.r1.type=avro<br>
a1.sources.r1.type=exec<br>
#a1.sources.r1.bind=0.0.0.0<br>
#a1.sources.r1.port=44444<br>
a1.sources.r1.command=tail -F /opt/first_project/data/access.log<br><br>
# Describe the sink<br>
a1.sinks.k1.type=hdfs<br>
a1.sinks.k1.hdfs.path=hdfs://mycluster/flume/data1/%y-%m-%d<br>
a1.sinks.k1.hdfs.rollInterval=0<br>
a1.sinks.k1.hdfs.rollCount=0<br>
a1.sinks.k1.hdfs.rollSize=10240000<br>
a1.sinks.k1.hdfs.fileType=DataStream<br>
a1.sinks.k1.hdfs.idleTimeout=5<br>
a1.sinks.k1.hdfs.useLocalTimeStamp=true<br>
a1.sinks.k1.hdfs.callTimeout=10000<br><br>
#a1.sinks.k1.type=hdfs<br>
#a1.sinks.k1.type=hdfs<br>
#a1.sinks.k1.type=hdfs<br><br>
# Use a channel which buffers events in memory<br>
a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br><br>
# Bind the source and sink to the channel<br>
a1.sources.r1.channels=c1<br>
a1.sinks.k1.channel=c1<br><br><br></p>
            </div>
                </div>