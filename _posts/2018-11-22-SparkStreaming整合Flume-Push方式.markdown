---
layout:     post
title:      SparkStreaming整合Flume-Push方式
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><strong>SparkStreaming整合Flume有俩种方式</strong></p><p>详细学习文档地址：https://spark.apache.org/docs/latest/streaming-flume-integration.html</p><p><strong>方式一：</strong></p><p><strong>Push方式整合</strong></p><br><strong>步骤一：编写flume配置文件<br>Flume Agent的编写： flume_push_streaming.conf</strong><br><br><br>simple-agent.sources = netcat-source<br>simple-agent.sinks = avro-sink<br>simple-agent.channels = memory-channel<br><br><br>simple-agent.sources.netcat-source.type = netcat<br>simple-agent.sources.netcat-source.bind = hadoop000<br>simple-agent.sources.netcat-source.port = 44444<br><br><br>simple-agent.sinks.avro-sink.type = avro<br>simple-agent.sinks.avro-sink.hostname = 192.168.199.203<br>simple-agent.sinks.avro-sink.port = 41414<br><br><br>simple-agent.channels.memory-channel.type = memory<br><br><br>simple-agent.sources.netcat-source.channels = memory-channel<br>simple-agent.sinks.avro-sink.channel = memory-channel<br><br><p><strong>步骤2：创建测试类（FlumePushWordCount）</strong></p><p>--------------------------------------------------------------------------------------------------------------------------------------</p><p>package com.imooc.spark<br><br><br>import org.apache.spark.SparkConf<br>import org.apache.spark.streaming.flume.FlumeUtils<br>import org.apache.spark.streaming.{Seconds, StreamingContext}<br><br><br>/**<br>  * Spark Streaming整合Flume的第一种方式<br>  */<br>object FlumePushWordCount {<br><br><br>  def main(args: Array[String]): Unit = {<br><br>//为了适应生产环境，我们一般不将hostname port写死，而是通过参数判断的形式<br>    if(args.length != 2) {//为什么是2 ，一个代表hostname 一个代表port<br>      System.err.println("Usage: FlumePushWordCount &lt;hostname&gt; &lt;port&gt;")<br>      System.exit(1)<br>    }<br><br><br>    val Array(hostname, port) = args<br><br><br>    val sparkConf = new SparkConf() //.setMaster("local[2]").setAppName("FlumePushWordCount")<br>    val ssc = new StreamingContext(sparkConf, Seconds(5))<br><br><br>    //TODO... 如何使用SparkStreaming整合Flume<br>    val flumeStream = FlumeUtils.createStream(ssc, hostname, port.toInt)<br><br><br>    flumeStream.map(x=&gt; new String(x.event.getBody.array()).trim)<br>      .flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).print()<br><br><br>    ssc.start()<br>    ssc.awaitTermination()<br>  }<br></p><p>}</p><p><strong>注意：如何在idea中传入参数呢 </strong>， 在右上角Edit Configuration中的，选择programe arguments输入 0.0.0.0 44444(注意中间有空格)，代表要传入俩个参数</p><p>---------------------------------------------------------------------------------------------------------------------------------------</p><p>          <strong> pom.xml核心依赖（注意版本一致性）</strong></p><p>---------------------------------------------------------------------------------------------------------------------------------------<br>        &lt;!-- Spark Streaming 依赖--&gt;<br>        &lt;dependency&gt;<br>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br>            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;<br>            &lt;version&gt;${spark.version}&lt;/version&gt;<br>        &lt;/dependency&gt;<br><br>        &lt;!-- Spark Streaming整合Flume 依赖--&gt;<br>        &lt;dependency&gt;<br>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br>            &lt;artifactId&gt;spark-streaming-flume_2.11&lt;/artifactId&gt;<br>            &lt;version&gt;${spark.version}&lt;/version&gt;<br></p><p>        &lt;/dependency&gt;</p>        &lt;dependency&gt;<br>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br>            &lt;artifactId&gt;spark-streaming-flume-sink_2.11&lt;/artifactId&gt;<br>            &lt;version&gt;${spark.version}&lt;/version&gt;<br><p>        &lt;/dependency&gt;</p><p>---------------------------------------------------------------------------------------------------------------------------------------</p><p><br></p><p><strong>注意：本地测试和线上测试的差异性</strong></p><p>本地测试：</p><p>flume配置文件：</p><p>simple-agent.sinks.avro-sink.hostname = 192.168.199.203 (本地IP)</p><p>测试类：hostname改为0.0.0.0（代表本地）</p><p> //TODO... 如何使用SparkStreaming整合Flume<br>    val flumeStream = FlumeUtils.createStream(ssc, "0.0.0.0", port.toInt)<br></p><p>-------------------------------------------------------------------------------------</p><p><strong>Flume启动（hadoop000上）</strong></p><p>flume-ng agent  \</p>--name simple-agent   \<br>--conf $FLUME_HOME/conf    \<br>--conf-file $FLUME_HOME/conf/flume_push_streaming.conf  \<br><p>-Dflume.root.logger=INFO,console</p><p>当看到avro-sink start代表启动成功！</p><p>再在另一个客户端上hadoop000上输入telnet localhost 44444， 输入a 回车 b回车 b回车</p><p>再去idea控制台上看有没有输出</p><p>------------------------------》证明：一切流程是没有问题</p><br><br><br><br>hadoop000:是服务器的地址<br>local的模式进行Spark Streaming代码的测试  192.168.199.203<br><br><br><strong><span style="font-size:18px;">本地测试总结</span></strong><br>1）启动sparkstreaming作业<br>2) 启动flume agent<br><p>3) 通过telnet输入数据，观察IDEA控制台的输出</p><p><strong>-------------------------------------------------<span style="font-size:18px;">接下来开始线上测试</span>------------------------------------------------------</strong></p><p><br></p><p>在idea中输入mvn clean package -DiskipTests 完成打包</p><p>线上客户端执行：</p><p>客户端1：</p><p>spark-submit \<br>--class com.imooc.spark.FlumePushWordCount \<br>--master local[2] \<br>--packages org.apache.spark:spark-streaming-flume_2.11:2.2.0 \<br>/home/hadoop/lib/sparktrain-1.0.jar \<br></p><p>hadoop000 41414</p><p>客户端2：</p><p></p><p>flume-ng agent  \</p>--name simple-agent   \<br>--conf $FLUME_HOME/conf    \<br>--conf-file $FLUME_HOME/conf/flume_push_streaming.conf  \<br><p>-Dflume.root.logger=INFO,console</p><p>客户端3：</p><p>telnet localhost 44444</p><p>测试：在客户端3上输入a a a d d d c回车</p><p>在客户端1上看到输出结果代表成功！！</p><p><br></p><p><br></p><p><br></p><p><br></p>            </div>
                </div>