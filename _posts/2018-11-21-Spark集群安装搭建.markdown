---
layout:     post
title:      Spark集群安装搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><strong>1.下载Spark</strong></p>

<p>Spark是一个独立的内存计算框架，如果不考虑存储的话，可以完全独立运行，因此这里就只安装Spark集群</p>

<p>Spark下载地址：       <a href="http://spark.apache.org/downloads.html" rel="nofollow">http://spark.apache.org/downloads.html</a></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20181008203401965?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lzXzIzMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>选择好Spark和Hadoop的版本之后就可以下载了，从2.0版本开始，Spark默认使用Scala2.11</p>

<p><strong>2.上传解压</strong></p>

<p>将Spark的压缩包上传到集群的某一台机器上，然后解压缩</p>

<p><strong>3.进行Spark的配置文件的配置</strong></p>

<p>进入到Spark的目录下</p>

<pre class="has">
<code>cd conf
mv  spark-env.sh.template spark-env.sh
vi spark-env.sh</code></pre>

<p>在该配置文件中添加如下配置</p>

<pre class="has">
<code>export JAVA_HOME=你的jdk所在目录</code></pre>

<p>配置slaves文件</p>

<pre class="has">
<code>mv slaves.template slaves
vi slaves</code></pre>

<p>在slaves中添加你的Spark集群子节点机器的主机名或者ip</p>

<p><strong>4.将配置好的Spark传输到集群的其他机器上</strong></p>

<p>使用scp命令，如果集群机器特别多的话，可以使用shell编程来循环自动传输，这里不在详细说明</p>

<p><strong>5.启动Spark</strong></p>

<p>进入到Spark的主目录下</p>

<pre class="has">
<code>sbin/start-all.sh</code></pre>

<p>使用jps命令可以看出，该Spark集群有一个Master，三个Work</p>

<p> </p>

<p>Spark集群的WEBUI界面：             Master所在的主机ip：8080</p>            </div>
                </div>