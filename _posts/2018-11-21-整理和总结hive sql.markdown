---
layout:     post
title:      整理和总结hive sql
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/Duke147/article/details/17286403				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
进入hive shell<br>
#hive或者hive --service cli<br>
Hive 的启动方式:<br>
hive  命令行模式，直接输入/hive/bin/hive的执行程序，或者输入 hive –service cli<br>
hive  web界面的启动方式，hive –service hwi  <br>
hive  远程服务 (端口号10000) 启动方式，hive --service hiveserver<br>
hive  远程后台启动(关闭终端hive服务不退出): nohup hive -–service hiveserver  &amp;<br>
显示所有函数：<br>
hive&gt; show functions;<br>
查看函数用法：<br>
hive&gt; describe function substr;<br>
查看hive为某个查询使用多少个MapReduce作业<br>
hive&gt; Explain select a.id from tbname a;<br>
--------------------------------------------------------------------------<br>
表结构操作：<br>
托管表和外部表<br>
托管表会将数据移入Hive的warehouse目录；外部表则不会。经验法则是，如果所有处理都由Hive完成，<br>
应该使用托管表；但如果要用Hive和其它工具来处理同一个数据集，则使用外部表。<br>
创建表(通常stored as textfile)：<br>
hive&gt; create table tbName (id int,name string) stored as textfile;<br>
创建表并且按分割符分割行中的字段值(即导入数据的时候被导入数据是以该分割符划分的，否则导入后为null，缺省列为null)；<br>
hive&gt; create table tbName (id int,name string) row format delimited fields terminated by ',';<br>
创建外部表:<br>
hive&gt;create external table extbName(id int, name string);<br>
创建表并创建单分区字段ds(分区表指的是在创建表时指定的partition的分区空间。):<br>
hive&gt; create table tbName2 (id int, name string) partitioned by (ds string); <br>
创建表并创建双分区字段ds:<br>
hive&gt; create table tbname3 (id int, content string) partitioned by (day string, hour string);<br>
表添加一列:<br>
hive&gt; alter table tbName add columns (new_col int);<br>
添加一列并增加列字段注释:<br>
hive&gt; alter table tbName add columns (new_col2 int comment 'a comment');<br>
更改表名:<br>
hive&gt; alter table tbName rename to tbName3;<br>
删除表(删除表的元数据，如果是托管表还会删除表的数据):<br>
hive&gt;drop table tbName;<br>
只删除内容(只删除表的内容，而保留元数据，则删除数据文件)：<br>
hive&gt;dfs –rmr ‘warehouse/my-table’;<br>
删除分区，分区的元数据和数据将被一并删除：<br>
hive&gt;alter table tbname2 drop partition (dt='2008-08-08', hour='09');<br>
--------------------------------------------------------------------------<br>
元数据存储(从HDFS中将数据导入到表中都是瞬时的):<br>
将文件中的数据加载到表中(文件要有后缀名，缺省列默认为null):<br>
hive&gt; load data local inpath 'myTest.txt' overwrite into table tbName;<br>
在已创立的表上添加单分区并指定数据：<br>
hive&gt; alter table tbname2 add partition (ds='20120701') location '/user/hadoop/his_trans/record/20120701';<br>
在已创立的表上添加双分区并指定数据：<br>
hive&gt; alter table tbname2 add partition (ds='2008-08-08', hour='08') location '/path/pv1.txt' partition (dt='2008-08-08', hour='09') location '/path/pv2.txt';<br>
加载本地数据，根据给定分区列信息:<br>
hive&gt; alter table tbname2 add partition (ds='2013-12-12');<br>
hdfs数据加载进分区表中语法(当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制至Hive表对应的位置)[不建议使用]：<br>
hive&gt; load data local inpath 'part.txt' overwrite into table tbName2 partition(ds='2013-12-12');<br>
hive&gt; load data inpath '/user/hadoop/*' into table tbname3 partition(dt='2008-08-08', hour='08'); <br>
--------------------------------------------------------------------------<br>
SQL 操作：<br>
查看表结构：<br>
hive&gt; describe tbname;<br>
hive&gt; desc tbname;<br>
显示所有表:<br>
hive&gt; show tables;<br>
按正条件（正则表达式）显示表：<br>
hive&gt; show tables '.*s';<br>
查询表数据不会做mapreduce操作：<br>
hive&gt; select * from tbName;<br>
查询一列数据，会做mapreduce操作：<br>
hive&gt; select a.id from tbname a ;<br>
基于分区的查询的语句：<br>
hive&gt; select tbname2.* from tbname2 a where a.ds='2013-12-12' ;<br>
查看分区语句：<br>
hive&gt; show partitions tbname2;<br>
函数avg/sum/count/group by/order by (desc)/limit:<br>
select logdate, count(logdate) as count from access_1 group by logdate order by count limit 5;<br>
内连接(inner join)：<br>
hive&gt; SELECT sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);<br>
外连接：<br>
hive&gt; SELECT sales.*, things.* FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);<br>
hive&gt; SELECT sales.*, things.* FROM sales RIGHT OUTER JOIN things ON (sales.id = things.id);<br>
hive&gt; SELECT sales.*, things.* FROM sales FULL OUTER JOIN things ON (sales.id = things.id);<br>
in查询：Hive不支持，但可以使用LEFT SEMI JOIN<br>
hive&gt; SELECT * FROM things LEFT SEMI JOIN sales ON (sales.id = things.id);<br>
相当于sql语句：SELECT * FROM things WHERE things.id IN (SELECT id from sales);<br>
Map连接：Hive可以把较小的表放入每个Mapper的内存来执行连接操作<br>
hive&gt; SELECT /*+ MAPJOIN(things) */ sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);<br>
INSERT OVERWRITE TABLE ..SELECT：新表预先存在<br>
hive&gt; FROM records2<br>
    &gt; INSERT OVERWRITE TABLE stations_by_year SELECT year, COUNT(DISTINCT station) GROUP BY year <br>
    &gt; INSERT OVERWRITE TABLE records_by_year SELECT year, COUNT(1) GROUP BY year<br>
    &gt; INSERT OVERWRITE TABLE good_records_by_year SELECT year, COUNT(1) WHERE temperature != 9999 AND <br>
    (quality = 0 OR quality = 1 OR quality = 4 OR quality = 5 OR quality = 9) GROUP BY year;  <br>
CREATE TABLE ... AS SELECT：新表表预先不存在<br>
hive&gt;CREATE TABLE target AS SELECT col1,col2 FROM source;<br>
创建视图：<br>
hive&gt; CREATE VIEW valid_records AS SELECT * FROM records2 WHERE temperature !=9999;<br>
查看视图详细信息：<br>
hive&gt; DESCRIBE EXTENDED valid_records;<br>
--------------------------------------------------------------------------<br>
将查询数据输出至目录<br>
hive&gt; insert overwrite directory '/tmp/hdfs_out' select a.* from tbname2 a where a.ds='2013-12-12';<br>
将查询结果输出至本地目录<br>
hive&gt; insert overwrite local directory '/tmp/local_out' select ds,count(1) from tbname group by ds;<br>
hive&gt; insert overwrite table events select a.* from tbname a where a.id &lt; 100;<br>
hive&gt; insert overwrite local directory '/tmp/sum' select sum(a.pc) from tbpc a ;<br>
将一个表的统计结果插入另一个表中<br>
hive&gt; from tbname a insert overwrite table events select a.bar,count(1) where a.foo &gt; 0 group by a.bar;<br>
hive&gt; insert overwrite table events select a.bar,count(1) from tbname a where a.foo &gt; 0 group by a.bar;<br>
JOIN:<br>
hive&gt; from tbname t1 join tbname2 t2 on (t1.id = t2.id) insert overwrite table events select t1.id,t1.name,t2,ds;<br>
将多表数据插入到同一表中<br>
FROM src<br>
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key &lt; 100<br>
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key &gt;= 100 and src.key &lt; 200<br>
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key &gt;= 200 and src.key &lt; 300<br>
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/dest4.out' SELECT src.value WHERE src.key &gt;= 300;<br>
将文件流直接插入文件<br>
hive&gt; FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds &gt; '2008-08-09';<br>
This streams the data in the map phase through the script /bin/cat (like hadoop streaming). Similarly - streaming can be used on the reduce <br>
side (please see the Hive Tutorial or examples) <br>
--------------------------------------------------------------------------<br>
### 错误信息 ### <br>
问题：load数据全部为null  <br>
原因：数据分隔符的问题，反序列化数据的时候出错了，定义表的时候需要定义数据分隔符。<br>
解决：row format delimited fields terminated by ',' stored as textfile;<br><br>
create table mytable(key int , value string ) row format delimited fields terminated by ',' escaped by '\\' stored as textfile;<br>
[row format delimited]是用来设置创建的表在加载数据的时候，支持的列分隔符，如以','为分隔符；row format delimited fields terminated by ',';<br>
[terminated by]分隔符：意思是以什么字符作为分隔符，默认情况下是tab字符（\t） <br>
[enclosed by]字段括起字符<br>
[escaped by]转义字符<br>
使用"\"符号转义或者写作:ALTER TABLE splitchar SET SERDEPROPERTIES ('escape.delim' = '\\');<br>
[stored as file_format]:是用来设置加载数据的数据类型。Hive本身支持的文件格式只有：Text File，Sequence File。<br>
如果文件数据是纯文本，可以使用 [stored as textfile]。<br>
如果数据需要压缩，使用 [stored as sequence] 通常情况，只要不需要保存序列化的对象，我们默认采用[STORED AS TEXTFILE]。<br>
将CSV中数据导入表中：<br>
add jar /home/hadoop/csv-serde-1.1.2.jar;//引用了这个jar包，关于这个表的所有操作都要引入这个jar。<br>
row format serde 'com.bizo.hive.serde.csv.CSVSerde'<br>
eg：create external table trans_data<br>
(<br>
id int,<br>
name string<br>
)<br>
partitioned by (pdate string) <br>
row format serde 'com.bizo.hive.serde.csv.CSVSerde' stored as textfile;<br>
alter table trans_data add partition (pdate='20120701') location '/user/hadoop/his_trans/record/20120701';<br>
--------------------------------------------------------------------------<br>
### 错误信息 ### <br>
问题：java.lang.OutOfMemoryError: Java heap space<br>
解决：检查hiveserver服务是否开启<br>
--------------------------------------------------------------------------<br>
### 错误信息 ###<br>
java.lang.NoSuchMethodError: com.facebook.fb303.FacebookService<br>
由于hadoop与hive版本不兼容导致(hadoop-0.20.2+320)<br>
解决方法：mv $HADOOP_HOME/lib/libfb303.jar $HADOOP_HOME/lib/libfb303.jar_backup &amp;&amp; ln -s $HIVE_HOME/lib/libfb303.jar $HADOOP_HOME/lib/libfb303.jar<br>
 
            </div>
                </div>