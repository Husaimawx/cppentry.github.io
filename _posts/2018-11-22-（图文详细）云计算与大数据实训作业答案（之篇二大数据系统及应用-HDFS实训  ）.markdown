---
layout:     post
title:      （图文详细）云计算与大数据实训作业答案（之篇二大数据系统及应用-HDFS实训  ）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-dracula">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><div class="toc">
<ul>
<li><ul>
<li><a href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E5%8F%8A%E5%BA%94%E7%94%A8-hdfs%E5%AE%9E%E8%AE%AD" rel="nofollow">大数据系统及应用-HDFS实训</a><ul>
<li><ul>
<li><a href="#%E7%AC%AC1%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99" rel="nofollow">第1关：HDFS Java API编程 ——文件读写</a></li>
<li><a href="#%E7%AC%AC2%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0" rel="nofollow">第2关：HDFS Java API编程——文件上传</a></li>
<li><a href="#%E7%AC%AC3%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B-%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD" rel="nofollow">第3关：HDFS Java API编程 ——文件下载</a></li>
<li><a href="#%E7%AC%AC4%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B-%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E6%B5%81%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE" rel="nofollow">第4关：HDFS Java API编程 ——使用字符流读取数据</a></li>
<li><a href="#%E7%AC%AC5%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B-%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6" rel="nofollow">第5关：HDFS Java API编程 ——删除文件</a></li>
<li><a href="#%E7%AC%AC6%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B-%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%A4%B9" rel="nofollow">第6关：HDFS Java API编程 ——删除文件夹</a></li>
<li><a href="#%E7%AC%AC7%E5%85%B3hdfs-java-api%E7%BC%96%E7%A8%8B-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E6%B5%81" rel="nofollow">第7关：HDFS Java API编程 ——自定义数据输入流</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>


<h2 id="大数据系统及应用-hdfs实训">大数据系统及应用-HDFS实训</h2>

<p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构，可以在不了解分布式底层细节的情况下，开发分布式程序，以满足在低性能的集群上实现对高容错，高并发的大数据集的高速运算和存储的需要。Hadoop支持超大文件（可达PB级），能够检测和快速应对硬件故障、支持流式数据访问、同时在简化的一致性模型的基础上保证了高容错性。因而被大规模部署在分布式系统中，应用十分广泛。</p>

<p>实验目的 <br>
1) 理解HDFS在Hadoop体系结构中的角色； <br>
2) 熟悉HDFS操作常用的Java API。</p>

<h4 id="第1关hdfs-java-api编程-文件读写">第1关：HDFS Java API编程 ——文件读写</h4>

<p><strong>任务描述</strong> <br>
利用HDFS文件系统开放的API对HDFS系统进行文件的创建和读写</p>

<p><strong>相关知识</strong></p>

<p><strong>HDFS文件系统</strong> <br>
HDFS设计成能可靠地在集群中大量机器之间存储大量的文件，它以块序列的形式存储文件。文件中除了最后一个块，其他块都有相同的大小（一般64M）。属于文件的块为了故障容错而被复制到不同节点备份（备份数量有复制因子决定）。块的大小和读写是以文件为单位进行配置的。HDFS中的文件是一次写的，并且任何时候都只有一个写操作，但是可以允许多次读。 <br>
<img src="https://img-blog.csdn.net/20180604144817859?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTk1MDEz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""> <br>
<strong>HDFS文件创建和读写</strong> <br>
HDFS文件创建和操作可分为三个步骤： <br>
1、获取<code>FileSystem</code>对象；</p>



<pre class="prettyprint"><code class="language-java hljs ">    Configuration conf = <span class="hljs-keyword">new</span> Configuration();<span class="hljs-comment">//configuration类实现hadoop各模块之间值的传递</span>
    FileSystem fs = FileSystem.get(conf);  <span class="hljs-comment">//获取文件系统</span>
    Path file = <span class="hljs-keyword">new</span> Path(xxx);        <span class="hljs-comment">//创建文件</span></code></pre>

<p>2、通过<code>FSDataOutputStream</code>进行写入；</p>



<pre class="prettyprint"><code class="language-java hljs ">    FSDataOutputStream outStream = fs.create(file); <span class="hljs-comment">//获取输出流</span>
    outStream.writeUTF(<span class="hljs-string">"XXX"</span>);<span class="hljs-comment">//可以写入任意字符</span>
    outStream.close();<span class="hljs-comment">//记得关闭输出流</span></code></pre>

<p>3、通过<code>FSDataInputStream</code>将文件内容输出。</p>

<pre class="prettyprint"><code class="language-java hljs ">    FSDataInputStream inStream = fs.open(file);  <span class="hljs-comment">//获取输入流</span>
    String data = inStream.readUTF();  <span class="hljs-comment">//读取文件</span></code></pre>

<p><img src="https://img-blog.csdn.net/20180604145318209?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NTk1MDEz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""> <br>
<strong>编程要求</strong> <br>
在右侧编辑器中补全代码，完成本关任务，具体要求如下：</p>

<ul>
  <li>  获取hadoop的系统设置，并在其中创建HDFS文件，文件路径为/user/hadoop/myfile；</li>
   <li> 在myfile文件中添加字符串https://www.educoder.net；   </li>
     <li>  读取刚刚创建myfile文件中的内容，并输出。   </li>
  </ul>

<p><strong>测试说明</strong> <br>
本关的评测预设文件时<code>/user/hadoop/myfile</code>所以创建文档的路径必须设置为<code>/user/hadoop/myfile</code>才能评测，否则会评测失败。 <br>
注：由于启动服务、编译等耗时，以及hdfs文件操作过程资源消耗较大且时间较长，因而评测时间较长，在40s左右.</p>

<p>实训使用软件环境为：JavaJDK1.8，hadoop2.7.4。</p>

<p>开始你的任务吧，gook luck！</p>

<p><strong>代码如下：</strong></p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.*;
<span class="hljs-keyword">import</span> java.sql.Date;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;


<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">hdfs</span> {</span>

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args) <span class="hljs-keyword">throws</span> IOException {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();<span class="hljs-comment">//configuration类实现hadoop各模块之间值的传递</span>
        FileSystem fs = FileSystem.get(conf);  <span class="hljs-comment">//获取文件系统</span>
        Path file = <span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/user/hadoop/myfile"</span>);        <span class="hljs-comment">//创建文件</span>
        FSDataOutputStream outStream = fs.create(file); <span class="hljs-comment">//获取输出流</span>
        outStream.writeUTF(<span class="hljs-string">"https://www.educoder.net"</span>);<span class="hljs-comment">//可以写入任意字符</span>
        outStream.close();<span class="hljs-comment">//记得关闭输出流</span>
        FSDataInputStream inStream = fs.open(file);  <span class="hljs-comment">//获取输入流</span>
        String data = inStream.readUTF();  <span class="hljs-comment">//读取文件</span>
    }
  }

</code></pre>

<h4 id="第2关hdfs-java-api编程文件上传">第2关：HDFS Java API编程——文件上传</h4>

<p><strong>任务描述</strong> <br>
本关任务：向HDFS中上传任意文本文件。</p>

<p><strong>相关知识</strong> <br>
<strong>判断HDFS中文件是否存在</strong> </p>

<pre class="prettyprint"><code class="language-java hljs ">  <span class="hljs-number">1.</span> FileSystem fs = FileSystem.get(conf);<span class="hljs-comment">//获取对象</span>
  <span class="hljs-number">2.</span> fs.exists(<span class="hljs-keyword">new</span> Path(path);    <span class="hljs-comment">//判断该路径的文件是否存在，是则返回true</span>
</code></pre>

<p><strong>文件拷贝</strong> <br>
关键代码如下：</p>



<pre class="prettyprint"><code class=" hljs cs"><span class="hljs-number">1.</span> <span class="hljs-comment">/* fs.copyFromLocalFile 第一个参数表示是否删除源文件，第二个参数表示是否覆盖 */</span>
<span class="hljs-number">2.</span> fs.copyFromLocalFile(<span class="hljs-keyword">false</span>, <span class="hljs-keyword">true</span>, localPath, remotePath);
</code></pre>

<p><strong>向HDFS文件追加数据</strong> <br>
向HDFS文件中追加信息，关键代码如下：</p>



<pre class="prettyprint"><code class="language-java hljs ">        FileSystem fs = FileSystem.get(conf);
        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
        <span class="hljs-comment">/* 创建一个文件读入流 */</span>
        FileInputStream in = <span class="hljs-keyword">new</span> FileInputStream(localFilePath);
        <span class="hljs-comment">/* 创建一个文件输出流，输出的内容将追加到文件末尾 */</span>
        FSDataOutputStream out = fs.append(remotePath);
        <span class="hljs-comment">/* 读写文件内容 */</span>
        <span class="hljs-keyword">byte</span>[] data = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">1024</span>];
        <span class="hljs-keyword">int</span> read = -<span class="hljs-number">1</span>;
        <span class="hljs-keyword">while</span> ( (read = in.read(data)) &gt; <span class="hljs-number">0</span> ) {
        out.write(data, <span class="hljs-number">0</span>, read);
        }</code></pre>

<p><strong>编程要求</strong></p>

<p>请在右侧start…end处填充代码实现相关功能，完成向HDFS中上传文本文件，如果指定的文件在HDFS中已经存在，由用户指定是追加到原有文件末尾还是覆盖原有的文件。</p>

<p><strong>测试说明</strong></p>

<p>文中要上传的文件路径和目标文件路径已经设置好，请不要修改，否则无法评测，因为Hadoop环境非常消耗资源，所以评测时间较长，需要40秒左右。</p>

<p>开始你的任务吧，good luck!</p>

<p><strong>代码如下：</strong></p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.*;
<span class="hljs-keyword">import</span> java.sql.Date;
<span class="hljs-keyword">import</span> java.util.Scanner;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">hdfs</span> {</span>

      <span class="hljs-javadoc">/**
     * 判断路径是否存在
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">test</span>(Configuration conf, String path) <span class="hljs-keyword">throws</span> IOException {
        <span class="hljs-javadoc">/*****start*****/</span>
        <span class="hljs-comment">//请在此处编写判断文件是否存在的代码</span>
        <span class="hljs-keyword">try</span>(FileSystem fs = FileSystem.get(conf)){
            <span class="hljs-keyword">return</span> fs.exists(<span class="hljs-keyword">new</span> Path(path));
        } <span class="hljs-keyword">catch</span> (IOException e){
            e.printStackTrace();
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;
        }   
        <span class="hljs-javadoc">/*****end*****/</span>
    }

    <span class="hljs-javadoc">/**
     * 复制文件到指定路径
     * 若路径已存在，则进行覆盖
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">copyFromLocalFile</span>(Configuration conf, String localFilePath, String remoteFilePath) <span class="hljs-keyword">throws</span> IOException {
        <span class="hljs-javadoc">/*****start*****/</span>
        <span class="hljs-comment">//请在此处编写复制文件到指定路径的代码</span>

        Path localPath = <span class="hljs-keyword">new</span> Path(localFilePath);  
        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);  
        <span class="hljs-keyword">try</span> (FileSystem fs = FileSystem.get(conf)) { 
        fs.copyFromLocalFile(<span class="hljs-keyword">false</span>, <span class="hljs-keyword">true</span>, localPath, remotePath);  
         } <span class="hljs-keyword">catch</span> (IOException e) {  
            e.printStackTrace();  
         }  

        <span class="hljs-javadoc">/*****end*****/</span>
    }

    <span class="hljs-javadoc">/**
     * 追加文件内容
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">appendToFile</span>(Configuration conf, String localFilePath, String remoteFilePath) <span class="hljs-keyword">throws</span> IOException {
       <span class="hljs-javadoc">/*****start*****/</span>
         <span class="hljs-comment">//请在此处编写追加文件内容的代码</span>

        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);  
    <span class="hljs-keyword">try</span> (FileSystem fs = FileSystem.get(conf);  
            FileInputStream in = <span class="hljs-keyword">new</span> FileInputStream(localFilePath);) {  
        FSDataOutputStream out = fs.append(remotePath);  
        <span class="hljs-keyword">byte</span>[] data = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">1024</span>];  
        <span class="hljs-keyword">int</span> read = -<span class="hljs-number">1</span>;  
        <span class="hljs-keyword">while</span> ((read = in.read(data)) &gt; <span class="hljs-number">0</span>) {  
            out.write(data, <span class="hljs-number">0</span>, read);  
        }  
        out.close();  
    } <span class="hljs-keyword">catch</span> (IOException e) {  
        e.printStackTrace();  
    }  

        <span class="hljs-javadoc">/*****end*****/</span>
    }

    <span class="hljs-javadoc">/**
     * 主函数
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args)<span class="hljs-keyword">throws</span> IOException  {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();

        createHDFSFile(conf);

        String localFilePath = <span class="hljs-string">"./file/text.txt"</span>;           <span class="hljs-comment">// 本地路径</span>
        String remoteFilePath = <span class="hljs-string">"/user/hadoop/text.txt"</span>;    <span class="hljs-comment">// HDFS路径</span>
        String choice = <span class="hljs-string">""</span>;    

        <span class="hljs-keyword">try</span> {
            <span class="hljs-comment">/* 判断文件是否存在 */</span>
            Boolean fileExists = <span class="hljs-keyword">false</span>;
            <span class="hljs-keyword">if</span> (hdfs.test(conf, remoteFilePath)) {
                fileExists = <span class="hljs-keyword">true</span>;
                System.out.println(remoteFilePath + <span class="hljs-string">" 已存在."</span>);
                choice = <span class="hljs-string">"append"</span>;      <span class="hljs-comment">//若文件存在则追加到文件末尾</span>
            } <span class="hljs-keyword">else</span> {
                System.out.println(remoteFilePath + <span class="hljs-string">" 不存在."</span>);
                choice = <span class="hljs-string">"overwrite"</span>;   <span class="hljs-comment">//覆盖</span>
            }


            <span class="hljs-javadoc">/*****start*****/</span>
            <span class="hljs-comment">//请在此处编写文件不存在则上传 文件choice等于overwrite则覆盖   choice 等于append 则追加的逻辑</span>

             <span class="hljs-keyword">if</span> (!fileExists) { <span class="hljs-comment">// 文件不存在，则上传</span>
                hdfs.copyFromLocalFile(conf, localFilePath, remoteFilePath);
                System.out.println(localFilePath + <span class="hljs-string">" 已上传至 "</span> + remoteFilePath);
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (choice.equals(<span class="hljs-string">"overwrite"</span>)) {    <span class="hljs-comment">// 选择覆盖</span>
                hdfs.copyFromLocalFile(conf, localFilePath, remoteFilePath);
                System.out.println(localFilePath + <span class="hljs-string">" 已覆盖 "</span> + remoteFilePath);
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (choice.equals(<span class="hljs-string">"append"</span>)) {   <span class="hljs-comment">// 选择追加</span>
                hdfs.appendToFile(conf, localFilePath, remoteFilePath);
                System.out.println(localFilePath + <span class="hljs-string">" 已追加至 "</span> + remoteFilePath);
            }

            <span class="hljs-javadoc">/*****end*****/</span>

        } <span class="hljs-keyword">catch</span> (Exception e) {
            e.printStackTrace();
        }
    }

    <span class="hljs-comment">//创建HDFS文件</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">createHDFSFile</span>(Configuration conf)<span class="hljs-keyword">throws</span> IOException{
        FileSystem fs = FileSystem.get(conf);  <span class="hljs-comment">//获取文件系统</span>
        Path file = <span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/user/hadoop/text.txt"</span>);        <span class="hljs-comment">//创建文件   </span>
        FSDataOutputStream outStream = fs.create(file); <span class="hljs-comment">//获取输出流</span>
        outStream.writeUTF(<span class="hljs-string">"hello"</span>);
        outStream.close();
        fs.close();
    }
}

</code></pre>

<h4 id="第3关hdfs-java-api编程-文件下载">第3关：HDFS Java API编程 ——文件下载</h4>

<p><strong>任务描述</strong> <br>
从HDFS中下载指定文件。</p>

<p><strong>相关知识</strong></p>

<p><strong>将文件从HDFS拷贝至本地</strong> <br>
将文件拷贝至本地只需要调用FileSystem中的一个方法即可，如下：</p>



<pre class="prettyprint"><code class="language-java hljs ">    FileSystem fs = FileSystem.get(conf);
    Path localPath = <span class="hljs-keyword">new</span> Path(localFilePath);
    fs.copyToLocalFile(remotePath, localPath);</code></pre>

<p><strong>编程要求</strong> <br>
填充右侧代码片段，完成从HDFS中下载文件的功能。</p>

<p><strong>测试说明</strong> <br>
文中要上传的文件路径和目标文件路径已经设置好，请不要修改，否则无法评测，因为Hadoop环境非常消耗资源，所以评测时间较长，需要40秒左右。</p>

<p>开始你的任务吧，祝你成功！</p>

<p><strong>代码如下：</strong></p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.*;
<span class="hljs-keyword">import</span> java.sql.Date;
<span class="hljs-keyword">import</span> java.util.Scanner;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">hdfs</span> {</span>
     <span class="hljs-javadoc">/**
     * 下载文件到本地
     * 判断本地路径是否已存在，若已存在，则自动进行重命名
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">copyToLocal</span>(Configuration conf, String remoteFilePath, String localFilePath) <span class="hljs-keyword">throws</span> IOException {
        FileSystem fs = FileSystem.get(conf);
        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
        File f = <span class="hljs-keyword">new</span> File(localFilePath);
        <span class="hljs-javadoc">/*****start*****/</span>
        <span class="hljs-comment">/*在此添加判断文件是否存在的代码，如果文件名存在，自动重命名(在文件名后面加上 _0, _1 ...) */</span>
        <span class="hljs-keyword">if</span> (f.exists()) {
            System.out.println(localFilePath + <span class="hljs-string">" 已存在."</span>);
            Integer i = <span class="hljs-number">0</span>;
            <span class="hljs-keyword">while</span> (  <span class="hljs-keyword">true</span>) {
                f = <span class="hljs-keyword">new</span> File(  localFilePath + <span class="hljs-string">"_"</span> + i.toString()     );
                <span class="hljs-keyword">if</span> (!f.exists()  ) {
                    localFilePath = localFilePath + <span class="hljs-string">"_"</span> + i.toString()      ;
                    <span class="hljs-keyword">break</span>;
                }
            }
            System.out.println(<span class="hljs-string">"将重新命名为: "</span> + localFilePath);
        }

        <span class="hljs-javadoc">/*****end*****/</span>

        <span class="hljs-javadoc">/*****start*****/</span>
        <span class="hljs-comment">// 在此添加将文件下载到本地的代码</span>

        Path localPath = <span class="hljs-keyword">new</span> Path(localFilePath);   
        fs.copyToLocalFile(remotePath, localPath);

       <span class="hljs-javadoc">/*****end*****/</span>
       fs.close();
    }

    <span class="hljs-javadoc">/**
     * 主函数
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args)<span class="hljs-keyword">throws</span> IOException {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        createHDFSFile(conf);
        String localFilePath = <span class="hljs-string">"/tmp/output/text.txt"</span>;    <span class="hljs-comment">// 本地路径</span>
        String remoteFilePath = <span class="hljs-string">"/user/hadoop/text.txt"</span>;    <span class="hljs-comment">// HDFS路径</span>

        <span class="hljs-keyword">try</span> {
            <span class="hljs-comment">//调用方法下载文件至本地</span>
            hdfs.copyToLocal(conf, remoteFilePath, localFilePath);
            System.out.println(<span class="hljs-string">"下载完成"</span>);
        } <span class="hljs-keyword">catch</span> (Exception e) {
            e.printStackTrace();
        }
    }

    <span class="hljs-comment">//创建HDFS文件</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">createHDFSFile</span>(Configuration conf)<span class="hljs-keyword">throws</span> IOException{
        FileSystem fs = FileSystem.get(conf);  <span class="hljs-comment">//获取文件系统</span>
        Path file = <span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/user/hadoop/text.txt"</span>);        <span class="hljs-comment">//创建文件   </span>
        FSDataOutputStream outStream = fs.create(file); <span class="hljs-comment">//获取输出流</span>
        outStream.writeUTF(<span class="hljs-string">"hello hadoop HDFS www.educoder.net"</span>);
        outStream.close();
        fs.close();
    }

}
</code></pre>

<h4 id="第4关hdfs-java-api编程-使用字符流读取数据">第4关：HDFS Java API编程 ——使用字符流读取数据</h4>

<p><strong>任务描述</strong></p>

<p>本关任务：使用字符流读取HDFS文件数据并输出到终端。</p>

<p><strong>相关知识</strong> <br>
<strong>使用字符流读取数据</strong></p>

<p>使用字符流读取数据简单来说分为三个步骤：</p>

<ol>
    <li>通过Configuration对象获取FileSystem对象；</li>
    <li>通过fs获取FSDataInputStream对象；</li>
    <li>通过字符流循环读取文件中数据并输出。</li>
 </ol>

<p>关键代码：</p>



<pre class="prettyprint"><code class="language-java hljs ">    Configuration conf = <span class="hljs-keyword">new</span> Configuration();
    FileSystem fs = FileSystem.get(conf);
    Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
    FSDataInputStream in = fs.open(remotePath);
    BufferedReader d = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> InputStreamReader(in));
    StringBuffer buffer = <span class="hljs-keyword">new</span> StringBuffer();
    String line = <span class="hljs-keyword">null</span>;
    <span class="hljs-keyword">while</span> ( (line = d.readLine()) != <span class="hljs-keyword">null</span> ) {
          buffer.append(line);
    }
</code></pre>

<p><strong>编程要求</strong></p>

<p>填充右侧代码片段，完成将HDFS中指定文件输出到指定文件中。</p>

<p><strong>测试说明</strong> <br>
文中要上传的文件路径和目标文件路径已经设置好，请不要修改，否则无法评测，因为Hadoop环境非常消耗资源，所以评测时间较长，需要40秒左右。</p>

<p>开始你的任务吧！</p>

<p><strong>代码如下：</strong></p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.*;
<span class="hljs-keyword">import</span> java.sql.Date;
<span class="hljs-keyword">import</span> java.util.Scanner;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">hdfs</span> {</span>
 <span class="hljs-javadoc">/**
     * 读取文件内容
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">cat</span>(Configuration conf, String remoteFilePath) <span class="hljs-keyword">throws</span> IOException {

        <span class="hljs-javadoc">/*****start*****/</span>
        <span class="hljs-comment">//1.读取文件中的数据</span>
        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath); 
        FileSystem fs = FileSystem.get(conf); 
        FSDataInputStream in = fs.open(remotePath); 
        BufferedReader d = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> InputStreamReader(in));      
        String line = <span class="hljs-keyword">null</span>; 
        StringBuffer buffer = <span class="hljs-keyword">new</span> StringBuffer();
        <span class="hljs-keyword">while</span> ((line = d.readLine()) != <span class="hljs-keyword">null</span>) 
        { buffer.append(line); } 
        String res = buffer.toString();

        <span class="hljs-comment">//2.将读取到的数据输出到  /tmp/output/text.txt 文件中  提示：可以使用FileWriter</span>

        FileWriter f1=<span class="hljs-keyword">new</span> FileWriter(<span class="hljs-string">"/tmp/output/text.txt"</span>);
        f1.write(res);
        f1.close();
       <span class="hljs-javadoc">/*****end*****/</span>
    }

    <span class="hljs-javadoc">/**
     * 主函数
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args)<span class="hljs-keyword">throws</span> IOException {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        createHDFSFile(conf);
        String remoteFilePath = <span class="hljs-string">"/user/hadoop/text.txt"</span>;    <span class="hljs-comment">// HDFS路径</span>

        <span class="hljs-keyword">try</span> {
            System.out.println(<span class="hljs-string">"读取文件: "</span> + remoteFilePath);
            hdfs.cat(conf, remoteFilePath);
            System.out.println(<span class="hljs-string">"\n读取完成"</span>);
        } <span class="hljs-keyword">catch</span> (Exception e) {
            e.printStackTrace();
        }
    }

    <span class="hljs-comment">//创建HDFS文件</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">createHDFSFile</span>(Configuration conf)<span class="hljs-keyword">throws</span> IOException{
        FileSystem fs = FileSystem.get(conf);  <span class="hljs-comment">//获取文件系统</span>
        Path file = <span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/user/hadoop/text.txt"</span>);        <span class="hljs-comment">//创建文件   </span>
        FSDataOutputStream outStream = fs.create(file); <span class="hljs-comment">//获取输出流</span>
        outStream.writeUTF(<span class="hljs-string">"hello hadoop HDFS step4 www.educoder.net"</span>);
        outStream.close();
        fs.close();
    }
}
</code></pre>

<h4 id="第5关hdfs-java-api编程-删除文件">第5关：HDFS Java API编程 ——删除文件</h4>

<p><strong>任务描述</strong></p>

<p>删除HDFS中指定的文件。</p>

<p><strong>相关知识</strong></p>

<p><strong>删除HDSF中的文件和目录</strong> <br>
删除HDFS中指定文件需要使用HDFS Java API中FileSystem的delete()方法。 <br>
如下：</p>



<pre class="prettyprint"><code class="language-java hljs ">    FileSystem fs = FileSystem.get(conf);
    Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
    <span class="hljs-keyword">boolean</span> result =  fs.delete(remotePath, <span class="hljs-keyword">false</span>);
</code></pre>

<p><code>public boolean delete(Path f, Boolean recursive)</code>永久性删除指定的文件或目录，如果f是一个空目录或者文件，那么recursive的值就会被忽略。只有<code>recursive＝true</code>时，一个非空目录及其内容才会被删除（即递归删除所有文件）。</p>

<p><strong>编程要求</strong> <br>
请在右侧代码区填充代码，删除<code>HDFS中/user/hadoop/text.txt</code>文件。</p>

<p><strong>测试说明</strong> <br>
因为Hadoop环境非常消耗资源，所以评测时间较长，需要40秒左右。</p>

<p>验货啦，验货啦，开始你的任务吧！</p>

<p><strong>代码如下：</strong></p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.*;
<span class="hljs-keyword">import</span> java.sql.Date;
<span class="hljs-keyword">import</span> java.util.Scanner;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;


<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">hdfs</span> {</span>

     <span class="hljs-javadoc">/**
     * 删除文件 
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">rm</span>(Configuration conf, String remoteFilePath) <span class="hljs-keyword">throws</span> IOException {
        <span class="hljs-javadoc">/*****start*****/</span>        
        <span class="hljs-comment">//请在此添加删除文件的代码</span>
        FileSystem fs = FileSystem.get(conf);
        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
        <span class="hljs-keyword">boolean</span> result = fs.delete(remotePath,<span class="hljs-keyword">false</span>);

        <span class="hljs-keyword">return</span>   <span class="hljs-keyword">true</span>    ;


        <span class="hljs-javadoc">/*****end*****/</span>
    }

    <span class="hljs-javadoc">/**
     * 主函数
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args) {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        String remoteFilePath = <span class="hljs-string">"/user/hadoop/text.txt"</span>;    <span class="hljs-comment">// HDFS文件</span>

        <span class="hljs-keyword">try</span> {
            <span class="hljs-keyword">if</span> (rm(conf, remoteFilePath) ) {
                System.out.println(<span class="hljs-string">"文件删除: "</span> + remoteFilePath);
            } <span class="hljs-keyword">else</span> {
                System.out.println(<span class="hljs-string">"操作失败（文件不存在或删除失败）"</span>);
            }
        } <span class="hljs-keyword">catch</span> (Exception e) {
            e.printStackTrace();
        }
    }

}

</code></pre>

<h4 id="第6关hdfs-java-api编程-删除文件夹">第6关：HDFS Java API编程 ——删除文件夹</h4>

<p><strong>任务描述</strong></p>

<p>删除HDFS中指定的目录。</p>

<p><strong>相关知识</strong></p>

<p><strong>验证目录下是否存在文件</strong></p>

<p>使用到的方法<code>public RemoteIterator&lt;LocatedFileStatus&gt; listFiles(Path f, Boolean recursive)</code> <br>
该方法的作用是：列出给定路径中文件的状态和块位置。如果f是一个目录，recursive是false，则返回目录中的文件；如果recursive是true，则在根目录中返回文件。如果路径是文件，则返回文件的状态和块位置。 <br>
例如：</p>



<pre class="prettyprint"><code class="language-java hljs ">    FileSystem fs = FileSystem.get(conf);
    Path dirPath = <span class="hljs-keyword">new</span> Path(remoteDir);
    RemoteIterator&lt;LocatedFileStatus&gt; remoteIterator = fs.listFiles(dirPath, <span class="hljs-keyword">true</span>);
    <span class="hljs-comment">//remoteIterator.hasNext() 会返回一个布尔类型的值，true即代表文件夹为空，false即代表非空。</span></code></pre>

<p><strong>删除HDSF中的文件和目录</strong></p>

<p>删除HDFS中指定文件需要使用HDFS Java API中FileSystem的delete()方法。 <br>
如下：</p>



<pre class="prettyprint"><code class="language-java hljs ">    FileSystem fs = FileSystem.get(conf);
    Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
    <span class="hljs-keyword">boolean</span> result =  fs.delete(remotePath, <span class="hljs-keyword">false</span>);    </code></pre>

<p><code>public boolean delete(Path f, Boolean recursive)</code>永久性删除指定的文件或目录，如果f是一个空目录或者文件，那么recursive的值就会被忽略。只有<code>recursive＝true</code>时，一个非空目录及其内容才会被删除（即递归删除所有文件）。</p>

<p><strong>编程要求</strong></p>

<p>请在右侧代码区填充代码，删除HDFS中/user/hadoop/tmp目录和/user/hadoop/dir目录，删除前，需要判断两个目录是否为空，若不为空则不删除，否则删除。</p>

<p><strong>测试说明</strong></p>

<p>因为Hadoop环境非常消耗资源，所以评测时间较长，需要40秒左右。</p>

<p>验货啦，验货啦，开始你的任务吧！</p>

<p><strong>代码如下：</strong></p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.*;
<span class="hljs-keyword">import</span> java.sql.Date;
<span class="hljs-keyword">import</span> java.util.Scanner;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.*;


<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">hdfs</span> {</span>
        <span class="hljs-javadoc">/**
     * 判断目录是否为空
     * true: 空，false: 非空
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">isDirEmpty</span>(Configuration conf, String remoteDir) <span class="hljs-keyword">throws</span> IOException {
        FileSystem fs = FileSystem.get(conf);
        Path dirPath = <span class="hljs-keyword">new</span> Path(remoteDir);
        RemoteIterator&lt;LocatedFileStatus&gt; remoteIterator = fs.listFiles(dirPath, <span class="hljs-keyword">true</span>);
        <span class="hljs-keyword">return</span> !remoteIterator.hasNext();
    }
    <span class="hljs-javadoc">/**
     * 删除目录
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">rmDir</span>(Configuration conf, String remoteDir, <span class="hljs-keyword">boolean</span> recursive) <span class="hljs-keyword">throws</span> IOException {
        FileSystem fs = FileSystem.get(conf);
        Path dirPath = <span class="hljs-keyword">new</span> Path(remoteDir);
        <span class="hljs-comment">/* 第二个参数表示是否递归删除所有文件 */</span>
        <span class="hljs-keyword">boolean</span> result = fs.delete(dirPath, recursive);
        fs.close();
        <span class="hljs-keyword">return</span> result;
    }

    <span class="hljs-javadoc">/**
     * 主函数
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args) {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        String remoteDir = <span class="hljs-string">"/user/hadoop/dir/"</span>;    <span class="hljs-comment">// HDFS目录</span>
        String remoteDir1 = <span class="hljs-string">"/user/hadoop/tmp/"</span>;    <span class="hljs-comment">// HDFS目录</span>
        Boolean forceDelete = <span class="hljs-keyword">false</span>;  <span class="hljs-comment">// 是否强制删除</span>

        <span class="hljs-keyword">try</span> {
            <span class="hljs-keyword">if</span> ( !isDirEmpty(conf, remoteDir) &amp;&amp; !forceDelete ) {
                System.out.println(<span class="hljs-string">"目录不为空，不删除"</span>);
            } <span class="hljs-keyword">else</span> {
               <span class="hljs-keyword">if</span> ( rmDir(conf, remoteDir, forceDelete) ) {
                    System.out.println(<span class="hljs-string">"目录已删除: "</span> + remoteDir);
               } <span class="hljs-keyword">else</span> {
                    System.out.println(<span class="hljs-string">"操作失败"</span>);
               }
            }

            <span class="hljs-keyword">if</span> ( !isDirEmpty(conf, remoteDir1) &amp;&amp; !forceDelete ) {
                System.out.println(<span class="hljs-string">"目录不为空，不删除"</span>);
            } <span class="hljs-keyword">else</span> {
                <span class="hljs-keyword">if</span> ( rmDir(conf, remoteDir1, forceDelete) ) {
                    System.out.println(<span class="hljs-string">"目录已删除: "</span> + remoteDir1);
                } <span class="hljs-keyword">else</span> {
                    System.out.println(<span class="hljs-string">"操作失败"</span>);
                }
            }
        } <span class="hljs-keyword">catch</span> (Exception e) {
            e.printStackTrace();
       }
    }

}
</code></pre>



<h4 id="第7关hdfs-java-api编程-自定义数据输入流">第7关：HDFS Java API编程 ——自定义数据输入流</h4>

<p><strong>任务描述</strong></p>

<p>本关任务：实现一个自定义的数据输入流。</p>

<p><strong>相关知识</strong></p>

<p><strong>BufferedReader相关方法</strong></p>



<pre class="prettyprint"><code class="language-java hljs "> <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">read</span>(<span class="hljs-keyword">char</span>[] cbuf,<span class="hljs-keyword">int</span> off,<span class="hljs-keyword">int</span> len)<span class="hljs-keyword">throws</span> IOException</code></pre>

<p>此方法实现 <code>Reader</code>类相应 read 方法的常规协定。另一个便捷之处在于，它将通过重复地调用底层流的 read 方法，尝试读取尽可能多的字符。这种迭代的 read 会一直继续下去，直到满足以下条件之一： 已经读取了指定的字符数， 底层流的 read 方法返回 -1，指示文件末尾（end-of-file），或者 底层流的 ready 方法返回 false，指示将阻塞后续的输入请求。 如果第一次对底层流调用 read 返回 -1（指示文件末尾），则此方法返回 -1，否则此方法返回实际读取的字符数。</p>

<p><strong>编程要求</strong></p>

<p>在右侧编辑器中填充代码，实现按行读取HDFS中指定文件的方法<code>readLine()</code>，如果读到文件末尾，则返回空，否则返回文件一行的文本，即实现和<code>BufferedReader</code>类的<code>readLine()</code>方法类似的效果。</p>

<p><strong>测试说明</strong></p>

<p>因为Hadoop环境非常消耗资源，所以评测时间较长，需要40秒左右。</p>

<p>开始你的任务吧！</p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> java.io.*;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyFSDataInputStream</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">FSDataInputStream</span> {</span>
    <span class="hljs-keyword">public</span> <span class="hljs-title">MyFSDataInputStream</span>(InputStream in) {
        <span class="hljs-keyword">super</span>(in);
    }

   <span class="hljs-javadoc">/**
    * 实现按行读取     * 每次读入一个字符，遇到"\n"结束，返回一行内容
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> String <span class="hljs-title">readline</span>(BufferedReader br) <span class="hljs-keyword">throws</span> IOException {
        <span class="hljs-keyword">char</span>[] data = <span class="hljs-keyword">new</span> <span class="hljs-keyword">char</span>[<span class="hljs-number">1024</span>];
        <span class="hljs-keyword">int</span> read = -<span class="hljs-number">1</span>;
        <span class="hljs-keyword">int</span> off = <span class="hljs-number">0</span>; <span class="hljs-comment">// 循环执行时，br 每次会从上一次读取结束的位置继续读取，因此该函数里，off 每次都从0开始</span>
        <span class="hljs-keyword">while</span> ( (read = br.read(data, off, <span class="hljs-number">1</span>)) != -<span class="hljs-number">1</span> ) {
            <span class="hljs-keyword">if</span> (String.valueOf(data[off]).equals(<span class="hljs-string">"\n"</span>) ) {
                off += <span class="hljs-number">1</span>;
                <span class="hljs-keyword">return</span> String.valueOf(data, <span class="hljs-number">0</span>, read);
            }
            off += <span class="hljs-number">1</span>;
            <span class="hljs-keyword">return</span> String.valueOf(data, <span class="hljs-number">0</span>, read);
        }
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;
    }

    <span class="hljs-javadoc">/**
     * 读取文件内容
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">cat</span>(Configuration conf, String remoteFilePath) <span class="hljs-keyword">throws</span> IOException {
        FileSystem fs = FileSystem.get(conf);
        Path remotePath = <span class="hljs-keyword">new</span> Path(remoteFilePath);
        FSDataInputStream in = fs.open(remotePath);
        BufferedReader br = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> InputStreamReader(in));
        FileWriter f = <span class="hljs-keyword">new</span> FileWriter(<span class="hljs-string">"/tmp/output/text.txt"</span>);
        String line = <span class="hljs-keyword">null</span>;
        <span class="hljs-keyword">while</span> ( (line = MyFSDataInputStream.readline(br)) != <span class="hljs-keyword">null</span> ) {
            f.write(line);
        }
        f.close();
        br.close();
        in.close();
        fs.close();
    }

    <span class="hljs-javadoc">/**
   * 主函数
     */</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args) {
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
       String remoteFilePath = <span class="hljs-string">"/user/hadoop/text.txt"</span>;    <span class="hljs-comment">// HDFS路径</span>
       <span class="hljs-keyword">try</span> {
           MyFSDataInputStream.cat(conf, remoteFilePath);
       } <span class="hljs-keyword">catch</span> (Exception e) {
            e.printStackTrace();
        }
    }

}
</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>