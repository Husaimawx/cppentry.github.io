---
layout:     post
title:      Spark sql DataFrame Datasets Guide
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><span style="font-size:24px;">Spark SQL不是SQL ！！！！！！！！</span></p><p><br></p><h5>一.Spark  SQL定义</h5><h5>Spark SQL is Apache Spark's module for working with structured data.<br>Spark结构查询语言是阿帕奇Spark用于处理结构化数据的模块。<br>（没有体现sql）<br>Spark SQL is not about SQL<br>Spark SQL is about more than SQL<br>不仅仅是sql框架而是超出范围<br></h5><div>二.Spark SQL的优势</div><p>1.集成<br>将SQL查询与Spark程序无缝混合。<br>2.统一的数据访问<br>以同样的方式连接到任何数据源。（不需要sqoop）<br>3.Hive集成<br>在现有仓库上运行SQL或HiveQL查询。<br>公用metastore为数据中心<br>4.标准连接<br></p><p>通过JDBC或ODBC连接。对稳定性，社区支持，效果非常好</p><p>也就是<span style="color:#ff0000;">Data Source API</span>！</p><p>三.<span style="color:rgb(29,31,34);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;background-color:rgb(255,255,255);">DataFrame、Datasets</span></p><p style="color:rgb(29,31,34);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;"><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">数据集是分布式数据集合。</span><span style="vertical-align:inherit;">数据集是Spark 1.6中添加的新接口，它提供了RDD的优点（强打字，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优势。</span><span style="vertical-align:inherit;">数据集可以被</span></span><a href="http://spark.apache.org/docs/2.2.0/sql-programming-guide.html#creating-datasets" rel="nofollow" style="color:rgb(0,136,204);">构造</a><span style="vertical-align:inherit;">从JVM对象，然后使用功能性的转换（操作</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">map</code><span style="vertical-align:inherit;">，</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">flatMap</code><span style="vertical-align:inherit;">，</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">filter</code><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">等等）。</span><span style="vertical-align:inherit;">数据集API可用于</span></span><a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.Dataset" rel="nofollow" style="color:rgb(0,136,204);">Scala</a><span style="vertical-align:inherit;">和 </span><a href="http://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/sql/Dataset.html" rel="nofollow" style="color:rgb(0,136,204);">Java</a><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">。</span><span style="vertical-align:inherit;">Python不支持数据集API。</span><span style="vertical-align:inherit;">但是由于Python的动态特性，数据集API的许多优点已经可用（例如，您可以自然地通过名称访问行的字段</span></span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">row.columnName</code><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">）。</span><span style="vertical-align:inherit;">R的情况类似。</span></span></p><p style="color:rgb(29,31,34);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;"><span style="vertical-align:inherit;">DataFrame是一个</span><em>数据集，</em><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">组织到命名列中。</span><span style="vertical-align:inherit;">它在概念上等同于关系数据库中的表或R / Python中的数据框，但在引擎盖下具有更丰富的优化。</span><span style="vertical-align:inherit;">DataFrame可以从各种</span></span><a href="http://spark.apache.org/docs/2.2.0/sql-programming-guide.html#data-sources" rel="nofollow" style="color:rgb(0,136,204);">来源</a><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">构建而成，</span><span style="vertical-align:inherit;">例如：结构化数据文件，Hive中的表格，外部数据库或现有的RDD。</span><span style="vertical-align:inherit;">数据帧API是Scala，Java的，可用</span></span><a href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="nofollow" style="color:rgb(0,136,204);">的Python</a><span style="vertical-align:inherit;">和</span><a href="http://spark.apache.org/docs/2.2.0/api/R/index.html" rel="nofollow" style="color:rgb(0,136,204);">[R </a><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">。</span><span style="vertical-align:inherit;">在Scala和Java中，DataFrame由</span></span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">Row</code><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">s </span><span style="vertical-align:inherit;">的数据集表示</span><span style="vertical-align:inherit;">。</span><span style="vertical-align:inherit;">在</span></span><a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.Dataset" rel="nofollow" style="color:rgb(0,136,204);">Scala API中</a><span style="vertical-align:inherit;">，</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">DataFrame</code><span style="vertical-align:inherit;">只是一个类型的别名</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">Dataset[Row]</code><span style="vertical-align:inherit;"><span style="vertical-align:inherit;">。</span><span style="vertical-align:inherit;">而在</span></span><a href="http://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/sql/Dataset.html" rel="nofollow" style="color:rgb(0,136,204);">Java API中</a><span style="vertical-align:inherit;">，用户需要</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">Dataset&lt;Row&gt;</code><span style="vertical-align:inherit;">用来表示一个</span><code style="font-family:Menlo, 'Lucida Console', monospace;color:rgb(68,68,68);background:#FFFFFF;border:none;">DataFrame</code><span style="vertical-align:inherit;">。</span></p><p style="color:rgb(29,31,34);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;"><span style="vertical-align:inherit;"><span style="font-size:16px;">四.操作</span></span></p><p style="color:rgb(29,31,34);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;"><span style="vertical-align:inherit;"><span style="font-size:16px;">import org.apache.spark.sql.SparkSession<br>val spark = SparkSession<br>  .builder()<br>  .appName("Spark SQL")<br>  .master("local [2]")<br>  .getOrCreate()<br>//用于隐式转换，如将RDD转换为DataFrames <br>import spark.implicits._<br>val df = spark.read.json("/usr/data/data1.json")<br>df.show()<br><br>//DataFrame操作<br>import spark.implicits._<br>df.printSchema()<br>//树形式打印模式<br></span></span></p><p><span style="vertical-align:inherit;"><span style="font-size:16px;">df.select("xxx").show()</span></span></p><p><span style="vertical-align:inherit;"><span style="font-size:16px;">//20行"xxx"数据   、、</span></span></p><p><span style="font-size:18px;color:rgb(29,31,34);">df.select($"xxx", $"ccc" + 1).show()</span></p><p><span style="font-size:18px;color:rgb(29,31,34);">//21行“xxx"数据</span></p><p><span style="vertical-align:inherit;"><span style="font-size:16px;">df.filter($"ccc" &gt; 21).show()</span></span></p><p><span style="vertical-align:inherit;"><span style="font-size:16px;">df.groupBy("ccc").count().show()</span></span></p><p><span style="vertical-align:inherit;"><span style="font-size:16px;"></span></span></p><h5></h5><h5><br></h5><h5>五、Datasets Guide</h5><h5>case class Person(name: String, age: Long)<br>val caseClassDS = Seq(Person("xxx", 00)).toDS()<br>caseClassDS.show()<br>val primitiveDS = Seq(1, 2, 3).toDS()<br>primitiveDS.map(_ + 1).collect() <br><br></h5><h5>val path = "/usr/data/people.json"<br>val peopleDS = spark.read.json(path).as[Person]<br><br></h5><h5>peopleDS.show()</h5><p></p><div>DataFrame 和 Datasets 对比 <br></div><p><br></p><p><img src="https://img-blog.csdn.net/2018040921582887" alt=""></p><p><br></p><p><br></p><p><span style="background-color:rgb(255,255,255);">【来自@若泽大数据】</span><br></p>            </div>
                </div>