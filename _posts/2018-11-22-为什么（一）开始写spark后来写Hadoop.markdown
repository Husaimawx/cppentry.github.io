---
layout:     post
title:      为什么（一）开始写spark后来写Hadoop
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/HereIcome/article/details/79496785				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                    看了标题估计也能猜到，<span style="color:#cc0000;">因为spark写不下去了！！</span>确实是这样的。实际生成环境中spark的好多处理都是建立在spark集群环境上的，需要Hadoop集群（数据转换存储等操作）来配合spark，所以还是从Hadoop再开始。            </div>
                </div>