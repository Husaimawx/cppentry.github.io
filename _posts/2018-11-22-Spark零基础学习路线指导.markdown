---
layout:     post
title:      Spark零基础学习路线指导
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><span style="color:#3333ff;"><u><a href="http://www.aboutyun.com/thread-21959-1-1.html" rel="nofollow">spark零基础学习路线指导</a></u></span></p>

<p> </p>

<p> </p>

<p><strong>问题导读</strong><br><br><strong><span style="color:#ff0000;">1.你认为spark该如何入门？</span></strong><br><strong><span style="color:#ff0000;">2.你认为spark入门编程需要哪些步骤？</span></strong><br><strong><span style="color:#ff0000;">3.本文介绍了spark哪些编程知识？</span></strong><br><br><img alt="" class="zoom" height="35" id="aimg_eC9cO" src="http://www.aboutyun.com/static/image/hrline/4.gif" width="500"><br><br><img alt="" class="zoom" height="346" id="aimg_30976" src="http://www.aboutyun.com/data/attachment/forum/201706/16/102914tnw8evppvnwsvxwu.jpg" width="353"> <br><br><span style="color:#444444;">spark学习一般都具有hadoop基础，所以学习起来更容易多了。如果没有基础，可以参考</span><a href="http://www.aboutyun.com/thread-6780-1-1.html" rel="nofollow">零基础学习hadoop到上手工作线路指导（初级篇）</a><span style="color:#444444;">。具有基础之后，一般都是按照官网或则视频、或则文档，比如搭建spark，运行spark例子。后面就不知道做什么了。这里整体梳理一下。希望对大家有所帮助。</span><br><strong>1.spark场景</strong><br><span style="color:#444444;">在入门spark之前，首先对spark有些基本的了解。比如spark场景，spark概念等。推荐参考</span><br><span style="color:#444444;">Spark简介：适用场景、核心概念、创建RDD、支持语言等介绍</span><br><a href="http://www.aboutyun.com/thread-9389-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=9389</a><br><br><br><strong>2.spark部署</strong><br><span style="color:#444444;">首先还是说些基础性的内容，非零基础的同学，可以跳过。</span><br><span style="color:#444444;">首先还是spark环境的搭建。</span><br><span style="color:#444444;">about云日志分析项目准备6：Hadoop、Spark集群搭建</span><br><a href="http://www.aboutyun.com/thread-20620-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20620</a><br><span style="color:#444444;">spark环境搭建完毕，例子运行完毕。后面就不知道干啥了。</span><br><br><br><span style="color:#444444;">这时候我们就需要了解spark。</span><br><span style="color:#444444;">从不同角度，可以有多种不同的方式：如果我们从实战工作的角度，下面我们就需要了解开发方面的知识</span><br><span style="color:#444444;">如果我们从知识、理论的角度，我们就需要了解spark生态系统</span><br><span style="color:#444444;">下面我们从不同角度来介绍</span><br><br><br><strong>3.spark实战</strong><br><strong>3.1spark开发环境</strong><br><span style="color:#444444;">比如我们从实战的角度，当我们部署完毕，下面我们就可以接触开发方面的知识。</span><br><span style="color:#444444;">对于开发,当然是首先是开发工具，比如eclipse，IDEA。对于eclipse和IDEA两个都有选择的，看你使用那个更顺手些。</span><br><br><strong>下面是个人总结希望对大家有帮助[</strong><span style="color:#ff0000;"><strong>二次修改新增内容</strong></span><strong>]</strong><br><span style="color:#444444;">spark开发环境详细教程1：IntelliJ IDEA使用详细说明</span><br><a href="http://www.aboutyun.com/thread-22320-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=22320</a><br><br><span style="color:#444444;">spark开发环境详细教程2：window下sbt库的设置</span><br><a href="http://www.aboutyun.com/thread-22409-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=22409</a><br><br><span style="color:#444444;">spark开发环境详细教程3：IntelliJ IDEA创建项目</span><br><a href="http://www.aboutyun.com/thread-22410-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=22410</a><br><br><span style="color:#444444;">spark开发环境详细教程4：创建spark streaming应用程序</span><br><a href="http://www.aboutyun.com/thread-22465-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=22465</a><br><br><br><span style="color:#444444;">更多了解即可：</span><br><span style="color:#444444;">Spark集成开发环境搭建-eclipse</span><br><a href="http://www.aboutyun.com/thread-6772-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=6772</a><br><br><br><span style="color:#444444;">用IDEA开发spark，源码提交任务到YARN</span><br><a href="http://www.aboutyun.com/thread-20316-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20316</a><br><br><br><br><span style="color:#444444;">Spark1.0.0 开发环境快速搭建</span><br><a href="http://www.aboutyun.com/thread-8403-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=8403</a><br><br><span style="color:#444444;">spark开发环境中，如何将源码打包提交到集群</span><br><a href="http://www.aboutyun.com/thread-20979-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20979</a><br><br><br><span style="color:#444444;">田毅-Spark开发及本地环境搭建指南</span><br><a href="http://www.aboutyun.com/thread-20313-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20313</a><br><br><span style="color:#444444;">Spark 开发环境IntelliJ IDEA图文教程、视频系统教程</span><br><a href="http://www.aboutyun.com/thread-10122-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=10122</a><br><br><strong>3.2spark开发基础</strong><br><span style="color:#444444;">开发环境中写代码，或则写代码的时候，遇到个严重的问题，Scala还不会。这时候我们就需要补Scala的知识。如果是会Java或则其它语言，可能会阅读C,.net,甚至Python，但是Scala，你可能会遇到困难，因为里面各种符号和关键字，所以我们需要真正的学习下Scala。下面内容，是个人的总结，仅供参考</span><br><span style="color:#444444;">#######################</span><br><span style="color:#444444;">about云spark开发基础之Scala快餐</span><br><a href="http://www.aboutyun.com/thread-20303-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20303</a><br><br><br><span style="color:#444444;">spark开发基础之从Scala符号入门Scala</span><br><a href="http://www.aboutyun.com/thread-20159-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20159</a><br><br><span style="color:#444444;">spark开发基础之从关键字入门Scala</span><br><a href="http://www.aboutyun.com/thread-20223-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20223</a><br><br><span style="color:#444444;">更多内容：</span><br><span style="color:#444444;">spark开发基础之Scala快餐：开发环境Intellij IDEA 快捷键整理【收藏备查】</span><br><a href="http://www.aboutyun.com/thread-20380-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20380</a><br><br><span style="color:#444444;">学习Scala的过程中，参考了以下资料</span><br><span style="color:#444444;">《快学Scala》完整版书籍分享</span><br><a href="http://www.aboutyun.com/thread-8713-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=8713</a><br><br><span style="color:#444444;">scala入门视频【限时下载】</span><br><a href="http://www.aboutyun.com/thread-12434-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=12434</a><br><br><span style="color:#444444;">更多可以搜索Scala</span><br><a href="http://so.aboutyun.com/" rel="nofollow">http://so.aboutyun.com/</a><br><br><span style="color:#444444;">#######################</span><br><span style="color:#444444;">相信上面的资料，足以让你搞懂Scala。Scala会了，开发环境、代码都写好了，下面我们就需要打包了。该如何打包。这里打包的方式有两种：</span><br><span style="color:#444444;">1.maven</span><br><span style="color:#444444;">2.sbt</span><br><span style="color:#444444;">有的同学要问，哪种方式更好。其实两种都可以，你熟悉那个就使用那个即可。</span><br><span style="color:#444444;">下面提供一些资料</span><br><span style="color:#444444;">scala eclipse sbt（ Simple Build Tool） 应用程序开发</span><br><a href="http://www.aboutyun.com/thread-9340-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=9340</a><br><br><br><span style="color:#444444;">使用maven编译Spark</span><br><a href="http://www.aboutyun.com/thread-11746-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=11746</a><br><br><span style="color:#444444;">更多资料</span><br><span style="color:#444444;">Spark大师之路：使用maven编译Spark</span><br><a href="http://www.aboutyun.com/thread-10842-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=10842</a><br><br><span style="color:#444444;">用SBT编译Spark的WordCount程序</span><br><a href="http://www.aboutyun.com/thread-8587-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=8587</a><br><br><br><span style="color:#444444;">如何用maven构建spark</span><br><a href="http://www.aboutyun.com/thread-12261-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=12261</a><br><br><strong>3.3spark开发知识</strong><br><span style="color:#444444;">spark 开发包括spark core的相关组件及运算，还有spark streaming，spark sql，spark mlib，GraphX.</span><br><span style="color:#444444;">###########################</span><br><span style="color:#444444;">下面的知识是关于spark1.x的，关于1.x其实有了基础，那么spark2.x学习来是非常快的。那么他们之间的区别在什么地方？最大的区别在编程方面是spark context，sqlcontext,hivecontext,都使用一个类即可，那就是SparkSession。他的编程是非常方便的。比如</span><br><span style="color:#444444;">通过SparkSession如何创建rdd,通过下面即可</span><br><img alt="" class="has" height="59" src="https://img-blog.csdn.net/20180807100355598?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Blbmd6b25nbHU3Mjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="604"><br><span style="color:#444444;">再比如如何执行spark sql</span></p>

<p><img alt="" class="has" height="345" src="https://img-blog.csdn.net/20180807100411653?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Blbmd6b25nbHU3Mjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="737"></p>

<p><span style="color:#444444;">更多参考：</span><br><span style="color:#444444;">spark2：SparkSession思考与总结</span><br><a href="http://www.aboutyun.com/thread-23381-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23381</a><br><br><br><br><span style="color:#444444;">SparkSession使用方法介绍【spark2.0】</span><br><a href="http://www.aboutyun.com/thread-19632-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=19632</a><br><br><br><br><span style="color:#444444;">spark2使用遇到问题总结</span><br><a href="http://www.aboutyun.com/thread-24050-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=24050</a><br><br><br><span style="color:#444444;">spark2.0文档【2016英文】</span><br><a href="http://www.aboutyun.com/thread-18970-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=18970</a><br><br><br><span style="color:#444444;">spark2 sql读取数据源编程学习样例1：程序入口、功能等知识详解</span><br><a href="http://www.aboutyun.com/thread-23484-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23484</a><br><br><br><span style="color:#444444;">spark2 sql读取数据源编程学习样例2：函数实现详解</span><br><a href="http://www.aboutyun.com/thread-23489-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23489</a><br><br><br><span style="color:#444444;">使用spark2 sql的方式有哪些</span><br><a href="http://www.aboutyun.com/thread-23541-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23541</a><br><br><br><span style="color:#444444;">spark2之DataFrame如何保存【持久化】为表</span><br><a href="http://www.aboutyun.com/thread-23523-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23523</a><br><br><br><span style="color:#444444;">spark2 sql编程样例：sql操作</span><br><a href="http://www.aboutyun.com/thread-23501-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23501</a><br><br><br><span style="color:#444444;">spark2 sql读取json文件的格式要求</span><br><a href="http://www.aboutyun.com/thread-23478-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23478</a><br><br><br><span style="color:#444444;">spark2 sql读取json文件的格式要求续：如何查询数据</span><br><a href="http://www.aboutyun.com/thread-23483-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23483</a><br><br><br><span style="color:#444444;">spark2的SparkSession思考与总结2：SparkSession包含哪些函数及功能介绍</span><br><a href="http://www.aboutyun.com/thread-23407-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23407</a><br><br><br><span style="color:#444444;">spark2.2以后版本任务调度将增加黑名单机制</span><br><a href="http://www.aboutyun.com/thread-23346-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=23346</a><br><br><br><br><br><span style="color:#444444;">##########################</span><br><br><strong>3.3.1spark 编程</strong><br><span style="color:#444444;">说到spark编程，有一个不能绕过的SparkContext，相信如果你接触过spark程序，都会见到SparkContext。那么他的作用是什么？</span><br><span style="color:#444444;">SparkContext其实是连接集群以及获取spark配置文件信息，然后运行在集群中。如下面程序可供参考</span><br>
 </p>

<p><span style="color:#444444;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#444444;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><span style="color:#afafaf;">1</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">2</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">3</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">4</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">5</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">6</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">7</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.SparkConf</code></span></p>

			<p><span style="color:#444444;"><code> </code> </span></p>

			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.SparkContext</code></span></p>

			<p><span style="color:#444444;"><code> </code> </span></p>

			<p><span style="color:#444444;"><code>val</code> <code>conf </code><code>=</code> <code>new</code> <code>SparkConf().setAppName(“MySparkDriverApp”).setMaster(“spark</code><code>:</code><code>//master:7077”).set(“spark.executor.memory”, “2g”)</code></span></p>

			<p><span style="color:#444444;"><code> </code> </span></p>

			<p><span style="color:#444444;"><code>val</code> <code>sc </code><code>=</code> <code>new</code> <code>SparkContext(conf)</code></span></p>
			</td>
		</tr></tbody></table><p><br><span style="color:#444444;">下面图示为SparkContext作用</span></p>

<p><img alt="" class="has" height="327" src="https://img-blog.csdn.net/20180807100457684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Blbmd6b25nbHU3Mjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="680"></p>

<p><span style="color:#444444;">当然还有 SQLContext 和HiveContext作用是类似的，同理还有hadoop的Context，它们的作用一般都是全局的。除了SparkContext，还有Master、worker、DAGScheduler、TaskScheduler、Executor、Shuffle、BlockManager等，留到后面理论部分。这里的入门更注重实战操作</span><br><br><span style="color:#444444;">我们通过代码连接上集群，下面就该各种内存运算了。</span><br><span style="color:#444444;">比如rdd,dataframe,DataSet。如果你接触过spark，相信rdd是经常看到的，DataFrame是后来加上的。但是他们具体是什么。可以详细参考</span><strong>spark core组件:RDD、DataFrame和DataSet介绍、场景与比较</strong><br><a href="http://www.aboutyun.com/thread-20902-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20902</a><br><br><span style="color:#444444;">看到上面我们其实可能对它们还没有认识到本质，其实他们就是内存的数据结构。那么数据结构相信我们应该都了解过，最简单、我们经常接触的就是数组了。而rdd，跟数组有一个相同的地方，都是用来装数据的，只不过复杂度不太一样而已。对于已经了解过人来说，这是理所当然的。这对于初学者来说，认识到这个程度，rdd就已经不再神秘了。那么DataFrame同样也是，DataFrame是一种以RDD为基础的分布式数据集.</span><br><br><span style="color:#444444;">rdd和DataFrame在spark编程中是经常用到的，那么该如何得到rdd，该如何创建DataFrame，他们之间该如何转换。</span><br><strong>创建rdd有三种方式，</strong><br><span style="color:#444444;">1.从scala集合中创建RDD</span><br><span style="color:#444444;">2.从本地文件系统创建RDD</span><br><span style="color:#444444;">3.从HDFS创建RDD</span><br><span style="color:#444444;">详细参考</span><br><span style="color:#444444;">spark小知识总结</span><br><a href="http://www.aboutyun.com/thread-20920-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20920</a><br><br><strong>如何创建dataframe</strong><br><span style="color:#444444;">df&lt;-data.frame(A=c(NA),B=c(NA))</span><br><span style="color:#444444;">当然还可以通过rdd转换而来,通过toDF()函数实现</span><br><span style="color:#444444;">rdd.toDF()</span><br><span style="color:#444444;">dataframe同样也可以转换为rdd,通过.rdd即可实现</span><br><span style="color:#444444;">如下面</span><br><span style="color:#444444;">val rdd = df.toJSON.rdd</span><br><span style="color:#444444;">为了更好的理解，在看下面例子</span></p>

<p><span style="color:#444444;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#444444;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><span style="color:#afafaf;">1</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">2</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">3</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">4</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><code>先创建一个类</code></span></p>

			<p><span style="color:#444444;"><code>case</code> <code>class</code> <code>Person(name</code><code>:</code> <code>String, age</code><code>:</code> <code>Int)</code></span></p>

			<p><span style="color:#444444;"><code>然后将Rdd转换成DataFrame</code></span></p>

			<p><span style="color:#444444;"><code>val</code> <code>people </code><code>=</code> <code>sc.textFile(</code><code>"/usr/people.txt"</code><code>).map(</code><code>_</code><code>.split(</code><code>","</code><code>)).map(p </code><code>=</code><code>&gt; Person(p(</code><code>0</code><code>), p(</code><code>1</code><code>).trim.toInt)).toDF()</code></span></p>
			</td>
		</tr></tbody></table><p><br><span style="color:#444444;">即为rdd转换为dataframe.</span><br><br><strong>RDD和DataFrame各种操作</strong><br><span style="color:#444444;">上面只是简单的操作，更多还有</span><span style="color:#ff00ff;">rdd的action和Transformation</span><span style="color:#444444;">Actions操作如：reduce，collect，count，foreach等</span><br><span style="color:#444444;">Transformation如，map,filter等</span><br><span style="color:#444444;">更多参考</span><br><span style="color:#444444;">Spark RDD详解</span><br><a href="http://www.aboutyun.com/thread-7214-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=7214</a><br><br><strong>DataFrame同理</strong><br><br><span style="color:#ff00ff;">DataFrame 的函数</span><br><br><span style="color:#444444;">collect，collectAsList等</span><br><span style="color:#ff00ff;">dataframe的基本操作</span><br><span style="color:#444444;">如</span><span style="color:#000000;">cache，</span><span style="color:#000000;">columns</span><span style="color:#000000;"> 等</span><br><span style="color:#000000;">更多参考</span><br><span style="color:#000000;">spark DataFrame 的函数|基本操作|集成查询记录</span><br><span style="color:#000000;">http://www.aboutyun.com/blog-1330-3165.html</span><br><br><br><strong>spark数据库操作</strong><br><span style="color:#444444;">很多初级入门的同学，想在spark中操作数据库，比如讲rdd或则dataframe数据导出到mysql或则oracle中。但是让他们比较困惑的是，该如何在spark中将他们导出到关系数据库中，spark中是否有这样的类。这是因为对编程的理解不够造成的误解。在spark程序中，如果操作数据库，spark是不会提供这样的类的，直接引入操作mysql的库即可，比如jdbc,odbc等。</span><br><span style="color:#444444;">比如下面Spark通过JdbcRDD整合 Mysql(JdbcRDD)开发</span><br><a href="http://www.aboutyun.com/thread-9826-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=9826</a><br><span style="color:#444444;">更多可百度。</span><br><span style="color:#ff0000;">经常遇到的问题</span><br><span style="color:#444444;">在操作数据中，很多同学遇到不能序列化的问题。因为类本身没有序列化.所以变量的定义与使用最好在同一个地方。</span><br><span style="color:#444444;">想了解更详细，可参考</span><br><span style="color:#444444;">不能序列化解决方法 org.apache.spark.sparkException:Task not serializable</span><br><a href="http://www.aboutyun.com/blog-29-3362.html" rel="nofollow">http://www.aboutyun.com/home.php?mod=space&amp;uid=29&amp;do=blog&amp;id=3362</a><br><strong>小总结</strong><br><span style="color:#444444;">如果上面已经都会了，那么spark基本编程和做spark相关项目外加一些个人经验相信应该没有问题。</span><br><br><strong>3.3.2spark sql编程</strong><br><span style="color:#444444;">spark sql为何会产生。原因很多，比如用spark编程完成比较繁琐，需要多行代码来完成，spark sql写一句sql就能搞定了。那么spark sql该如何使用。</span><br><br><span style="color:#444444;">1.初始化spark sql</span><br><span style="color:#444444;">为了开始spark sql，我们需要添加一些imports 到我们程序。如下面例子1</span><br><span style="color:#444444;">例子1Scala SQL imports</span></p>

<p><span style="color:#444444;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#444444;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><span style="color:#afafaf;">1</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">2</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">3</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">4</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><code>// Import Spark SQL</code></span></p>

			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.sql.hive.HiveContext</code></span></p>

			<p><span style="color:#444444;"><code>// Or if you can't have the hive dependencies</code></span></p>

			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.sql.SQLContext</code></span></p>
			</td>
		</tr></tbody></table><p><br><span style="color:#444444;">下面引用一个例子</span></p>

<blockquote><span style="color:#666666;">首先在maven项目的pom.xml中添加Spark SQL的依赖。<br>
&lt;dependency&gt;<br>
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br>
    &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;<br>
    &lt;version&gt;1.5.2&lt;/version&gt;<br>
&lt;/dependency&gt;</span>

<blockquote>
<p><span style="color:#666666;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#666666;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#666666;"><span style="color:#afafaf;">01</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">02</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">03</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">04</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">05</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">06</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">07</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">08</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">09</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">10</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">11</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">12</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">13</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">14</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">15</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">16</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">17</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">18</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">19</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">20</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">21</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">22</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">23</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">24</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">25</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">26</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">27</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">28</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">29</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">30</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">31</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">32</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">33</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">34</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">35</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">36</span></span></p>

			<p><span style="color:#666666;"><span style="color:#afafaf;">37</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#666666;"><code>package</code> <code>[url</code><code>=</code><code>http</code><code>:</code><code>//www.aboutyun.com]www.aboutyun.com[/url]</code></span></p>

			<p><span style="color:#666666;"> </span></p>

			<p><span style="color:#666666;"><code>import</code> <code>org.apache.spark.{SparkConf, SparkContext}</code></span></p>

			<p><span style="color:#666666;"><code>import</code> <code>org.apache.spark.sql.SQLContext</code></span></p>

			<p><span style="color:#666666;"> </span></p>

			<p><span style="color:#666666;"><code>object</code> <code>InferringSchema {</code></span></p>

			<p><span style="color:#666666;"><code>  </code><code>def</code> <code>main(args</code><code>:</code> <code>Array[String]) {</code></span></p>

			<p><span style="color:#666666;"> </span></p>

			<p><span style="color:#666666;"><code>    </code><code>//创建SparkConf()并设置App名称</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>conf </code><code>=</code> <code>new</code> <code>SparkConf().setAppName(</code><code>"aboutyun"</code><code>)</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//SQLContext要依赖SparkContext</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>sc </code><code>=</code> <code>new</code> <code>SparkContext(conf)</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//创建SQLContext</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>sqlContext </code><code>=</code> <code>new</code> <code>SQLContext(sc)</code></span></p>

			<p><span style="color:#666666;"> </span></p>

			<p><span style="color:#666666;"><code>    </code><code>//从指定的地址创建RDD</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>lineRDD </code><code>=</code> <code>sc.textFile(args(</code><code>0</code><code>)).map(</code><code>_</code><code>.split(</code><code>" "</code><code>))</code></span></p>

			<p><span style="color:#666666;"> </span></p>

			<p><span style="color:#666666;"><code>    </code><code>//创建case class</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//将RDD和case class关联</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>personRDD </code><code>=</code> <code>lineRDD.map(x </code><code>=</code><code>&gt; Person(x(</code><code>0</code><code>).toInt, x(</code><code>1</code><code>), x(</code><code>2</code><code>).toInt))</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//导入隐式转换，如果不到人无法将RDD转换成DataFrame</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//将RDD转换成DataFrame</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>import</code> <code>sqlContext.implicits.</code><code>_</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>personDF </code><code>=</code> <code>personRDD.toDF</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//注册表</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>personDF.registerTempTable(</code><code>"person"</code><code>)</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//传入SQL</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>val</code> <code>df </code><code>=</code> <code>sqlContext.sql(</code><code>"select * from person order by age desc "</code><code>)</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//将结果以JSON的方式存储到指定位置</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>df.write.json(args(</code><code>1</code><code>))</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>//停止Spark Context</code></span></p>

			<p><span style="color:#666666;"><code>    </code><code>sc.stop()</code></span></p>

			<p><span style="color:#666666;"><code>  </code><code>}</code></span></p>

			<p><span style="color:#666666;"><code>}</code></span></p>

			<p><span style="color:#666666;"><code>//case class一定要放到外面</code></span></p>

			<p><span style="color:#666666;"><code>case</code> <code>class</code> <code>Person(id</code><code>:</code> <code>Int, name</code><code>:</code> <code>String, age</code><code>:</code> <code>Int)</code></span></p>
			</td>
		</tr></tbody></table></blockquote>
</blockquote>

<p><br><span style="color:#444444;">参考：csdn 绛门人，更多例子大家也可网上搜索</span><br><span style="color:#444444;">我们看到上面例子中</span> sqlContext.sql可以将sql语句放入到函数中。<br><span style="color:#444444;">关于spark sql的更多内容推荐Spark Sql系统入门1：什么是spark sql及包含哪些组件</span><br><a href="http://www.aboutyun.com/thread-20910-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20910</a><br><br><span style="color:#444444;">Spark Sql系统入门2：spark sql精简总结</span><br><a href="http://www.aboutyun.com/thread-21002-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21002</a><br><br><br><span style="color:#444444;">Spark Sql系统入门3：spark sql运行计划精简</span><br><a href="http://www.aboutyun.com/thread-21032-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21032</a><br><br><span style="color:#444444;">about云日志分析项目准备6-5-2：spark应用程序中如何嵌入spark sql</span><br><a href="http://www.aboutyun.com/thread-21078-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21078</a><br><br><span style="color:#444444;">spark sql完毕，后面我们继续spark streaming。</span><br><br><strong>3.3.3spark streaming编程</strong><br><span style="color:#444444;">我么知道spark具有实时性，那么spark的实时性就是通过spark streaming来实现的。spark streaming可以实时跟踪页面统计，训练机器学习模型或则自动检测异常等.</span><br><br><strong>如何使用spark streaming</strong><br><span style="color:#444444;">大数据编程很多都是类似的，我们还是需要看下</span><span style="color:#ff0000;"><strong>StreamingContext</strong></span><span style="color:#444444;">.</span><br><br><span style="color:#444444;">为了初始化Spark Streaming程序，一个StreamingContext对象必需被创建，它是Spark Streaming所有流操作的主要入口。一个StreamingContext 对象可以用SparkConf对象创建。StreamingContext这里可能不理解，其实跟SparkContext也差不多的。（可参考</span><a href="http://www.aboutyun.com/thread-21018-1-1.html" rel="nofollow">让你真正理解什么是SparkContext, SQLContext 和HiveContext</a><span style="color:#444444;">）。同理也有hadoop Context，它们都是全文对象，并且会获取配置文件信息。那么配置文件有哪些？比如hadoop的core-site.xml,hdfs-site.xml等，spark如spark-defaults.conf等。这时候我们可能对StreamingContext有了一定的认识。下面一个例子</span><br><br><span style="color:#444444;">为了初始化Spark Streaming程序，一个StreamingContext对象必需被创建，它是Spark Streaming所有流操作的主要入口。</span><br><span style="color:#444444;">一个StreamingContext 对象可以用SparkConf对象创建。</span></p>

<p><span style="color:#444444;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#444444;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><span style="color:#afafaf;">1</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">2</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">3</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">4</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.</code><code>_</code></span></p>

			<p><span style="color:#444444;"><code>impoty org.apache.spark.streaming.</code><code>_</code></span></p>

			<p><span style="color:#444444;"><code>val</code> <code>conf </code><code>=</code> <code>new</code> <code>SparkConf().setAppName(appName).setMaster(master)</code></span></p>

			<p><span style="color:#444444;"><code>val</code> <code>ssc</code><code>=</code><code>new</code> <code>StreamingContext(conf,Seconds(</code><code>1</code><code>))</code></span></p>
			</td>
		</tr></tbody></table><p><br><span style="color:#444444;">appName表示你的应用程序显示在集群UI上的名字,master 是一个Spark、Mesos、YARN集群URL 或者一个特殊字符串“local”，它表示程序用本地模式运行。当程序运行在集群中时，你并不希望在程序中硬编码 master ，而是希望用 sparksubmit启动应用程序，并从 spark-submit 中得到 master 的值。对于本地测试或者单元测试，你可以传递“local”字符串在同</span><br><span style="color:#444444;">一个进程内运行Spark Streaming。需要注意的是，它在内部创建了一个SparkContext对象，你可以通过 ssc.sparkContext访问这个SparkContext对象。</span><br><span style="color:#444444;">批时间片需要根据你的程序的潜在需求以及集群的可用资源来设定，你可以在性能调优那一节获取详细的信息.可以利用已经存在的 SparkContext 对象创建 StreamingContext 对象。</span></p>

<p><span style="color:#444444;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#444444;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><span style="color:#afafaf;">1</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">2</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">3</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.streaming.</code><code>_</code></span></p>

			<p><span style="color:#444444;"><code>val</code> <code>sc </code><code>=</code> <code>... </code><code>// existing SparkContext</code></span></p>

			<p><span style="color:#444444;"><code>val</code> <code>ssc </code><code>=</code> <code>new</code> <code>StreamingContext(sc, Seconds(</code><code>1</code><code>))</code></span></p>
			</td>
		</tr></tbody></table><p><br><br><span style="color:#444444;">当一个上下文（context）定义之后，你必须按照以下几步进行操作</span><br>
 </p>

<ul style="margin-left:14px;"><li>定义输入源；</li>
	<li>准备好流计算指令；</li>
	<li>利用 streamingContext.start() 方法接收和处理数据；</li>
	<li>处理过程将一直持续，直到 streamingContext.stop() 方法被调用。</li>
</ul><p><br><span style="color:#444444;">StreamingContext了解了，还有个重要的概念需要了解</span><span style="color:#ff0000;"><strong>DStream</strong></span><span style="color:#444444;">.</span><br><span style="color:#444444;">Spark Streaming支持一个高层的抽象，叫做离散流( discretized stream )或者 DStream ，它代表连续的数据流。DStream既可以利用从Kafka, Flume和Kinesis等源获取的输入数据流创建，也可以 在其他DStream的基础上通过高阶函数获得。在内部，DStream是由一系列RDDs组成。</span><br><span style="color:#444444;">举例：</span><br><span style="color:#444444;">一个简单的基于Streaming的workCount代码如下：</span></p>

<p><span style="color:#444444;">[Scala] <em>纯文本查看</em> <em>复制代码</em></span></p>

<p><span style="color:#444444;"><span style="color:#ffffff;"><a class="toolbar_item command_help help" href="http://www.aboutyun.com/#" rel="nofollow">?</a></span></span></p>

<table cellspacing="0"><tbody><tr><td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><span style="color:#afafaf;">01</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">02</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">03</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">04</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">05</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">06</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">07</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">08</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">09</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">10</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">11</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">12</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">13</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">14</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">15</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">16</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">17</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">18</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">19</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">20</span></span></p>

			<p><span style="color:#444444;"><span style="color:#afafaf;">21</span></span></p>
			</td>
			<td style="text-align:left;vertical-align:baseline;">
			<p><span style="color:#444444;"><code>package</code> <code>com.debugo.example</code></span></p>

			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.streaming.{Seconds, StreamingContext}</code></span></p>

			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.streaming.StreamingContext.</code><code>_</code></span></p>

			<p><span style="color:#444444;"><code>import</code> <code>org.apache.spark.SparkConf</code></span></p>

			<p><span style="color:#444444;"><code>  </code> </span></p>

			<p><span style="color:#444444;"><code>object</code> <code>WordCountStreaming {</code></span></p>

			<p><span style="color:#444444;"><code>  </code><code>def</code> <code>main(args</code><code>:</code> <code>Array[String])</code><code>:</code> <code>Unit </code><code>=</code><code>{</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>val</code> <code>sparkConf </code><code>=</code> <code>new</code> <code>SparkConf().setAppName(</code><code>"HDFSWordCount"</code><code>).setMaster(</code><code>"spark://172.19.1.232:7077"</code><code>)</code></span></p>

			<p><span style="color:#444444;"><code>  </code> </span></p>

			<p><span style="color:#444444;"><code>    </code><code>//create the streaming context</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>val</code>  <code>ssc </code><code>=</code> <code>new</code> <code>StreamingContext(sparkConf, Seconds(</code><code>30</code><code>))</code></span></p>

			<p><span style="color:#444444;"><code>  </code> </span></p>

			<p><span style="color:#444444;"><code>    </code><code>//process file when new file be found.</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>val</code> <code>lines </code><code>=</code> <code>ssc.textFileStream(</code><code>"file:///home/spark/data"</code><code>)</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>val</code> <code>words </code><code>=</code> <code>lines.flatMap(</code><code>_</code><code>.split(</code><code>" "</code><code>))</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>val</code> <code>wordCounts </code><code>=</code> <code>words.map(x </code><code>=</code><code>&gt; (x, </code><code>1</code><code>)).reduceByKey(</code><code>_</code> <code>+ </code><code>_</code><code>)</code><code>//这里不是rdd,而是dstream</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>wordCounts.print()</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>ssc.start()</code></span></p>

			<p><span style="color:#444444;"><code>    </code><code>ssc.awaitTermination()</code></span></p>

			<p><span style="color:#444444;"><code>  </code><code>}</code></span></p>

			<p><span style="color:#444444;"><code>}</code></span></p>
			</td>
		</tr></tbody></table><p><br><span style="color:#444444;">这段代码实现了当指定的路径有新文件生成时，就会对这些文件执行wordcount，并把结果print。具体流程如下：</span></p>

<p><img alt="" class="has" height="206" src="https://img-blog.csdn.net/20180807100600259?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Blbmd6b25nbHU3Mjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="656"></p>

<p><span style="color:#444444;">代码诠释：</span><br><span style="color:#444444;">使用Spark Streaming就需要创建StreamingContext对象（类似SparkContext）。创建StreamingContext对象所需的参数与SparkContext基本一致，包括设定Master节点(setMaster），设定应用名称(setAppName)。第二个参数Seconds(30)，指定了Spark Streaming处理数据的时间间隔为30秒。需要根据具体应用需要和集群处理能力进行设置。</span><br><span style="color:#ff0000;">val lines = ssc.textFileStream("file:///home/spark/data")为创建lines Dstream</span><br><br><span style="color:#ff0000;">val words = lines.flatMap(_.split(" "))为通过flatMap转换为words Dstream</span><br><br><span style="color:#444444;">我们在引一例，比如创建Twitter</span><br><span style="color:#444444;">val tweets=ssc.twitterStream()</span><br><img alt="" class="has" height="216" src="https://img-blog.csdn.net/20180807100625703?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Blbmd6b25nbHU3Mjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="684"></p>

<p><span style="color:#444444;">其中为tweets为DStream上面内容来自</span><br><span style="color:#444444;">让你真正明白spark streaming</span><br><a href="http://www.aboutyun.com/thread-21141-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21141</a><br><br><br><br><strong>DStream的transformation</strong><br><span style="color:#444444;">DStream与RDD，DataFrame类似的，也有自己的transformation。如下</span></p>

<table border="1" cellspacing="0"><tbody><tr><td style="background-color:#ffffff;border-color:#e3edf5;">  
			<p>Transformation</p>
			  </td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>Meaning</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>map(func)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>对 DStream 中的各个元素进行 func 函数操作， 然后返回一个新的 DStream.</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>flatMap(func)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>与 map 方法类似， 只不过各个输入项可以被输出为零个或多个输出项</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>filter(func)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>过滤出所有函数 func 返回值为 true 的 DStream 元素并返回一个新的 DStream</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>repartition(numPartitions)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>增加或减少 DStream 中的分区数， 从而改变 DStream 的并行度</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>union(otherStream)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>将源 DStream 和输入参数为 otherDStream 的元素合并，<br>
			  并返回一个新的 DStream.</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>count()</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>通过对 DStreaim 中的各个 RDD 中的元素进行计数， 然后返回只有一个元素<br>
			  的 RDD 构成的 DStream</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>reduce(func)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>对源 DStream 中的各个 RDD 中的元素利用 func 进行聚合操作，<br>
			  然后返回只有一个元素的 RDD 构成的新的 DStream.</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>countByValue()</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>对于元素类型为 K 的 DStream， 返回一个元素为（ K,Long） 键值对形式的<br>
			  新的 DStream， Long 对应的值为源 DStream 中各个 RDD 的 key 出现的次数</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>reduceByKey(func,<br>
			  [numTasks])</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>利用 func 函数对源 DStream 中的 key 进行聚合操作， 然后返回新的（ K， V） 对<br>
			  构成的 DStream</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>join(otherStream,<br>
			  [numTasks])</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>输入为（ K,V)、 （ K,W） 类型的 DStream， 返回一个新的（ K， （ V， W） 类型的<br>
			  DStream</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>cogroup(otherStream,<br>
			  [numTasks])</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>输入为（ K,V)、 （ K,W） 类型的 DStream， 返回一个新的 (K, Seq[V], Seq[W])<br>
			  元组类型的 DStream</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>transform(func)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>通过 RDD-to-RDD 函数作用于源码 DStream 中的各个 RDD，可以是任意的 RDD 操作， 从而返回一个新的 RDD</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>updateStateByKey(func)</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>根据于 key 的前置状态和 key 的新值， 对 key 进行更新，<br>
			  返回一个新状态的 DStream</p>
			</td>
		</tr><tr><td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>window</p>
			</td>
			<td style="background-color:#ffffff;border-color:#e3edf5;">
			<p>对滑动窗口数据执行操作</p>
			</td>
		</tr></tbody></table><p><br><span style="color:#444444;">除了DStream，还有个重要的概念，需要了解</span><br><strong>windows滑动窗体</strong><br><span style="color:#444444;">我们知道spark streaming的数据流是Dstream，而Dstream由RDD组成，但是我们将这些RDD进行有规则的组合，比如我们以3个RDD进行组合，那么组合起来，我们需要给它起一个名字，就是</span><span style="color:#ff0000;">windows滑动窗体</span><br><span style="color:#444444;">更多内容可参考</span><br><br><span style="color:#444444;">spark streaming知识总结2</span><br><a href="http://www.aboutyun.com/thread-21173-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21173</a><br><br><span style="color:#444444;">上面对spark streaming有了一定的了解，更多编程知识可参考下面内容</span><br><span style="color:#444444;">SparkStreaming运行三种方式</span><br><a href="http://www.aboutyun.com/thread-18892-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=18892</a><br><br><span style="color:#444444;">spark streaming知识总结1</span><br><a href="http://www.aboutyun.com/thread-21307-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21307</a><br><br><span style="color:#444444;">sparkstreaming数据通过Scala实现存储到数据库</span><br><a href="http://www.aboutyun.com/thread-20753-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20753</a><br><br><br><span style="color:#444444;">Spark Streaming日志分析思考、选择方案及部分代码实现</span><br><a href="http://www.aboutyun.com/thread-21593-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21593</a><br><br><br><span style="color:#444444;">透过WordCount案例快速理解SparkStreaming工作原理分享</span><br><a href="http://www.aboutyun.com/thread-19688-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=19688</a><br><br><br><span style="color:#444444;">刘永平-Spark-streaming在京东的项目实践</span><br><a href="http://www.aboutyun.com/thread-18924-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=18924</a><br><br><br><span style="color:#444444;">Spark-Streaming编程指南</span><br><a href="http://www.aboutyun.com/thread-21257-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21257</a><br><br><br><span style="color:#444444;">上面具备spark streaming知识后，下面是关于about云日志分析使用到的spark streaming大家可参考</span><br><br><span style="color:#444444;">使用Spark Streaming + Kafka 实现有容错性的实时统计程序</span><br><a href="http://www.aboutyun.com/thread-20427-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20427</a><br><br><br><span style="color:#444444;">about云日志分析项目准备10：使用Intellij Idea搭建Spark Streaming开发环境(SBT版本)</span><br><a href="http://www.aboutyun.com/thread-20855-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20855</a><br><br><br><span style="color:#444444;">about云日志分析项目准备10-4：将Spark Streaming程序运行在Spark集群上</span><br><a href="http://www.aboutyun.com/thread-21034-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21034</a><br><br><br><span style="color:#444444;">about云日志分析项目准备11：spark streaming 接收 flume 监控目录的日志文件</span><br><a href="http://www.aboutyun.com/thread-21229-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21229</a><br><br><br><span style="color:#444444;">about云日志分析项目准备11-1：spark streaming+spark sql 实现业务</span><br><a href="http://www.aboutyun.com/thread-21599-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21599</a><br><br><br><span style="color:#444444;">下面就是spark MLlib和GraphX编程，对于初级入门来说用到的不多，也可以不用看。MLlib 是Spark的可以扩展的机器学习库，由以下部分组成：通用的学习算法和工具类，包括分类，回归，聚类，协同过滤，降维。GraphX是spark的一个新组件用于图和并行图计算.下面给大家推荐一些资料</span><br><br><strong>3.4.spark MLlib编程</strong><br><br><span style="color:#444444;">使用Spark MLlib给豆瓣用户推荐电影</span><br><a href="http://www.aboutyun.com/thread-16430-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=16430</a><br><br><br><span style="color:#444444;">MLlib回归算法（线性回归、决策树）实战演练--Spark学习（机器学习）</span><br><a href="http://www.aboutyun.com/thread-17183-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=17183</a><br><br><br><br><span style="color:#444444;">Spark_Mllib_实践与优化_雷宗雄</span><br><a href="http://www.aboutyun.com/thread-18739-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=18739</a><br><br><br><span style="color:#444444;">Spark MLlib系列——程序框架</span><br><a href="http://www.aboutyun.com/thread-8552-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=8552</a><br><br><br><span style="color:#444444;">Spark MLlib算法之KMeans应用实例讲解【附代码下载】</span><br><a href="http://www.aboutyun.com/thread-21436-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21436</a><br><br><br><span style="color:#444444;">Spark MLlib Statistics统计</span><br><a href="http://www.aboutyun.com/thread-13054-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=13054</a><br><br><br><br><span style="color:#444444;">Spark MLlib之 KMeans聚类算法详解</span><br><a href="http://www.aboutyun.com/thread-19745-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=19745</a><br><br><br><span style="color:#444444;">MLlib分类算法实战演练--Spark学习（机器学习）</span><br><a href="http://www.aboutyun.com/thread-17184-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=17184</a><br><br><br><span style="color:#444444;">about云系列spark入门5：MLlib 介绍</span><br><a href="http://www.aboutyun.com/thread-14183-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=14183</a><br><br><br><br><span style="color:#444444;">Spark0.9分布式运行MLlib的线性回归算法</span><br><a href="http://www.aboutyun.com/thread-10793-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=10793</a><br><br><span style="color:#444444;">求一spark mllib视频</span><br><a href="http://www.aboutyun.com/thread-19061-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=19061</a><br><br><span style="color:#444444;">ALS 在 Spark MLlib 中的实现--孟祥瑞</span><br><a href="http://www.aboutyun.com/thread-12988-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=12988</a><br><br><br><br><strong>3.5.spark  GraphX</strong><br><br><span style="color:#444444;">Spark GraphX详细介绍</span><br><a href="http://www.aboutyun.com/thread-13783-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=13783</a><br><br><span style="color:#444444;">Spark GraphX在淘宝的实践</span><br><a href="http://www.aboutyun.com/thread-12970-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=12970</a><br><br><br><span style="color:#444444;">Spark中文手册8：spark GraphX编程指南（1）</span><br><a href="http://www.aboutyun.com/thread-11589-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=11589</a><br><br><span style="color:#444444;">Spark中文手册9：spark GraphX编程指南（2）</span><br><a href="http://www.aboutyun.com/thread-11601-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=11601</a><br><br><br><span style="color:#444444;">Apache Spark源码走读之14 -- Graphx实现剖析</span><br><a href="http://www.aboutyun.com/thread-10957-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=10957</a><br><br><br><span style="color:#444444;">spark图感知及图数据挖掘：图流合壁，基于Spark Streaming和GraphX的动态图计算</span><br><a href="http://www.aboutyun.com/thread-12277-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=12277</a><br><br><br><span style="color:#444444;">图流合璧——基于Spark Streaming和GraphX的动态图计算</span><br><a href="http://www.aboutyun.com/thread-13799-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=13799</a><br><br><span style="color:#444444;">用Apache Spark进行大数据处理 -用Spark GraphX进行图数据分析</span><br><a href="http://www.aboutyun.com/thread-21913-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21913</a><br><br><br><span style="color:#444444;">about云系列spark入门6：GraphX  介绍</span><br><a href="http://www.aboutyun.com/thread-14220-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=14220</a><br><br><br><span style="color:#444444;">最新100份开源大数据架构论文之45:spark graphx</span><br><a href="http://www.aboutyun.com/thread-14239-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=14239</a><br><br><br><span style="color:#444444;">上面介绍了从实战学习的角度去入门学习，后面有时间从理论角度来入门spark。</span><br><br><strong>更多资料推荐</strong><br><span style="color:#444444;">Spark1.0.0 学习路线指导</span><br><a href="http://www.aboutyun.com/thread-8421-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=8421</a><br><br><span style="color:#444444;">Spark学习总结---入门</span><br><a href="http://www.aboutyun.com/thread-20710-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=20710</a><br><br><span style="color:#444444;">spark个人学习总结</span><br><a href="http://www.aboutyun.com/thread-14584-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=14584</a><br><br><br><span style="color:#444444;">spark入门教程及经验总结</span><br><a href="http://www.aboutyun.com/thread-11128-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=11128</a><br><br><br><br><span style="color:#444444;">本文链接：</span><a href="http://www.aboutyun.com/thread-21959-1-1.html" rel="nofollow">http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=21959</a><br><br><strong>pdf下载</strong><br><br><span style="color:#444444;">链接：</span><a href="http://pan.baidu.com/s/1eROBzrW" rel="nofollow">http://pan.baidu.com/s/1eROBzrW</a><span style="color:#444444;"> 密码：5n7a</span></p>            </div>
                </div>