---
layout:     post
title:      Hive介绍及部署最详细文档
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<table cellspacing="0" cellpadding="0" style="border-collapse:collapse;table-layout:fixed;color:rgb(68,68,68);font-family:Tahoma, 'Microsoft Yahei', Simsun;font-size:12px;line-height:18px;"><tbody><tr><td class="t_f" id="postmessage_116352" style="font-size:14px;">
<div class="a_pr" style="overflow:hidden;margin-left:10px;">
<div id="BAIDU_SSP__wrapper_u1728839_0">&lt;iframe id="iframeu1728839_0" src="http://pos.baidu.com/gcdm?rdid=1728839&amp;amp;dc=2&amp;amp;di=u1728839&amp;amp;dri=0&amp;amp;dis=0&amp;amp;dai=3&amp;amp;ps=454x1130&amp;amp;dcb=BAIDU_SSP_define&amp;amp;dtm=BAIDU_DUP_SETJSONADSLOT&amp;amp;dvi=0.0&amp;amp;dci=-1&amp;amp;dpt=none&amp;amp;tsr=0&amp;amp;tpr=1463104811653&amp;amp;ti=Hive%E4%BB%8B%E7%BB%8D%E5%8F%8A%E9%83%A8%E7%BD%B2%E6%9C%80%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3-%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%88hadoop%E7%B3%BB%E5%88%97%EF%BC%89%E5%AD%A6%E4%B9%A0-about%E4%BA%91%E5%BC%80%E5%8F%91&amp;amp;ari=1&amp;amp;dbv=2&amp;amp;drs=1&amp;amp;pcs=1343x667&amp;amp;pss=1343x766&amp;amp;cfv=0&amp;amp;cpl=6&amp;amp;chi=1&amp;amp;cce=true&amp;amp;cec=GBK&amp;amp;tlm=1463104811&amp;amp;ltu=http%3A%2F%2Fwww.aboutyun.com%2Fthread-14958-1-1.html&amp;amp;ltr=http%3A%2F%2Fso.aboutyun.com%2Fcse%2Fsearch%3Fs%3D18335994984162479371%26ie%3Dgbk%26q%3Dhive%26searchsubmit%3Dtrue&amp;amp;ecd=1&amp;amp;psr=1360x768&amp;amp;par=1360x728&amp;amp;pis=-1x-1&amp;amp;ccd=24&amp;amp;cja=true&amp;amp;cmi=8&amp;amp;col=zh-CN&amp;amp;cdo=-1&amp;amp;tcn=1463104812&amp;amp;qn=dfc03c912c82696c&amp;amp;tt=1463104811609.156.531.532" width="120" height="240" align="center,center" vspace="0" hspace="0" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" allowtransparency="true" style="word-wrap: break-word; border-width: 0px; border-style: initial; vertical-align: bottom; margin: 0px;"&gt;&lt;/iframe&gt;</div>
</div>
<span style="font-weight:700;">问题导读<br><span style="color:#FF0000;">1、Hive的概念以及优缺点。<br>
2、Hive的执行流程。<br>
3、了解Hive的运行框架，以及Hive架构包括哪些组件？<br>
4、Hive的搭建过程。<br>
5、如何设置安装Mysql、设置root密码、Hive用户和创建Hive数据库？<br>
6、安装Hive过程中如何设置相关的环境变量？</span></span><span style="color:#FF0000;"><br><span style="font-weight:700;">7、如何验证Hive是否安装成功？<br>
8、解决问题：设置MySql数据库root用户密码报错、Hive启动，报CommandNeedRetryException异常。</span><br></span><br><img id="aimg_xrZrK" class="zoom" border="0" alt="" src="http://www.aboutyun.com/static/image/hrline/2.gif" width="485" height="21"><br><br><span style="font-size:14px;"><span style="font-weight:700;">1、Hive介绍</span><br>
1.1 Hive介绍</span><br>
Hive是一个基于Hadoop的开源数据仓库工具，用于存储和处理海量结构化数据。它是Facebook 2008年8月开源的一个数据仓库框架，提供了类似于SQL语法的HQL语句作为数据访问接口，Hive有如下优缺点：<br>
优点：<br>
1.Hive 使用类SQL 查询语法, 最大限度的实现了和SQL标准的兼容，大大降低了传统数据分析人员学习的曲线；<br>
2.使用JDBC 接口/ODBC接口，开发人员更易开发应用；<br>
3.以MR 作为计算引擎、HDFS 作为存储系统，为超大数据集设计的计算/ 扩展能力；<br>
4.统一的元数据管理（Derby、MySql等），并可与Pig 、Presto 等共享；<br>
l缺点：<br>
1.Hive 的HQL 表达的能力有限，有些复杂运算用HQL 不易表达；<br>
2.由于Hive自动生成MapReduce 作业， HQL 调优困难；<br>
3.粒度较粗，可控性差<br><br><span style="font-size:14px;">1.2  Hive运行架构</span><br><img id="aimg_18472" src="http://www.aboutyun.com/data/attachment/forum/201508/27/143122kzwa3qkkdcipwl33.jpg" class="zoom" width="440" alt=""> <br>
由上图可知，Hadoop的MapReduce是Hive架构的根基。Hive架构包括如下组件：CLI（Command Line Interface）、JDBC/ODBC、Thrift Server、WEB GUI、Metastore和Driver(Complier、Optimizer和Executor)，这些组件分为两大类：服务端组件和客户端组件。<br>
服务端组件：<br>
1、lDriver组件：该组件包括Complier、Optimizer和Executor，它的作用是将HiveQL（类SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的MapReduce计算框架；<br>
2、lMetastore组件：元数据服务组件，这个组件存储Hive的元数据，Hive的元数据存储在关系数据库里，Hive支持的关系数据库有Derby和Mysql。元数据对于Hive十分重要，因此Hive支持把Metastore服务独立出来，安装到远程的服务器集群里，从而解耦Hive服务和Metastore服务，保证Hive运行的健壮性；<br>
3、lThrift服务：Thrift是Facebook开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，Hive集成了该服务，能让不同的编程语言调用Hive的接口。<br>
客户端组件：<br>
lCLI：Command Line Interface，命令行接口。<br>
1、lThrift客户端：上面的架构图里没有写上Thrift客户端，但是Hive架构的许多客户端接口是建立在Thrift客户端之上，包括JDBC和ODBC接口。<br>
2、lWEBGUI：Hive客户端提供了一种通过网页的方式访问Hive所提供的服务。这个接口对应Hive的HWI组件（Hive Web Interface），使用前要启动HWI服务。<br>
Hive的执行流程：<br><img id="aimg_18473" src="http://www.aboutyun.com/data/attachment/forum/201508/27/143321koc6budfrgvag5za.gif" class="zoom" width="600" alt=""> <br>
三种部署模式<br>
1.单用户模式 此模式连接到一个In-Memory 的数据库Derby，一般用于Unit Test。<br><img id="aimg_18474" src="http://www.aboutyun.com/data/attachment/forum/201508/27/143442mz0pq7qndhf67cqo.jpg" class="zoom" width="398" alt=""> <br>
2.多用户模式 通过网络连接到一个数据库中，是最经常使用到的模式。<br><img id="aimg_18475" src="http://www.aboutyun.com/data/attachment/forum/201508/27/143442j0wvwdixyexxfe96.jpg" class="zoom" width="395" alt=""> <br>
3.  远程服务器模式 用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库。<br><img id="aimg_18476" src="http://www.aboutyun.com/data/attachment/forum/201508/27/143443n94e98v80m9w8i1g.jpg" class="zoom" width="596" alt=""> <br><br><span style="font-size:14px;">1.3 Hive数据模型</span><br>
Hive没有专门的数据存储格式，用户可以自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。Hive中所有的数据都存储在HDFS中，存储结构主要包括数据库、文件、表和视图。Hive中包含以下数据模型：Table内部表，External Table外部表，Partition分区，Bucket桶。Hive默认可以直接加载文本文件，还支持sequence file 、RCFile。<br><br>
1.Hive数据库 <br>
类似传统数据库的DataBase，在第三方数据库里实际是一张表<br>
简单示例命令行： create database test_database;<br>
2.内部表 <br>
Hive的内部表与数据库中的Table在概念上是类似。每一个Table在Hive中都有一个相应的目录存储数据。例如一个表tbInner，它在HDFS中的路径为/user/hive/warehouse/tbInner，其中/user/hive/warehouse是在hive-site.xml中由${hive.metastore.warehouse.dir} 指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。内部表删除时，元数据与数据都会被删除。<br>
内部表简单示例：<br>
创建数据文件：test_inner_table.txt<br>
创建表：create table test_inner_table (key string);<br>
加载数据：LOAD DATA LOCAL INPATH 'filepath' INTO TABLE test_inner_table;<br>
查看数据：select * from test_inner_table;<br>
删除表：drop table test_inner_table;<br>
3. 外部表 <br>
外部表指向已经在HDFS中存在的数据，并可以创建Partition。它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。内部表的创建过程和数据加载过程这两个过程可以分别独立完成，也可以在同一个语句中完成，在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。而外部表只有一个过程，加载数据和创建表同时完成（CREATE EXTERNAL TABLE ……LOCATION），实际数据是存储在LOCATION后面指定的
 HDFS 路径中，并不会移动到数据仓库目录中。当删除一个External Table时，仅删除该链接。<br>
外部表简单示例：<br>
创建数据文件：test_external_table.txt<br>
创建表：create external table test_external_table (key string);<br>
加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_inner_table;<br>
查看数据：select * from test_external_table;<br>
删除表：drop table test_external_table;<br>
4.分区 <br>
Partition对应于数据库中的Partition列的密集索引，但是Hive中Partition的组织方式和数据库中的很不相同。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition的数据都存储在对应的目录中。例如pvs表中包含ds和city两个Partition，则对应于ds = 20090801, ctry = US 的HDFS子目录为/user/hive/warehouse/pvs/ds=20090801/ctry=US；对应于 ds = 20090801, ctry
 = CA 的HDFS子目录为/user/hive/warehouse/pvs/ds=20090801/ctry=CA。<br>
分区表简单示例：<br>
创建数据文件：test_partition_table.txt<br>
创建表：create table test_partition_table (key string) partitioned by (dt string);<br>
加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_partition_table partition (dt=‘2006’);<br>
查看数据：select * from test_partition_table;<br>
删除表：drop table test_partition_table;<br>
5.桶 <br>
Buckets是将表的列通过Hash算法进一步分解成不同的文件存储。它对指定列计算Hash，根据Hash值切分数据，目的是为了并行，每一个Bucket对应一个文件。例如将user列分散至32个bucket，首先对user列的值计算Hash，对应Hash值为0的HDFS目录为/user/hive/warehouse/pvs/ds=20090801/ctry=US/part-00000；Hash值为20的HDFS目录为/user/hive/warehouse/pvs/ds=20090801/ctry=US/part-00020。如果想应用很多的Map任务这样是不错的选择。<br>
桶的简单示例：<br>
创建数据文件：test_bucket_table.txt<br>
创建表：create table test_bucket_table (key string) clustered by (key) into 20 buckets；<br>
加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_bucket_table；<br>
查看数据：select * from test_bucket_table;  set hive.enforce.bucketing = true;<br>
6.Hive的视图 <br>
视图与传统数据库的视图类似。视图是只读的，它基于的基本表，如果改变，数据增加不会影响视图的呈现；如果删除，会出现问题。如果不指定视图的列，会根据select语句后的生成。<br>
示例：create view test_view as select * from test；<br><span style="font-size:14px;"><br></span><span style="font-size:14px;">1.4 Hive数据类型</span><br>
Hive支持两种数据类型，一类叫原子数据类型，一类叫复杂数据类型。<br>
1、原子数据类型包括数值型、布尔型和字符串类型，具体如下表所示：<br><img id="aimg_18477" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144400yrrrrrpmip8pi173.jpg" class="zoom" width="600" alt=""> <br>
由上表我们看到Hive不支持日期类型，在Hive里日期都是用字符串来表示的，而常用的日期格式转化操作则是通过自定义函数进行操作。<br>
Hive是用Java开发的，Hive里的基本数据类型和java的基本数据类型也是一一对应的，除了String类型。有符号的整数类型：TINYINT、SMALLINT、INT和BIGINT分别等价于Java的Byte、Short、Int和Long原子类型，它们分别为1字节、2字节、4字节和8字节有符号整数。Hive的浮点数据类型FLOAT和DOUBLE,对应于Java的基本类型Float和Double类型。而Hive的BOOLEAN类型相当于Java的基本数据类型Boolean。对于Hive的String类型相当于数据库的Varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。<br>
2、复杂数据类型包括数组（ARRAY）、映射（MAP）和结构体（STRUCT），具体如下所示：<br><img id="aimg_18478" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144403cgl6aoqw7owiwdlg.gif" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">1.5 Hive与关系数据库的区别</span><br>
由于Hive采用了SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性。<br>
Hive和数据库的比较如下表：<br><img id="aimg_18479" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144601uk4xxnkk1bqknnb1.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-weight:700;"><span style="font-size:14px;">2、Hive搭建过程</span></span><br><span style="font-size:14px;">2.1 安装MySql数据库<br>
2.1.1 下载MySql安装文件</span><br>
下载地址：<a href="http://dev.mysql.com/downloads/mysql/#downloads" rel="nofollow" style="color:rgb(51,102,153);">http://dev.mysql.com/downloads/mysql/#downloads</a>，使用系统为CentOS选择 Red Hat Enterprise Linux/Oracle系列：<br><img id="aimg_18480" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144602gdbg3lhhfqfwlrb3.jpg" class="zoom" width="600" alt=""> <br>
操作系统为64位，选择对应安装包进行下载：<br><img id="aimg_18481" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144603v9d9gi39mzrmis36.jpg" class="zoom" width="600" alt=""> <br><img id="aimg_18482" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144612ywiq6u5httrmmwuu.jpg" class="zoom" width="600" alt=""> <br><img id="aimg_18483" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144612ama8t88s44zezp0x.jpg" class="zoom" width="600" alt=""> <br>
下载在本地目录如下图：<br><img id="aimg_18484" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144614njfacl8wazyaxal0.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.1.2 上传MySql安装文件</span><br>
把下载的mysql安装包，使用SSH Secure File Transfer工具（参见《Spark编译与部署（上）--基础环境搭建》1.3.1介绍）上传到/home/hadoop/upload 目录下，如下图所示：<br><img id="aimg_18485" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144904f53g0ghfhgt1ztxh.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.1.3 卸载旧的MySql</span><br>
1、查找以前是否安装有mysql<br>
使用命令查看是否已经安装过mysql：<br>
$rpm -qa | grep -i mysql<br>
可以看到如下图的所示：<br><img id="aimg_18486" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144905j2qwt7tq6884fwqd.jpg" class="zoom" width="600" alt=""> <br>
说明之前安装了：<br>
MySQL-client-5.6.21-1.el6.x86_64<br>
MySQL-server-5.6.21-1.el6.x86_64<br>
MySQL-devel-5.6.21-1.el6.x86_64<br>
如果没有结果，可以进行mysql数据库安装<br>
2、停止mysql服务、删除之前安装的mysql<br>
停止mysql服务、删除之前安装的mysql删除命令：rpm -e –nodeps 包名<br>
$sudo rpm -ev MySQL-server-5.6.21-1.el6.x86_64<br>
$sudo rpm -ev MySQL-devel-5.6.21-1.el6.x86_64<br>
$sudo rpm -ev MySQL-client-5.6.21-1.el6.x86_64<br><br>
如果存在CentOS自带mysql-libs-5.1.71-1.el6.x86_64使用下面的命令卸载即可<br>
$sudo rpm -ev --nodeps mysql-libs-5.1.71-1.el6.x86_64<br><img id="aimg_18487" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144906nll5fyf9775x12sl.jpg" class="zoom" width="600" alt=""> <br>
3、查找之前老版本mysql的目录并且删除老版本mysql的文件和库<br>
$sudo find / -name mysql<br><img id="aimg_18488" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144627nlbjr04zlym4y3ml.jpg" class="zoom" width="600" alt=""> <br>
删除对应的mysql目录<br>
$sudo rm -rf /usr/lib64/mysql<br>
$sudo rm -rf /var/lib/mysql<br><img id="aimg_18489" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144628voh0kn8fh8nqnx1k.jpg" class="zoom" width="600" alt=""> <br>
4、再次查找机器是否安装mysql<br>
$sudo rpm -qa | grep -i mysql<br>
无结果，说明已经卸载彻底、接下来直接安装mysql即可<br><img id="aimg_18490" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144757eh6j9r46mh6mzzs5.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.1.4 安装MySql</span><br>
进入安装文件的目录，安装mysql服务端<br>
$cd /home/hadoop/upload<br>
$sudo rpm -ivh MySQL-server-5.6.21-1.el6.x86_64.rpm<br><img id="aimg_18491" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144758v6sh57n6vv58vc5s.jpg" class="zoom" width="600" alt=""> <br>
安装mysql客户端、mysql-devel<br>
$sudo rpm -ivh MySQL-client-5.6.21-1.el6.x86_64.rpm<br>
$sudo rpm -ivh MySQL-devel-5.6.21-1.el6.x86_64.rpm<br><img id="aimg_18492" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144932ctlb454ggtv9xcgn.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.1.5 设置root密码</span><br>
在CentOS6.5下安装mysql设置root密码时，出现如下错误：<br><img id="aimg_18493" src="http://www.aboutyun.com/data/attachment/forum/201508/27/144933usqmb6gembsrsre0.jpg" class="zoom" width="600" alt=""> <br>
/usr/bin/mysqladmin: connect to server at 'localhost' failed<br>
error: 'Access denied for user 'root'@'localhost' (using password: NO)'<br>
可以进入安全模式进行设置root密码<br>
1、停止mysql服务<br>
使用如下命令停止mysql服务：<br>
$sudo service mysql stop<br>
$sudo service mysql status<br><img id="aimg_18494" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145144nzziyj5i9l0bjf7o.jpg" class="zoom" width="600" alt=""> <br><br>
2、跳过验证启动mysql<br>
使用如下命令验证启动mysql，由于&amp;结尾是后台运行进程，运行该命令可以再打开命令窗口或者Ctr+C继续进行下步操作，由于mysql启动时间会长点，需要等待几分钟再查看启动状态：<br>
$sudo mysqld_safe --skip-grant-tables &amp;<br>
$sudo service mysql status<br><img id="aimg_18495" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145239o7zl8li3qitll3ll.jpg" class="zoom" width="600" alt=""> <br><br>
3、跳过验证启动MySQL<br>
验证mysql服务已经在后台运行后，执行如下语句，其中后面三条命令是在mysql语句：<br>
mysql -u root<br>
mysql&gt;use mysql;<br>
mysql&gt;update user set password = password('root') where user = 'root';<br>
mysql&gt;flush privileges;<br>
mysql&gt;quit;<br><img id="aimg_18496" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145240t6636z323pa8bvc4.jpg" class="zoom" width="600" alt=""> <br><br>
4、跳过验证启动MySQL<br>
重启mysql服务并查看状态<br>
$sudo service mysql stop<br>
$sudo service mysql start<br>
$sudo service mysql status<br><img id="aimg_18497" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145358u80ipd6djizil703.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.1.6 设置Hive用户</span><br>
进入mysql命令行，创建Hive用户并赋予所有权限：<br>
mysql -uroot -proot<br>
mysql&gt;set password=password('root');<br>
mysql&gt;create user 'hive' identified by 'hive';<br>
mysql&gt;grant all on *.* TO 'hive'@'%' with grant option;<br>
mysql&gt;flush privileges;<br>
mysql&gt;quit;<br><img id="aimg_18498" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145519cso02y3c6j3pptl6.jpg" class="zoom" width="600" alt=""> <br>
（注意：如果是root第一次登录数据库，需要重新设置一下密码，所报异常信息如下：ERROR 1820 (HY000): You must SET PASSWORD before executing this statement）<br><br><span style="font-size:14px;">2.1.7 创建Hive数据库</span><br>
使用hive用户登录，创建Hive数据库：<br>
mysql -uhive -phive<br>
mysql&gt;create database hive;<br>
mysql&gt;show databases;<br><img id="aimg_18499" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145626rspl0ugngo4phhg5.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2 安装Hive<br>
2.2.1 下载Hive安装文件</span><br>
可以到Apache基金hive官网<a href="http://hive.apache.org/downloads.html" rel="nofollow" style="color:rgb(51,102,153);">http://hive.apache.org/downloads.html</a>，选择镜像下载地址：<a href="http://mirrors.cnnic.cn/apache/hive/" rel="nofollow" style="color:rgb(51,102,153);">http://mirrors.cnnic.cn/apache/hive/</a>下载一个稳定版本，这里选择下载apache-hive-0.13.1-bin.tar.gz文件，如下图所示：<br><img id="aimg_18500" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145750nhxxx9xfwodoecoo.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.2 下载MySql驱动</span><br>
到mysql官网进入下载页面：<a href="http://dev.mysql.com/downloads/connector/j/" rel="nofollow" style="color:rgb(51,102,153);">http://dev.mysql.com/downloads/connector/j/</a>，默认情况下是Windows安装包，这里需要选择Platform Independent版本下载zip格式的文件<br><img id="aimg_18501" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145846l8gfi3v98miam66d.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.3 上传Hive安装文件和MySql驱动</span><br>
把下载的hive安装包和mysql驱动包，使用SSH Secure File Transfer工具（参见《Spark编译与部署（上）--基础环境搭建》1.3.1介绍）上传到/home/hadoop/upload 目录下，如下图所示：<br><img id="aimg_18502" src="http://www.aboutyun.com/data/attachment/forum/201508/27/145919crb18brr8rn0778r.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.4 解压缩</span><br>
到上传目录下，用如下命令解压缩hive安装文件：<br>
cd /home/hadoop/upload<br>
tar -zxf hive-0.13.1-bin.tar.gz<br><img id="aimg_18503" src="http://www.aboutyun.com/data/attachment/forum/201508/27/150006ymaz2ipzq7qjadpi.jpg" class="zoom" width="600" alt=""> <br>
改名并迁移到/app/hadoop目录下：<br>
sudo mv apache-hive-0.13.1-bin /app/hadoop/hive-0.13.1<br>
ll /app/hadoop<br><img id="aimg_18504" src="http://www.aboutyun.com/data/attachment/forum/201508/27/150051mto2ajfvkqjxtt8a.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.5 把MySql驱动放到Hive的lib目录下</span><br>
把下载的hive安装包和mysql驱动包，使用<br>
cd /home/hadoop/upload<br>
cp mysql-connector-java-5.1.34-bin.jar /app/hadoop/hive-0.13.1/lib<br><img id="aimg_18524" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165048x2k31mr141h4jalz.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.6 配置/etc/profile环境变量</span><br>
使用如下命令打开/etc/profile文件：<br>
sudo vi /etc/profile<br><img id="aimg_18525" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165049k5gmr10kqabb00r1.jpg" class="zoom" width="600" alt=""> <br>
设置如下参数：<br>
export HIVE_HOME=/app/hadoop/hive-0.13.1<br>
export PATH=$PATH:$HIVE_HOME/bin<br>
export CLASSPATH=$CLASSPATH:$HIVE_HOME/bin<br><img id="aimg_18526" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165050i0lziqdqqfqcwf60.jpg" class="zoom" width="600" alt=""> <br>
使配置文件生效：<br>
source /etc/profile<br><img id="aimg_18527" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165051wp1adp0d1rmr1a53.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.7 设置hive-env.sh配置文件</span><br>
进入hive-0.13.1/conf目录，复制hive-env.sh.templaete为hive-env.sh：<br>
cd /app/hadoop/hive-0.13.1/conf<br>
cp hive-env.sh.template hive-env.sh<br>
ls<br><img id="aimg_18528" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165052pb56336dndnd773a.jpg" class="zoom" width="600" alt=""> <br>
使用如下命令边界配置文件<br>
sudo vi hive-env.sh<br>
分别设置HADOOP_HOME和HIVE_CONF_DIR两个值：<br>
# Set HADOOP_HOME to point to a specific hadoop install directory<br>
export HADOOP_HOME=/app/hadoop/hadoop-2.2.0<br>
# Hive Configuration Directory can be controlled by:<br>
export HIVE_CONF_DIR=/app/hadoop/hive-0.13.1/conf<br><img id="aimg_18529" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165053l131plzuz8mza37z.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.2.8 设置hive-site.xml配置文件</span><br>
复制hive-default.xml.templaete为hive-site.xml<br>
cp hive-default.xml.template hive-site.xml<br>
sudo vi hive-site.xml<br><img id="aimg_18530" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165055zikcsnn37yktt1c1.jpg" class="zoom" width="600" alt=""> <br>
1、加入配置项<br>
默认metastore在本地，添加配置改为非本地<br>
&lt;property&gt;<br>
  &lt;name&gt;hive.metastore.local&lt;/name&gt;<br>
  &lt;value&gt;false&lt;/value&gt;<br>
&lt;/property&gt;<br><img id="aimg_18531" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165056c0vlt6zwa4twvg0c.jpg" class="zoom" width="600" alt=""> <br>
2、修改配置项<br>
hive默认为derby数据库，derby数据只运行单个用户进行连接，这里需要调整为mysql数据库<br>
&lt;property&gt;<br>
  &lt;name&gt;hive.metastore.uris&lt;/name&gt;<br>
  &lt;value&gt;thrift://hadoop1:9083&lt;/value&gt;<br>
  &lt;description&gt;Thrift URI for the remote metastore. ...&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>
  &lt;value&gt;jdbc:mysql://hadoop1:3306/hive?=createDatabaseIfNotExist=true&lt;/value&gt;<br>
  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br>
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br>
  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
.......<br>
&lt;property&gt;<br>
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br>
  &lt;value&gt;hive&lt;/value&gt;<br>
  &lt;description&gt;username to use against metastore database&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br>
  &lt;value&gt;hive&lt;/value&gt;<br>
  &lt;description&gt;password to use against metastore database&lt;/description&gt;<br>
&lt;/property&gt;<br><img id="aimg_18532" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165057i1ursy9d69izhvz1.jpg" class="zoom" width="600" alt=""> <br>
把hive.metastore.schema.verification配置项值修改为false<br>
&lt;property&gt;<br>
  &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;<br>
  &lt;value&gt;false&lt;/value&gt;<br>
   &lt;desc....&gt;<br>
&lt;/property&gt;<br><img id="aimg_18533" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165058om1vhfpfc1whvmi9.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.3 启动并验证Hive<br>
2.3.1 启动Hive</span><br>
实际使用时，一般通过后台启动metastore和hiveserver实现服务，命令如下：<br>
hive --service metastore &amp;<br>
hive --service hiveserver &amp;<br><img id="aimg_18534" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165100zgq4fuw21zjd97ig.jpg" class="zoom" width="600" alt=""> <br>
启动用通过jps命令可以看到两个进行运行在后台<br><img id="aimg_18535" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165101u9quu17ip1pt3miv.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">2.3.2 在Hive中操作</span><br>
登录hive，在hive创建表并查看该表，命令如下：<br>
hive<br>
hive&gt;create table test(a string, b int);<br>
hive&gt;show tables;<br>
hive&gt;desc test;<br><img id="aimg_18536" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165102x4jm30163bb1304o.jpg" class="zoom" width="600" alt=""> <br>
登录mysql，在TBLS表中查看新增test表：<br>
mysql -uhive -phive<br>
mysql&gt;use hive;<br>
mysql&gt;select TBL_ID, CREATE_TIME, DB_ID, OWNER, TBL_NAME,TBL_TYPE from TBLS;<br><img id="aimg_18537" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165103ra21jijyoug2pihz.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;"><span style="font-weight:700;">3、问题解决</span><br>
3.1 设置MySql数据库root用户密码报错</span><br><img id="aimg_18538" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165104or91xrb1glyrrudj.jpg" class="zoom" width="600" alt=""> <span style="font-size:14px;"><br></span>在CentOS6.5下安装mysql设置root密码时，出现如下错误：<br>
/usr/bin/mysqladmin: connect to server at 'localhost' failed<br>
error: 'Access denied for user 'root'@'localhost' (using password: NO)'<br><br>
1、停止mysql服务<br>
使用如下命令停止mysql服务：<br>
sudo service mysql stop<br>
sudo service mysql status<br><img id="aimg_18539" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165104la8r3rc05hadr53b.jpg" class="zoom" width="600" alt=""> <br><br>
2、跳过验证启动mysql<br>
使用如下命令验证启动mysql，由于&amp;结尾是后台运行进程，运行该命令可以再打开命令窗口或者Ctr+C继续进行下步操作：<br>
mysqld_safe --skip-grant-tables &amp;<br>
sudo service mysql status<br><img id="aimg_18540" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165105q56yaj5yjsza75j6.jpg" class="zoom" width="600" alt=""> <br><br>
3、跳过验证启动MySQL<br>
验证mysql服务已经在后台运行后，执行如下语句，其中后面三条命令是在mysql语句：<br>
mysql -u root<br>
mysql&gt;use mysql;<br>
mysql&gt;update user set password = password('root') where user = 'root';<br>
mysql&gt;flush privileges;<br><img id="aimg_18541" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165107b5s666szszofe56f.jpg" class="zoom" width="600" alt=""> <br><br>
4、跳过验证启动MySQL<br>
重启mysql服务并查看状态<br>
sudo service mysql stop<br>
sudo service mysql start<br>
sudo service mysql status<br><img id="aimg_18542" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165108gi4fqtqz72i4ioi7.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">3.2 Hive启动，报CommandNeedRetryException异常</span><br>
启动Hive时，出现CommandNeedRetryException异常，具体信息如下：<br><img id="aimg_18543" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165109eoo7b74zobn8n4nf.jpg" class="zoom" width="600" alt=""> <br>
Exception in thread "main" java.lang.NoClassDefFoundError:org/apache/hadoop/hive/ql/CommandNeedRetryException<br>
        at java.lang.Class.forName0(Native Method)<br>
        at java.lang.Class.forName(Class.java:270)<br>
        at org.apache.hadoop.util.RunJar.main(RunJar.java:149)<br>
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.CommandNeedRetryException<br>
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)<br>
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)<br>
        at java.security.AccessController.doPrivileged(Native Method)<br>
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)<br>
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)<br>
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)<br>
由于以前使用hadoop时，修改hadoop-env.sh的HADOOP_CLASSPATH配置项，由以前的：<br>
export HADOOP_CLASSPATH=/usr/local/hadoop-1.1.2/myclass<br>
修改为：<br>
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/local/hadoop-1.1.2/myclass<br><img id="aimg_18544" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165110gmmfcmn2gao2jgge.jpg" class="zoom" width="600" alt=""> <br><img id="aimg_18545" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165114lbfldto3ikb3j8dt.jpg" class="zoom" width="600" alt=""> <span style="font-size:14px;"><br></span><br><span style="font-size:14px;">3.3 在Hive中使用操作语言</span><br>
启动Hive后，使用HSQL出现异常，需要启动metastore和hiveserver<br><img id="aimg_18546" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165116mbn7nkbnn5rva7b5.jpg" class="zoom" width="600" alt=""> <br>
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient<br>
在使用hive之前需要启动metastore和hiveserver服务，通过如下命令启用：<br>
hive --service metastore &amp;<br>
hive --service hiveserver &amp;<br><img id="aimg_18547" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165117cbzags4ph7pcaqgs.jpg" class="zoom" width="600" alt=""> <br>
启动用通过jps命令可以看到两个进行运行在后台<br><img id="aimg_18548" src="http://www.aboutyun.com/data/attachment/forum/201508/27/165117zb362ow7x7d6w6jo.jpg" class="zoom" width="600" alt=""> <br><br><span style="font-size:14px;">参考资料：</span><br>
1、《Hive体系结构》 <a href="http://blog.csdn.net/zhoudaxia/article/details/8855937" rel="nofollow" style="color:rgb(51,102,153);">http://blog.csdn.net/zhoudaxia/article/details/8855937</a><br>
2、《大数据时代的技术hive：hive的数据类型和数据模型》<a href="http://www.cnblogs.com/sharpxiajun/archive/2013/06/03/3114560.html" rel="nofollow" style="color:rgb(51,102,153);">http://www.cnblogs.com/sharpxiajun/archive/2013/06/03/3114560.html</a></td>
</tr></tbody></table>            </div>
                </div>