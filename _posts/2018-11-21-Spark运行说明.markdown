---
layout:     post
title:      Spark运行说明
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/chengqiuming/article/details/79254170				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<div><strong>一 Spark运行环境</strong></div>
<div>Spark是Scala写的，运行在JVM上，所以运行环境Java7+</div>
<div>如果使用Python API，需要安装Python 2.6+或者运行Python3.4+</div>
<div>Spark 1.6.2-Scala 2.10  Spark 2.0.0+Scala2.11</div>
<div><br></div>
<div><strong>二 Spark下载</strong></div>
<div>下载地址：</div>
<div><a href="http://spark.apache.org/downloads.html" rel="nofollow">http://spark.apache.org/downloads.html</a></div>
<div>搭Spark不需要Hadoop，如有hadoop集群，可下载相应的版本。</div>
<div>解压</div>
<div><br></div>
<div><strong>三 Spark目录</strong></div>
<div>bin包含用来和Spark交互的可执行文件，如Spark shell</div>
<div><span style="line-height:1.45;">core，streaming，python，...包含主要组件的源代码。</span></div>
<div>examples包含一些单机spark job,你可以研究和运行这些例子。</div>
<div><br></div>
<div><strong>四 Spark的Shell</strong></div>
<div>Spark的shell使你能够处理分布在集群上的数据。</div>
<div>Spark把数据加载到节点的内存中，因此分布式处理可在秒级完成。</div>
<div>快速使迭代式计算，实时查询、分析一般能够在shells中完成。</div>
<div>Spark提供了Python shells和Scala shells。</div>
<div><br></div>
<div><strong>五 Python Shell进入方法</strong></div>
<div><pre><code class="language-plain">[root@master bin]# ./pyspark
Python 2.7.2 (default, Jan  6 2018, 08:58:52)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux3
Type "help", "copyright", "credits" or "license" for more information.
Traceback (most recent call last):
  File "/opt/spark-2.0.0-bin-hadoop2.7/python/pyspark/shell.py", line 28, in &lt;module&gt;
    import py4j
zipimport.ZipImportError: can't decompress data; zlib not available
&gt;&gt;&gt; exit();</code></pre><br></div>
<div><strong>六 Scala Shell进入方法</strong></div>
<div><pre><code class="language-plain">[root@master bin]# ./spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
18/02/04 18:40:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/04 18:40:45 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
Spark context Web UI available at http://192.168.0.110:4040
Spark context available as 'sc' (master = local[*], app id = local-1517740844632).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_152)
Type in expressions to have them evaluated.
Type :help for more information.
scala&gt;</code></pre><br></div>
<div><strong>七 实战</strong></div>
<div><pre><code class="language-plain">[root@master ~]# cat helloSpark.txt
go to home hello java
so many to hello word kafka java
go to so
scala&gt; val lines = sc.textFile("/root/helloSpark.txt")
lines: org.apache.spark.rdd.RDD[String] = /root/helloSpark.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24
scala&gt; lines.count()
res0: Long = 3
scala&gt; lines.first()
res1: String = go to home hello java</code></pre><br></div>
<div><strong>八 修改日志级别</strong></div>
<div><pre><code class="language-plain">[root@master conf]# cat log4j.properties
log4j.rootCategory=WARN, console</code></pre><br><br></div>
            </div>
                </div>