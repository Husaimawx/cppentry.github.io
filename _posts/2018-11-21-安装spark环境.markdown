---
layout:     post
title:      安装spark环境
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>一、安装spark包</p>

<p>1、将spark-1.3.0-bin-hadoop2.4.tgz使用WinSCP上传到/usr/local目录下。</p>

<p>2、解压缩spark包：tar zxvf spark-1.3.0-bin-hadoop2.4.tgz。</p>

<p>3、更改spark目录名：mv spark-1.3.0-bin-hadoop2.4 spark</p>

<p>4、设置spark环境变量</p>

<p>vi .bashrc</p>

<p>export SPARK_HOME=/usr/local/spark</p>

<p>export PATH=$SPARK_HOME/bin</p>

<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</p>

<p>source .bashrc</p>

<p>二、修改spark-env.sh</p>

<p>1、cd /usr/local/spark/conf</p>

<p>2、cp spark-env.sh.template spark-env.sh</p>

<p>3、vi spark-env.sh</p>

<p>export JAVA_HOME=/usr/java/latest</p>

<p>export SCALA_HOME=/usr/local/scala</p>

<p>export SPARK_MASTER_IP=192.168.1.107</p>

<p>export SPARK_WORKER_MEMORY=1g</p>

<p>export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</p>

<p>三、修改slaves文件</p>

<p>spark1 spark2 spark3</p>

<p>四、安装spark集群</p>

<p>在另外两个节点进行一模一样的配置，使用scp将spark和.bashrc拷贝到spark2和spark3即可。</p>

<p>五、启动spark集群</p>

<p>1、在spark目录下的sbin目录</p>

<p>2、执行./start-all.sh</p>

<p>3、使用jsp和8080端口可以检查集群是否启动成功</p>

<p>4、进入spark-shell查看是否正常</p>

<p> </p>            </div>
                </div>