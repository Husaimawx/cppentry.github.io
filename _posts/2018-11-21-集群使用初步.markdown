---
layout:     post
title:      集群使用初步
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <strong>5 <span style="font-family:'宋体';">集群使用初步</span></strong>
<h2><strong>5.1 HDFS<span style="font-family:'宋体';">使用</span></strong></h2>
<p>1<span style="font-family:'宋体';">、查看集群状态</span></p>
<p><span style="font-family:'宋体';">命令：</span>   hdfs  dfsadmin  –report</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140703243-79291935.png" alt=""></p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';">可以看出，集群共有</span>3<span style="font-family:'宋体';">个</span><span style="font-family:Calibri;">datanode</span><span style="font-family:'宋体';">可用</span></p>
<p><span style="font-family:'宋体';">也可打开</span>web<span style="font-family:'宋体';">控制台查看</span><span style="font-family:Calibri;">HDFS</span><span style="font-family:'宋体';">集群信息，在浏览器打开</span><a href="http://hdp-node-01:50070/" rel="nofollow"><span style="text-decoration:underline;">http://hdp-node-01:50070/</span></a></p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140712118-68229323.png" alt=""></p>
<p> </p>
<p> </p>
<p>2<span style="font-family:'宋体';">、上传文件到</span><span style="font-family:Calibri;">HDFS</span></p>
<p>² <span style="font-family:'宋体';">查看</span>HDFS<span style="font-family:'宋体';">中的目录信息</span></p>
<p><span style="font-family:'宋体';">命令</span><span style="font-family:'宋体';">：</span>   hadoop  fs  –ls  /</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140720478-537226839.png" alt=""></p>
<p> </p>
<p> </p>
<p> </p>
<p>² <span style="font-family:'宋体';">上传文件</span></p>
<p><span style="font-family:'宋体';">命令：</span>   hadoop  fs  -put  ./ scala-2.10.6.tgz  to  /</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140729509-1857004629.png" alt=""></p>
<p> </p>
<p> </p>
<p> </p>
<p>² <span style="font-family:'宋体';">从</span>HDFS<span style="font-family:'宋体';">下载文件</span></p>
<p><span style="font-family:'宋体';">命令：</span>  hadoop  fs  -get  /yarn-site.xml</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140805821-508503056.png" alt=""></p>
<p> </p>
<p> </p>
<h2><strong>5.2 MAPREDUCE<span style="font-family:'宋体';">使用</span></strong></h2>
<p>mapreduce<span style="font-family:'宋体';">是</span><span style="font-family:Calibri;">hadoop</span><span style="font-family:'宋体';">中的分布式运算编程框架，只要按照其编程规范，只需要编写少量的业务逻辑代码即可实现一个强大的海量数据并发处理程序</span></p>
<h3><strong>5.2.1 Demo<span style="font-family:'宋体';">开发——</span><span style="font-family:Calibri;">wordcount</span></strong></h3>
<p>1<span style="font-family:'宋体';">、需求</span></p>
<p><span style="font-family:'宋体';">从大量（比如</span>T<span style="font-family:'宋体';">级别）文本文件中，统计出每一个单词出现的总次数</span></p>
<p> </p>
<p>2<span style="font-family:'宋体';">、</span><span style="font-family:Calibri;">mapreduce</span><span style="font-family:'宋体';">实现思路</span></p>
<p>Map<span style="font-family:'宋体';">阶段：</span></p>
<p>a) <span style="font-family:'宋体';">从</span>HDFS<span style="font-family:'宋体';">的源数据文件中逐行读取数据</span></p>
<p>b) <span style="font-family:'宋体';">将每一行数据切分出单词</span></p>
<p>c) <span style="font-family:'宋体';">为每一个单词构造一个键值对</span>(<span style="font-family:'宋体';">单词，</span><span style="font-family:Calibri;">1)</span></p>
<p>d) <span style="font-family:'宋体';">将键值对发送给</span>reduce</p>
<p> </p>
<p>Reduce<span style="font-family:'宋体';">阶段</span><span style="font-family:'宋体';">：</span></p>
<p>a) <span style="font-family:'宋体';">接收</span>map<span style="font-family:'宋体';">阶段输出的单词键值对</span></p>
<p>b) <span style="font-family:'宋体';">将相同单词的键值对汇聚成一组</span></p>
<p>c) <span style="font-family:'宋体';">对每一组，遍历组中的所有</span>“值”，累加求和，即得到每一个单词的总次数</p>
<p>d) <span style="font-family:'宋体';">将</span>(<span style="font-family:'宋体';">单词，总次数</span><span style="font-family:Calibri;">)</span><span style="font-family:'宋体';">输出到</span><span style="font-family:Calibri;">HDFS</span><span style="font-family:'宋体';">的文件中</span></p>
<p> </p>
<p> </p>
<p>1、 <span style="font-family:'宋体';">具体编码实现</span></p>
<p>(1)<span style="font-family:'宋体';">定义一个</span><span style="font-family:Calibri;">mapper</span><span style="font-family:'宋体';">类</span></p>
<table><tbody><tr><td valign="top" width="568">
<p>//<span style="font-family:'宋体';">首先要定义四个泛型的类型</span></p>
<p>//keyin:  LongWritable    valuein: Text</p>
<p>//keyout: Text            valueout:IntWritable</p>
<p> </p>
<p>public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{</p>
<p>//map<span style="font-family:'宋体';">方法的生命周期：  框架每传一行数据就被调用一次</span></p>
<p>//key :  <span style="font-family:'宋体';">这一行的起始点在文件中的偏移量</span></p>
<p>//value: <span style="font-family:'宋体';">这一行的内容</span></p>
<p>@Override</p>
<p>protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</p>
<p>//<span style="font-family:'宋体';">拿到一行数据转换为</span><span style="font-family:Calibri;">string</span></p>
<p>String line = value.toString();</p>
<p>//<span style="font-family:'宋体';">将这一行切分出各个单词</span></p>
<p>String[] words = line.split(" ");</p>
<p>//<span style="font-family:'宋体';">遍历数组，输出</span><span style="font-family:Calibri;">&lt;</span><span style="font-family:'宋体';">单词，</span><span style="font-family:Calibri;">1&gt;</span></p>
<p>for(String word:words){</p>
<p>context.write(new Text(word), new IntWritable(1));</p>
<p>}</p>
<p>}</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<p>(2)<span style="font-family:'宋体';">定义一个</span><span style="font-family:Calibri;">reducer</span><span style="font-family:'宋体';">类</span></p>
<table><tbody><tr><td valign="top" width="568">
<p>//<span style="font-family:'宋体';">生命周期：框架每传递进来一个</span><span style="font-family:Calibri;">kv </span><span style="font-family:'宋体';">组，</span><span style="font-family:Calibri;">reduce</span><span style="font-family:'宋体';">方法被调用一次</span></p>
<p>@Override</p>
<p>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {</p>
<p>//<span style="font-family:'宋体';">定义一个计数器</span></p>
<p>int count = 0;</p>
<p>//<span style="font-family:'宋体';">遍历这一组</span><span style="font-family:Calibri;">kv</span><span style="font-family:'宋体';">的所有</span><span style="font-family:Calibri;">v</span><span style="font-family:'宋体';">，累加到</span><span style="font-family:Calibri;">count</span><span style="font-family:'宋体';">中</span></p>
<p>for(IntWritable value:values){</p>
<p>count += value.get();</p>
<p>}</p>
<p>context.write(key, new IntWritable(count));</p>
<p>}</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<p>(3)<span style="font-family:'宋体';">定义一个主类，用来描述</span><span style="font-family:Calibri;">job</span><span style="font-family:'宋体';">并提交</span><span style="font-family:Calibri;">job</span></p>
<table><tbody><tr><td valign="top" width="568">
<p>public class WordCountRunner {</p>
<p>//<span style="font-family:'宋体';">把业务逻辑相关的信息（哪个是</span><span style="font-family:Calibri;">mapper</span><span style="font-family:'宋体';">，哪个是</span><span style="font-family:Calibri;">reducer</span><span style="font-family:'宋体';">，要处理的数据在哪里，输出的结果放哪里。。。。。。）描述成一个</span><span style="font-family:Calibri;">job</span><span style="font-family:'宋体';">对象</span></p>
<p>//<span style="font-family:'宋体';">把这个描述好的</span><span style="font-family:Calibri;">job</span><span style="font-family:'宋体';">提交给集群去运行</span></p>
<p>public static void main(String[] args) throws Exception {</p>
<p>Configuration conf = new Configuration();</p>
<p>Job wcjob = Job.getInstance(conf);</p>
<p>//<span style="font-family:'宋体';">指定我这个</span><span style="font-family:Calibri;">job</span><span style="font-family:'宋体';">所在的</span><span style="font-family:Calibri;">jar</span><span style="font-family:'宋体';">包</span></p>
<p>// wcjob.setJar("/home/hadoop/wordcount.jar");</p>
<p>wcjob.setJarByClass(WordCountRunner.class);</p>
<p> </p>
<p>wcjob.setMapperClass(WordCountMapper.class);</p>
<p>wcjob.setReducerClass(WordCountReducer.class);</p>
<p>//<span style="font-family:'宋体';">设置我们的业务逻辑</span><span style="font-family:Calibri;">Mapper</span><span style="font-family:'宋体';">类的输出</span><span style="font-family:Calibri;">key</span><span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">value</span><span style="font-family:'宋体';">的数据类型</span></p>
<p>wcjob.setMapOutputKeyClass(Text.class);</p>
<p>wcjob.setMapOutputValueClass(IntWritable.class);</p>
<p>//<span style="font-family:'宋体';">设置我们的业务逻辑</span><span style="font-family:Calibri;">Reducer</span><span style="font-family:'宋体';">类的输出</span><span style="font-family:Calibri;">key</span><span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">value</span><span style="font-family:'宋体';">的数据类型</span></p>
<p>wcjob.setOutputKeyClass(Text.class);</p>
<p>wcjob.setOutputValueClass(IntWritable.class);</p>
<p> </p>
<p>//<span style="font-family:'宋体';">指定要处理的数据所在的位置</span></p>
<p>FileInputFormat.setInputPaths(wcjob, "hdfs://hdp-server01:9000/wordcount/data/big.txt");</p>
<p>//<span style="font-family:'宋体';">指定处理完成之后的结果所保存的位置</span></p>
<p>FileOutputFormat.setOutputPath(wcjob, new Path("hdfs://hdp-server01:9000/wordcount/output/"));</p>
<p> </p>
<p>//<span style="font-family:'宋体';">向</span><span style="font-family:Calibri;">yarn</span><span style="font-family:'宋体';">集群提交这个</span><span style="font-family:Calibri;">job</span></p>
<p>boolean res = wcjob.waitForCompletion(true);</p>
<p>System.exit(res?0:1);</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<p> </p>
<p> </p>
<p> </p>
<h3><strong>5.2.2 <span style="font-family:'宋体';">程序打包运行</span></strong></h3>
<p>1. <span style="font-family:'宋体';">将程序打包</span></p>
<p>2. <span style="font-family:'宋体';">准备输入数据</span></p>
<p>vi  /home/hadoop/test.txt</p>
<table><tbody><tr><td valign="top" width="568">
<p>Hello tom</p>
<p>Hello jim</p>
<p>Hello ketty</p>
<p>Hello world</p>
<p>Ketty tom</p>
</td>
</tr></tbody></table><p><span style="font-family:'宋体';">在</span>hdfs<span style="font-family:'宋体';">上创建输入数据文件夹</span><span style="font-family:'宋体';">：</span></p>
<p>hadoop   fs  mkdir  -p  /wordcount/input</p>
<p><span style="font-family:'宋体';">将</span>words.txt<span style="font-family:'宋体';">上传到</span><span style="font-family:Calibri;">hdfs</span><span style="font-family:'宋体';">上</span></p>
<p>hadoop  fs  –put  /home/hadoop/words.txt  /wordcount/input</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140830759-820210247.png" alt=""></p>
<p> </p>
<p> </p>
<p> </p>
<p>3. <span style="font-family:'宋体';">将程序</span>jar<span style="font-family:'宋体';">包上传到集群的任意一台服务器上</span></p>
<p> </p>
<p>4. <span style="font-family:'宋体';">使用命令启动执行</span>wordcount<span style="font-family:'宋体';">程序</span><span style="font-family:Calibri;">jar</span><span style="font-family:'宋体';">包</span></p>
<p>$ hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver /wordcount/input /wordcount/out</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140838353-2033736059.png" alt=""></p>
<p> </p>
<p> </p>
<p>5. <span style="font-family:'宋体';">查看执行结果</span></p>
<p>$ hadoop fs –cat /wordcount/out/part-r-00000</p>
<p><img src="http://images2015.cnblogs.com/blog/1168946/201706/1168946-20170617140845743-1605891859.png" alt=""></p>
<p> </p>            </div>
                </div>