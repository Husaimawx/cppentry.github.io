---
layout:     post
title:      了解 spark
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                一、spark 简介<br>spark 是快速的<br>spark 扩充了mapreduce<br>spark 通用的<br>spark 容纳了其它分布系统，批处理，迭代式计算，交互式查询，流处理<br>spark 降低成本<br>spark 多种开发语言接口<br><br><br>spark 生态系统<br>诞生于2009年，加州大学伯克利分校RDD实验室的一个研究项目，最初是基于hadoop mapreduce的，发现mapreduce迭代式计算和交互式上低效，引入了内存存储，2010年3月spark 开源，2011年amp实验室在spark上开发了高级组件，像spark streaming，2013年转移到apache下，不久成为顶级项目<br><br><br>二、spark 组件<br><br><br>spark core : 包含spark的基本功能，包含任务调度，内存管理，容错机制等<br>内部定义了RDDS（弹性分布式数据集）<br>提供了很多API来创建和操作这些RDDS<br>为其他的组件提供底层的服务<br><br><br>spark sql: spark 处理结构化数据的库，就像hive sql,mysql一样，一般做报表统计<br>spark streaming: 是实时数据流处理组件，类似storm<br>spark streaming 提供了API来操作实时流数据<br>应用场景，从kafka接收数据做实时统计<br><br><br>Mlib: 一个包含通用机器学习功能的包，machine learning lib<br>包含分类，聚类，回归等，还包括模型评估和数据导入<br>MLIB 提供的上面这些方法，都支持集群上的横向扩展<br>应用场景 机器学习<br><br><br>Graphx：是处理图的库，例如，社交网络图，并进行图的并行计算<br>像spark streaming,spark sql 一样，它也继承了RDD API<br>它提供了各种图的操作，包括常用的图算法，例如PangRank算法<br>应用场景 图计算<br><br><br>Cluster Managers:集群管理，Spark自带一个集群管理是单独调度器。常见集群管理包括Hadoop YARN,Apache Mesos<br><br><br>紧密集成的优点：<br>Spark底层优化了，基于Spark底层的组件，也得到了相应的优化<br>紧密集成，节省了各个组件组合使用时的部署、测试时间<br>当增加新组件时，可即刻使用<br><br><br>三、Spark 与 Hadoop 的比较<br>Hadoop 主要应用于离线、对时效性要求不高的处理，主要原因是处理过程中的数据要写盘<br>Spark 主要用于对时效性要求较高的场景，主要原因它是基于内存，中间数据尽量不进行写盘操作，另一个场景是用于机器学习等领域<br>Hadoop 之父Doug Cutting的观点：<br>各种分布式系统的生态环境，每个组件都有其作用，各善其职即可<br>Spark不具备HDFS的存储能力，要借助HDFS等持久化数据<br>大数据将会孕育出更多的新技术<br><br><br>四、Spark的安装<br>Spark是Scala写的，运行在JVM上的，所以运行环境Java7+。<br>如果使用python api，需要安装python 2.6+ 或者 python3.4+<br>版本对应关系<br>Spark1.6.2 - Scala2.10   Spark2.0.0-Scala2.11<br>Spark下载：<br>下载地址： http://spark.apache.org/downloads.html<br>Spark不需要Hadoop,如有hadoop集群，可下载相应的版本<br>下载后解压<br><br><br>Spark目录：<br>bin包含用来和Spark交互的可执行文件，如Spark shell。<br>core、streaming、python ...包含主要是组件的源代码。<br>examples 包含一些单机Spark job，可以作为参考和运行的例子。<br><br><br>cd ./bin<br><br><br>下面介绍下Spark的shell:<br><span style="white-space:pre;">	</span>Spark的shell使你能够处理分布在集群上的数据<br><span style="white-space:pre;">	</span>Spark把数据加载到节点的内存中，因此分布式处理可在称级完成。<br><span style="white-space:pre;">	</span>快速迭代式计算，实时查询、分析一般能够在spark shell中完成<br><span style="white-space:pre;">	</span>spaark提供了python shell和scala shell<br><span style="white-space:pre;">	</span><br><span style="white-space:pre;">	</span>Python Shell:<br><span style="white-space:pre;">	</span>bin/pyspark<br><span style="white-space:pre;">	</span>Scala Shell:<br><span style="white-space:pre;">	</span>bin/spark-shell<br><span style="white-space:pre;">	</span>例子：<br><span style="white-space:pre;">	</span>val lines=sc.textFile("./hellospark")<br><span style="white-space:pre;">	</span>lines.count()<br><span style="white-space:pre;">	</span>lines.first()<br><span style="white-space:pre;">	</span>修改日志级别 log4j.rootCategory=WARN,console<br><br><br>五、开发环境<br>Scala http://www.scala-lang.org/download/<br>强调下，scala和spark 版本匹配问题<br>spark 1.6.2 -- scala 2.10<span style="white-space:pre;">			</span>spark 2.0.0 -- scala 2.11<br>IntelliJ IDEA的下载 安装<br>https://www.jetbrains.com/idea/<br>http://idea.lanyus.com<br>插件安装<br>plugins,搜索Scala直接安装，插件有scala和sbt<br>版本匹配 scala2.10.5 JDK1.8 spark1.6.2 sbt0.13.8<br>开发第一个Spark程序<br>配置ssh无密码登录<br>    ssh -keygen<br>    .ssh目录下 cat xxx_rsa.pub &gt; authorized_keys<br>    chmod 600 authorized_keys<br>WordCount<br>    创建一个Spark Context<br>    加载数据<br>    把每一行分割成单词<br>    转换成pairs并且计数            </div>
                </div>