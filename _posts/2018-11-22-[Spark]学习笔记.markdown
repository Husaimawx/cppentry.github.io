---
layout:     post
title:      [Spark]学习笔记
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p><strong>Spark是一个快速且通用的集群计算框架</strong></p>
<h4><a id="Spark_1"></a>一、Spark的特点</h4>
<p>1.Spark是<mark>快速</mark>的：<br>
Spark扩充了流行的MapReduce计算框架<br>
Spark是<mark>基于内存</mark>的计算</p>
<p>2.Spark是<mark>通用</mark>的：<br>
Spark的涉及容纳了其他分布式系统拥有的功能：批处理、迭代式计算、交互查询和流处理等<br>
其优势是：降低了维护的成本</p>
<p>3.Spark是<mark>高度开放</mark>的：<br>
Spark提供了Python、Java、Scala、SQL的API和丰富的内置库</p>
<h4><a id="Spark_13"></a>二、Spark的组件</h4>
<p><img src="https://img-blog.csdnimg.cn/20181026144802655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjE4Mzcz,size_27,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
1.<mark>Spark core</mark>：<br>
包含Spark的基本功能，包含任务调度，内存管理，容错机制等<br>
内部定义了RDDs<br>
提供了很多APIs来创建和操作这些RDDs<br>
应用场景：为其他组件提供底层服务</p>
<p>2.<mark>Spark SQL</mark>：<br>
是Spark处理结构化数据的库，就像Hive SQL，Mysql一样<br>
应用场景：企业中用来做报表的统计</p>
<p>3.<mark>Spark Streaming</mark>：<br>
是实时数据流处理组件，类似Storm<br>
Spark Streaming提供API来操作实时数据流<br>
应用场景：企业中用来从kafka接受数据做实时统计</p>
<p>4.Mlib：–机器学习</p>
<p>5.Graphx：–处理图</p>
<p>6.<mark>Cluster Managers</mark>：<br>
就是集群管理，Spark自带一个集群管理，是单独调度器<br>
常见的集群内管理包括：Hadoop YARN，Apache Mesos</p>
<p>7.密集集成优点：<br>
Spark底层优化了，基于Spark底层的组件，也得到了相应的优化<br>
紧密集成，节省了各个组件组合使用时的部署，测试等时间<br>
向Spark增加新的组件时，其他组件，可立刻享用新组件的功能</p>
<h4><a id="SparkHadoop_43"></a>三、Spark与Hadoop的比较——应用场景的比较</h4>
<p>1.Hadoop的应用场景：离线处理、对时效性要求不高<br>
2.Spark的应用场景：时效性要求高的场景，机器学习等领域<br>
3.<mark>比较</mark>：（中立的观点）<br>
这是一个生态系统，每个组件都有其作用，各善其职即可<br>
Spark不具有HDFS的存储功能，要借助HDFS等持久化数据<br>
大数据将会孕育除更多的新技术</p>
<h4><a id="SparkWordCount_51"></a>四、通过命令行执行Spark——WordCount</h4>
<p>1.启动Spark：spark-shell --master local[2]<br>
2.spark实现word count<br>
1）读取数据：var file=sc.textFile(“file:///home/hadoop/data/hello.txt”)<br>
2）拆分每一行：var a=file.flatMap(line=&gt;line.split(" "))<br>
3）为每个单词赋上1，用于计数：var b=a.map(word=&gt;(word,1))<br>
4）将相同单词的计数加和：var c=b.reduceByKey(_ + _)</p>
<p>上述执行，可以通过下面这一条语句执行，得到相同的结果：<br>
sc.textFile(“file:///home/hadoop/data/hello.txt”).flatMap(line=&gt;line.split(" ")).map(word=&gt;(word,1)).reduceByKey(_ + _).collect</p>
<h4><a id="ideaSpark_62"></a>五、通过idea写好Spark程序，打包，在集群上执行</h4>
<p>1.在虚拟机上使用Spark时，同样需要配置ssh免密登陆，同hadoop的配置相同，每台虚拟机仅需要配置一次即可<br>
2.写好程序后，需要打成jar包，这里介绍使用idea打jar包<br>
file-&gt;project structure-&gt;artifacts-&gt;左上角加号-&gt;jar-&gt;选择下面的那个-&gt;要打包的项目名-&gt;main class的名字-&gt;上面的选项是连依赖包一起打成jar包/下面的选择是不包含依赖包-&gt;apply-&gt;ok<br>
3.build-&gt;build artifacts-&gt;build<br>
4.虚拟机啊启动master和worker：sbin目录下执行：./start-all<br>
5.把jar包copy到集群上：idea上打好的jar包在output中<br>
在C:\Users\PYN\IdeaProjects\scalatrain\out\artifacts\scalatrain_jar 下执行命令：scp scalatrain.jar hadoop@***.***.***.***:~/lib<br>
6.提交，运行<br>
在spark目录下执行：<br>
./bin/spark-submit --master local[2] --class com.scala.scala.com.spark.WordCount  /home/hadoop/lib/scalatrain.jar</p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>