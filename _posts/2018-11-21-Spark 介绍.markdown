---
layout:     post
title:      Spark 介绍
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>﻿﻿</p>

<p style="text-indent:0px;">安装在集群上的spark版本：spark-1.6.3-bin-hadoop2.6.tgz             scala版本：scala-2.10.4.tgz</p>

<p style="text-indent:0px;">1、spark是什么</p>

<p style="text-indent:0px;">      Spark， 是一种通用的大数据计算框架， 正如传统大数据技术Hadoop的MapReduce、 Hive引擎， 以及Storm流式实时计算引擎等。 </p>

<p style="text-indent:0px;">      Spark包含了大数据领域常见的各种计算框架， 比如: </p>

<p style="text-indent:0px;">               Spark Core用于离线计算  </p>

<p style="text-indent:0px;">               Spark SQL用于交互式查询 <br>
               Spark Streaming用于实时流式计算 </p>

<p style="text-indent:0px;">               Spark MLlib用于机器学习 </p>

<p style="text-indent:0px;">               Spark GraphX用于图计算 </p>

<p style="text-indent:0px;">      Spark主要用于大数据的计算， 而Hadoop以后主要用于大数据的存储（ 比如HDFS、 Hive、 HBase等） ， 以及资源调度（ Yarn） </p>

<p style="text-indent:0px;">      Spark+Hadoop的组合， 是未来大数据领域最热门的组合， 也是最有前景的组合！ </p>

<p style="text-indent:0px;">2、spark介绍</p>

<p style="text-indent:0px;">Spark， 是一种"One Stack to rule them all"的大数据计算框架， 期望使用一个技术堆栈就 完美地解决大数据领域的各种计算任务。 Apache官方， 对Spark的定义就是： 通用的大数据快 速处理引擎。<br>
Spark使用Spark RDD、 Spark SQL、 Spark Streaming、 MLlib、 GraphX成功解决了大数 据领域中， 离线批处理、 交互式查询、 实时流计算、 机器学习与图计算等最重要的任务和问题。<br>
Spark除了一站式的特点之外， 另外一个最重要的特点， 就是基于内存进行计算， 从而让 它的速度可以达到MapReduce、 Hive的数倍甚至数十倍！<br>
现在已经有很多大公司正在生产环境下深度地使用Spark作为大数据的计算框架， 包括 eBay、 Yahoo!、 BAT、 网易、 京东、 华为、 大众点评、 优酷土豆、 搜狗等等。<br>
Spark同时也获得了多个世界顶级IT厂商的支持， 包括IBM、 Intel等。 </p>

<p style="text-indent:0px;">3、spark的生态系统</p>

<p style="text-indent:0px;"><img alt="" class="has" src="https://images2015.cnblogs.com/blog/1122015/201703/1122015-20170320094006502-51744707.png"></p>

<p style="text-indent:0px;"> </p>

<p style="text-indent:0px;">Mesos和yarn 作用一样，资源调度平台，用yarn的比较多</p>

<p style="text-indent:0px;">Tachyon:（1）内存当中hdfs(内存中的分布式存储系统，加快spark在内存中读取和处理速度)</p>

<p style="text-indent:0px;">             （2）在不同应用程序之间实现数据共享</p>

<p style="text-indent:0px;">spark core：spark的核心，用于离线计算</p>

<p style="text-indent:0px;"> </p>

<p style="text-indent:0px;">4、spark计算处理数据速度快的原因：</p>

<p style="text-indent:0px;">     （1）基于内存的计算方式</p>

<p style="text-indent:0px;">      （2）基于DAG有向无环图      （ A经过相关运算产生B，B经过相关运算产生C，  D经过运算产生E，  C和E经过相关运算产生F）</p>

<p style="text-indent:0px;">5、spark的历史沿革</p>

<p style="text-indent:0px;">·2009年， Spark诞生于伯克利大学的AMPLab实验室。 最出Spark只是一个实验性的项目， 代码量非常少， 属于轻量级的框架。<br>
·2010年， 伯克利大学正式开源了Spark项目。<br>
·2013年， Spark成为了Apache基金会下的项目， 进入高速发展期。 第三方开发者贡献了 大量的代码， 活跃度非常高。<br>
·2014年， Spark以飞快的速度称为了Apache的顶级项目。<br>
·2015年~， Spark在国内IT行业变得愈发火爆， 大量的公司开始重点部署或者使用Spark来 替代MapReduce、 Hive、 Storm等传统的大数据计算框架。 </p>

<p style="text-indent:0px;">6、spark特点</p>

<p style="text-indent:0px;">·速度快： Spark基于内存进行计算（ 当然也有部分计算基于磁盘， 比如shuffle） 。 </p>

<p style="text-indent:0px;"> </p>

<p style="text-indent:0px;">·容易上手开发： Spark的基于RDD的计算模型， 比Hadoop的基于Map-Reduce的计算模型要更加易于 理解， 更加易于上手开发， 实现各种复杂功能， 比如二次排序、 topn等复杂操作时， 更加便捷。 </p>

<p style="text-indent:0px;">超强的通用性： Spark提供了Spark RDD、 Spark SQL、 Spark Streaming、 Spark MLlib、 Spark GraphX等技术组件， 可以一站式地完成大数据领域的离线批处理、 交互式查询、 流式计算、 机器学习、<br>
                     图计算等常见的任务。</p>

<p style="text-indent:0px;">集成Hadoop： Spark并不是要成为一个大数据领域的“独裁者” ， 一个人霸占大数据领域所有的 “地盘” ， 而是与Hadoop进行了高度的集成， 两者可以完美的配合使用。 Hadoop的HDFS、 Hive、<br>
                    HBase负责存储， YARN负责资源调度； Spark复杂大数据计算。 实际上， Hadoop+Spark的组合， 是 一种“ double win” 的组合。 <br>
 </p>

<p style="text-indent:0px;">·极高的活跃度： Spark目前是Apache基金会的顶级项目， 全世界有大量的优秀工程师是Spark的 committer。 并且世界上很多顶级的IT公司都在大规模地使用Spark。 </p>

<p style="text-indent:0px;">7、Spark VS MapReduce </p>

<p style="text-indent:0px;">     MapReduce能够完成的各种离线批处理功能， 以及常见算法（ 比如二次排序、 topn等） ， 基于Spark RDD的核心编程， 都可以实现， 并且可以更好地、 更容易地实现。 而且基于Spark RDD编写的</p>

<p style="text-indent:0px;">离线批处理程序， 运行速度是MapReduce的数倍， 速度上有非常明显的优势。<br><br>
Spark相较于MapReduce速度快的最主要原因就在于， MapReduce的计算模型太死板， 必须是mapreduce模式， 有时候即使完成一些诸如过滤之类的操作， 也必须经过map-reduce过程， 这样就必须经过</p>

<p style="text-indent:0px;">shuffle 过程。 而MapReduce的shuffle过程是最消耗性能的， 因为shuffle中间的过程必须基于磁盘来读写。 而Spark的 shuffle虽然也要基于磁盘， 但是其大量transformation操作， 比如单纯的map或者</p>

<p style="text-indent:0px;">filter等操作， 可以直接基于内 存进行pipeline操作， 速度性能自然大大提升。<br><br>
但是Spark也有其劣势。 由于Spark基于内存进行计算， 虽然开发容易， 但是真正面对大数据的时候（ 比如 一次操作针对10亿以上级别） ， 在没有进行调优的情况下， 可能会出现各种各样的问题， 比如</p>

<p style="text-indent:0px;">OOM内存溢出等 等。 导致Spark程序可能都无法完全运行起来， 就报错挂掉了， 而MapReduce即使是运行缓慢， 但是至少可以 慢慢运行完。 </p>

<p style="text-indent:0px;">  </p>

<p style="text-indent:0px;">8、Spark SQL VS Hive </p>

<p style="text-indent:0px;">     Spark SQL实际上并不能完全替代Hive， 因为Hive是一种基于HDFS的数据仓库， 并且提供了基于SQL模型的， 针对 存储了大数据的数据仓库， 进行分布式交互查询的查询引擎。 </p>

<p style="text-indent:0px;">     严格的来说， Spark SQL能够替代的， 是Hive的查询引擎， 而不是Hive本身， 实际上即使在生产环境下， Spark SQL 也是针对Hive数据仓库中的数据进行查询， Spark本身自己是不提供存储的，</p>

<p style="text-indent:0px;">自然也不可能替代Hive作为数据仓库的这个 功能。 </p>

<p style="text-indent:0px;">      Spark SQL的一个优点， 相较于Hive查询引擎来说， 就是速度快， 同样的SQL语句， 可能使用Hive的查询引擎， 由于 其底层基于MapReduce， 必须经过shuffle过程走磁盘， 因此速度是非常缓慢的。</p>

<p style="text-indent:0px;">很多复杂的SQL语句， 在hive中执行都需 要一个小时以上的时间。 而Spark SQL由于其底层基于Spark自身的基于内存的特点， 因此速度达到了Hive查询引擎的数 倍以上。 </p>

<p style="text-indent:0px;">      但是Spark SQL由于与Spark一样， 是大数据领域的新起的新秀， 因此还不够完善， 有少量的Hive支持的高级特性， Spark SQL还不支持， 导致Spark SQL暂时还不能完全替代Hive的查询引擎。</p>

<p style="text-indent:0px;">而只能在部分Spark SQL功能特性可以满足 需求的场景下， 进行使用。 </p>

<p style="text-indent:0px;">       而Spark SQL相较于Hive的另外一个优点， 就是支持大量不同的数据源， 包括hive、 json、 parquet、 jdbc等等。 此外， Spark SQL由于身处Spark技术堆栈内， 也是基于RDD来工作，</p>

<p style="text-indent:0px;">因此可以与Spark的其他组 件无缝整合使用， 配合起来实现许多复杂的功能。 比如Spark SQL支持可以直接针对hdfs文件执行sql语句！ </p>

<p style="text-indent:0px;">9、Spark Streaming VS Storm   （实时流计算，storm精确到毫秒级，对时间要求非常严格，纯实时，而streaming是准实时，精确到秒级）</p>

<p style="text-indent:0px;">        Spark Streaming与Storm都可以用于进行实时流计算。 但是他们两者的区别是非常大的。 其中区别之一， 就是， Spark Streaming和Storm的计算模型完全不一样， Spark Streaming是基于RDD的， 因此需要将一小段时间内的， 比如1秒内的数据，<br>
收集起来， 作为一个RDD， 然后再针对这个batch的数据进行处理。 而Storm却可以做到每来一条数据， 都可以立即进行处理 和计算。 因此， Spark Streaming实际上严格意义上来说， 只能称作准实时的</p>

<p style="text-indent:0px;">流计算框架； 而Storm是真正意义上的实时计算 框架。</p>

<p style="text-indent:0px;">       此外， Storm支持的一项高级特性， 是Spark Streaming暂时不具备的， 即Storm支持在分布式流式计算程序（ Topology） 在运行过程中， 可以动态地调整并行度， 从而动态提高并发处理能力。</p>

<p style="text-indent:0px;">而Spark Streaming是无法动态调整并行度的。 </p>

<p style="text-indent:0px;">        但是Spark Streaming也有其优点， 首先Spark Streaming由于是基于batch进行处理的， 因此相较于Storm基于单条数据 进行处理， 具有数倍甚至数十倍的吞吐量。 </p>

<p style="text-indent:0px;">         此外， Spark Streaming由于也身处于Spark生态圈内， 因此Spark Streaming可以与Spark Core、 Spark SQL， 甚至是 Spark MLlib、 Spark GraphX进行无缝整合。 流式处理完的数据，</p>

<p style="text-indent:0px;">可以立即进行各种map、 reduce转换操作， 可以立即使用 sql进行查询， 甚至可以立即使用machine learning或者图计算算法进行处理。 这种一站式的大数据处理功能和优势， 是Storm 无法匹敌的。 </p>            </div>
                </div>