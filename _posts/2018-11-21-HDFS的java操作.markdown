---
layout:     post
title:      HDFS的java操作
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/freefish_yzx/article/details/76566175				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><span style="font-family:'KaiTi_GB2312';"><span style="background-color:rgb(255,204,153);"><strong><em>HDFS的客户端操作分为 Java和shell命令行</em></strong></span></span></p>
<p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span>hdfs</span><span>在生产应用中主要是客户端的开发，其核心步骤是从</span><span>hdfs</span><span>提供的</span><span>api</span><span>中构造一个</span><span>HDFS</span><span>的访问客户端对象，然后通过该客户端对象操作（增删改查）</span><span>HDFS</span><span>上的文件</span></span></strong></em></span></p>
<p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span><br></span></span></strong></em></span></p>
<p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span>1. 文件的增删改查</span></span></strong></em></span></p>
<p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span></span></span></strong></em></span></p><pre><code class="language-java"> package cn.yzx.bigdata.hdfs;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.Iterator;
import java.util.Map.Entry;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.RemoteIterator;
import org.apache.hadoop.hdfs.DFSClient.Conf;
import org.junit.Before;
import org.junit.Test;

public class HdfsClientDemo {
	FileSystem fs=null;
	Configuration conf = null;
	@Before
	public void init() throws IOException {
	 conf = new Configuration();
	  conf.set("fs.defaultFS", "hdfs://hadoop01:9000");
	  conf.set("dfs.defaultFS", "5");
	  //拿到一个文件系统操作的客户端实例对象
		 fs = FileSystem.get(conf);
	
	}
	/*
	 * 上传文件
	 */
	
	@Test
	public void testUpload() throws Exception {
		fs.copyFromLocalFile(new Path("c:/AMTAG.BIN"), new Path("/AMTAG.BIN.copy"));
		fs.close();
	}
	/*
	 * 打印参数
	 */
	@Test
	public void testconf() {
		Iterator&lt;Entry&lt;String, String&gt;&gt; iterator = conf.iterator();
		while(iterator.hasNext()) {
			Entry&lt;String, String&gt; next = iterator.next();
			System.out.println(next.getKey()+" :  "+next.getValue());
		}
		
	}
	/*
	 * 创建文件夹
	 */
	@Test
	public void testMkdir() throws Exception {
		boolean mkdirs = fs.mkdirs(new Path("/testmkdirs/aaa"));
		System.out.println(mkdirs);
	}
	/*
	 * 删除
	 */
	@Test
	public void testdelete() throws Exception {
		boolean delete = fs.delete(new Path("/testmkdirs/aaa"), true);
		System.out.println(delete);
	}
	/*
	 * 查看 递归
	 */
	@Test
	public void testLs() throws Exception {
		RemoteIterator&lt;LocatedFileStatus&gt; files = fs.listFiles(new Path("/"), true);
		while(files.hasNext()) {
			LocatedFileStatus fileStatus = files.next();
			System.out.println("BlockSize   "+fileStatus.getBlockSize());
			System.out.println("Owner   "+fileStatus.getOwner());
			System.out.println("Replication   "+fileStatus.getReplication());
			System.out.println("Permission   "+fileStatus.getPermission());
			System.out.println("name   "+fileStatus.getPath().getName());
			BlockLocation[] blockLocations = fileStatus.getBlockLocations();
			for(BlockLocation blockLocation:blockLocations) {
				System.out.println("块的ID  "+blockLocation.getNames());
				System.out.println("块起始偏移量  "+blockLocation.getOffset());
				System.out.println("datanode "+blockLocation.getLength());
				String[] datanodes = blockLocation.getHosts();
				for(String s:datanodes) {
					System.out.println("datanode "+s);
				}
				
			}
			System.out.println("+++++++++++++++++++++++++++");
		}
	}
	/*
	 * 查看不用递归
	 */
	@Test
	public void testLs2() throws Exception {
		FileStatus[] listStatus = fs.listStatus(new Path("/"));
		for(FileStatus s:listStatus) {
			System.out.println("name "+s.getPath().getName());
			System.out.println(s.isFile()?"f":"dir");
		}
	}
	
}
</code></pre><br>
2 。通过流的方式访问hdfs
<p><span style="background-color:rgb(255,204,153);"><span style="font-style:italic;"><span style="font-weight:bold;"><span style="font-family:'KaiTi_GB2312';"><span><span></span></span></span></span></span></span></p>
<p><em>hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度</em></p>
<p><em><br></em></p>
<p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span></span></span></strong></em></span></p><pre><code class="language-java">package cn.yzx.bigdata.hdfs;

import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;

import org.apache.commons.io.IOUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.junit.Before;
import org.junit.Test;

/*
 * 用流的方式操作HDFS上的文件
 * 可以指定偏移量的范围的数据
 */
public class HdfsStreamAccess {
	FileSystem fs=null;
	Configuration conf = null;
	@Before
	public void init() throws IOException {
	 conf = new Configuration();
	  conf.set("fs.defaultFS", "hdfs://hadoop01:9000");
	  //拿到一个文件系统操作的客户端实例对象
		 fs = FileSystem.get(conf);
	
	}
	@Test
	public void testUpload() throws Exception {
		 //上传文件 anglebaby.love  true表示如果有就覆盖
		FSDataOutputStream outputstream = fs.create(new Path("/anglababy.txt"),true);
		FileInputStream inputStream = new FileInputStream("c:/anglababy.txt");
		IOUtils.copy(inputStream, outputstream);
	}
	/*
	 * 从指定位置开始读
	 */
	@Test
	public  void testRandAccess () throws IllegalArgumentException, IOException {
		FSDataInputStream inputStream = fs.open(new Path("/anglababy.txt"));
		inputStream.seek(12);
		FileOutputStream outputStream = new FileOutputStream("f:/anglababy.txt");
		IOUtils.copy(inputStream, outputStream);
		 
	}
}
</code></pre><br><br><p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span><br></span></span></strong></em></span></p>
<p><span style="background-color:rgb(255,204,153);"><em><strong><span style="font-family:'KaiTi_GB2312';"><span><br></span></span></strong></em></span></p>
            </div>
                </div>