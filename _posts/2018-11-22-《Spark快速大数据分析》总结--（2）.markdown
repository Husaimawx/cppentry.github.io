---
layout:     post
title:      《Spark快速大数据分析》总结--（2）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1><strong>第一章 Spark数据分析导论</strong></h1>

<h2><strong>1、Spark核心</strong></h2>

<p>Spark的核心是一个对由很多计算任务组成的、运行在多个工作机器或者是一个计算集群上的应用进行调度、分发以及监控的计算引擎。</p>

<h2><strong>2、Spark组件</strong></h2>

<p><img alt="" class="has" height="307" src="https://img-blog.csdn.net/20181017212203638?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTAwMjQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="746"></p>

<p>依次介绍各组件：</p>

<p><strong>（1）Spark Core</strong></p>

<p>Spark Core实现了Spark的基本功能，包括任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core还包括了对弹性分布式数据集（RDD）的API定义。RDD表示分布在多个节点上可以并行操作的元素集合，是Spark主要的编程抽象<strong>。</strong></p>

<p><strong>（2）Spark SQL</strong></p>

<p>Spark SQL是Spark用来操作结构化数据的程序包。</p>

<p><strong>（3）Spark Streaming</strong></p>

<p>Spark Streaming是Spark提供的对实时数据进行流失计算的组件。Spark Streaming提供用来操作数据集的API，并且与Spark Core中的RDD API高度对应。从底层设计来看，Spark Streaming支持与Spark Core同级别的容错性、吞吐量以及可伸缩性。</p>

<p><strong>（4）MLib</strong></p>

<p>MLib是Spark提供的包含常见的机器学习功能的程序库，MLlib提供很多机器学习算法，包括分类、回归、聚类、协同过滤等，还提供模型评估、数据导入等额外的支持功能。MLlib还提供一些更底层的机器学习原语，包括一个通用的梯度下降优化算法。</p>

<p><strong>（5）GraphX</strong></p>

<p>GraphX是用来操作图的程序库，可以进行并行的图计算。GraphX扩展了Spark的RDD API，能用来创建一个顶点和边都包含的任意属性的有向图。</p>

<p><strong>（6）集群管理器</strong></p>

<p>为了获得最大的灵活性，Spark支持在各种集群管理器上运行，Spark自带一个简易调度器，叫作独立调度器。</p>

<p> </p>            </div>
                </div>