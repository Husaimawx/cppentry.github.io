---
layout:     post
title:      Spark SQL 笔记
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/youngbit007/article/details/78332772				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>官方参考文档：</p>

<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//spark<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.org</span>/docs/<span class="hljs-number">2.1</span><span class="hljs-number">.0</span>/sql-programming-guide<span class="hljs-preprocessor">.html</span><span class="hljs-preprocessor">#creating-dataframes</span></code></pre>



<h2 id="dataframe">DataFrame</h2>



<pre class="prettyprint"><code class=" hljs livecodeserver">A DataFrame is <span class="hljs-operator">a</span> Dataset organized <span class="hljs-keyword">into</span> named columns. It is conceptually equivalent <span class="hljs-built_in">to</span> <span class="hljs-operator">a</span> table <span class="hljs-operator">in</span> <span class="hljs-operator">a</span> relational database <span class="hljs-operator">or</span> <span class="hljs-operator">a</span> data frame <span class="hljs-operator">in</span> R/Python, but <span class="hljs-operator">with</span> richer optimizations under <span class="hljs-operator">the</span> hood. DataFrames can be constructed <span class="hljs-built_in">from</span> <span class="hljs-operator">a</span> wide array <span class="hljs-operator">of</span> sources such <span class="hljs-keyword">as</span>: structured data <span class="hljs-built_in">files</span>, tables <span class="hljs-operator">in</span> Hive, external databases, <span class="hljs-operator">or</span> existing RDDs. The DataFrame API is available <span class="hljs-operator">in</span> Scala, Java, Python, <span class="hljs-operator">and</span> R.</code></pre>

<p>对于熟悉python的同学，Spark的DataFrame和python的DF很像。对于structured data files同学比较熟知的有xml、jason、parquet等。</p>

<p>关于parquet，请参考：</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//blog<span class="hljs-preprocessor">.csdn</span><span class="hljs-preprocessor">.net</span>/yu616568/article/details/<span class="hljs-number">50993491</span></code></pre>

<p>具体的df的操作请参考官网：</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//spark<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.org</span>/docs/<span class="hljs-number">2.1</span><span class="hljs-number">.0</span>/sql-programming-guide<span class="hljs-preprocessor">.html</span><span class="hljs-preprocessor">#creating-dataframes</span></code></pre>

<p>python操作演示：</p>



<pre class="prettyprint"><code class=" hljs asciidoc">&gt;&gt;&gt; df = sqlContext.read.json("file:///home/zkpk/spark-1.5.2-bin-2.5.2/examples/src/main/resources/people.json")
<span class="hljs-header">&gt;&gt;&gt; df.show()
+----+-------+</span>
<span class="hljs-header">| age|   name|
+----+-------+</span>
|null|Michael|
|  30|   Andy|
<span class="hljs-header">|  19| Justin|
+----+-------+</span>
&gt;&gt;&gt; df.printSchema()
root
<span class="hljs-code"> |-- age: long (nullable = true)</span>
<span class="hljs-code"> |-- name: string (nullable = true)</span>

<span class="hljs-header">&gt;&gt;&gt; df.select("name").show()
+-------+</span>
<span class="hljs-header">|   name|
+-------+</span>
|Michael|
|   Andy|
<span class="hljs-header">| Justin|
+-------+</span>

&gt;&gt;&gt; df.select(df[<span class="hljs-emphasis">'name'</span>], df[<span class="hljs-emphasis">'age'</span>] + 1).show()
<span class="hljs-code">+-------+</span>---------+
<span class="hljs-header">|   name|(age + 1)|
+-------+---------+</span>
|Michael|     null|
|   Andy|       31|
<span class="hljs-header">| Justin|       20|
+-------+---------+</span>

&gt;&gt;&gt; df.filter(df[<span class="hljs-emphasis">'age'</span>] &gt; 21).show()
<span class="hljs-code">+---+</span>----+
<span class="hljs-header">|age|name|
+---+----+</span>
<span class="hljs-header">| 30|Andy|
+---+----+</span>
</code></pre>

<p>scala在eclipse中的实现</p>



<pre class="prettyprint"><code class=" hljs avrasm">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkConf</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkContext</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.SQLContext</span>

object DataFrameOperation {
  def main(args: Array[String]): Unit = {
    //create  datarame
    //首先创建程序入口
    val conf=new SparkConf()<span class="hljs-preprocessor">.setAppName</span>(<span class="hljs-string">"DataFrameOperation"</span>)
    val sc=new SparkContext(conf)<span class="hljs-comment">;</span>
    //create sqlcontext
    val sqlContext=new SQLContext(sc)<span class="hljs-comment">;</span>
    val df=sqlContext<span class="hljs-preprocessor">.read</span><span class="hljs-preprocessor">.json</span>(<span class="hljs-string">"hdfs://hadoop1:9000/examples/src/main/resources/people.json"</span>)
    df<span class="hljs-preprocessor">.show</span>()<span class="hljs-comment">;  </span>

    //print schema
    df<span class="hljs-preprocessor">.printSchema</span>()
    //name age
    df<span class="hljs-preprocessor">.select</span>(<span class="hljs-string">"name"</span>)<span class="hljs-preprocessor">.show</span>()<span class="hljs-comment">;</span>
    //
    df<span class="hljs-preprocessor">.select</span>(df(<span class="hljs-string">"name"</span>),df(<span class="hljs-string">"age"</span>)+<span class="hljs-number">1</span>)<span class="hljs-preprocessor">.show</span>()
    //where
    df<span class="hljs-preprocessor">.filter</span>(df(<span class="hljs-string">"age"</span>) &gt; <span class="hljs-number">21</span>)<span class="hljs-preprocessor">.show</span>()
    //groupby
    df<span class="hljs-preprocessor">.groupBy</span>(<span class="hljs-string">"age"</span>)<span class="hljs-preprocessor">.count</span>()<span class="hljs-preprocessor">.show</span>()<span class="hljs-comment">;   </span>
  }
}</code></pre>

<p>关于代码的提交：</p>

<ul>
<li>将上面的代码的代码文件（文件 即可不用导出工程） export成jar</li>
<li>提交请参考 Run on a YARN cluster 在上一篇的spark教程里</li>
</ul>



<h2 id="interoperating-with-rdds和rdd交互">Interoperating with RDDs（和RDD交互）</h2>

<p>Spark SQL supports two different methods for converting existing RDDs into DataFrames. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.</p>

<p>The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct DataFrames when the columns and their types are not known until runtime.</p>



<h2 id="通过reflection的方式将rdd转换成dataframe">通过reflection的方式将RDD转换成dataframe</h2>

<p>注意：RDD是读取非结构化数据 <br>
java版本</p>



<pre class="prettyprint"><code class=" hljs avrasm">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkConf</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.api</span><span class="hljs-preprocessor">.java</span><span class="hljs-preprocessor">.JavaRDD</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.api</span><span class="hljs-preprocessor">.java</span><span class="hljs-preprocessor">.JavaSparkContext</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.api</span><span class="hljs-preprocessor">.java</span><span class="hljs-preprocessor">.function</span><span class="hljs-preprocessor">.Function</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.api</span><span class="hljs-preprocessor">.java</span><span class="hljs-preprocessor">.function</span><span class="hljs-preprocessor">.VoidFunction</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.DataFrame</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.Row</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.SQLContext</span><span class="hljs-comment">;</span>

public class RDD2DataFrameReflection {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf()<span class="hljs-comment">;</span>
        conf<span class="hljs-preprocessor">.setAppName</span>(<span class="hljs-string">"RDD2DataFrameReflection"</span>)<span class="hljs-comment">;</span>
        JavaSparkContext sc = new JavaSparkContext(conf)<span class="hljs-comment">;</span>
        SQLContext sqlContext = new SQLContext(sc)<span class="hljs-comment">;</span>
        //生成一个RDD
        JavaRDD&lt;Person&gt; PersonRDD = sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"hdfs://hadoop1:9000/examples/src/main/resources/people.txt"</span>)
        <span class="hljs-preprocessor">.map</span>(new Function&lt;String, Person&gt;() {

            public Person <span class="hljs-keyword">call</span>(String line) throws Exception {
                String[] strs = line<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">","</span>)<span class="hljs-comment">;</span>
                String name=strs[<span class="hljs-number">0</span>]<span class="hljs-comment">;</span>
                int age=Integer<span class="hljs-preprocessor">.parseInt</span>(strs[<span class="hljs-number">1</span>]<span class="hljs-preprocessor">.trim</span>())<span class="hljs-comment">;</span>
                Person person=new Person(age,name)<span class="hljs-comment">;     </span>
                return person<span class="hljs-comment">;</span>
            }

        })<span class="hljs-comment">;</span>
        //生成一个df
        DataFrame personDF = sqlContext<span class="hljs-preprocessor">.createDataFrame</span>(PersonRDD, Person<span class="hljs-preprocessor">.class</span>)<span class="hljs-comment">;</span>
        //将df注册成临时表，之后就可以像操作表一样
        personDF<span class="hljs-preprocessor">.registerTempTable</span>(<span class="hljs-string">"person"</span>)<span class="hljs-comment">;</span>

        DataFrame resultperson = sqlContext<span class="hljs-preprocessor">.sql</span>(<span class="hljs-string">"select name,age from person where age &gt; 13 and age &lt;= 19"</span>)<span class="hljs-comment">;</span>
        //当想使用RDD的算子是，将df转换为RDD就可以使用类似foreach的操作了
        resultperson<span class="hljs-preprocessor">.javaRDD</span>()<span class="hljs-preprocessor">.foreach</span>(new VoidFunction&lt;Row&gt;() {

            <span class="hljs-comment">/**
             * 
             */</span>
            private static final long serialVersionUID = <span class="hljs-number">1</span>L<span class="hljs-comment">;</span>

            public void <span class="hljs-keyword">call</span>(Row row) throws Exception {
            //把每一条数据都看成是一个row  row(<span class="hljs-number">0</span>)=name  row(<span class="hljs-number">1</span>)=age 
                System<span class="hljs-preprocessor">.out</span><span class="hljs-preprocessor">.println</span>(<span class="hljs-string">"name"</span>+row<span class="hljs-preprocessor">.getString</span>(<span class="hljs-number">0</span>))<span class="hljs-comment">;</span>
                System<span class="hljs-preprocessor">.out</span><span class="hljs-preprocessor">.println</span>(<span class="hljs-string">"age"</span>+row<span class="hljs-preprocessor">.getInt</span>(<span class="hljs-number">1</span>))<span class="hljs-comment">;</span>
            }
        })<span class="hljs-comment">;</span>

        resultperson<span class="hljs-preprocessor">.javaRDD</span>()<span class="hljs-preprocessor">.saveAsTextFile</span>(<span class="hljs-string">"hdfs://hadoop1:9000/reflectionresult"</span>)<span class="hljs-comment">;</span>

    }

}
</code></pre>

<p>python版本(reflection.py)：</p>



<pre class="prettyprint"><code class=" hljs sql"># sc is an existing SparkContext.
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext, Row
appName= "reflection"
master = "local"
conf1 = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf1)

sqlContext = SQLContext(sc)

# <span class="hljs-operator"><span class="hljs-keyword">Load</span> a text file <span class="hljs-keyword">and</span> convert <span class="hljs-keyword">each</span> line <span class="hljs-keyword">to</span> a <span class="hljs-keyword">Row</span>.
lines = sc.textFile(<span class="hljs-string">"file:///home/zkpk/spark-1.5.2-bin-2.5.2/examples/src/main/resources/people.txt"</span>)
parts = lines.map(lambda l: l.split(<span class="hljs-string">","</span>))
people = parts.map(lambda p: <span class="hljs-keyword">Row</span>(name=p[<span class="hljs-number">0</span>], age=<span class="hljs-keyword">int</span>(p[<span class="hljs-number">1</span>])))

# Infer the <span class="hljs-keyword">schema</span>, <span class="hljs-keyword">and</span> register the DataFrame <span class="hljs-keyword">as</span> a <span class="hljs-keyword">table</span>.
schemaPeople = sqlContext.createDataFrame(people)
schemaPeople.registerTempTable(<span class="hljs-string">"people"</span>)

# <span class="hljs-keyword">SQL</span> can be run over DataFrames that have been registered <span class="hljs-keyword">as</span> a <span class="hljs-keyword">table</span>.
teenagers = sqlContext.<span class="hljs-keyword">sql</span>(<span class="hljs-string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)

# The results <span class="hljs-keyword">of</span> <span class="hljs-keyword">SQL</span> queries <span class="hljs-keyword">are</span> RDDs <span class="hljs-keyword">and</span> support <span class="hljs-keyword">all</span> the normal RDD operations.
teenNames = teenagers.map(lambda p: <span class="hljs-string">"Name: "</span> + p.name)
<span class="hljs-keyword">for</span> teenName <span class="hljs-keyword">in</span> teenNames.collect():
  print(teenName)
</span></code></pre>

<p>代码提交时，使用spark-submit reflection.py</p>



<h2 id="用编程programmatically的方式将rdd转换成df">用编程（programmatically）的方式将RDD转换成DF</h2>



<pre class="prettyprint"><code class=" hljs python"><span class="hljs-prompt">&gt;&gt;&gt; </span>lines = sc.textFile(<span class="hljs-string">"file:///home/zkpk/spark-1.5.2-bin-2.5.2/examples/src/main/resources/people.txt"</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>parts = lines.map(<span class="hljs-keyword">lambda</span> l: l.split(<span class="hljs-string">","</span>))
<span class="hljs-prompt">&gt;&gt;&gt; </span>people = parts.map(<span class="hljs-keyword">lambda</span> p: (p[<span class="hljs-number">0</span>], p[<span class="hljs-number">1</span>].strip()))
<span class="hljs-prompt">&gt;&gt;&gt; </span>parts.collect()
[[<span class="hljs-string">u'Michael'</span>, <span class="hljs-string">u' 29'</span>], [<span class="hljs-string">u'Andy'</span>, <span class="hljs-string">u' 30'</span>], [<span class="hljs-string">u'Justin'</span>, <span class="hljs-string">u' 19'</span>]]
<span class="hljs-prompt">&gt;&gt;&gt; </span>people.collect()
[(<span class="hljs-string">u'Michael'</span>, <span class="hljs-string">u'29'</span>), (<span class="hljs-string">u'Andy'</span>, <span class="hljs-string">u'30'</span>), (<span class="hljs-string">u'Justin'</span>, <span class="hljs-string">u'19'</span>)]
<span class="hljs-prompt">&gt;&gt;&gt; </span>schemaString = <span class="hljs-string">"name age"</span>
<span class="hljs-prompt">&gt;&gt;&gt; </span>fields = [StructField(field_name, StringType(), <span class="hljs-keyword">True</span>) <span class="hljs-keyword">for</span> field_name <span class="hljs-keyword">in</span> schemaString.split()]
Traceback (most recent call last):
  File <span class="hljs-string">"&lt;stdin&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;
NameError: name <span class="hljs-string">'StructField'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> defined
<span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> pyspark.sql.types <span class="hljs-keyword">import</span> *
<span class="hljs-prompt">&gt;&gt;&gt; </span>fields = [StructField(field_name, StringType(), <span class="hljs-keyword">True</span>) <span class="hljs-keyword">for</span> field_name <span class="hljs-keyword">in</span> schemaString.split()]
<span class="hljs-prompt">&gt;&gt;&gt; </span>fields.collect()
Traceback (most recent call last):
  File <span class="hljs-string">"&lt;stdin&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;
AttributeError: <span class="hljs-string">'list'</span> object has no attribute <span class="hljs-string">'collect'</span>
<span class="hljs-prompt">&gt;&gt;&gt; </span>schema = StructType(fields)
<span class="hljs-prompt">&gt;&gt;&gt; </span>schemaPeople = sqlContext.createDataFrame(people, schema)
<span class="hljs-prompt">&gt;&gt;&gt; </span>schemaPeople.registerTempTable(<span class="hljs-string">"people"</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>results = sqlContext.sql(<span class="hljs-string">"SELECT name FROM people"</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>results.collect()
[Row(name=<span class="hljs-string">u'Michael'</span>), Row(name=<span class="hljs-string">u'Andy'</span>), Row(name=<span class="hljs-string">u'Justin'</span>)]
<span class="hljs-prompt">&gt;&gt;&gt; </span>names = results.map(<span class="hljs-keyword">lambda</span> p: <span class="hljs-string">"Name: "</span> + p.name)
<span class="hljs-prompt">&gt;&gt;&gt; </span>names.collect()
[<span class="hljs-string">u'Name: Michael'</span>, <span class="hljs-string">u'Name: Andy'</span>, <span class="hljs-string">u'Name: Justin'</span>]
<span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> names.collect():
<span class="hljs-prompt">... </span>  print(name)
<span class="hljs-prompt">... </span>
Name: Michael
Name: Andy
Name: Justin
<span class="hljs-prompt">&gt;&gt;&gt; </span>
</code></pre>

<p>不懂StructField和StructType的还请参考</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//spark<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.org</span>/docs/<span class="hljs-number">1.5</span><span class="hljs-number">.2</span>/api/python/pyspark<span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.html</span>?highlight=structfield<span class="hljs-preprocessor">#pyspark.sql.types.StructField</span></code></pre>



<h2 id="dataframe-vs-rdd">DataFrame VS RDD</h2>

<p><img src="https://img-blog.csdn.net/20171030191411171?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveW91bmdiaXQwMDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>当生成DF的时候会返回schema类型（类似于postgresql的表结构），而RDD不会，仅仅返回的是一个Person，不知道里面具体的数据类型</p>



<h2 id="数据的load以及save">数据的load以及save</h2>

<p>参考：</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//spark<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.org</span>/docs/<span class="hljs-number">2.1</span><span class="hljs-number">.0</span>/sql-programming-guide<span class="hljs-preprocessor">.html</span><span class="hljs-preprocessor">#data-sources</span></code></pre>

<p>spark sql默认支持的是parquet文件格式，所以不制定load、save类型，默认都是parquet。关于parquet 参考</p>



<pre class="prettyprint"><code class=" hljs livecodeserver"><span class="hljs-keyword">http</span>://www.infoq.com/cn/articles/<span class="hljs-operator">in</span>-depth-analysis-<span class="hljs-operator">of</span>-parquet-column-storage-<span class="hljs-built_in">format</span></code></pre>

<p>Parquet是语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与Parquet配合的组件有：</p>

<p>查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL</p>

<p>计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite</p>

<p>数据模型: Avro, Thrift, Protocol Buffers, POJOs</p>

<p>保存时是追加还是覆盖，请参考SaveModes</p>



<h2 id="parquet-文件操作">parquet 文件操作</h2>

<p>通用文件操作：</p>



<pre class="prettyprint"><code class=" hljs asciidoc">df1 = sqlContext.read.load("file:///home/zkpk/spark-1.5.2-bin-2.5.2/examples/src/main/resources/users.parquet")

<span class="hljs-header">&gt;&gt;&gt; df1.show()
+------+--------------+----------------+</span>
<span class="hljs-header">|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+</span>
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
<span class="hljs-code">+------+</span>--------------<span class="hljs-code">+----------------+</span>
<span class="hljs-header">df1.select("name", "favorite_color").show()
+------+--------------+</span>
<span class="hljs-header">|  name|favorite_color|
+------+--------------+</span>
|Alyssa|          null|
<span class="hljs-header">|   Ben|           red|
+------+--------------+</span>

df1.select("name", "favorite<span class="hljs-emphasis">_color").write.save("file:///home/zkpk/ecaoyng/input/namesAndFavColors.parquet")</span></code></pre>



<h2 id="自动推断分区partition-discovery">自动推断分区（Partition Discovery）</h2>

<p>看完下面的例子就知道什么是自动推断分区了</p>



<pre class="prettyprint"><code class=" hljs asciidoc">#在hdfs上新建一个目录country=US
hadoop fs -mkdir -p /ecaoyng/country=US
#将parquet文件上传到新建的目录
hadoop fs -put users.parquet /ecaoyng/country=US

#之前读到的df1的内容
<span class="hljs-header">&gt;&gt;&gt; df1.show()
+------+--------------+----------------+</span>
<span class="hljs-header">|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+</span>
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
<span class="hljs-code">+------+</span>--------------<span class="hljs-code">+----------------+</span>
#在新建目录之后读取到的parquet文件，可以看出多了country这一列
&gt;&gt;&gt; df2= sqlContext.read.parquet("hdfs:///ecaoyng/country=US/users.parquet")
<span class="hljs-header">&gt;&gt;&gt; df2.show()
+------+--------------+----------------+-------+</span>
<span class="hljs-header">|  name|favorite_color|favorite_numbers|country|
+------+--------------+----------------+-------+</span>
|Alyssa|          null|  [3, 9, 15, 20]|     US|
|   Ben|           red|              []|     US|
<span class="hljs-code">+------+</span>--------------<span class="hljs-code">+----------------+</span>-------+
</code></pre>



<h2 id="schema的合并">schema的合并</h2>



<pre class="prettyprint"><code class=" hljs smalltalk">
&gt;&gt;&gt; df3 = sqlContext.createDataFrame(sc.parallelize(range(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)).map(lambda i : <span class="hljs-class">Row</span>(single=i, double=i*<span class="hljs-number">2</span>))
... )
&gt;&gt;&gt; df3.write.parquet(<span class="hljs-comment">"hdfs:///ecaoyng/test_table/key=1"</span>)
&gt;&gt;&gt; df4=sqlContext.createDataFrame(sc.parallelize(range(<span class="hljs-number">6</span>,<span class="hljs-number">11</span>)).map(lambda i : <span class="hljs-class">Row</span>(single=i, triple=i*<span class="hljs-number">3</span>)))
&gt;&gt;&gt; df4.write.parquet(<span class="hljs-comment">"hdfs:///ecaoyng/test_table/key=2"</span>)
&gt;&gt;&gt; df5 = sqlContext.read.option(<span class="hljs-comment">"mergeSchema"</span>, <span class="hljs-comment">"true"</span>).parquet(<span class="hljs-comment">"hdfs:///ecaoyng/test_table"</span>)
&gt;&gt;&gt; df5.printSchema()
root
 |-- <span class="hljs-method">double:</span> long (nullable = <span class="hljs-keyword">true</span>)
 |-- <span class="hljs-method">single:</span> long (nullable = <span class="hljs-keyword">true</span>)
 |-- <span class="hljs-method">triple:</span> long (nullable = <span class="hljs-keyword">true</span>)
 |-- <span class="hljs-method">key:</span> integer (nullable = <span class="hljs-keyword">true</span>)

&gt;&gt;&gt; df5.show()
+------+------+------+---+
<span class="hljs-localvars">|double|single|triple|key|</span>
+------+------+------+---+
<span class="hljs-localvars">|  null|</span>     <span class="hljs-number">6</span>|    <span class="hljs-number">18</span>|  <span class="hljs-number">2</span>|
<span class="hljs-localvars">|  null|</span>     <span class="hljs-number">7</span>|    <span class="hljs-number">21</span>|  <span class="hljs-number">2</span>|
<span class="hljs-localvars">|  null|</span>     <span class="hljs-number">8</span>|    <span class="hljs-number">24</span>|  <span class="hljs-number">2</span>|
<span class="hljs-localvars">|  null|</span>     <span class="hljs-number">9</span>|    <span class="hljs-number">27</span>|  <span class="hljs-number">2</span>|
<span class="hljs-localvars">|  null|</span>    <span class="hljs-number">10</span>|    <span class="hljs-number">30</span>|  <span class="hljs-number">2</span>|
|     <span class="hljs-number">2</span>|     <span class="hljs-number">1</span><span class="hljs-localvars">|  null|</span>  <span class="hljs-number">1</span>|
|     <span class="hljs-number">4</span>|     <span class="hljs-number">2</span><span class="hljs-localvars">|  null|</span>  <span class="hljs-number">1</span>|
|     <span class="hljs-number">6</span>|     <span class="hljs-number">3</span><span class="hljs-localvars">|  null|</span>  <span class="hljs-number">1</span>|
|     <span class="hljs-number">8</span>|     <span class="hljs-number">4</span><span class="hljs-localvars">|  null|</span>  <span class="hljs-number">1</span>|
|    <span class="hljs-number">10</span>|     <span class="hljs-number">5</span><span class="hljs-localvars">|  null|</span>  <span class="hljs-number">1</span>|
+------+------+------+---+

&gt;&gt;&gt; 
</code></pre>



<h2 id="数据源之jdbc之mysql">数据源之JDBC之mysql</h2>

<p>以mysql为例，首先启动mysql. 遇到错误信息如下</p>

<pre class="prettyprint"><code class=" hljs livecodeserver">nother MySQL daemon already running <span class="hljs-operator">with</span> <span class="hljs-operator">the</span> same unix <span class="hljs-built_in">socket</span>.
Starting mysqld:                                           [FAILED]
</code></pre>

<p>解决方案，重命名/var/lib/mysql/mysql.sock 即可，重新启动mysql <br>
其他的请参考如下链接</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">https:</span>//www<span class="hljs-preprocessor">.cnblogs</span><span class="hljs-preprocessor">.com</span>/wwxbi/p/<span class="hljs-number">6978774.</span>html
<span class="hljs-label">http:</span>//spark<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.org</span>/docs/<span class="hljs-number">1.6</span><span class="hljs-number">.0</span>/sql-programming-guide<span class="hljs-preprocessor">.html</span><span class="hljs-preprocessor">#jdbc-to-other-databases</span></code></pre>

<p>注意，如果没有驱动包会报错：</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">java.sql.SQLException:</span> No suitable driver
        at java<span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.DriverManager</span><span class="hljs-preprocessor">.getDriver</span>(DriverManager<span class="hljs-preprocessor">.java</span>:<span class="hljs-number">278</span>)
</code></pre>

<p>解决方法是： 下载驱动包</p>



<pre class="prettyprint"><code class=" hljs ruby">在spark-env.sh中设置
export <span class="hljs-constant">SPARK_CLASSPATH</span>=<span class="hljs-variable">$SPARK_CLASSPATH</span><span class="hljs-symbol">:/usr/local/soft/hive/lib/mysql-connector-java-</span><span class="hljs-number">5.1</span>.<span class="hljs-number">10</span>.jar
那么在提交jdbc的脚本的时候就不可以设置--driver-<span class="hljs-class"><span class="hljs-keyword">class</span>-<span class="hljs-title">path</span>路径</span>

如果在spark-env.sh脚本里没有设置<span class="hljs-constant">MySQL</span>驱动包的spark_classpath
那么需要在提交任务的时候在脚本里天如下内容：
--driver-<span class="hljs-class"><span class="hljs-keyword">class</span>-<span class="hljs-title">path</span> /<span class="hljs-title">usr</span>/<span class="hljs-title">local</span>/<span class="hljs-title">soft</span>/<span class="hljs-title">hive</span>/<span class="hljs-title">lib</span>/<span class="hljs-title">mysql</span>-<span class="hljs-title">connector</span>-<span class="hljs-title">java</span>-5.1.10.<span class="hljs-title">jar</span></span>
</code></pre>



<h2 id="数据源之hive">数据源之Hive</h2>

<p>在spark开发中spark开发占了很大一部分，而基于hive的开发，又占了其中很大的比重。下面我们就来进行hive和spark集成</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>