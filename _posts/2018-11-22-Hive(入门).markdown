---
layout:     post
title:      Hive(入门)
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                复习Hadoop<br>1) Hadoop是什么<br><span>	</span>存储  HDFS<br><span>	</span>计算  MapReduce<br><span>	</span>资源调度  YARN<br><br><br>2) HDFS的访问方式<br><span>	</span>Shell CLI : hadoop/hdfs fs ....<br><span>	</span>Java API : FileSystem<br><span>	</span>Web UI : HUE/Hadoop自带UI<br> <br>3) Hadoop的常用操作<br><span>	</span>HDFS: mkdir put get rm mv ...<br><span>		</span>Q: copy vs mv <br><span>	</span>MR: mr的执行流程(一定要掌握的)<br><span>	</span>YARN：mr作业跑在yarn之上，杀死yarn上的作业，提交的时候指定一些重要的参数<br> <br> <br>Q: 让你们使用mapreduce来实现join、mapjoin的功能<br>   ==&gt; 非常繁琐<br>       wordcount<br> <br>一点：MR是非常麻烦的 <br> <br> <br>Hive的产生背景 <br>1) MR编程不便性<br>2) 传统RDBMS人员的需要 <br><span>	</span>HDFS上面的文件就是普通的文件，它并没有schema的概念<br><span>	</span>schema: RDBMS中的表结构<br><span>		</span>people.txt  &lt;==  id  name age  address<br><span>	</span>sql ===&gt;  搞定海量数据的统计分析<br><br><br>==&gt; Hive<span>	</span><br><span>		</span><br><br><br> <br>Hive <br><span>	</span>distributed storage： HDFS/S3<br><span>		</span>Q: HDFS存的是近期的数据<br><span>			</span>1min：几百G<br><span>			</span>冷数据: 定期的移走S3   table的location指向s3<br><span>	</span>Facebook  解决海量结构化日志数据的统计文件<br><span>	</span>构建在Hadoop之上的数据仓库<br><span>		</span>数据存储在HDFS之上<br><span>		</span>计算是使用MR<br><span>		</span>弹性：线性扩展<br><span>	</span>Hive底层的执行引擎：MapReduce、Tez、Spark<br><span>	</span>Hive定义一种类SQL的查询语言: HQL<br><span>		</span>Q: HQL和SQL的关系：毛线都没有，只是语法类似<br><span>		</span>很多的SQL on Hadoop的语法都是和RDBMS非常类似的<br><span>	</span>Hive常用于：离线批处理<br><span>	</span>SQL ==&gt; MR ：把SQL语句翻译成MapReduce作业，并提交到YARN上运行<br><span>		</span>Q：是否智能、执行计划(sql是如何翻译成mr作业,打死都要知道!!!)<br>    <br><span>	</span>高级：UDF   一般不用,一般用scala自己开发 <br> <br>Hive的优化 线下班要求:必要要源源不断20分钟..从数据倾斜,压缩文件格式之类 !!!!!!!!!!!!!!!!!<br><br><br>Stinger Plan<br><span>	</span>Phase 1  0.11  ORC HiveServer2<br><span>	</span>Phase 2  0.12  ORC improvement<br><span>	</span>Phase 3  0.13  Vectorized query engine &amp; Tez<br>Stinger.next Phase 1  0.14  CBO<span>	</span>    <br><br><br>CBO?????  <br><span>	</span><br><span>	</span><br>为什么要使用Hive<br>1) 简单易用<br>2) 弹性<br>3) 统一的元数据管理<span>	</span><br><span>	</span>元数据存放在哪里呢? metadata  mysql<br><span>	</span>Q: Hive的数据存放在哪里<br><span>	</span>统一：SQL on Hadoop都是能够共享的<br><span>		</span>Hive/Impala/Spark SQL/Presto<br> <span>	</span>就是再Hive创建了一张表,可以在Impala/Spark SQL/Presto的架构上使用,很方便移植<br> <br> <br>Hive体系架构<br>ODBC???<br><br><br>Hive部署架构 <br> <br><br><br><br><br>Hive VS RDBMS <br>Hive的缺点 <br> <br>Hive环境搭建 <br><span>	</span>wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz<br><br><br><span>	</span>tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app<br><span>	</span><br>hadoop/hadoop<br>~<br><span>	</span>app<br><span>	</span>software<br><span>	</span>source<br><span>	</span><br>1) 添加HIVE_HOME到系统环境变量<br>mvn clean package -Pdist<br><br><br>export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0<br>export PATH=$HIVE_HOME/bin:$PATH<span>	</span><br><span>	</span><br>2) Hive配置修改<br><span>	</span>hive-env.sh<br><span>		</span>HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0<br><span>	</span><br><span>	</span>hive-site.xml   统一元数据管理<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br><br>&lt;configuration&gt;<br>&lt;property&gt;<br>  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>    &lt;value&gt;jdbc:mysql://localhost:3306/ruozedata_basic02?createDatabaseIfNotExist=true&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br>        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br>        &lt;/property&gt;<br><br><br>&lt;property&gt;<br>  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br>    &lt;value&gt;root&lt;/value&gt;<br>    &lt;/property&gt;<br><br><br>&lt;property&gt;<br>  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br>    &lt;value&gt;root&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br><span>	</span><br>3) 拷贝mysql驱动包到$HIVE_HOME/lib<span>	</span><br><span>	</span><br>The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. <br>Please check your CLASSPATH specification, <br>and the name of the driver.<br><span>	</span><br><span>		</span><br>4) 权限问题<span>	</span><br> <br>创建表失败<br>FAILED: Execution Error, <br>return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. <br>MetaException(message:For direct MetaStore DB <br>connections, we don't support retries at the client <br>level.)<br><br><br><br><br>思路：找日志 <br>日志在哪里： $HIVE_HOME/conf/hive-log4j.properties.template<br><br><br>hive.log.dir=${java.io.tmpdir}/${user.name}<br>hive.log.file=hive.log<br><br><br>能不能改？如何改？<br><br><br><br><br>ERROR [main]: Datastore.Schema (Log4JLogger.java:error(115)) - An exception was thrown while adding/validating class(es) : <br>Specified key was too long; max key length is 767 bytes<br>com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes <br> <br><br><br> <br> <br>作业<br>1) 下载Hive的源码：hive-1.1.0-cdh5.7.0-src.tar.gz<br><span>	</span>编译出来Hive的安装包，并部署Hive<br>2) 使用hive完成wordcount统计<span>	</span><br><span>	</span><br><span>	</span><br><span>	</span><br><span>	</span>create table helloworld (id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';<span>	</span><br><span>	</span>alter database ruozedata_basic02 character set latin1;<br><span>	</span><br> <br> <br>             </div>
                </div>