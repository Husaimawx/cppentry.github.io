---
layout:     post
title:      Spark 1.6.2 + Hadoop 2.7.2 集群搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/u010048823/article/details/51925977				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<blockquote>
  <p>版本信息：Centos7 + Hadoop 2.7.2 + Spark 1.6.2 + Scala 2.11.8</p>
</blockquote>

<p>Hadoop + Spark 集群搭建系列文章，建议按顺序参考：</p>

<blockquote>
  <p><a href="http://blog.csdn.net/u010048823/article/details/51931012" rel="nofollow">Hadoop &amp; Spark 集群搭建 理念思想</a></p>
  
  <p><a href="http://blog.csdn.net/u010048823/article/details/51908817" rel="nofollow">Hadoop 2.7.2 集群搭建－预备工作</a></p>
  
  <p><a href="http://blog.csdn.net/u010048823/article/details/51913608" rel="nofollow">Hadoop 2.7.2 集群搭建</a></p>
  
  <p><a href="#" rel="nofollow">Spark 1.6.2 + Hadoop 2.7.2 集群搭建</a> (不用点了，就是本文)</p>
</blockquote>

<hr>

<h1 id="预备工作">预备工作</h1>



<h2 id="jdk-安装配置">JDK 安装配置</h2>

<p>请参考 <a href="http://blog.csdn.net/u010048823/article/details/51908817" rel="nofollow">Hadoop 2.7.2 集群搭建－预备工作</a>中 JDK 的安装和配置。</p>



<h2 id="scala-安装配置">Scala 安装配置</h2>

<p>1.下载后解压至合适目录</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> ~]<span class="hljs-variable">$ </span>cd <span class="hljs-number">03</span>Software/<span class="hljs-number">04</span>BigData/
[liuyao<span class="hljs-variable">@master</span> <span class="hljs-number">04</span>BigData]<span class="hljs-variable">$ </span>tar -xzvf scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span>.tgz --directory=<span class="hljs-regexp">/home/liuyao</span><span class="hljs-regexp">/00Hadoop/</span>
scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span>/
scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span>/man/
scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span>/man/man1/
……</code></pre>

<p>2.配置环境变量，在 /etc/profile 中添加如下内容</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-keyword">export</span> SCALA_HOME=/home/liuyao/<span class="hljs-number">00</span>Hadoop/scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span>
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">${SCALA_HOME}</span>/bin</code></pre>

<p>3.使 /etc/profile 文件更新生效</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> ~]<span class="hljs-variable">$ </span>source /etc/profile</code></pre>

<p>4.在 master 主机上配置完毕后，把Scala整个文件夹分发到slave各节点，并配置环境变量</p>



<h2 id="ssh配置">SSH配置</h2>

<p>主要是配置 SSH 以实现集群各节点间免密码登录（公钥登录），请参考 <a href="http://blog.csdn.net/u010048823/article/details/51908817" rel="nofollow">Hadoop 2.7.2 集群搭建－预备工作</a>中 SSH 的安装和配置。</p>

<hr>



<h1 id="hadoop安装配置">Hadoop安装配置</h1>

<blockquote>
  <p>此部分所有步骤一一列出，但部分步骤省略操作细节，详情请参考 <a href="http://blog.csdn.net/u010048823/article/details/51913608" rel="nofollow">Hadoop 2.7.2 集群搭建</a> 一文中的内容。</p>
</blockquote>

<p>1.下载后解压至合适目录</p>

<p>2.配置 Hadoop 环境变量</p>

<p>3.编辑配置文件</p>

<p>(1) 配置hadoop-env.sh文件</p>

<p>(2) 配置core-site.xml文件</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>hdfs://master:9000<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span> 
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> 
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>/home/liuyao/00Hadoop/hadoop-2.7.2/tmp<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> 
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>io.file.buffer.size<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>131072<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.proxyuser.hduser.hosts<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #Hadoop全分布模式里，此处是spark而非hduser?下同
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.proxyuser.hduser.groups<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>(3) 配置yarn-site.xml文件</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8032<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8030<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8031<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #Hadoop全分布模式里，此处为8035而非8031?
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8033<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
       <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
       <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8088<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>(4) 配置mapred-site.xml文件，完全同Hadoop全分布模式</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:10020<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:19888<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>(5) 配置hdfs-site.xml文件，完全同Hadoop全分布模式</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:9001<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.webhdfs.enabled<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>(6) 配置slaves文件，完全同 Hadoop 全分布模式</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2</code></pre>

<p>(7) 将以上配置文件分发到各个节点，实际上应该直接把整个 Hadoop 目录分发过去（若从节点没安装Hadoop的话）</p>

<p>4.格式化namenode，启动并测试 Hadoop</p>

<hr>



<h1 id="spark-安装配置">Spark 安装配置</h1>



<h2 id="下载安装并配置环境">下载安装并配置环境</h2>

<p>1.从官网上下载后解压至合适目录</p>

<blockquote>
  <p>选择的版本是<code>spark-1.6.2-bin-hadoop2.6.tgz</code>，虽然Hadoop是2.7.2的，之前下载的是<code>without-hadoop</code>版本，运行出错。</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> <span class="hljs-number">04</span>BigData]<span class="hljs-variable">$ </span>tar -xzvf spark-<span class="hljs-number">1.6</span>.<span class="hljs-number">2</span>-bin-hadoop2.<span class="hljs-number">6</span>.tgz --directory=<span class="hljs-regexp">/home/liuyao</span><span class="hljs-regexp">/00Hadoop/</span>
spark-<span class="hljs-number">1.6</span>.<span class="hljs-number">2</span>-bin-hadoop2.<span class="hljs-number">6</span>/
spark-<span class="hljs-number">1.6</span>.<span class="hljs-number">2</span>-bin-hadoop2.<span class="hljs-number">6</span>/<span class="hljs-constant">NOTICE</span>
spark-<span class="hljs-number">1.6</span>.<span class="hljs-number">2</span>-bin-hadoop2.<span class="hljs-number">6</span>/<span class="hljs-constant">CHANGES</span>.txt
……</code></pre>

<p>2.配置 Spark 环境变量，在 /etc/profie 里添加如下两行</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-keyword">export</span> SPARK_HOME=/home/liuyao/<span class="hljs-number">00</span>Hadoop/spark-<span class="hljs-number">1.6</span>.<span class="hljs-number">2</span>-bin-hadoop2.<span class="hljs-number">6</span>
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">${SPARK_HOME}</span>/bin</code></pre>



<h2 id="编辑配置文件">编辑配置文件</h2>

<p>1.配置spark-env.sh文件，加入下面内容</p>



<pre class="prettyprint"><code class=" hljs rust"><span class="hljs-keyword">export</span> JAVA_HOME=/usr/lib/jvm/java-<span class="hljs-number">1.8</span>.<span class="hljs-number">0</span>-openjdk-<span class="hljs-number">1.8</span>.<span class="hljs-number">0.91</span>-<span class="hljs-number">0</span>.b14.el7_2.x86_64
<span class="hljs-keyword">export</span> SCALA_HOME=/home/liuyao/<span class="hljs-number">00</span>Hadoop/scala-<span class="hljs-number">2.11</span>.<span class="hljs-number">8</span>
<span class="hljs-keyword">export</span> SPARK_WORKER_MEMORY=<span class="hljs-number">4</span>g
<span class="hljs-keyword">export</span> SPARK_MASTER_IP=<span class="hljs-number">101</span>.X.XX.XX0
<span class="hljs-keyword">export</span> MASTER=spark:<span class="hljs-comment">//101.X.XX.XX0:7077</span>
<span class="hljs-keyword">export</span> HADOOP_CONF_DIR=/home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>/etc/hadoop</code></pre>

<p>2.配置slaves文件，加入下面内容</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2</code></pre>

<p>3.将以上配置文件分发到各个节点，实际上应该直接把整个 Spark 目录分发过去（若从节点没安装 Spark 的话）</p>



<h2 id="启动集群">启动集群</h2>

<p>1.启动 Hadoop</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># master节点</span>
[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ start-dfs<span class="hljs-preprocessor">.sh</span>
Starting namenodes on [master]
<span class="hljs-label">master:</span> starting namenode, logging to ……
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2: starting datanode, logging to ……
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1: starting datanode, logging to ……
Starting secondary namenodes [master]
<span class="hljs-label">master:</span> starting secondarynamenode, logging to ……

[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ start-yarn<span class="hljs-preprocessor">.sh</span>
starting yarn daemons
starting resourcemanager, logging to ……
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2: starting nodemanager, logging to ……
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1: starting nodemanager, logging to ……

[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ jps
<span class="hljs-number">6441</span> Jps
<span class="hljs-number">5755</span> SecondaryNameNode
<span class="hljs-number">5373</span> NameNode
<span class="hljs-number">6126</span> ResourceManager</code></pre>



<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-comment"># slave1节点，slave2节点与之一样</span>
[liuyao<span class="hljs-variable">@slave1</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">5328</span> <span class="hljs-constant">DataNode</span>
<span class="hljs-number">5958</span> <span class="hljs-constant">Jps</span>
<span class="hljs-number">5661</span> <span class="hljs-constant">NodeManager</span></code></pre>

<p>2.启动 Spark</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># master节点，可以看到，成功启动了Master进程</span>
[liuyao@master spark-<span class="hljs-number">1.6</span><span class="hljs-number">.2</span>-bin-hadoop2<span class="hljs-number">.6</span>]$ ./sbin/start-all<span class="hljs-preprocessor">.sh</span> 
starting org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.deploy</span><span class="hljs-preprocessor">.master</span><span class="hljs-preprocessor">.Master</span>, logging to ……
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2: starting org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.deploy</span><span class="hljs-preprocessor">.worker</span><span class="hljs-preprocessor">.Worker</span>, logging to ……
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1: starting org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.deploy</span><span class="hljs-preprocessor">.worker</span><span class="hljs-preprocessor">.Worker</span>, logging to ……

[liuyao@master spark-<span class="hljs-number">1.6</span><span class="hljs-number">.2</span>-bin-hadoop2<span class="hljs-number">.6</span>]$ jps
<span class="hljs-number">6485</span> Master
<span class="hljs-number">6584</span> Jps
<span class="hljs-number">5755</span> SecondaryNameNode
<span class="hljs-number">5373</span> NameNode
<span class="hljs-number">6126</span> ResourceManager</code></pre>



<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-comment"># slave1节点，可以看到，成功启动了Worker进程，slave2节点与之一样</span>
[liuyao<span class="hljs-variable">@slave1</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">5328</span> <span class="hljs-constant">DataNode</span>
<span class="hljs-number">6090</span> <span class="hljs-constant">Worker</span>
<span class="hljs-number">6282</span> <span class="hljs-constant">Jps</span>
<span class="hljs-number">5661</span> <span class="hljs-constant">NodeManager</span></code></pre>



<h2 id="集群测试">集群测试</h2>

<p>Spark支持两种方式运行样例： run-example 方式和 Spark Shell 方式</p>

<p>1.run-example 方式</p>

<p>(1) 输入代码</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master spark-<span class="hljs-number">1.6</span><span class="hljs-number">.2</span>-bin-hadoop2<span class="hljs-number">.6</span>]$ ./bin/run-example org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.examples</span><span class="hljs-preprocessor">.SparkPi</span>
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">22</span>:<span class="hljs-number">39</span>:<span class="hljs-number">22</span> INFO spark<span class="hljs-preprocessor">.SparkContext</span>: Running Spark version <span class="hljs-number">1.6</span><span class="hljs-number">.2</span>
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">22</span>:<span class="hljs-number">39</span>:<span class="hljs-number">22</span> WARN util<span class="hljs-preprocessor">.NativeCodeLoader</span>: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
……</code></pre>

<p>(2) 通过Web UI 查看集群状态</p>

<p>浏览器中输入<a href="http://master:8080" rel="nofollow">http://master:8080</a>，可以观察集群整个状态，如下图所示</p>

<p><img src="https://img-blog.csdn.net/20160716230049398" alt="这里写图片描述" title=""></p>

<p>2.Spark Shell 方式</p>

<p>(1) 上传文件至HDFS</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>hdfs dfs -ls /
<span class="hljs-constant">Found</span> <span class="hljs-number">1</span> items
drwx-wx-wx   - liuyao supergroup          <span class="hljs-number">0</span> <span class="hljs-number">2016</span>-<span class="hljs-number">07</span>-<span class="hljs-number">16</span> <span class="hljs-number">23</span><span class="hljs-symbol">:</span><span class="hljs-number">33</span> /tmp
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>hdfs dfs -put <span class="hljs-constant">README</span>.txt /tmp/
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>hdfs dfs -ls /tmp/
<span class="hljs-constant">Found</span> <span class="hljs-number">2</span> items
-rw-r--r--   <span class="hljs-number">2</span> liuyao supergroup       <span class="hljs-number">1366</span> <span class="hljs-number">2016</span>-<span class="hljs-number">07</span>-<span class="hljs-number">16</span> <span class="hljs-number">23</span><span class="hljs-symbol">:</span><span class="hljs-number">45</span> /tmp/<span class="hljs-constant">README</span>.txt
drwx-wx-wx   - liuyao supergroup          <span class="hljs-number">0</span> <span class="hljs-number">2016</span>-<span class="hljs-number">07</span>-<span class="hljs-number">16</span> <span class="hljs-number">23</span><span class="hljs-symbol">:</span><span class="hljs-number">33</span> /tmp/hive</code></pre>

<p>(2) 启动Spark-shell</p>



<pre class="prettyprint"><code class=" hljs vbnet">[liuyao@master bin]$ spark-shell 
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">33</span>:<span class="hljs-number">27</span> WARN util.NativeCodeLoader: Unable <span class="hljs-keyword">to</span> load native-hadoop library <span class="hljs-keyword">for</span> your platform... <span class="hljs-keyword">using</span> builtin-java classes <span class="hljs-keyword">where</span> applicable
……
Welcome <span class="hljs-keyword">to</span>
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  <span class="hljs-comment">'_/</span>
   /___/ .__/\_,_/_/ /_/\_\   version <span class="hljs-number">1.6</span><span class="hljs-number">.2</span>
      /_/

<span class="hljs-keyword">Using</span> Scala version <span class="hljs-number">2.10</span><span class="hljs-number">.5</span> (OpenJDK <span class="hljs-number">64</span>-Bit Server VM, Java <span class="hljs-number">1.8</span><span class="hljs-number">.0</span>_91)
……
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">33</span>:<span class="hljs-number">30</span> INFO repl.SparkILoop: Created spark context..
Spark context available <span class="hljs-keyword">as</span> sc.
……
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">34</span>:<span class="hljs-number">07</span> INFO repl.SparkILoop: Created sql context (<span class="hljs-keyword">with</span> Hive support)..
SQL context available <span class="hljs-keyword">as</span> sqlContext.</code></pre>

<p>(3) 交互式操作</p>



<pre class="prettyprint"><code class=" hljs autohotkey">scala&gt; val readmeFile = sc.textFile(<span class="hljs-string">"hdfs://master:9000/tmp/README.txt"</span>)
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">46</span>:<span class="hljs-number">21</span> INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size <span class="hljs-number">62.4</span> KB, free <span class="hljs-number">62.4</span> KB)
……
<span class="hljs-label">readmeFile: org.apache.spark.rdd.RDD[String] = hdfs://master:9000/tmp/README.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:</span><span class="hljs-number">27</span>

scala&gt; var theCount = readmeFile.filter(line=&gt;line.contains(<span class="hljs-string">"The"</span>))
<span class="hljs-label">theCount: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at &lt;console&gt;:</span><span class="hljs-number">29</span>

scala&gt; theCount.count
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">47</span>:<span class="hljs-number">19</span> INFO mapred.FileInputFormat: Total input paths to process : <span class="hljs-number">1</span>
…… 
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">47</span>:<span class="hljs-number">21</span> INFO scheduler.DAGScheduler: Job <span class="hljs-number">0</span> finished: count at &lt;console&gt;:<span class="hljs-number">32</span>, took <span class="hljs-number">1.251228</span> s
<span class="hljs-label">res0:</span> Long = <span class="hljs-number">4</span>

scala&gt; val wordCount = readmeFile.flatMap(line=&gt;line.split(<span class="hljs-string">" "</span>)).map(word=&gt;(word,<span class="hljs-number">1</span>)).reduceByKey(_+_)
<span class="hljs-label">wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at &lt;console&gt;:</span><span class="hljs-number">29</span>

scala&gt; wordCount.collect
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">48</span>:<span class="hljs-number">49</span> INFO spark.SparkContext: Starting job: collect at &lt;console&gt;:<span class="hljs-number">32</span>
…… 
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">23</span>:<span class="hljs-number">48</span>:<span class="hljs-number">49</span> INFO scheduler.DAGScheduler: Job <span class="hljs-number">1</span> finished: collect at &lt;console&gt;:<span class="hljs-number">32</span>, took <span class="hljs-number">0.222501</span> s
<span class="hljs-label">res1: Array[(String, Int)] = Array((under,1), (this,3), (distribution,2), (Technology,1), (country,1), (is,1), (Jetty,1), (currently,1), (permitted.,1), (check,1), (have,1), (Security,1), (U.S.,1), (with,1), (BIS,1), (This,1), (mortbay.org.,1), ((ECCN),1), (using,2), (security,1), (Department,1), (export,1), (reside,1), (any,1), (algorithms.,1), (from,1), (re-export,2), (has,1), (SSL,1), (Industry,1), (Administration,1), (details,1), (provides,1), (http:</span>//hadoop.apache.org/core/,<span class="hljs-number">1</span>), (country's,<span class="hljs-number">1</span>), (Unrestricted,<span class="hljs-number">1</span>), (<span class="hljs-number">740.13</span>),<span class="hljs-number">1</span>), (policies,<span class="hljs-number">1</span>), (country,,<span class="hljs-number">1</span>), (concerning,<span class="hljs-number">1</span>), (uses,<span class="hljs-number">1</span>), (Apache,<span class="hljs-number">1</span>), (possession,,<span class="hljs-number">2</span>), (information,<span class="hljs-number">2</span>), (our,<span class="hljs-number">2</span>), (as,<span class="hljs-number">1</span>), (<span class="hljs-string">""</span>,<span class="hljs-number">18</span>), (Bureau,<span class="hljs-number">1</span>), (wiki,,<span class="hljs-number">1</span>), (please,<span class="hljs-number">2</span>), (form,<span class="hljs-number">1</span>), (information.,<span class="hljs-number">1</span>), (ENC,<span class="hljs-number">1</span>), (Export,<span class="hljs-number">2</span>), (included,<span class="hljs-number">1</span>), (asymmetric,<span class="hljs-number">1</span>), (Commodity,<span class="hljs-number">1</span>), (Softwar...</code></pre>

<p>(4) 通过Web UI 查看集群状态 <br>
Spark - Job 界面：<a href="http://master:4040/jobs/" rel="nofollow">http://master:4040/jobs/</a></p>

<p><img src="https://img-blog.csdn.net/20160717001202586" alt="这里写图片描述" title=""></p>

<p>Spark - Stage 界面：<a href="http://master:4040/stages/" rel="nofollow">http://master:4040/stages/</a></p>

<p><img src="https://img-blog.csdn.net/20160717001234182" alt="这里写图片描述" title=""></p>

<p>Spark - Master &amp; Workers 界面：<a href="http://master:8080/" rel="nofollow">http://master:8080/</a></p>

<p><img src="https://img-blog.csdn.net/20160717001446153" alt="这里写图片描述" title=""></p>

<hr>



<h1 id="参考资料">参考资料</h1>

<blockquote>
  <p><a href="http://www.cnblogs.com/tonylp/p/5233369.html" rel="nofollow">http://www.cnblogs.com/tonylp/p/5233369.html</a></p>
  
  <p><a href="http://www.open-open.com/lib/view/open1419490748562.html" rel="nofollow">http://www.open-open.com/lib/view/open1419490748562.html</a></p>
  
  <p><a href="http://blog.csdn.net/stark_summer/article/details/42458081" rel="nofollow">http://blog.csdn.net/stark_summer/article/details/42458081</a></p>
</blockquote>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>