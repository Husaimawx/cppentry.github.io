---
layout:     post
title:      hadoop集群数据迁移
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/lookqlp/article/details/52096296				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                hadoop distcp hdfs://namenode1/foo hdfs://namenode2/bar 
<br>如果想在两个运行着不同版本HDFS的集群上利用distcp，使用hdfs协议是会失败的，因为RPC系统是不兼容的。想要弥补这种情况，可以使用基于HTTP的HFTP文件系统从源中进行读取。这个<span style="color:#FF0000;">作业必须运行在目标集群</span>上，使得HDFS RPC版本是兼容的。
<br>例如：hadoop distcp hftp://namenode1:50070/foo hdfs://namenode2/bar 
<br><br>cdh如下异常：
<br>Caused by: java.io.IOException: Check-sum mismatch between hftp://ip:50070/flume/CC/normal/2014-06-20/FlumeData.1403222404996.snappy and hdfs://ip:8020/flume/.distcp.tmp.attempt_1404355744480_0004_m_000015_2.
<br>    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:190)
<br>    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:125)
<br>    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:95)
<br>    at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
<br>    ... 11 more
<br>解决方法：
<br>hdfs增加如此参数
<br>&lt;property&gt;
<br>&lt;name&gt;dfs.checksum.type&lt;/name&gt;
<br>&lt;value&gt;CRC32&lt;/value&gt;
<br>&lt;/property&gt;
<br><br>另外若目标集群若配置了安全机制，例如kerberos，distcp运行失败，暂时解决办法是禁用安全机制，先将数据迁移。            </div>
                </div>