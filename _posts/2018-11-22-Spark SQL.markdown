---
layout:     post
title:      Spark SQL
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h2>1.Spark SQL  </h2><p><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164819438-1352452013.jpg" alt="" border="0"></p><div><strong>    Spark SQL 和 Hive on Spark<span style="line-height:1.6;"> 两者的区别？</span></strong></div><div><strong>        spark on hive：</strong>hive只是作为元数据存储的角色，解析，优化，执行都是spark做的<strong>   </strong> </div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">  </span><span style="line-height:1.6;">   </span><strong style="line-height:1.6;">hive on spark：</strong><span style="line-height:1.6;"> </span><span style="line-height:1.6;">hive既作为存储的角色，又作为计算角色的一部分，hive将sql解析Spark任务，</span><strong style="line-height:1.6;">底层是Spark引擎</strong><span style="line-height:1.6;">（</span><span style="line-height:1.6;">hive2.0以后推荐使用Spark引擎，转化为Spark任务，hvie2.0以前都是转化为MR任务）</span></div><div>            <span style="line-height:1.6;"><br></span></div><div><span style="line-height:1.6;">    </span><strong>Spark SQL 转化的过程（底层架构）<span style="line-height:1.6;"><br></span></strong></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164820251-723759118.jpg" alt="" border="0"><div> </div></div><div>【SQL/HQL--&gt;解析器--&gt;分析器--&gt;优化器--&gt;CostModel消耗模型（选出消耗最低的，就是效率最高的），最终将传入的SQL转换为RDD的计算】</div><div> </div><div><span style="color:#ff0000;background-color:#ffff00;"><strong>须知：</strong></span></div><div><span><span style="color:#ff0000;background-color:#ffffff;">       </span> <strong>若想使用SparkSQL必须创建SQLContext 必须是传入SparkContext 不能是SparkConf<br></strong></span></div><div><span><span><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164820673-1956712757.png" alt="" border="0"></span></span><div> </div><div><strong style="line-height:1.6;">1.DataFrame与RDD的区别？<span style="line-height:1.6;">   ||</span><span style="line-height:1.6;">   什么是DataFrame?</span></strong></div></div><p><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164821188-1836456422.jpg" alt="" border="0"></p><div><span style="line-height:1.6;"><strong>区别：</strong></span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><strong style="line-height:1.6;">Spark core是基于RDD的编程，Spark SQL是基于DataFrame的编程</strong><span style="line-height:1.6;">，DataFrame的底层就是封装的</span><span style="line-height:1.6;">RDD，只不过DataFrame底层RDD的泛型是ROW（DataFrame &lt;==&gt; RDD&lt;ROW&gt;），另外，</span><span style="line-height:1.6;">DataFrame中有对列的描述，但是RDD没有对列的描述。</span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span></div><div><span style="line-height:1.6;"><strong>What is DataFrame：</strong></span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;"><strong>DataFrame</strong></span><span style="line-height:1.6;font-weight:bold;"> </span><span style="line-height:1.6;">与 <strong>RDD </strong>类似，<strong>DataFrame 是一个分布式数据容器，更像传统数据库的二维表格，除了数据以外，还掌握数据的结构信息</strong>（比如对列的描述）<strong>， 即 schema。</strong>同时，与 Hive 类似，<strong>DataFrame 也支持嵌套数据类型</strong>（struct、 array 和 map）。 从 API 易用性的角度上 看，<strong>DataFrameAPI 提供的是一套高层的关系操作</strong>，<strong>比</strong>函数式的 <strong>RDDAPI </strong>要<strong>更加友好，门槛更低</strong>。 </span></div><h4><span style="line-height:1.6;"><strong>3.创建DataFrame的来源和方<span>式</span></strong></span><span style="font-size:15px;line-height:1.6;"><span>   ||</span></span><span style="font-size:15px;line-height:1.6;"><span>   如何对DataFrame中封装的数据进行操作？</span></span></h4><div><strong style="line-height:1.6;">3.1创建DataFrame的来源和方<span>式</span></strong></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164821641-112548315.png" alt="" border="0"><div> </div></div><div><h4><span style="font-size:15px;line-height:1.6;"><span>3.2如何对DataFrame中封装的数据进行操作？</span></span></h4></div><div>    当我们的DataFrame构建好之后，里面封装了我们的数据，需要对数据进行操作即对DataFrame进行操作，有两种方式</div><div><strong>3.2.1</strong><span style="line-height:1.6;">   <strong>通过方法</strong></span></div><div>    <span style="line-height:1.6;"> </span><span style="line-height:1.6;">   </span><strong style="line-height:1.6;">sqlContext.read() </strong><span style="line-height:1.6;">   </span><span style="line-height:1.6;">返回DataFrameReader对象</span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">     </span><strong style="line-height:1.6;">sqlContext.read().json("student.json")</strong><span style="line-height:1.6;">   </span><span style="line-height:1.6;">读取一个json文件（这个json文件中的内容不能是嵌套的）读进来变成DataFrame,</span></div><div>        <strong>df.select("age").show()</strong>，如果没有show,这个程序就不会执行，这个show就类似与Spark中Action类型的算子，触发执行</div><div> </div><div><span style="background-color:#c0ffff;">示例代码：</span></div><div><div class="cnblogs_code"><img class="code_img_opened" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" alt=""><div class="cnblogs_code_hide"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div><pre><span style="color:#008080;"> 1</span> <span style="color:#0000ff;">package</span><span style="color:#000000;"> com.hzf.spark.exercise;
</span><span style="color:#008080;"> 2</span> 
<span style="color:#008080;"> 3</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkConf;
</span><span style="color:#008080;"> 4</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaSparkContext;
</span><span style="color:#008080;"> 5</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrame;
</span><span style="color:#008080;"> 6</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.SQLContext;
</span><span style="color:#008080;"> 7</span> 
<span style="color:#008080;"> 8</span> <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span><span style="color:#000000;"> TestSparkSQL {
</span><span style="color:#008080;"> 9</span>     <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> main(String[] args) {
</span><span style="color:#008080;">10</span>         SparkConf conf = <span style="color:#0000ff;">new</span> SparkConf().setAppName("DataFrameOps").setMaster("local"<span style="color:#000000;">);
</span><span style="color:#008080;">11</span>         
<span style="color:#008080;">12</span>         JavaSparkContext sc = <span style="color:#0000ff;">new</span><span style="color:#000000;"> JavaSparkContext(conf);
</span><span style="color:#008080;">13</span>         SQLContext sqlContext = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SQLContext(sc);
</span><span style="color:#008080;">14</span>         
<span style="color:#008080;">15</span>         DataFrame df = sqlContext.read().json("people.json"<span style="color:#000000;">);
</span><span style="color:#008080;">16</span>         
<span style="color:#008080;">17</span>         
<span style="color:#008080;">18</span>         <span style="color:#008000;">/*</span>
<span style="color:#008080;">19</span> <span style="color:#008000;">         * 操作DataFrame的第一种方式
</span><span style="color:#008080;">20</span> <span style="color:#008000;">         * </span><span style="color:#008000;">*/</span>
<span style="color:#008080;">21</span>         <span style="color:#008000;">//</span><span style="color:#008000;">类似 SQL的select from table;</span>
<span style="color:#008080;">22</span> <span style="color:#000000;">        df.show();
</span><span style="color:#008080;">23</span>         <span style="color:#008000;">//</span><span style="color:#008000;">desc table</span>
<span style="color:#008080;">24</span> <span style="color:#000000;">        df.printSchema();
</span><span style="color:#008080;">25</span>         
<span style="color:#008080;">26</span>         <span style="color:#008000;">//</span><span style="color:#008000;">select age from table;</span>
<span style="color:#008080;">27</span>         df.select("age"<span style="color:#000000;">).show();
</span><span style="color:#008080;">28</span>         <span style="color:#008000;">//</span><span style="color:#008000;">select name from table;</span>
<span style="color:#008080;">29</span>         df.select("name"<span style="color:#000000;">).show();
</span><span style="color:#008080;">30</span>         <span style="color:#008000;">//</span><span style="color:#008000;">select name,age+10 from table;</span>
<span style="color:#008080;">31</span>         df.select(df.col("name"),df.col("age").plus(10<span style="color:#000000;">)).show();
</span><span style="color:#008080;">32</span>         <span style="color:#008000;">//</span><span style="color:#008000;">select * from table where age &gt; 20</span>
<span style="color:#008080;">33</span>         df.filter(df.col("age").gt(20<span style="color:#000000;">)).show();
</span><span style="color:#008080;">34</span> <span style="color:#000000;">    }
</span><span style="color:#008080;">35</span> }</pre><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div></div></div></div><div><div> </div><div><span style="background-color:#c0ffff;">result:</span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164822048-207693703.png" alt="" border="0"><div> </div></div></div><div><strong>3.2.2   通过注册临时表，传入SQL语句（推荐使用）</strong></div><div> </div><div><span style="background-color:#c0ffff;">示例代码：</span></div><div><div class="cnblogs_code"><img class="code_img_opened" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" alt=""><div class="cnblogs_code_hide"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div><pre><span style="color:#008080;"> 1</span> <span style="color:#0000ff;">package</span><span style="color:#000000;"> com.hzf.spark.exercise;
</span><span style="color:#008080;"> 2</span> 
<span style="color:#008080;"> 3</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkConf;
</span><span style="color:#008080;"> 4</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaSparkContext;
</span><span style="color:#008080;"> 5</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrame;
</span><span style="color:#008080;"> 6</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.SQLContext;
</span><span style="color:#008080;"> 7</span> 
<span style="color:#008080;"> 8</span> <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span><span style="color:#000000;"> TestSparkSQL01 {
</span><span style="color:#008080;"> 9</span>     <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> main(String[] args) {
</span><span style="color:#008080;">10</span>         SparkConf conf = <span style="color:#0000ff;">new</span> SparkConf().setAppName("DataFrameOps").setMaster("local"<span style="color:#000000;">);
</span><span style="color:#008080;">11</span>         
<span style="color:#008080;">12</span>         JavaSparkContext sc = <span style="color:#0000ff;">new</span><span style="color:#000000;"> JavaSparkContext(conf);
</span><span style="color:#008080;">13</span>         SQLContext sqlContext = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SQLContext(sc);
</span><span style="color:#008080;">14</span>         
<span style="color:#008080;">15</span>         DataFrame df = sqlContext.read().json("people.json"<span style="color:#000000;">);
</span><span style="color:#008080;">16</span>         
<span style="color:#008080;">17</span>         <span style="color:#008000;">//</span><span style="color:#008000;">将DataFrame中封装的数据注册为一张临时表，对临时表进行sql操作</span>
<span style="color:#008080;">18</span>         df.registerTempTable("people"<span style="color:#000000;">);
</span><span style="color:#008080;">19</span>         DataFrame sql = sqlContext.sql("SELECT * FROM people WHERE age IS NOT NULL"<span style="color:#000000;">);
</span><span style="color:#008080;">20</span> <span style="color:#000000;">        sql.show(); 
</span><span style="color:#008080;">21</span> <span style="color:#000000;">    }
</span><span style="color:#008080;">22</span> }</pre><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div></div></div></div><div> </div><div><span style="background-color:#c0ffff;">result:</span><span style="background-color:#c0ffff;"><br></span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164822423-1727569018.png" alt="" border="0"></div><div> </div><div><strong style="line-height:1.6;">3.3创建DataFrame的几种方式，来源（json，jsonRDD，parquet，非json格式，mysql）</strong></div><div><strong style="line-height:1.6;"> </strong></div><div><strong style="line-height:1.6;">&lt;1&gt;读取Json格式文件--&gt;DataFrame：</strong><span style="line-height:1.6;"><span style="color:#ff0000;background-color:#ffff00;"><strong>Json 文件中不能有嵌套的格式</strong></span></span></div><div>      <strong>加载json格式文件--&gt;DataFrame有两种方式</strong>：</div><div>            <strong>方式一：</strong><span><strong><span style="background-color:#c0ffff;">DataFrame </span><span style="color:#6a3e3e;"><span style="background-color:#c0ffff;">df</span></span><span style="background-color:#c0ffff;"> = </span><span style="color:#6a3e3e;"><span style="background-color:#c0ffff;">sqlContext</span></span><span style="background-color:#c0ffff;">.read().format(</span><span style="color:#2a00ff;"><span style="background-color:#c0ffff;">"json"</span></span><span style="background-color:#c0ffff;">).load(</span><span style="color:#2a00ff;"><span style="background-color:#c0ffff;">"people.json"</span></span><span style="background-color:#c0ffff;">);</span></strong></span></div><div>          <strong> 方式二：<span><span style="background-color:#c0ffff;">DataFrame </span><span style="color:#6a3e3e;"><span style="background-color:#c0ffff;">df</span></span><span style="background-color:#c0ffff;"> = </span><span style="color:#6a3e3e;"><span style="background-color:#c0ffff;">sqlContext</span></span><span style="background-color:#c0ffff;">.read().json(</span><span style="color:#2a00ff;"><span style="background-color:#c0ffff;">"people.json"</span></span><span style="background-color:#c0ffff;">);</span></span></strong></div><div> </div><div><span style="background-color:#c0ffff;">数据集：</span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164823141-1853919396.png" alt="" border="0"><div> </div><div><span style="background-color:#c0ffff;">示例代码：</span></div><div><div class="cnblogs_code"><img class="code_img_opened" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" alt=""><div class="cnblogs_code_hide"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div><pre><span style="color:#008080;"> 1</span> <span style="color:#0000ff;">package</span><span style="color:#000000;"> com.bjsxt.java.spark.sql.json;
</span><span style="color:#008080;"> 2</span> 
<span style="color:#008080;"> 3</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> java.util.ArrayList;
</span><span style="color:#008080;"> 4</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> java.util.List;
</span><span style="color:#008080;"> 5</span> 
<span style="color:#008080;"> 6</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkConf;
</span><span style="color:#008080;"> 7</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkContext;
</span><span style="color:#008080;"> 8</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaPairRDD;
</span><span style="color:#008080;"> 9</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaRDD;
</span><span style="color:#008080;">10</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaSparkContext;
</span><span style="color:#008080;">11</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.function.Function;
</span><span style="color:#008080;">12</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.function.PairFunction;
</span><span style="color:#008080;">13</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrame;
</span><span style="color:#008080;">14</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.Row;
</span><span style="color:#008080;">15</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.RowFactory;
</span><span style="color:#008080;">16</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.SQLContext;
</span><span style="color:#008080;">17</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.types.DataTypes;
</span><span style="color:#008080;">18</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.types.StructField;
</span><span style="color:#008080;">19</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.types.StructType;
</span><span style="color:#008080;">20</span> 
<span style="color:#008080;">21</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> scala.Tuple2;
</span><span style="color:#008080;">22</span> 
<span style="color:#008080;">23</span> <span style="color:#008000;">/**</span>
<span style="color:#008080;">24</span> <span style="color:#008000;"> * JSON数据源
</span><span style="color:#008080;">25</span> <span style="color:#008000;"> * </span><span style="color:#808080;">@author</span><span style="color:#008000;"> Administrator
</span><span style="color:#008080;">26</span> <span style="color:#008000;"> *
</span><span style="color:#008080;">27</span>  <span style="color:#008000;">*/</span>
<span style="color:#008080;">28</span> <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span><span style="color:#000000;"> JSONDataSource {
</span><span style="color:#008080;">29</span> 
<span style="color:#008080;">30</span>     <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> main(String[] args) {
</span><span style="color:#008080;">31</span>         SparkConf conf = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SparkConf()
</span><span style="color:#008080;">32</span>                 .setAppName("JSONDataSource"<span style="color:#000000;">)
</span><span style="color:#008080;">33</span> <span style="color:#008000;">//</span><span style="color:#008000;">                .set("spark.default.parallelism", "100")</span>
<span style="color:#008080;">34</span>                 .setMaster("local"<span style="color:#000000;">);  
</span><span style="color:#008080;">35</span>         JavaSparkContext sc = <span style="color:#0000ff;">new</span><span style="color:#000000;"> JavaSparkContext(conf);
</span><span style="color:#008080;">36</span>         SQLContext sqlContext = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SQLContext(sc);
</span><span style="color:#008080;">37</span>         
<span style="color:#008080;">38</span>         DataFrame studentScoresDF = sqlContext.read().json("student.json"<span style="color:#000000;">);  
</span><span style="color:#008080;">39</span>         
<span style="color:#008080;">40</span>         studentScoresDF.registerTempTable("student_scores"<span style="color:#000000;">);
</span><span style="color:#008080;">41</span>         DataFrame goodStudentScoresDF =<span style="color:#000000;"> sqlContext.sql(
</span><span style="color:#008080;">42</span>                 "select name,count(score) from student_scores where score&gt;=80 group by name"<span style="color:#000000;">);
</span><span style="color:#008080;">43</span>         
<span style="color:#008080;">44</span>         List&lt;String&gt; goodStudentNames = goodStudentScoresDF.javaRDD().map(<span style="color:#0000ff;">new</span> Function&lt;Row, String&gt;<span style="color:#000000;">() {
</span><span style="color:#008080;">45</span>                     <span style="color:#0000ff;">private</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> <span style="color:#0000ff;">long</span> serialVersionUID = 1L<span style="color:#000000;">;
</span><span style="color:#008080;">46</span>                     
<span style="color:#008080;">47</span> <span style="color:#000000;">                    @Override
</span><span style="color:#008080;">48</span>                     <span style="color:#0000ff;">public</span> String call(Row row) <span style="color:#0000ff;">throws</span><span style="color:#000000;"> Exception {
</span><span style="color:#008080;">49</span>                         <span style="color:#0000ff;">return</span> row.getString(0<span style="color:#000000;">);
</span><span style="color:#008080;">50</span> <span style="color:#000000;">                    }
</span><span style="color:#008080;">51</span>                     
<span style="color:#008080;">52</span> <span style="color:#000000;">        }).collect();
</span><span style="color:#008080;">53</span>         
<span style="color:#008080;">54</span>         <span style="color:#0000ff;">for</span><span style="color:#000000;">(String str: goodStudentNames){
</span><span style="color:#008080;">55</span> <span style="color:#000000;">            System.out.println(str);
</span><span style="color:#008080;">56</span> <span style="color:#000000;">        }
</span><span style="color:#008080;">57</span> <span style="color:#000000;">    }
</span><span style="color:#008080;">58</span> }</pre><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div></div></div></div><div><div> </div><div><span style="background-color:#c0ffff;">result:</span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164824048-1023358200.png" alt="" border="0"><div> </div></div><div><strong>&lt;2&gt;jsonRDD--&gt;DataFrame</strong></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164825204-2118732035.png" alt="" border="0"><div> </div></div></div></div><div><span style="line-height:1.6;"><strong>&lt;3&gt;读取Parquet格式文件</strong></span><strong style="line-height:1.6;">--&gt;DataFrame：</strong><span style="line-height:1.6;">自动推测分区，合并 Schema。</span></div><div><div><strong><span style="color:#ff0000;">经验：</span>将Spark中的文本转换为Parquet以提升性能</strong></div><div><p style="padding:6px 0px;border:0px;vertical-align:baseline;">    <strong>parquet</strong>是一个基于列的存储格式，<strong>列式存储布局</strong>可以加速查询，<strong>因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储</strong>。</p><p style="padding:6px 0px;border:0px;vertical-align:baseline;">   如果在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！</p></div><div><strong><span style="background-color:#ffff00;">parquet的压缩比高，将一个普通的文本转化为parquet格式，如何去转？</span></strong></div><div>       val lineRDD = sc.textFile()<strong><br></strong></div><div>       DF.save(parquet) //<span style="line-height:1.6;">将RDD转化为DF</span></div></div><div><strong><span style="background-color:#ffffff;">parquet操作示例</span></strong></div><div><span style="line-height:1.6;">   </span><strong>是否指定format</strong>--若存储时，指定format为json格式，那么则生成json格式文件，否则不指定format,默认文件以parquet形式进行存储 </div><div><strong>测试一：指定format为json格式，存储在本地</strong></div><div><span style="background-color:#c0ffff;">测试数据:</span><span style="line-height:1.6;"><span style="background-color:#c0ffff;">   </span></span><span style="line-height:1.6;"><span style="background-color:#c0ffff;">top.txt</span></span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164825626-1509507347.png" alt="" border="0"></div><div><span style="background-color:#c0ffff;">测试代码</span></div><p><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164826407-591566056.png" alt="" border="0"></p><div><div><span style="background-color:#c0ffff;">测试结果</span></div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164826907-1728156653.png" alt="" border="0"><div> </div><div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164827438-208949336.png" alt="" border="0"></div><div> </div><div><strong>测试二：不指定format,那么文件默认以parquet形式进行存储，存储在本地</strong></div><div> </div><div><span style="line-height:1.6;background-color:#c0ffff;">测试数据:</span><span style="line-height:1.6;"><span style="background-color:#c0ffff;">   people.json</span></span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164827720-389377542.png" alt="" border="0"></div><div><span style="background-color:#c0ffff;">测试代码</span></div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164828126-1554934084.png" alt="" border="0"><div><div><span style="background-color:#c0ffff;">测试结果</span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164828360-342053479.png" alt="" border="0"></div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164828860-1242405187.png" alt="" border="0"><div><div> </div><div><div><strong>测试三：读取本地parquet存储格式的文件</strong></div></div></div></div></div><div><span style="background-color:#c0ffff;">测试代码</span><strong><br></strong></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164829157-350345041.png" alt="" border="0"><strong><strong><br></strong></strong><div><span style="font-weight:normal;background-color:#c0ffff;">测试结果</span></div><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164829376-1115532325.png" alt="" border="0"><div> </div></div><div><strong>测试四：读取hdfs上parquet形式的文件</strong><span style="font-weight:normal;background-color:#c0ffff;"><span style="background-color:#ffffff;"><br></span></span></div><div><strong><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164830251-58634137.png" alt="" border="0"></strong></div><div><span style="font-weight:normal;background-color:#c0ffff;">测试代码</span><strong><br></strong></div><div><span style="font-weight:normal;background-color:#c0ffff;"><span style="font-weight:normal;background-color:#c0ffff;"><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164830563-1140320329.png" alt="" border="0"></span></span></span><div><strong><span style="font-weight:normal;background-color:#c0ffff;">测试结果</span></strong></div><div><strong><span style="font-weight:normal;background-color:#c0ffff;"><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164831048-172652917.png" alt="" border="0"></span></span></strong></div><div><strong><span style="font-weight:normal;background-color:#c0ffff;"><span style="background-color:#ffffff;"> </span></span></strong></div></div></div></div><div><strong><span style="line-height:1.6;">&lt;4&gt;</span><span style="line-height:1.6;"> RDD（非json格式变成DataFrame）</span></strong></div><div><strong><span style="line-height:1.6;">读取</span><span style="line-height:1.6;">txt 文件</span>--&gt;DataFrame<span style="line-height:1.6;">：</span></strong><span style="line-height:1.6;">从 txt 文件读取，然后转为 RDD，最后转为 DataFrame</span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;"><strong><span style="background-color:#c0ffff;">RDD 转为 DataFrame 有两种方式</span></strong></span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;"><strong>(1)反射机制</strong>，</span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;"><strong><span style="color:#ff0000;">注意点：</span>自定义的类一定要是 public</strong>，并且要<strong>实现序列化接口 Serializable</strong>，</span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   <strong>取数据的时候，</strong></span><span style="line-height:1.6;"><strong>在 JavaAPI 中会有顺序问题</strong>（因为 DataFrame 转为 RDD&lt;Row&gt; 的时候，会进行一次字典排序改变 Row 的位置，而Scala 的 API 则没有这个问题）</span></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><span style="line-height:1.6;"><strong>(2)动态创建 Schema</strong>，先将<strong> RDD</strong> 中的每一行类型变 为<strong> RDD&lt;Row&gt; </strong>类型，然后创建 DataFrame 的元数据--&gt;<strong>构建 StructType</strong>，用于最后 DataFrame 元数据的描述，<strong>基于现有的 StructType 以及 RDD&lt;Row&gt; 来构造 DataFrame</strong>。(如果列的信息比较长可以存到数据库</span><span style="line-height:1.6;">里) </span></div><div><strong>&lt;4.1&gt;反射机制</strong><span style="line-height:1.6;"><br></span></div><div><span style="background-color:#c0ffff;">数据</span></div><div><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164831423-99862671.png" alt="" border="0"></span></div><div><span style="background-color:#c0ffff;">示例代码：</span><strong><br></strong></div><div><span style="background-color:#c0ffff;"><strong><span style="color:#000000;background-color:#ffffff;">自定义类</span></strong></span></div><div><span style="background-color:#ffffff;"><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164831845-1736143445.png" alt="" border="0"></span></span></div><div><div class="cnblogs_code"><img class="code_img_opened" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" alt=""><div class="cnblogs_code_hide"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div><pre><span style="color:#008080;"> 1</span> <span style="color:#0000ff;">package</span><span style="color:#000000;"> com.bjsxt.java.spark.sql.createdf;
</span><span style="color:#008080;"> 2</span> 
<span style="color:#008080;"> 3</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> java.util.List;
</span><span style="color:#008080;"> 4</span> 
<span style="color:#008080;"> 5</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkConf;
</span><span style="color:#008080;"> 6</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaRDD;
</span><span style="color:#008080;"> 7</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaSparkContext;
</span><span style="color:#008080;"> 8</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.function.Function;
</span><span style="color:#008080;"> 9</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrame;
</span><span style="color:#008080;">10</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.Row;
</span><span style="color:#008080;">11</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.SQLContext;
</span><span style="color:#008080;">12</span> 
<span style="color:#008080;">13</span> <span style="color:#008000;">/**</span>
<span style="color:#008080;">14</span> <span style="color:#008000;"> * 使用反射的方式将RDD转换成为DataFrame
</span><span style="color:#008080;">15</span> <span style="color:#008000;"> * 1、自定义的类必须是public
</span><span style="color:#008080;">16</span> <span style="color:#008000;"> * 2、自定义的类必须是可序列化的
</span><span style="color:#008080;">17</span> <span style="color:#008000;"> * 3、RDD转成DataFrame的时候，他会根据自定义类中的字段名进行排序。
</span><span style="color:#008080;">18</span> <span style="color:#008000;"> * </span><span style="color:#808080;">@author</span><span style="color:#008000;"> zfg
</span><span style="color:#008080;">19</span> <span style="color:#008000;"> *
</span><span style="color:#008080;">20</span>  <span style="color:#008000;">*/</span>
<span style="color:#008080;">21</span> 
<span style="color:#008080;">22</span> <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span><span style="color:#000000;"> RDD2DataFrameByReflection {
</span><span style="color:#008080;">23</span> 
<span style="color:#008080;">24</span>     <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> main(String[] args) {
</span><span style="color:#008080;">25</span>         SparkConf conf = <span style="color:#0000ff;">new</span> SparkConf().setMaster("local").setAppName("RDD2DataFrameByReflection"<span style="color:#000000;">);
</span><span style="color:#008080;">26</span>         JavaSparkContext sc = <span style="color:#0000ff;">new</span><span style="color:#000000;"> JavaSparkContext(conf);
</span><span style="color:#008080;">27</span>         SQLContext sqlcontext = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SQLContext(sc);
</span><span style="color:#008080;">28</span>         
<span style="color:#008080;">29</span>         JavaRDD&lt;String&gt; lines = sc.textFile("Peoples.txt"<span style="color:#000000;">);
</span><span style="color:#008080;">30</span>         JavaRDD&lt;Person&gt; personsRdd = lines.map(<span style="color:#0000ff;">new</span> Function&lt;String, Person&gt;<span style="color:#000000;">() {
</span><span style="color:#008080;">31</span>             
<span style="color:#008080;">32</span>             <span style="color:#0000ff;">private</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> <span style="color:#0000ff;">long</span> serialVersionUID = 1L<span style="color:#000000;">;
</span><span style="color:#008080;">33</span> 
<span style="color:#008080;">34</span> <span style="color:#000000;">            @Override
</span><span style="color:#008080;">35</span>             <span style="color:#0000ff;">public</span> Person call(String line) <span style="color:#0000ff;">throws</span><span style="color:#000000;"> Exception {
</span><span style="color:#008080;">36</span>                 String[] split = line.split(","<span style="color:#000000;">);
</span><span style="color:#008080;">37</span>                 Person p = <span style="color:#0000ff;">new</span><span style="color:#000000;"> Person();
</span><span style="color:#008080;">38</span>                 p.setId(Integer.valueOf(split[0<span style="color:#000000;">].trim()));
</span><span style="color:#008080;">39</span>                 p.setName(split[1<span style="color:#000000;">]);
</span><span style="color:#008080;">40</span>                 p.setAge(Integer.valueOf(split[2<span style="color:#000000;">].trim()));
</span><span style="color:#008080;">41</span>                 <span style="color:#0000ff;">return</span><span style="color:#000000;"> p;
</span><span style="color:#008080;">42</span> <span style="color:#000000;">            }
</span><span style="color:#008080;">43</span> <span style="color:#000000;">        });
</span><span style="color:#008080;">44</span>         
<span style="color:#008080;">45</span>         <span style="color:#008000;">//</span><span style="color:#008000;">传入进去Person.class的时候，sqlContext是通过反射的方式创建DataFrame
</span><span style="color:#008080;">46</span>         <span style="color:#008000;">//</span><span style="color:#008000;">在底层通过反射的方式或得Person的所有field，结合RDD本身，就生成了DataFrame</span>
<span style="color:#008080;">47</span>         DataFrame df = sqlcontext.createDataFrame(personsRdd, Person.<span style="color:#0000ff;">class</span><span style="color:#000000;">);
</span><span style="color:#008080;">48</span>      
<span style="color:#008080;">49</span>         <span style="color:#008000;">//</span><span style="color:#008000;">命名table的名字为person</span>
<span style="color:#008080;">50</span>         df.registerTempTable("personTable"<span style="color:#000000;">);
</span><span style="color:#008080;">51</span>         
<span style="color:#008080;">52</span>         DataFrame resultDataFrame = sqlcontext.sql("select * from personTable where age &gt; 7"<span style="color:#000000;">);
</span><span style="color:#008080;">53</span> <span style="color:#000000;">        resultDataFrame.show();
</span><span style="color:#008080;">54</span>         
<span style="color:#008080;">55</span>          <span style="color:#008000;">//</span><span style="color:#008000;">将df转成rdd</span>
<span style="color:#008080;">56</span>         JavaRDD&lt;Row&gt; resultRDD =<span style="color:#000000;"> resultDataFrame.javaRDD();
</span><span style="color:#008080;">57</span>         JavaRDD&lt;Person&gt; result = resultRDD.map(<span style="color:#0000ff;">new</span> Function&lt;Row, Person&gt;<span style="color:#000000;">() {
</span><span style="color:#008080;">58</span>             
<span style="color:#008080;">59</span>             <span style="color:#0000ff;">private</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> <span style="color:#0000ff;">long</span> serialVersionUID = 1L<span style="color:#000000;">;
</span><span style="color:#008080;">60</span> 
<span style="color:#008080;">61</span> <span style="color:#000000;">            @Override
</span><span style="color:#008080;">62</span>             <span style="color:#0000ff;">public</span> Person call(Row row) <span style="color:#0000ff;">throws</span><span style="color:#000000;"> Exception {
</span><span style="color:#008080;">63</span>                  Person p = <span style="color:#0000ff;">new</span><span style="color:#000000;"> Person();
</span><span style="color:#008080;">64</span>                  p.setAge(row.getInt(0<span style="color:#000000;">));
</span><span style="color:#008080;">65</span>                  p.setId(row.getInt(1<span style="color:#000000;">));
</span><span style="color:#008080;">66</span>                  p.setName(row.getString(2<span style="color:#000000;">));
</span><span style="color:#008080;">67</span>                 <span style="color:#0000ff;">return</span><span style="color:#000000;"> p;
</span><span style="color:#008080;">68</span> <span style="color:#000000;">            }
</span><span style="color:#008080;">69</span> <span style="color:#000000;">        });
</span><span style="color:#008080;">70</span>         
<span style="color:#008080;">71</span>          List&lt;Person&gt; personList =<span style="color:#000000;"> result.collect();
</span><span style="color:#008080;">72</span>          
<span style="color:#008080;">73</span>          <span style="color:#0000ff;">for</span><span style="color:#000000;"> (Person person : personList) {
</span><span style="color:#008080;">74</span> <span style="color:#000000;">            System.out.println(person.toString());
</span><span style="color:#008080;">75</span> <span style="color:#000000;">        } 
</span><span style="color:#008080;">76</span> <span style="color:#000000;">    }
</span><span style="color:#008080;">77</span> }</pre><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div></div></div></div><div> </div><div> </div><div><span style="background-color:#c0ffff;">result：</span><span style="background-color:#c0ffff;"><br></span></div><div><span style="background-color:#c0ffff;"><span style="background-color:#c0ffff;"><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164832501-1545746097.png" alt="" border="0"></span></span></span><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164832782-918466585.png" alt="" border="0"></div><span style="background-color:#c0ffff;"><br></span></div><div><strong>&lt;4.2&gt;动态创建Schema方式</strong></div><div><div><span style="background-color:#c0ffff;">数据</span></div><div><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164831423-99862671.png" alt="" border="0"></span></div></div><div> </div><div><span style="background-color:#c0ffff;">示例代码：</span></div><div><div class="cnblogs_code"><img class="code_img_opened" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" alt=""><div class="cnblogs_code_hide"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div><pre><span style="color:#008080;"> 1</span> <span style="color:#0000ff;">package</span><span style="color:#000000;"> com.bjsxt.java.spark.sql.createdf;
</span><span style="color:#008080;"> 2</span> 
<span style="color:#008080;"> 3</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> java.util.ArrayList;
</span><span style="color:#008080;"> 4</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> java.util.List;
</span><span style="color:#008080;"> 5</span> 
<span style="color:#008080;"> 6</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkConf;
</span><span style="color:#008080;"> 7</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaRDD;
</span><span style="color:#008080;"> 8</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaSparkContext;
</span><span style="color:#008080;"> 9</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.function.Function;
</span><span style="color:#008080;">10</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrame;
</span><span style="color:#008080;">11</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.Row;
</span><span style="color:#008080;">12</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.RowFactory;
</span><span style="color:#008080;">13</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.SQLContext;
</span><span style="color:#008080;">14</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.types.DataTypes;
</span><span style="color:#008080;">15</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.types.StructField;
</span><span style="color:#008080;">16</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.types.StructType;
</span><span style="color:#008080;">17</span> 
<span style="color:#008080;">18</span> 
<span style="color:#008080;">19</span> <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span><span style="color:#000000;"> RDD2DataFrameByProgrammatically {
</span><span style="color:#008080;">20</span>     <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> main(String[] args) {
</span><span style="color:#008080;">21</span>         SparkConf conf = <span style="color:#0000ff;">new</span> SparkConf().setMaster("local").setAppName("RDD2DataFrameByReflection"<span style="color:#000000;">);
</span><span style="color:#008080;">22</span>         JavaSparkContext sc = <span style="color:#0000ff;">new</span><span style="color:#000000;"> JavaSparkContext(conf);
</span><span style="color:#008080;">23</span>         SQLContext sqlcontext = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SQLContext(sc);
</span><span style="color:#008080;">24</span>         <span style="color:#008000;">/**</span>
<span style="color:#008080;">25</span> <span style="color:#008000;">         * 在RDD的基础上创建类型为Row的RDD
</span><span style="color:#008080;">26</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">27</span>         JavaRDD&lt;String&gt; lines = sc.textFile("Peoples.txt"<span style="color:#000000;">);
</span><span style="color:#008080;">28</span>         JavaRDD&lt;Row&gt; rowRDD = lines.map(<span style="color:#0000ff;">new</span> Function&lt;String, Row&gt;<span style="color:#000000;">() {
</span><span style="color:#008080;">29</span>             
<span style="color:#008080;">30</span>             <span style="color:#0000ff;">private</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> <span style="color:#0000ff;">long</span> serialVersionUID = 1L<span style="color:#000000;">;
</span><span style="color:#008080;">31</span> 
<span style="color:#008080;">32</span> <span style="color:#000000;">            @Override
</span><span style="color:#008080;">33</span>             <span style="color:#0000ff;">public</span> Row call(String line) <span style="color:#0000ff;">throws</span><span style="color:#000000;"> Exception {
</span><span style="color:#008080;">34</span>                 String[] split = line.split(","<span style="color:#000000;">);
</span><span style="color:#008080;">35</span>                 <span style="color:#0000ff;">return</span> RowFactory.create(Integer.valueOf(split[0]),split[1],Integer.valueOf(split[2<span style="color:#000000;">]));
</span><span style="color:#008080;">36</span> <span style="color:#000000;">            }
</span><span style="color:#008080;">37</span> <span style="color:#000000;">        });
</span><span style="color:#008080;">38</span>         
<span style="color:#008080;">39</span>         <span style="color:#008000;">/**</span>
<span style="color:#008080;">40</span> <span style="color:#008000;">         * 动态构造DataFrame的元数据，一般而言，有多少列以及每列的具体类型可能来自于Json，也可能来自于DB
</span><span style="color:#008080;">41</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">42</span>         ArrayList&lt;StructField&gt; structFields = <span style="color:#0000ff;">new</span> ArrayList&lt;StructField&gt;<span style="color:#000000;">();
</span><span style="color:#008080;">43</span>         structFields.add(DataTypes.createStructField("id", DataTypes.IntegerType, <span style="color:#0000ff;">true</span><span style="color:#000000;">));
</span><span style="color:#008080;">44</span>         structFields.add(DataTypes.createStructField("name", DataTypes.StringType, <span style="color:#0000ff;">true</span><span style="color:#000000;">));
</span><span style="color:#008080;">45</span>         structFields.add(DataTypes.createStructField("age", DataTypes.IntegerType, <span style="color:#0000ff;">true</span><span style="color:#000000;">));
</span><span style="color:#008080;">46</span>         <span style="color:#008000;">//</span><span style="color:#008000;">构建StructType，用于最后DataFrame元数据的描述</span>
<span style="color:#008080;">47</span>         StructType schema =<span style="color:#000000;"> DataTypes.createStructType(structFields);
</span><span style="color:#008080;">48</span>         
<span style="color:#008080;">49</span>         <span style="color:#008000;">/**</span>
<span style="color:#008080;">50</span> <span style="color:#008000;">         * 基于已有的MetaData以及RDD&lt;Row&gt; 来构造DataFrame
</span><span style="color:#008080;">51</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">52</span>         DataFrame df =<span style="color:#000000;"> sqlcontext.createDataFrame(rowRDD, schema);
</span><span style="color:#008080;">53</span>         
<span style="color:#008080;">54</span>         <span style="color:#008000;">/**</span>
<span style="color:#008080;">55</span> <span style="color:#008000;">         *注册成为临时表以供后续的SQL操作查询
</span><span style="color:#008080;">56</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">57</span>         df.registerTempTable("persons"<span style="color:#000000;">);
</span><span style="color:#008080;">58</span>         
<span style="color:#008080;">59</span>         <span style="color:#008000;">/**</span>
<span style="color:#008080;">60</span> <span style="color:#008000;">         * 进行数据的多维度分析
</span><span style="color:#008080;">61</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">62</span>         DataFrame result = sqlcontext.sql("select * from persons where age &gt; 7"<span style="color:#000000;">);
</span><span style="color:#008080;">63</span> <span style="color:#000000;">        result.show();
</span><span style="color:#008080;">64</span> 
<span style="color:#008080;">65</span>         <span style="color:#008000;">/**</span>
<span style="color:#008080;">66</span> <span style="color:#008000;">         * 对结果进行处理，包括由DataFrame转换成为RDD&lt;Row&gt;
</span><span style="color:#008080;">67</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">68</span>          List&lt;Row&gt; listRow =<span style="color:#000000;"> result.javaRDD().collect();
</span><span style="color:#008080;">69</span>          <span style="color:#0000ff;">for</span><span style="color:#000000;"> (Row row : listRow) {
</span><span style="color:#008080;">70</span> <span style="color:#000000;">            System.out.println(row);
</span><span style="color:#008080;">71</span> <span style="color:#000000;">        }
</span><span style="color:#008080;">72</span> <span style="color:#000000;">    }
</span><span style="color:#008080;">73</span> }</pre><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div></div></div></div><div><div> </div><div><span style="background-color:#c0ffff;">result：</span></div><div><span style="background-color:#c0ffff;"><span style="background-color:#c0ffff;"><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164833516-762230096.png" alt="" border="0"></span></span></span><div><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164833735-1426054821.png" alt="" border="0"></div><span style="background-color:#c0ffff;"><br></span></div></div><div><strong style="line-height:1.6;"><span style="line-height:1.6;">&lt;5&gt;</span><span style="line-height:1.6;"> 读取MySql 中表里的数据</span>--&gt;DataFrame</strong></div><div><div>       <strong>Spark Build-in</strong>内置支持的<strong>json jdbc mysql,hive...如果数据库支持jdbc连接，Spark 就可以基于这个数据库尽行数据的处理</strong></div><div> </div><div><span style="background-color:#c0ffff;">示例代码：</span></div><div><div class="cnblogs_code"><img class="code_img_opened" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" alt=""><div class="cnblogs_code_hide"><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div><pre><span style="color:#008080;"> 1</span> <span style="color:#0000ff;">package</span><span style="color:#000000;"> com.bjsxt.java.spark.sql.jdbc;
</span><span style="color:#008080;"> 2</span> 
<span style="color:#008080;"> 3</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.SparkConf;
</span><span style="color:#008080;"> 4</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.api.java.JavaSparkContext;
</span><span style="color:#008080;"> 5</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrame;
</span><span style="color:#008080;"> 6</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.DataFrameReader;
</span><span style="color:#008080;"> 7</span> <span style="color:#0000ff;">import</span><span style="color:#000000;"> org.apache.spark.sql.SQLContext;
</span><span style="color:#008080;"> 8</span> 
<span style="color:#008080;"> 9</span> <span style="color:#008000;">/**</span>
<span style="color:#008080;">10</span> <span style="color:#008000;"> * JDBC数据源
</span><span style="color:#008080;">11</span> <span style="color:#008000;"> * 
</span><span style="color:#008080;">12</span> <span style="color:#008000;"> * </span><span style="color:#808080;">@author</span><span style="color:#008000;"> Administrator
</span><span style="color:#008080;">13</span> <span style="color:#008000;"> *
</span><span style="color:#008080;">14</span>  <span style="color:#008000;">*/</span>
<span style="color:#008080;">15</span> <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span><span style="color:#000000;"> JDBCDataSource {
</span><span style="color:#008080;">16</span> 
<span style="color:#008080;">17</span>     <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> main(String[] args) {
</span><span style="color:#008080;">18</span>         SparkConf conf = <span style="color:#0000ff;">new</span> SparkConf().setAppName("JDBCDataSource").setMaster("local"<span style="color:#000000;">);
</span><span style="color:#008080;">19</span>         JavaSparkContext sc = <span style="color:#0000ff;">new</span><span style="color:#000000;"> JavaSparkContext(conf);
</span><span style="color:#008080;">20</span>         SQLContext sqlContext = <span style="color:#0000ff;">new</span><span style="color:#000000;"> SQLContext(sc);
</span><span style="color:#008080;">21</span>         
<span style="color:#008080;">22</span>         <span style="color:#008000;">//</span><span style="color:#008000;"> 方法1、分别将mysql中两张表的数据加载为DataFrame</span>
<span style="color:#008080;">23</span>         <span style="color:#008000;">/*</span>
<span style="color:#008080;">24</span> <span style="color:#008000;">         * Map&lt;String, String&gt; options = new HashMap&lt;String, String&gt;();
</span><span style="color:#008080;">25</span> <span style="color:#008000;">         * options.put("url", "jdbc:mysql://hadoop1:3306/testdb");
</span><span style="color:#008080;">26</span> <span style="color:#008000;">         * options.put("driver", "com.mysql.jdbc.Driver"); 
</span><span style="color:#008080;">27</span> <span style="color:#008000;">         * options.put("user","spark");
</span><span style="color:#008080;">28</span> <span style="color:#008000;">         * options.put("password", "spark2016");
</span><span style="color:#008080;">29</span>          
<span style="color:#008080;">30</span> <span style="color:#008000;">         * options.put("dbtable", "student_info"); 
</span><span style="color:#008080;">31</span> <span style="color:#008000;">         * DataFrame studentInfosDF = sqlContext.read().format("jdbc").options(options).load();
</span><span style="color:#008080;">32</span> <span style="color:#008000;">         * options.put("dbtable", "student_score"); 
</span><span style="color:#008080;">33</span> <span style="color:#008000;">         * DataFrame studentScoresDF = sqlContext.read().format("jdbc") .options(options).load();
</span><span style="color:#008080;">34</span>          <span style="color:#008000;">*/</span>
<span style="color:#008080;">35</span>         <span style="color:#008000;">//</span><span style="color:#008000;"> 方法2、分别将mysql中两张表的数据加载为DataFrame</span>
<span style="color:#008080;">36</span>         DataFrameReader reader = sqlContext.read().format("jdbc"<span style="color:#000000;">);
</span><span style="color:#008080;">37</span>         reader.option("url", "jdbc:mysql://node4:3306/testdb"<span style="color:#000000;">);
</span><span style="color:#008080;">38</span>         reader.option("driver", "com.mysql.jdbc.Driver"<span style="color:#000000;">);
</span><span style="color:#008080;">39</span>         reader.option("user", "root"<span style="color:#000000;">);
</span><span style="color:#008080;">40</span>         reader.option("password", "123"<span style="color:#000000;">);
</span><span style="color:#008080;">41</span>         
<span style="color:#008080;">42</span>         reader.option("dbtable", "student_info"<span style="color:#000000;">);
</span><span style="color:#008080;">43</span>         DataFrame studentInfosDF =<span style="color:#000000;"> reader.load();
</span><span style="color:#008080;">44</span>         reader.option("dbtable", "student_score"<span style="color:#000000;">);
</span><span style="color:#008080;">45</span>         DataFrame studentScoresDF =<span style="color:#000000;"> reader.load();
</span><span style="color:#008080;">46</span>         
<span style="color:#008080;">47</span>         <span style="color:#008000;">//</span><span style="color:#008000;"> 将两个DataFrame转换为JavaPairRDD，执行join操作</span>
<span style="color:#008080;">48</span>         studentInfosDF.registerTempTable("studentInfos"<span style="color:#000000;">);
</span><span style="color:#008080;">49</span>         studentScoresDF.registerTempTable("studentScores"<span style="color:#000000;">);
</span><span style="color:#008080;">50</span>         
<span style="color:#008080;">51</span>         String sql = "SELECT studentInfos.name,age,score "
<span style="color:#008080;">52</span>                 + "        FROM studentInfos JOIN studentScores"
<span style="color:#008080;">53</span>                 + "         ON (studentScores.name = studentInfos.name)"
<span style="color:#008080;">54</span>                 + "     WHERE studentScores.score &gt; 80"<span style="color:#000000;">;
</span><span style="color:#008080;">55</span>         
<span style="color:#008080;">56</span>         DataFrame sql2 =<span style="color:#000000;"> sqlContext.sql(sql);
</span><span style="color:#008080;">57</span> <span style="color:#000000;">        sql2.show();
</span><span style="color:#008080;">58</span> <span style="color:#000000;">    }
</span><span style="color:#008080;">59</span> }</pre><div class="cnblogs_code_toolbar"><span class="cnblogs_code_copy"><a title="复制代码"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></span></div></div></div><p> </p><p><span style="background-color:#c0ffff;">result:</span></p></div><div><div style="font-weight:bold;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164834251-798093763.png" alt="" border="0"></div><div><h4 style="font-weight:bold;"><strong style="line-height:1.6;">4. 如何将DataFrame中的值写入到外部存储中去？</strong></h4><div><span style="background-color:#ffff00;"><strong>存储模式</strong>（<strong>SaveMode.Overwrite || Ignore || Ap<span style="background-color:#ffff00;">pend</span></strong></span><span style="line-height:1.6;"><span style="background-color:#ffff00;">  <strong> ||</strong></span></span><span style="line-height:1.6;"><span style="background-color:#ffff00;"><strong>   ErrorifExit）</strong></span></span></div><div style="font-weight:bold;"><strong style="line-height:1.6;"><span style="line-height:1.6;">&lt;1&gt;</span><span style="line-height:1.6;"> 读取本地json格式文件，并以json形式写入到hdfs（不指定format,默认是parquet）</span></strong></div><div><span style="background-color:#c0ffff;">测试代码</span></div></div><div><span style="background-color:#c0ffff;"><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164834516-2052128823.png" alt="" border="0"><br></span>测试结果<br></span></div><div><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164834813-748634451.png" alt="" border="0"></span></div><div><span style="background-color:#ffffff;"><img src="https://images2015.cnblogs.com/blog/1008304/201703/1008304-20170305164835563-1848618863.png" alt="" border="0"></span></div><div style="font-weight:bold;"> </div></div></div><div><div><strong style="line-height:1.6;">补充：</strong></div><div><strong style="line-height:1.6;">1.什么是下推过滤器？</strong></div><div>       在join之前过滤，而不是join之后进行过滤</div></div><div><strong> </strong></div><div><strong>2.select * from table 在SparkSQL和Hive on MR中的区别?</strong></div><div><span style="line-height:1.6;">   </span><span style="line-height:1.6;">   </span><strong style="line-height:1.6;">SparkSQL </strong><span style="line-height:1.6;">中 select * from table 在spark中是要具体执行spark任务的，而在</span><span style="line-height:1.6;"> </span><strong style="line-height:1.6;">Hive on MR</strong><span style="line-height:1.6;"> </span><span style="line-height:1.6;">中 select * from table直接读取数据，所以</span><strong style="line-height:1.6;">SparkSQL </strong><span style="line-height:1.6;">中执行select * from不一定比</span><strong style="line-height:1.6;">Hive on MR</strong><span style="line-height:1.6;">中的快</span></div><div><span style="line-height:1.6;"> </span></div><div><strong>3.如何将一个DataFrame变成一个RDD？</strong><span style="line-height:1.6;"><br></span></div><div><strong>    <span style="background-color:#c0ffff;">JavaRDD&lt;ROW&gt; rdd = resultFrame.javaRDD()</span></strong></div><div><strong><span style="background-color:#c0ffff;"> </span></strong></div><h4><strong>5.整合Spark和Hive?</strong></h4><div>    6.1Spark 目录下面的 <strong>conf </strong>下放一个配置文件 <strong>hive-site.xml</strong> 文件。</div><div>    6.2在 hive 的服务端<strong>启动 MetaStore Server【</strong>因为 HiveContext 会用到 metastore 服务。(<span style="background-color:#c0ffff;">在 Spark-shell 里面使用 HiveContext 的时候，要记住导入 HiveContext</span>）】（hive --service metastore）</div><div>    6.3<strong>启动hdfs【</strong>因为hive的数据是存在hdfs上的<strong>】</strong>和<strong>Spark集群</strong>（start-all.sh spark-start-all.sh）</div><div>    6.4进入<strong>Spark shell</strong>,测试Spark 和 Hive是否整合成功</div><div><div><ol class="linenums"><li class="L0"><code class="language-java"><span class="pln">scala</span><span class="pun">&gt;</span><span class="kwd">import</span><span class="pln"> org</span><span class="pun">.</span><span class="pln">apache</span><span class="pun">.</span><span class="pln">spark</span><span class="pun">.</span><span class="pln">sql</span><span class="pun">.</span><span class="pln">hive</span><span class="pun">.</span><span class="typ">HiveContext</span></code></li><li class="L1"><code class="language-java"><span class="pln">scala</span><span class="pun">&gt;</span><span class="pln">val hiveContext </span><span class="pun">=</span><span class="kwd">new</span><span class="typ">HiveContext</span><span class="pun">(</span><span class="pln">sc</span><span class="pun">)</span></code></li><li class="L2"><code class="language-java"><span class="pln">scala</span><span class="pun">&gt;</span><span class="pln">hiveContext</span><span class="pun">.</span><span class="pln">sql</span><span class="pun">(</span><span class="str">"show tables"</span><span class="pun">).</span><span class="pln">show</span></code></li></ol></div><div><div><span><span style="line-height:1.6;">    </span><span style="line-height:1.6;">6.5</span>整合测试（详见Spark_some配置）<strong>，<span style="background-color:#c0ffff;">注意！将代码提交到Spark集群上运行时，需要将hdfs-site.xml拷贝到SPARK_HOME/conf下</span></strong></span></div><h4><strong>6.SqlContext和HiveContext的关系？</strong></h4><div>       <strong>SQLcontext </strong>是 <strong>HiveContext </strong>的父类</div><div>       在<strong>集群中</strong>运行的时候用 <strong>HiveContext</strong>，可以<strong>基于 Hive </strong>来操作 Hive 表，对源数据进行CRUD的操作。 </div></div></div><br>            </div>
                </div>