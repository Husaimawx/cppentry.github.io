---
layout:     post
title:      Hadoop入门
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <div class="iteye-blog-content-contain" style="font-size:14px;">
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">Hadoop核心</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">        Hadoop的核心就是HDFS和MapReduce，而两者只是理论基础，不是具体可使用的高级应用，Hadoop旗下有很多经典子项目，比如HBase、Hive等，这些都是基于HDFS和MapReduce发展出来的。要想了解Hadoop，就必须知道HDFS和MapReduce是什么。</span></p>
<p> </p>
<ul><li><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">HDFS</span></li>
</ul><p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">HDFS（Hadoop Distributed File System，Hadoop分布式文件系统），它是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。</span></p>
<p> </p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">HDFS的设计特点是：</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">1、大数据文件，非常适合上T级别的大文件或者一堆大数据文件的存储，如果文件只有几个G甚至更小就没啥意思了。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">2、文件分块存储，HDFS会将一个完整的大文件平均分块存储到不同计算器上，它的意义在于读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多得都。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">3、流式数据访问，一次写入多次读写，这种模式跟传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化也只能在文件末添加内容。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">4、廉价硬件，HDFS可以应用在普通PC机上，这种机制能够让给一些公司用几十台廉价的计算机就可以撑起一个大数据集群。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">5、硬件故障，HDFS认为所有计算机都可能会出问题，为了防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其它某几个主机上，如果其中一台主机失效，可以迅速找另一块副本取文件。</span></p>
<p> </p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">HDFS的关键元素：</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">Block：将一个文件进行分块，通常是64M。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">NameNode：保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式----如果主NameNode失效，启动备用主机运行NameNode。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">DataNode：分布在廉价的计算机上，用于存储Block块文件。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;"><img src="http://dl2.iteye.com/upload/attachment/0099/4698/4bd65132-5351-38d9-91f3-351b3625f77b.jpg" alt=""></span><br><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;"> </span></p>
<p> </p>
<ul><li><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">MapReduce</span></li>
</ul><p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">通俗说MapReduce是一套从海量·源数据提取分析元素最后返回结果集的编程模型，将文件分布式存储到硬盘是第一步，而从海量数据中提取分析我们需要的内容就是MapReduce做的事了。</span></p>
<p> </p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">下面以一个计算海量数据最大值为例：一个银行有上亿储户，银行希望找到存储金额最高的金额是多少，按照传统的计算方式，我们会这样：</span></p>
<div class="bar">
<div class="tools">Java代码 <a title="复制代码" href="http://blessht.iteye.com/blog/2095675" rel="nofollow"><img src="http://blessht.iteye.com/images/icon_copy.gif" alt="复制代码"></a> <a title="收藏这段代码"><img class="spinner" src="http://blessht.iteye.com/images/spinner.gif" alt=""></a><span> </span>
</div>
</div>
<pre><code class="language-java">Long moneys[] ...
Long max = 0L;
for(int i=0;i&lt;moneys.length;i++){
  if(moneys[i]&gt;max){
    max = moneys[i];
  }
}</code></pre>
<p> </p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;"> 如果计算的数组长度少的话，这样实现是不会有问题的，还是面对海量数据的时候就会有问题。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">MapReduce会这样做：首先数字是分布存储在不同块中的，以某几个块为一个Map，计算出Map中最大的值，然后将每个Map中的最大值做Reduce操作，Reduce再取最大值给用户。</span></p>
<p><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;"><img src="http://dl2.iteye.com/upload/attachment/0099/4705/4c674209-2c2a-3f67-ad07-8b24a4cdd205.jpg" alt=""></span><br><span style="font-family:'Microsoft YaHei', '微软雅黑', SimHei, tahoma, arial, helvetica, sans-serif;">        MapReduce的基本原理就是：将大的数据分析分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。当然怎么分块分析，怎么做Reduce操作非常复杂，Hadoop已经提供了数据分析的实现，我们只需要编写简单的需求命令即可达成我们想要的数据。</span></p>
<p> </p>
</div>            </div>
                </div>