---
layout:     post
title:      spark 环境搭建及几种模式测试
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="spark安装部署">spark安装部署</h1>

<p>标签（空格分隔）： spark</p>

<hr>



<h1 id="hadoopsparkkafka交流群224209501">hadoop,spark,kafka交流群：224209501</h1>

<h2 id="1spark环境的安装">1，spark环境的安装</h2>

<p>创建四个目录</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">sudo</span> mkdir /opt/modules
<span class="hljs-built_in">sudo</span> mkdir /opt/softwares
<span class="hljs-built_in">sudo</span> mkdir /opt/tools
<span class="hljs-built_in">sudo</span> mkdir /opt/datas

<span class="hljs-built_in">sudo</span> chmod <span class="hljs-number">777</span> -R /opt/</code></pre>



<h3 id="1安装jdk17">1，安装jdk1.7</h3>

<p>先卸载自带的jdk</p>



<pre class="prettyprint"><code class=" hljs bash">rpm –qa | grep java

<span class="hljs-built_in">sudo</span> rpm <span class="hljs-operator">-e</span> --nodeps (自带java包)</code></pre>

<p>安装jdk1.7</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-keyword">export</span> JAVA_HOME=/opt/modules/jdk1.<span class="hljs-number">7.0</span>_67
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$JAVA_HOME</span>/bin</code></pre>



<h2 id="2spark编译">2，spark编译</h2>

<p>安装mvn</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-keyword">export</span> MAVEN_HOME=/usr/local/apache-maven-<span class="hljs-number">3.0</span>.<span class="hljs-number">5</span>
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$MAVEN_HOME</span>/bin</code></pre>



<h3 id="3安装scala">3，安装scala</h3>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-keyword">export</span> SCALA_HOME=/opt/modules/scala-<span class="hljs-number">2.10</span>.<span class="hljs-number">4</span>
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$SCALA_HOME</span>/bin</code></pre>



<h3 id="4修改mvn镜像源">4，修改mvn镜像源</h3>

<p>编译之前先配置镜像及域名服务器，来提高下载速度，进而提高编译速度，用nodepad++打开/opt/compileHadoop/apache-maven-3.0.5/conf/setting.xml。（nodepad已经通过sftp链接到了机器）</p>



<pre class="prettyprint"><code class=" hljs xml">  <span class="hljs-tag">&lt;<span class="hljs-title">mirror</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">id</span>&gt;</span>nexus-spring<span class="hljs-tag">&lt;/<span class="hljs-title">id</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">mirrorOf</span>&gt;</span>cdh.repo<span class="hljs-tag">&lt;/<span class="hljs-title">mirrorOf</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>spring<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">url</span>&gt;</span>http://repo.spring.io/repo/<span class="hljs-tag">&lt;/<span class="hljs-title">url</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-title">mirror</span>&gt;</span>

  <span class="hljs-tag">&lt;<span class="hljs-title">mirror</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">id</span>&gt;</span>nexus-spring2<span class="hljs-tag">&lt;/<span class="hljs-title">id</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">mirrorOf</span>&gt;</span>cdh.releases.repo<span class="hljs-tag">&lt;/<span class="hljs-title">mirrorOf</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>spring2<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-title">url</span>&gt;</span>http://repo.spring.io/repo/<span class="hljs-tag">&lt;/<span class="hljs-title">url</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-title">mirror</span>&gt;</span></code></pre>



<h3 id="5配置域名解析服务器">5，配置域名解析服务器</h3>



<pre class="prettyprint"><code class=" hljs nginx"><span class="hljs-title">sudo</span> vi /etc/resolv.conf
添加内容：
    nameserver <span class="hljs-number">8.8.8.8</span>
    nameserver <span class="hljs-number">8.8.4.4</span></code></pre>



<h3 id="6编译spark">6，编译spark</h3>

<p>为了提高编译速度，修改如下内容</p>



<pre class="prettyprint"><code class=" hljs ruleslanguage">VERSION=<span class="hljs-number">1.3</span><span class="hljs-number">.0</span>
SPARK_HADOOP_VERSION=<span class="hljs-number">2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.4</span><span class="hljs-number">.0</span>
SPARK_HIVE=<span class="hljs-number">1</span>
<span class="hljs-array">#VERSION</span>=$(<span class="hljs-string">"$MVN"</span> help:evaluate -Dexpression=project.version <span class="hljs-number">2</span>&gt;/dev/null | grep -v <span class="hljs-string">"INFO"</span> | tail -n <span class="hljs-number">1</span>)
<span class="hljs-array">#SPARK</span>_HADOOP_VERSION=$(<span class="hljs-string">"$MVN"</span> help:evaluate -Dexpression=hadoop.version $@ <span class="hljs-number">2</span>&gt;/dev/null\
<span class="hljs-array">#    </span>| grep -v <span class="hljs-string">"INFO"</span>\
<span class="hljs-array">#    </span>| tail -n <span class="hljs-number">1</span>)
<span class="hljs-array">#SPARK</span>_HIVE=$(<span class="hljs-string">"$MVN"</span> help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ <span class="hljs-number">2</span>&gt;/dev/null\
<span class="hljs-array">#    </span>| grep -v <span class="hljs-string">"INFO"</span>\
<span class="hljs-array">#    </span>| fgrep --count <span class="hljs-string">"&lt;id&gt;hive&lt;/id&gt;"</span>;\
<span class="hljs-array">#    </span><span class="hljs-array"># Reset exit status to </span><span class="hljs-number">0</span>, otherwise the script stops here if the last grep finds nothing\
<span class="hljs-array">#    </span><span class="hljs-array"># because we use </span><span class="hljs-string">"set -o pipefail"</span>
<span class="hljs-array">#    echo </span>-n)</code></pre>

<p>执行编译指令：</p>



<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-built_in">.</span>/make<span class="hljs-attribute">-distribution</span><span class="hljs-built_in">.</span>sh <span class="hljs-subst">--</span>tgz <span class="hljs-attribute">-Pyarn</span> <span class="hljs-attribute">-Phadoop</span><span class="hljs-subst">-</span><span class="hljs-number">2.4</span> <span class="hljs-attribute">-Dhadoop</span><span class="hljs-built_in">.</span>version<span class="hljs-subst">=</span><span class="hljs-number">2.6</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.4</span><span class="hljs-number">.0</span> <span class="hljs-attribute">-Phive</span><span class="hljs-subst">-</span><span class="hljs-number">0.13</span><span class="hljs-number">.1</span> <span class="hljs-attribute">-Phive</span><span class="hljs-attribute">-thriftserver</span>
去掉下面编译会很快，即使编译失败也不会每次都清除
<span class="hljs-attribute">-DskipTests</span> clean package</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/irjlpglfdygt5nafayuv9ljl/spark%E7%BC%96%E8%AF%91%E6%88%90%E5%8A%9F%E5%90%8E.png" alt="spark编译成功后.png-52.8kB" title=""></p>



<h3 id="4-安装hadoop26">4 安装hadoop2.6</h3>



<h4 id="1添加java主目录位置">1，添加java主目录位置</h4>



<pre class="prettyprint"><code class=" hljs avrasm">hadoop-env<span class="hljs-preprocessor">.sh</span>
mapred-env<span class="hljs-preprocessor">.sh</span>
yarn-env<span class="hljs-preprocessor">.sh</span>
添加如下：
export JAVA_HOME=/opt/modules/jdk1<span class="hljs-number">.7</span><span class="hljs-number">.0</span>_67</code></pre>



<h4 id="2core-sitexml配置">2，core-site.xml配置</h4>



<pre class="prettyprint"><code class=" hljs xml">    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>/opt/modules/hadoop-2.5.0/data/tmp<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>hdfs://spark.learn.com:8020<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span></code></pre>



<h4 id="3hdfs-sitexml配置">3，hdfs-site.xml配置</h4>



<pre class="prettyprint"><code class=" hljs xml">    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span></code></pre>



<h4 id="4mapred-sitexml配置">4，mapred-site.xml配置</h4>



<pre class="prettyprint"><code class=" hljs xml">    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span></code></pre>



<h4 id="5yarn-sitexml配置">5，yarn-site.xml配置</h4>



<pre class="prettyprint"><code class=" hljs xml">    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>miaodonghua.host<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>

    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span></code></pre>



<h4 id="6slaves配置">6，slaves配置</h4>



<pre class="prettyprint"><code class=" hljs cs">miaodonghua.host<span class="hljs-comment">//主机名：nodemanager和datanode地址</span></code></pre>



<h4 id="7格式化namenode">7，格式化namenode</h4>



<pre class="prettyprint"><code class=" hljs rsl">bin/hdfs namenode -<span class="hljs-built_in">format</span></code></pre>



<h2 id="3spark几种模式的安装部署">3，spark几种模式的安装部署</h2>



<h3 id="1-spark本地模式的安装">1 spark本地模式的安装</h3>

<p>本地模式基础语法测试 <br>
1，直接运行</p>



<pre class="prettyprint"><code class=" hljs livecodeserver">bin/spark-<span class="hljs-built_in">shell</span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/m1g0fri9c2dnl7i4pi63ip4f/%E8%BF%90%E8%A1%8C%E6%88%90%E5%8A%9F.png" alt="运行成功.png-54.4kB" title=""> <br>
2，spark的webAPP</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//spark<span class="hljs-preprocessor">.learn</span><span class="hljs-preprocessor">.com</span>:<span class="hljs-number">4040</span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/mqe6j34oj78yyudqm96570m0/webapp.png" alt="webapp.png-73.9kB" title=""> <br>
3，读取readme.cm</p>



<pre class="prettyprint"><code class=" hljs fsharp"><span class="hljs-keyword">val</span> textFile = sc.textFile(<span class="hljs-string">"README.md"</span>)</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/laif6zlv83upcga07gu3cey7/%E8%AF%BB%E5%8F%96readmemd.png" alt="读取readmemd.png-25.6kB" title=""> <br>
4，count</p>



<pre class="prettyprint"><code class=" hljs axapta">testFile.<span class="hljs-keyword">count</span>()</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/12zsapgpwa9xu162eafkkg50/count.png" alt="count.png-82.7kB" title=""> <br>
5，first</p>



<pre class="prettyprint"><code class=" hljs livecodeserver">testFile.<span class="hljs-keyword">first</span>()</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/2cx8i06fbo2hlit07tme1ctw/first.png" alt="first.png-64.1kB" title=""> <br>
6，filter</p>



<pre class="prettyprint"><code class=" hljs livecodeserver">val linesWithSpark = testFile.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">line</span> =&gt; <span class="hljs-built_in">line</span>.<span class="hljs-operator">contains</span>(<span class="hljs-string">"Spark"</span>))</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/ht8aruz4x1jqcuc9ye289ar1/lineFilter.png" alt="lineFilter.png-5kB" title=""></p>



<pre class="prettyprint"><code class=" hljs livecodeserver">testFile.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">line</span> =&gt; <span class="hljs-built_in">line</span>.<span class="hljs-operator">contains</span>(<span class="hljs-string">"Spark"</span>)).count()</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/dqjbi4f20r1gvlw6qehawthi/lineFilter2.png" alt="lineFilter2.png-83.6kB" title=""> <br>
相对复杂计算 complex computations</p>



<pre class="prettyprint"><code class=" hljs coffeescript">testFile.map<span class="hljs-function"><span class="hljs-params">(line =&gt; line.split(<span class="hljs-string">" "</span>).size)</span>.<span class="hljs-title">reduce</span><span class="hljs-params">((a, b) =&gt; <span class="hljs-keyword">if</span> (a &gt; b) a <span class="hljs-keyword">else</span> b)</span></span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/j5locgmouk7qamcl3j31rbjs/%E5%A4%8D%E6%9D%821.png" alt="复杂1.png-81.8kB" title=""></p>



<pre class="prettyprint"><code class=" hljs coffeescript"><span class="hljs-reserved">import</span> java.lang.Math
testFile.map<span class="hljs-function"><span class="hljs-params">(line =&gt; line.split(<span class="hljs-string">" "</span>).size)</span>.<span class="hljs-title">reduce</span><span class="hljs-params">((a, b) =&gt; Math.max(a, b))</span></span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/w75zwp5u280715qkdd3x8b1q/JAVAmax.png" alt="JAVAmax.png-87.1kB" title=""></p>



<pre class="prettyprint"><code class=" hljs coffeescript">val wordCounts = testFile.flatMap<span class="hljs-function"><span class="hljs-params">(line =&gt; line.split(<span class="hljs-string">" "</span>))</span>.<span class="hljs-title">map</span><span class="hljs-params">(word =&gt; (word, <span class="hljs-number">1</span>))</span>.<span class="hljs-title">reduceByKey</span><span class="hljs-params">((a, b) =&gt; a + b)</span></span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/b5lqlnpwjdxkv3mjkf9zdf9a/wordcount.png" alt="wordcount.png-8kB" title=""></p>



<pre class="prettyprint"><code class=" hljs avrasm"> wordCounts<span class="hljs-preprocessor">.collect</span>()</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/d8c3n5b477lzrwps4b8d68he/%E6%98%BE%E7%A4%BA.png" alt="显示.png-52.3kB" title=""></p>

<p>cache</p>



<pre class="prettyprint"><code class=" hljs avrasm">scala&gt; linesWithSpark<span class="hljs-preprocessor">.cache</span>()</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/o7rw3q2nb51lg3dkkliqjkgj/cahe.png" alt="cache.png-49.7kB" title=""></p>



<pre class="prettyprint"><code class=" hljs axapta">scala&gt; linesWithSpark.<span class="hljs-keyword">count</span>()</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/t3fxncgjoz5dnsg2cmdxd5no/cache1.png" alt="cache1.png-14.6kB" title=""></p>



<h3 id="2-spark-standalone模式的安装">2 spark standalone模式的安装</h3>

<p><img src="http://static.zybuluo.com/hadoopMan/er7ks66oj9zaqampjmc4wfp1/Cluster%20Mode.png" alt="Cluster Mode.png-25.4kB" title=""> <br>
1，配置spark-env.sh</p>



<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">HADOOP_CONF_DIR=<span class="hljs-value">/opt/modules/hadoop-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4.0</span>/etc/hadoop</span></span>
<span class="hljs-setting">JAVA_HOME=<span class="hljs-value">/opt/modules/jdk1.<span class="hljs-number">7.0</span>_67</span></span>
<span class="hljs-setting">SCALA_HOME=<span class="hljs-value">/opt/modules/scala-<span class="hljs-number">2.10</span>.<span class="hljs-number">4</span></span></span>

<span class="hljs-setting">SPARK_MASTER_IP=<span class="hljs-value">spark.learn.com</span></span>
<span class="hljs-setting">SPARK_MASTER_PORT=<span class="hljs-value"><span class="hljs-number">7077</span></span></span>
<span class="hljs-setting">SPARK_MASTER_WEBUI_PORT=<span class="hljs-value"><span class="hljs-number">8080</span></span></span>
<span class="hljs-setting">SPARK_WORKER_CORES=<span class="hljs-value"><span class="hljs-number">1</span></span></span>
<span class="hljs-setting">SPARK_WORKER_MEMORY=<span class="hljs-value"><span class="hljs-number">1000</span>m</span></span>
<span class="hljs-setting">SPARK_WORKER_PORT=<span class="hljs-value"><span class="hljs-number">7078</span></span></span>
<span class="hljs-setting">SPARK_WORKER_WEBUI_PORT=<span class="hljs-value"><span class="hljs-number">8081</span></span></span>
<span class="hljs-setting">SPARK_WORKER_INSTANCES=<span class="hljs-value"><span class="hljs-number">1</span></span></span></code></pre>

<p>2，配置spark-defaults.conf</p>



<pre class="prettyprint"><code class=" hljs profile"><span class="hljs-filename">spark.master spark</span>://hadoop-<span class="hljs-filename">spark.com</span>:<span class="hljs-number">7077</span></code></pre>

<p>3，配置slaves</p>



<pre class="prettyprint"><code class=" hljs avrasm">spark<span class="hljs-preprocessor">.learn</span><span class="hljs-preprocessor">.com</span></code></pre>

<p>4，启动spark</p>



<pre class="prettyprint"><code class=" hljs sql">sbin/<span class="hljs-operator"><span class="hljs-keyword">start</span>-master.sh
sbin/<span class="hljs-keyword">start</span>-slaves.sh</span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/nfmaxwf1o645pg0km2nw7ok4/%E5%90%AF%E5%8A%A8%E6%88%90%E5%8A%9F.png" alt="启动成功.png-7kB" title=""></p>

<p>5，命令测试 <br>
读取本地的需要注释掉</p>



<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-constant">HADOOP_CONF_DIR</span>=<span class="hljs-regexp">/opt/modules</span><span class="hljs-regexp">/hadoop-2.6.0-cdh5.4.0/etc</span><span class="hljs-regexp">/hadoop</span></code></pre>



<pre class="prettyprint"><code class=" hljs profile">val textFile = <span class="hljs-filename">sc.textFile("hdfs</span>://<span class="hljs-filename">spark.learn.com</span>:<span class="hljs-number">8020</span>/user/hadoop/spark<span class="hljs-string">")</span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/ydrp4kltv7chdqn33h87nyjj/%E8%AF%BB%E5%8F%96hdfs%E6%88%90%E5%8A%9F.png" alt="读取hdfs成功.png-28.6kB" title=""></p>



<pre class="prettyprint"><code class=" hljs coffeescript">val wordcount = textFile.flatMap<span class="hljs-function"><span class="hljs-params">(x=&gt;x.split(<span class="hljs-string">" "</span>))</span>.<span class="hljs-title">map</span><span class="hljs-params">(x=&gt;(x,<span class="hljs-number">1</span>))</span>.<span class="hljs-title">reduceByKey</span><span class="hljs-params">((a,b)=&gt;a+b)</span>.<span class="hljs-title">collect</span><span class="hljs-params">()</span></span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/nut724zv6ekzvazhgfmxrv2u/%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C.png" alt="输出结果.png-27.8kB" title=""></p>



<pre class="prettyprint"><code class=" hljs coffeescript">val wordcount = textFile.flatMap<span class="hljs-function"><span class="hljs-params">(x=&gt;x.split(<span class="hljs-string">" "</span>))</span>.<span class="hljs-title">map</span><span class="hljs-params">(x=&gt;(x,<span class="hljs-number">1</span>))</span>.<span class="hljs-title">reduceByKey</span><span class="hljs-params">((a,b)=&gt;a+b)</span>.<span class="hljs-title">sortByKey</span><span class="hljs-params">(<span class="hljs-literal">true</span>)</span>.<span class="hljs-title">collect</span><span class="hljs-params">()</span></span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/hpx9y143x0gvznzoyxd1j4um/%E8%BE%93%E5%87%BA%E6%9C%89%E5%BA%8F%E7%BB%93%E6%9E%9C.png" alt="输出有序结果.png-23kB" title=""></p>



<pre class="prettyprint"><code class=" hljs coffeescript">val wordcount = textFile.flatMap<span class="hljs-function"><span class="hljs-params">(x=&gt;x.split(<span class="hljs-string">" "</span>))</span>.<span class="hljs-title">map</span><span class="hljs-params">(x=&gt;(x,<span class="hljs-number">1</span>))</span>.<span class="hljs-title">reduceByKey</span><span class="hljs-params">((a,b)=&gt;a+b)</span>.<span class="hljs-title">map</span><span class="hljs-params">(x=&gt;(x._2,x._1))</span>.<span class="hljs-title">sortByKey</span><span class="hljs-params">(<span class="hljs-literal">false</span>)</span>.<span class="hljs-title">collect</span><span class="hljs-params">()</span></span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/0tr7559dm3lbdzaiverlzbdm/%E6%8C%89value%E6%8E%92%E5%BA%8F.png" alt="按value排序.png-17.6kB" title=""></p>



<pre class="prettyprint"><code class=" hljs avrasm">sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"hdfs://spark.learn.com:8020/user/cyhp/spark/wc.input"</span>)<span class="hljs-preprocessor">.flatMap</span>(_<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">" "</span>))<span class="hljs-preprocessor">.map</span>((_,<span class="hljs-number">1</span>))<span class="hljs-preprocessor">.reduceByKey</span>(_ + _)<span class="hljs-preprocessor">.collect</span></code></pre>



<h3 id="3-spark-on-yarn">3 spark on yarn</h3>

<p>查看spark-submit参数</p>



<pre class="prettyprint"><code class=" hljs ruby">[hadoop<span class="hljs-variable">@spark</span> spark-<span class="hljs-number">1.3</span>.<span class="hljs-number">0</span>-bin-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">4.0</span>]<span class="hljs-variable">$ </span>bin/spark-submit --help</code></pre>



<pre class="prettyprint"><code class=" hljs haml">Spark assembly has been built with Hive, including Datanucleus jars on classpath
Usage: spark-submit [options] &lt;app jar | python file&gt; [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]

Options:
  -<span class="ruby">-master <span class="hljs-constant">MASTER_URL</span>         <span class="hljs-symbol">spark:</span>/<span class="hljs-regexp">/host:port, mesos:/</span><span class="hljs-regexp">/host:port, yarn, or local.
</span></span>  -<span class="ruby">-deploy-mode <span class="hljs-constant">DEPLOY_MODE</span>   <span class="hljs-constant">Whether</span> to launch the driver program locally (<span class="hljs-string">"client"</span>) <span class="hljs-keyword">or</span>
</span>                              on one of the worker machines inside the cluster ("cluster")
                              (Default: client).
  -<span class="ruby">-<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CLASS_NAME</span>          <span class="hljs-title">Your</span> <span class="hljs-title">application</span>'<span class="hljs-title">s</span> <span class="hljs-title">main</span> <span class="hljs-title">class</span> (<span class="hljs-title">for</span> <span class="hljs-title">Java</span> / <span class="hljs-title">Scala</span> <span class="hljs-title">apps</span>).</span>
</span>  -<span class="ruby">-name <span class="hljs-constant">NAME</span>                 <span class="hljs-constant">A</span> name of your application.
</span>  -<span class="ruby">-jars <span class="hljs-constant">JARS</span>                 <span class="hljs-constant">Comma</span>-separated list of local jars to <span class="hljs-keyword">include</span> on the driver
</span>                              and executor classpaths.
  -<span class="ruby">-packages                  <span class="hljs-constant">Comma</span>-separated list of maven coordinates of jars to <span class="hljs-keyword">include</span>
</span>                              on the driver and executor classpaths. Will search the local
                              maven repo, then maven central and any additional remote
                              repositories given by --repositories. The format for the
                              coordinates should be groupId:artifactId:version.
  -<span class="ruby">-repositories              <span class="hljs-constant">Comma</span>-separated list of additional remote repositories to
</span>                              search for the maven coordinates given with --packages.
  -<span class="ruby">-py-files <span class="hljs-constant">PY_FILES</span>         <span class="hljs-constant">Comma</span>-separated list of .zip, .egg, <span class="hljs-keyword">or</span> .py files to place
</span>                              on the PYTHONPATH for Python apps.
  -<span class="ruby">-files <span class="hljs-constant">FILES</span>               <span class="hljs-constant">Comma</span>-separated list of files to be placed <span class="hljs-keyword">in</span> the working
</span>                              directory of each executor.

  -<span class="ruby">-conf <span class="hljs-constant">PROP</span>=<span class="hljs-constant">VALUE</span>           <span class="hljs-constant">Arbitrary</span> <span class="hljs-constant">Spark</span> configuration property.
</span>  -<span class="ruby">-properties-file <span class="hljs-constant">FILE</span>      <span class="hljs-constant">Path</span> to a file from which to load extra properties. <span class="hljs-constant">If</span> <span class="hljs-keyword">not</span>
</span>                              specified, this will look for conf/spark-defaults.conf.

  -<span class="ruby">-driver-memory <span class="hljs-constant">MEM</span>         <span class="hljs-constant">Memory</span> <span class="hljs-keyword">for</span> driver (e.g. <span class="hljs-number">1000</span>M, <span class="hljs-number">2</span>G) (<span class="hljs-constant">Default</span><span class="hljs-symbol">:</span> <span class="hljs-number">512</span>M).
</span>  -<span class="ruby">-driver-java-options       <span class="hljs-constant">Extra</span> <span class="hljs-constant">Java</span> options to pass to the driver.
</span>  -<span class="ruby">-driver-library-path       <span class="hljs-constant">Extra</span> library path entries to pass to the driver.
</span>  -<span class="ruby">-driver-<span class="hljs-class"><span class="hljs-keyword">class</span>-<span class="hljs-title">path</span>         <span class="hljs-title">Extra</span> <span class="hljs-title">class</span> <span class="hljs-title">path</span> <span class="hljs-title">entries</span> <span class="hljs-title">to</span> <span class="hljs-title">pass</span> <span class="hljs-title">to</span> <span class="hljs-title">the</span> <span class="hljs-title">driver</span>. <span class="hljs-title">Note</span> <span class="hljs-title">that</span></span>
</span>                              jars added with --jars are automatically included in the
                              classpath.

  -<span class="ruby">-executor-memory <span class="hljs-constant">MEM</span>       <span class="hljs-constant">Memory</span> per executor (e.g. <span class="hljs-number">1000</span>M, <span class="hljs-number">2</span>G) (<span class="hljs-constant">Default</span><span class="hljs-symbol">:</span> <span class="hljs-number">1</span>G).
</span>
  -<span class="ruby">-proxy-user <span class="hljs-constant">NAME</span>           <span class="hljs-constant">User</span> to impersonate <span class="hljs-keyword">when</span> submitting the application.
</span>
  -<span class="ruby">-help, -h                  <span class="hljs-constant">Show</span> this help message <span class="hljs-keyword">and</span> exit
</span>  -<span class="ruby">-verbose, -v               <span class="hljs-constant">Print</span> additional debug output
</span>  -<span class="ruby">-version,                  <span class="hljs-constant">Print</span> the version of current <span class="hljs-constant">Spark</span>
</span>
 Spark standalone with cluster deploy mode only:
  -<span class="ruby">-driver-cores <span class="hljs-constant">NUM</span>          <span class="hljs-constant">Cores</span> <span class="hljs-keyword">for</span> driver (<span class="hljs-constant">Default</span><span class="hljs-symbol">:</span> <span class="hljs-number">1</span>).
</span>  -<span class="ruby">-supervise                 <span class="hljs-constant">If</span> given, restarts the driver on failure.
</span>  -<span class="ruby">-kill <span class="hljs-constant">SUBMISSION_ID</span>        <span class="hljs-constant">If</span> given, kills the driver specified.
</span>  -<span class="ruby">-status <span class="hljs-constant">SUBMISSION_ID</span>      <span class="hljs-constant">If</span> given, requests the status of the driver specified.
</span>
 Spark standalone and Mesos only:
  -<span class="ruby">-total-executor-cores <span class="hljs-constant">NUM</span>  <span class="hljs-constant">Total</span> cores <span class="hljs-keyword">for</span> all executors.
</span>
 YARN-only:
  -<span class="ruby">-driver-cores <span class="hljs-constant">NUM</span>          <span class="hljs-constant">Number</span> of cores used by the driver, only <span class="hljs-keyword">in</span> cluster mode
</span>                              (Default: 1).
  -<span class="ruby">-executor-cores <span class="hljs-constant">NUM</span>        <span class="hljs-constant">Number</span> of cores per executor (<span class="hljs-constant">Default</span><span class="hljs-symbol">:</span> <span class="hljs-number">1</span>).
</span>  -<span class="ruby">-queue <span class="hljs-constant">QUEUE_NAME</span>          <span class="hljs-constant">The</span> <span class="hljs-constant">YARN</span> queue to submit to (<span class="hljs-constant">Default</span><span class="hljs-symbol">:</span> <span class="hljs-string">"default"</span>).
</span>  -<span class="ruby">-num-executors <span class="hljs-constant">NUM</span>         <span class="hljs-constant">Number</span> of executors to launch (<span class="hljs-constant">Default</span><span class="hljs-symbol">:</span> <span class="hljs-number">2</span>).
</span>  -<span class="ruby">-archives <span class="hljs-constant">ARCHIVES</span>         <span class="hljs-constant">Comma</span> separated list of archives to be extracted into the
</span>                              working directory of each executor.</code></pre>

<p>1，submit本地模式 <br>
<img src="http://static.zybuluo.com/hadoopMan/944kur2l8nj1gbt8gvyl54x0/ClientMode.png" alt="ClientMode.png-13.1kB" title=""></p>



<pre class="prettyprint"><code class=" hljs avrasm">bin/spark-submit \
    --class org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.examples</span><span class="hljs-preprocessor">.SparkPi</span> \
    lib/spark-examples-<span class="hljs-number">1.3</span><span class="hljs-number">.0</span>-hadoop2<span class="hljs-number">.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.4</span><span class="hljs-number">.0</span><span class="hljs-preprocessor">.jar</span> \
    <span class="hljs-number">10</span>  </code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/7fdfeig160xivhkzobqb7w96/submit.png" alt="submit.png-15.3kB" title=""> <br>
2，submit运行于standalone模式 <br>
<img src="http://static.zybuluo.com/hadoopMan/ah37n50kc58ys9fb5aewh3og/Cluster%20Mode.png" alt="Cluster Mode.png-14.2kB" title=""></p>



<pre class="prettyprint"><code class=" hljs haml">bin/spark-submit \
    -<span class="ruby">-deploy-mode cluster \
</span>    -<span class="ruby">-<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">org</span>.<span class="hljs-title">apache</span>.<span class="hljs-title">spark</span>.<span class="hljs-title">examples</span>.<span class="hljs-title">SparkPi</span> \</span>
</span>    lib/spark-examples-1.3.0-hadoop2.6.0-cdh5.4.0.jar \
    10</code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/wzg1tjfkygdsd9ted5ahnrzv/submit%E7%9A%84standalone%E6%A8%A1%E5%BC%8F.png" alt="submit的standalone模式.png-35.6kB" title=""></p>

<p>3，submit提交到yarn上 <br>
yarn Cluster mode <br>
<img src="http://static.zybuluo.com/hadoopMan/vrocvpzdl91ktbto0bgkwj5j/yarnCluster.png" alt="yarnCluster.png-42.4kB" title=""> <br>
yarn Client mode <br>
<img src="http://static.zybuluo.com/hadoopMan/67fshemyvbtydrfgixspyayl/yarn%20Client.png" alt="yarn Client.png-107.6kB" title=""></p>



<pre class="prettyprint"><code class=" hljs haml">bin/spark-submit \
    -<span class="ruby">-master yarn \
</span>    -<span class="ruby">-<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">org</span>.<span class="hljs-title">apache</span>.<span class="hljs-title">spark</span>.<span class="hljs-title">examples</span>.<span class="hljs-title">SparkPi</span> \</span>
</span>    lib/spark-examples-1.3.0-hadoop2.6.0-cdh5.4.0.jar \
    10  </code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/kbt218ywlbrb10gihl1dkebl/submit%E5%88%B0yarn%E4%B8%8A.png" alt="submit到yarn上.png-89.6kB" title=""></p>



<h2 id="4spark监控界面">4，spark监控界面</h2>



<h3 id="1启动spark监控服务">1，启动spark监控服务</h3>

<p>执行下面指令启动spark historyserver</p>



<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-built_in">.</span>/sbin/start<span class="hljs-attribute">-history</span><span class="hljs-attribute">-server</span><span class="hljs-built_in">.</span>sh</code></pre>

<p>查看方式</p>



<pre class="prettyprint"><code class=" hljs cs">http:<span class="hljs-comment">//&lt;server-url&gt;:18080</span></code></pre>



<h3 id="2spark监控的配置项介绍">2，spark监控的配置项介绍</h3>



<h4 id="1整体配置项">1，整体配置项</h4>

<p>我们可以对historyserver进行如下配置：</p>

<table>
<thead>
<tr>
  <th>Environment Variable</th>
  <th>Meaning</th>
</tr>
</thead>
<tbody><tr>
  <td>SPARK_DAEMON_MEMORY</td>
  <td>Memory to allocate to the history server (default: 512m).</td>
</tr>
<tr>
  <td>SPARK_DAEMON_JAVA_OPTS</td>
  <td>JVM options for the history server (default: none).</td>
</tr>
<tr>
  <td>SPARK_PUBLIC_DNS</td>
  <td>The public address for the history server. If this is not set, links to application history may use the internal address of the server, resulting in broken links (default: none).</td>
</tr>
<tr>
  <td>SPARK_HISTORY_OPTS</td>
  <td>spark.history.* configuration options for the history server (default: none).</td>
</tr>
</tbody></table>




<h4 id="2sparkhistoryopts配置">2，SPARK_HISTORY_OPTS配置</h4>

<table>
<thead>
<tr>
  <th>Property Name</th>
  <th>Default</th>
  <th>Meaning</th>
</tr>
</thead>
<tbody><tr>
  <td>spark.history.provider</td>
  <td>org.apache.spark.deploy.history.FsHistoryProvider</td>
  <td>Name of the class implementing the application history backend. Currently there is only one implementation, provided by Spark, which looks for application logs stored in the file system.</td>
</tr>
<tr>
  <td>spark.history.fs.logDirectory</td>
  <td>file:/tmp/spark-events</td>
  <td>Directory that contains application event logs to be loaded by the history server</td>
</tr>
<tr>
  <td>spark.history.fs.updateInterval</td>
  <td>10</td>
  <td>The period, in seconds, at which information displayed by this history server is updated. Each update checks for any changes made to the event logs in persisted storage.</td>
</tr>
<tr>
  <td>spark.history.retainedApplications</td>
  <td>50</td>
  <td>The number of application UIs to retain. If this cap is exceeded, then the oldest applications will be removed.</td>
</tr>
<tr>
  <td>spark.history.ui.port</td>
  <td></td>
  <td>18080</td>
</tr>
<tr>
  <td>spark.history.kerberos.enabled</td>
  <td>false</td>
  <td>Indicates whether the history server should use kerberos to login. This is useful if the history server is accessing HDFS files on a secure Hadoop cluster. If this is true, it uses the configs spark.history.kerberos.principal and spark.history.kerberos.keytab.</td>
</tr>
<tr>
  <td>spark.history.kerberos.principal</td>
  <td>(none)</td>
  <td>Kerberos principal name for the History Server.</td>
</tr>
<tr>
  <td>spark.history.kerberos.keytab</td>
  <td>(none)</td>
  <td>Location of the kerberos keytab file for the History Server.</td>
</tr>
<tr>
  <td>spark.history.ui.acls.enable</td>
  <td>false</td>
  <td>Specifies whether acls should be checked to authorize users viewing the applications. If enabled, access control checks are made regardless of what the individual application had set for spark.ui.acls.enable when the application was run. The application owner will always have authorization to view their own application and any users specified via spark.ui.view.acls when the application was run will also have authorization to view that application. If disabled, no access control checks are made.</td>
</tr>
</tbody></table>




<h3 id="3标记一个任务是否完成">3，标记一个任务是否完成</h3>

<p>要注意的是historyserver仅仅显示完成了的spark任务。标记任务完成的一种方式是直接调用sc.stop()。</p>



<h4 id="4spark-defaultsconf中的配置">4，spark-defaults.conf中的配置</h4>

<table>
<thead>
<tr>
  <th>Property Name</th>
  <th>Default Meaning</th>
</tr>
</thead>
<tbody><tr>
  <td>spark.eventLog.compress</td>
  <td>false</td>
</tr>
<tr>
  <td>spark.eventLog.dir</td>
  <td>file:///tmp/spark-events</td>
</tr>
<tr>
  <td>spark.eventLog.enabled</td>
  <td>false</td>
</tr>
</tbody></table>




<h3 id="2具体配置">2，具体配置</h3>



<h4 id="1在spark-envsh">1，在spark-env.sh</h4>



<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">SPARK_HISTORY_OPTS=<span class="hljs-value"><span class="hljs-string">"-Dspark.history.fs.logDirectory=hdfs://spark.learn.com:8020/user/hadoop/spark/history"</span></span></span></code></pre>



<h4 id="2在spark-defaultsconf">2，在spark-defaults.conf</h4>



<pre class="prettyprint"><code class=" hljs avrasm">spark<span class="hljs-preprocessor">.eventLog</span><span class="hljs-preprocessor">.enabled</span>           true
spark<span class="hljs-preprocessor">.eventLog</span><span class="hljs-preprocessor">.dir</span>               hdfs://spark<span class="hljs-preprocessor">.learn</span><span class="hljs-preprocessor">.com</span>:<span class="hljs-number">8020</span>/user/hadoop/spark/history
spark<span class="hljs-preprocessor">.eventLog</span><span class="hljs-preprocessor">.compress</span>         true</code></pre>



<h4 id="3测试">3，测试</h4>

<p>启动相关服务</p>



<pre class="prettyprint"><code class=" hljs lasso">sbin/start<span class="hljs-attribute">-master</span><span class="hljs-built_in">.</span>sh
sbin/start<span class="hljs-attribute">-slaves</span><span class="hljs-built_in">.</span>sh 
sbin/start<span class="hljs-attribute">-history</span><span class="hljs-attribute">-server</span><span class="hljs-built_in">.</span>sh
bin/spark<span class="hljs-attribute">-shell</span></code></pre>

<p>执行spark应用</p>



<pre class="prettyprint"><code class=" hljs avrasm">val textFile = sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"hdfs://spark.learn.com:8020/user/hadoop/spark/input/"</span>)
textFile<span class="hljs-preprocessor">.count</span>
sc<span class="hljs-preprocessor">.stop</span></code></pre>

<p><img src="http://static.zybuluo.com/hadoopMan/6tuzcgphvh9xotr89zzhhrn6/%E6%89%A7%E8%A1%8C%E6%88%90%E5%8A%9F.png" alt="执行成功.png-25.9kB" title=""></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>