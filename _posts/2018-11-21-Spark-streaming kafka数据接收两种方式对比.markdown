---
layout:     post
title:      Spark-streaming kafka数据接收两种方式对比
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <div><h4>1.1 Receiver-based Approach</h4><p>这种方式利用接收器（Receiver）来接收kafka中的数据，其最基本是使用Kafka高阶用户API接口。对于所有的接收器，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据。<br>Receiver-based的Kafka读取方式是基于Kafka高阶(high-level) api来实现对Kafka数据的消费。在提交Spark Streaming任务后，Spark集群会划出指定的Receivers来专门、持续不断、异步读取Kafka的数据，读取时间间隔以及每次读取offsets范围可以由参数来配置。读取的数据保存在Receiver中，具体StorageLevel方式由用户指定，诸如MEMORY_ONLY等。当driver 触发batch任务的时候，Receivers中的数据会转移到剩余的Executors中去执行。在执行完之后，Receivers会相应更新ZooKeeper的offsets。如要确保at least once的读取方式，可以设置spark.streaming.receiver.writeAheadLog.enable为true。具体Receiver执行流程见下图：</p><div class="image-package"><div class="image-container" style="max-width:542px;max-height:454px;"><div class="image-container-fill" style="padding-bottom:83.76%;"><img src="https://upload-images.jianshu.io/upload_images/2989357-7e80edb427d6964e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/542" alt=""></div><div class="image-view"><br></div></div></div><pre class="hljs scala"><code class="scala"><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka._

 <span class="hljs-keyword">val</span> kafkaStream = <span class="hljs-type">KafkaUtils</span>.createStream(streamingContext, 
     [<span class="hljs-type">ZK</span> quorum], [consumer group id], [per-topic number of <span class="hljs-type">Kafka</span> partitions to consume])
</code></pre><p>还有几个需要注意的点：</p><ul><li>在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度。</li><li>对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。</li><li>如果我们启用了Write Ahead Logs复制到文件系统如HDFS，那么storage level需要设置成StorageLevel.MEMORY_AND_DISK_SER，也就<br>是KafkaUtils.createStream(...,StorageLevel.MEMORY_AND_DISK_SER)</li></ul><h4>4.1.1 源码分析</h4><div class="image-package"><div class="image-container" style="max-width:700px;max-height:161px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-c13f644e74dbdb16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div></div><div class="image-package"><div class="image-container" style="max-width:516px;max-height:87px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-912376a0b2dc1265.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/516" alt=""></div></div><div class="image-caption"><br><img src="//upload-images.jianshu.io/upload_images/2989357-c79182541713a253.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/631" alt=""></div></div><div class="image-package"><div class="image-container" style="max-width:478px;max-height:243px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-20d834dd452596db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/478" alt=""></div></div><div class="image-caption"><img src="//upload-images.jianshu.io/upload_images/2989357-51d822791e405f47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-package"><div class="image-container" style="max-width:519px;max-height:78px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-929b5706e80ea248.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/519" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:620px;max-height:72px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-ff060931c43721c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:131px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-27a470e171bd4aa1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div></div><div class="image-package"><div class="image-container" style="max-width:645px;max-height:284px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-1b5d197e83fd398f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/645" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:289px;max-height:84px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-c81f26fcf6826483.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/289" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:548px;max-height:96px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-8b39f4b88367eb9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/548" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:294px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-544e3a491ab0b8e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div></div>需要查看<code>currentBuffer</code>在哪里被处理，搜索代码，发现<div class="image-package"><div class="image-container" style="max-width:612px;max-height:408px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-15eaad73bc8fc608.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/612" alt=""></div></div><div class="image-caption">而<code>updateCurrentBuffer</code>方法又在这里被回调</div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:385px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-0a60851b9c318e08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:32px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-883c0cbf1853be05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:522px;max-height:87px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-7bf06560508ee41e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/522" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:448px;max-height:199px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-7ef6a0abe4832c10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/448" alt=""></div></div></div><code>period</code>就是上面传入的，<code>blockIntervalMs</code>,默认200ms切一个block，这也是一个优化参数<div class="image-package"><div class="image-container" style="max-width:534px;max-height:128px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-ea050885284a8cd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/534" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:663px;max-height:404px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-0ab6b3a36d50d794.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/663" alt=""></div></div></div><p>这里参考博客，暂时未找到从哪里找到的</p><div class="image-package"><div class="image-container" style="max-width:334px;max-height:251px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-6a88dda5b7db4804.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/334" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:468px;"><div class="image-container-fill" style="padding-bottom:60.150000000000006%;"><img src="//upload-images.jianshu.io/upload_images/2989357-dfd216a6b6b7b46e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div></div><br><div class="image-package"><div class="image-container" style="max-width:700px;max-height:663px;"><div class="image-container-fill" style="padding-bottom:94.72%;"><img src="//upload-images.jianshu.io/upload_images/2989357-3c04e3aacccb516e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption"><code>BlockRDD</code>类中<code>getPartitions</code>方法是说将这个batch的blocks作为partitions。<code>Compute</code>方法则按照入参<code>BlockRDDPartition</code>的<code>blockId</code>，从<code>blockManager</code>中获取该block作为partition的数据。<code>getPreferredLocations</code>则是将<code>BlockRDDPartition</code>所在的host作为partition的首选位置,移动计算，不移动数据原则。</div></div><p></p><div class="image-package"><div class="image-container" style="max-width:696px;max-height:393px;"><div class="image-container-fill" style="padding-bottom:56.47%;"><img src="//upload-images.jianshu.io/upload_images/2989357-a71f2ef7b73dc9da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/696" alt=""></div></div><div class="image-caption">image.png</div></div><h4>1.2 Direct  Approach (No Receivers)</h4><p>Direct方式采用Kafka简单的consumer api方式来读取数据，无需经由ZooKeeper，此种方式不再需要专门Receiver来持续不断读取数据。当batch任务触发时，由Executor读取数据，并参与到其他Executor的数据计算过程中去。driver来决定读取多少offsets，并将offsets交由checkpoints来维护。将触发下次batch任务，再由Executor读取Kafka数据并计算。从此过程我们可以发现Direct方式无需Receiver读取数据，而是需要计算时再读取数据，所以Direct方式的数据消费对内存的要求不高，只需要考虑批量计算所需要的内存即可；另外batch任务堆积时，也不会影响数据堆积。其具体读取方式如下图：</p><br><div class="image-package"><div class="image-container" style="max-width:530px;max-height:369px;"><div class="image-container-fill" style="padding-bottom:69.62%;"><img src="//upload-images.jianshu.io/upload_images/2989357-b2062e1781497ff2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/530" alt=""></div></div><div class="image-caption">image.png</div></div><p>这种方法相较于Receiver方式的优势在于：</p><ul><li>简化的并行：在Receiver的方式中我们提到创建多个Receiver之后利用union来合并成一个Dstream的方式提高数据传输并行度。而在Direct方式中，Kafka中的partition与RDD中的partition是一一对应的并行读取Kafka数据，这种映射关系也更利于理解和优化。</li><li>高效：在Receiver的方式中，为了达到0数据丢失需要将数据存入Write Ahead Log中，这样在Kafka和日志中就保存了两份数据，浪费！而第二种方式不存在这个问题，只要我们Kafka的数据保留时间足够长，我们都能够从Kafka进行数据恢复。</li><li>精确一次：在Receiver的方式中，使用的是Kafka的高阶API接口从Zookeeper中获取offset值，这也是传统的从Kafka中读取数据的方式，但由于Spark Streaming消费的数据和Zookeeper中记录的offset不同步，这种方式偶尔会造成数据重复消费。而第二种方式，直接使用了简单的低阶Kafka API，Offsets则利用Spark Streaming的checkpoints进行记录，消除了这种不一致性。</li></ul><h4>4.2.1 源码分析</h4><pre class="hljs scala"><code class="scala"><span class="hljs-keyword">val</span> stream = <span class="hljs-type">KafkaUtils</span>.createDirectStream()
</code></pre><div class="image-package"><div class="image-container" style="max-width:700px;max-height:465px;"><div class="image-container-fill" style="padding-bottom:64.05%;"><img src="//upload-images.jianshu.io/upload_images/2989357-350ccfc0526e6701.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption">image.png</div></div><br><p>这里两个优化参数：</p><pre class="hljs bash"><code class="bash"> val refreshLeaderBackoffMs = props.getInt(<span class="hljs-string">"refresh.leader.backoff.ms"</span>, RefreshMetadataBackoffMs)
 val maxRetries = context.sparkContext.getConf.getInt(<span class="hljs-string">"spark.streaming.kafka.maxRetries"</span>, 1)</code></pre><p><img src="//upload-images.jianshu.io/upload_images/2989357-1bb2c45b4e69a2ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></p><div class="image-package"><div class="image-caption"><img src="//upload-images.jianshu.io/upload_images/2989357-66aed59a9b59ba5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:383px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-887549fce09af79f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption"><span style="background-color:rgb(249,242,244);color:rgb(199,37,78);font-family:Consolas, Inconsolata, Courier, monospace;font-size:12px;">优化参数 "spark.streaming.kafka.maxRatePerPartition"</span></div></div><div class="image-package"><div class="image-container" style="max-width:671px;max-height:47px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-41d64c3c30b33c95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/671" alt=""></div></div><div class="image-caption"><img src="//upload-images.jianshu.io/upload_images/2989357-b7f447d1c73fa4a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div>在getPartitions方法中可以看到，KafkaRDD的partition个数就是topic的partition个数之和。<br><div class="image-package"><div class="image-container" style="max-width:700px;max-height:122px;"><div class="image-container-fill" style="padding-bottom:16.07%;"><img src="//upload-images.jianshu.io/upload_images/2989357-5f5ac180b83f830e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption">compute方法是RDD用来构建一个partition的数据的。</div></div><div class="image-package"><div class="image-container" style="max-width:688px;max-height:198px;"><div class="image-container-fill" style="padding-bottom:28.78%;"><img src="//upload-images.jianshu.io/upload_images/2989357-d356496270127f50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/688" alt=""></div></div><div class="image-caption"><br><img src="//upload-images.jianshu.io/upload_images/2989357-52e794b2a84a25fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:216px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-4a8013f38d3f6065.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption"><br></div></div><div class="image-package"><div class="image-container" style="max-width:700px;max-height:281px;"><div class="image-view"><img src="//upload-images.jianshu.io/upload_images/2989357-c4077952422d5dc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></div><div class="image-caption"><br></div></div></div>作者：LancerLin_LX<br>链接：https://www.jianshu.com/p/d031132d131c<br>來源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。            </div>
                </div>