---
layout:     post
title:      spark之5：配置文件
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，转载请注明来自http://blog.csdn.net/jediael_lu/					https://blog.csdn.net/jediael_lu/article/details/76735281				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="spark之5配置文件">spark之5：配置文件</h1>

<p>@(SPARK)[spark]</p>



<h1 id="一spark的参数设置方式">一、spark的参数设置方式</h1>



<h2 id="1spark配置文件加载顺序">1、spark配置文件加载顺序</h2>

<p>spark按以下优先级加载配置文件： <br>
（1）用户代码中显式调用set()方法设置的选项 <br>
（2）通过spark-submit传递的参数 <br>
（3）配置文件中的值 <br>
（4）spark的默认值</p>

<p>以下会分别介绍各种方式。</p>



<h2 id="2set方法">2、set()方法</h2>

<p>val conf = new SparkConf() <br>
conf.set(“spark.app.name”, “ljh_test”) <br>
conf.set(“spark.master”,”yarn-client”) <br>
val sc = new SparkContext(conf)</p>



<h2 id="3spark-submit方式">3、spark-submit方式</h2>

<p>bin/spark-submit  <br>
–class com.lujinhong.MyTest <br>
–master yarn-client <br>
–name “ljh_test” <br>
myTest.jar</p>



<h2 id="4配置文件的方式">4、配置文件的方式</h2>



<h3 id="1spark-defaultsconf">（1）spark-defaults.conf</h3>

<p>主要是指conf/spark-defaults.conf，如：</p>

<pre><code># For monitoring
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://mycluster/tmp/spark-events
spark.history.fs.logDirectory    hdfs://mycluster/tmp/spark-events
spark.yarn.historyServer.address 10.1.1.100:18080
spark.ui.showConsoleProgress     true
spark.history.kerberos.enabled   true
spark.history.kerberos.principal hadoop/sparkhistoryserver@LUJINHONG.COM
spark.history.kerberos.keytab    /home/hadoop/conf/spark/spark.keytab

# For executor
spark.cores.max                  300
spark.driver.memory              2g
spark.executor.memory            6g
spark.executor.cores             6
spark.driver.extraJavaOptions -XX:PermSize=512M -XX:MaxPermSize=2048M
</code></pre>

<p>文件中是以空格分开的键值对，默认加载conf/spark-defaults.conf，也可以在spark-submit中通过–properties-file指定路径。</p>



<h3 id="2spark-envsh">（2）spark-env.sh</h3>

<p>主要用于指定一些环境变量，尤其是指定YARN相关的目录，如</p>

<pre><code>#!/usr/bin/env bash
export SPARK_HOME=/home/hadoop/spark
export SPARK_LOG_DIR=/home/hadoop/logs
export SPARK_PID_DIR=/home/hadoop/pids
export YARN_CONF_DIR=/home/hadoop/conf
export HADOOP_CONF_DIR=/home/hadoop/conf

# for exporting for enviroment, such as lib/native
export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native
</code></pre>



<h3 id="3其它">（3）其它</h3>

<p>其它的配置文件还有log4j.properties, metircs.properties等。</p>



<h2 id="5spark的默认值">5、spark的默认值</h2>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>