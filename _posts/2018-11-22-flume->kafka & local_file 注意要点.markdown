---
layout:     post
title:      flume->kafka & local_file 注意要点
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>2018-04-19:</p><p>   转载请注明：https://blog.csdn.net/qq_35946969/article/details/80006563<br></p> flume-&gt;kafka &amp; local_file 注意要点：<br>  备用命令：    <br>    flume启动：<br>    flume/bin/flume-ng agent --conf-file  flume/conf/kafka.properties -c conf/ --name a1 -Dflume.root.logger=DEBUG,console<br><br><br>    kafka创建topic：<br>     kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test<br><br><br>    kafka启动生产者：<br>    kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test<br><br><br>    kafka启动消费者：<br>    kafka/bin/kafka-console-consumer.sh --zookeeper 192.168.132.62:2181  --topic test --from-beginnin<br><br><br>  1、正常流程配置zookeeper及kafka，并创建一个kafkaTopic。<br>  2、启动一个kafka-console-consumer，监控查看flume-&gt;kafka的输出。tail -f监控查看flume-&gt;localfile的输出。<br>  3、配置flume，一个source、一个channel、两个sink，配置如下：<br>          a1.sources = s1<br>          a1.channels = c1 c2<br>          a1.sinks = k1 k2<br><br><br>          # define the source<br>          a1.sources.s1.type = exec<br>          a1.sources.s1.command = tail -F /opt/modules/nginx/logs/access.log<br>          a1.sources.s1.shell = /bin/sh -c<br><br><br><br><br>          #define the channel c1<br>          a1.channels.c1.type = memory<br>          a1.channels.c1.capacity = 1000<br>          a1.channels.c1.transactionCapacity = 100<br><br><br>          # define the sink k1<br>          a1.sinks.k1.type = file_roll<br>          a1.sinks.k1.channel = c1<br>          a1.sinks.k1.sink.directory = /opt/modules/apache-flume-1.8.0-bin/nginx_logs<br><br><br>          #define the sink k2<br>          a1.sinks.k2.channel = c1<br>          a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink<br>          a1.sinks.k2.kafka.topic = test<br>          a1.sinks.k2.kafka.bootstrap.servers = localhost:9092<br>          a1.sinks.k2.kafka.flumeBatchSize = 20<br>          a1.sinks.k2.kafka.producer.acks = 1<br>          a1.sinks.k2.kafka.producer.linger.ms = 1<br>          a1.sinks.k2.kafka.producer.compression.type = snappy<br><br><br><br><br>          # zuhe<br>          a1.sources.s1.channels = c1<br>          a1.sinks.k1.channel = c1<br>          a1.sources.s2.channels = c1<br>          a1.sinks.k2.channel = c1<br>  4、启动flume。在/opt/modules/nginx/logs/access.log随机增加内容，查看输出端。<br>  5、个人遇到的问题：<br>      （1）、kafka启动时zookeeper提示ERROR：brokerids已存在，并且kafka创建消费者时提示no brokers found in zk.<br>          原因：推测因为之前kafka非正常退出。<br>          解决方法：没有深入了解，我的解决方法是重装zookeeper，当然实际生产中不能这么干。<br>      （2）、上述问题解决后，kafka创建消费者时仍然提示no brokers found in zk.<br>          原因：使用的9092端口不对外开放。<br>          解决方法：开放端口，或者启动消费者时使用localhost<br>      （3）、flume配置文件一开始尝试一个source、两个channel对应两个sink，但是只有一个sink起作用。<br>          解决方法：换成了一个source-&gt;一个channel-&gt;两个sink            </div>
                </div>