---
layout:     post
title:      Hive入门小结转发
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <div style="margin:20px auto 0px;padding:0px;width:1200px;clear:both;color:rgb(125,139,141);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;background-color:rgb(240,238,245);"><div style="margin:0px;padding:0px;background:none;float:left;width:890px;"><div class="forFlow" style="margin:0px;padding:0px;float:none;width:890px;"><div style="margin:0px;padding:0px;"><div style="margin:0px;padding:20px;background:rgb(255,255,255);border:1px solid rgb(222,222,222);"><div class="post" style="margin:0px;padding:0px;"><div class="postBody" style="margin:0px;padding:0px;"><div class="blogpost-body" style="margin:0px 0px 20px;padding:0px;color:rgb(51,51,51);line-height:1.8;"><div style="margin:0px;padding:0px;">HIve总结：</div><div style="margin:0px;padding:0px;">首先要学习Hive，第一步是了解Hive，Hive是基于Hadoop的一个数据仓库，可以将结构化的数据文件映射为一张表，并提供类sql查询功能，Hive底层将sql语句转化为mapreduce任务运行。相对于用java代码编写mapreduce来说，Hive的优势明显：快速开发，人员成本低，可扩展性（自由扩展集群规模），延展性（支持自定义函数）。</div><div style="margin:0px;padding:0px;">Hive的构架：</div><div style="margin:0px;padding:0px;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205214177-782883965.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"><p style="margin:10px auto;"> </p></div><div style="margin:0px;padding:0px;"></div><div style="margin:0px;padding:0px;">Hive提供了三种用户接口：CLI、HWI和客户端。客户端是使用JDBC驱动通过thrift，远程操作Hive。HWI即提供Web界面远程访问Hive。但是最常见的使用方式还是使用CLI方式。（在linux终端操作Hive）</div><div style="margin:0px;padding:0px;">Hive有三种安装方式：</div><div style="margin:0px;padding:0px;">1、内嵌模式（元数据保村在内嵌的derby种，允许一个会话链接，尝试多个会话链接时会报错，不适合开发环境）</div><div style="margin:0px;padding:0px;"> 2、本地模式（本地安装mysql 替代derby存储元数据）</div><div style="margin:0px;padding:0px;"> 3、远程模式（远程安装mysql 替代derby存储元数据）</div><div style="margin:0px;padding:0px;">安装Hive：（本地模式）</div><div style="margin:0px;padding:0px;">首先Hive的安装是在Hadoop集群正常安装的基础上，并且集群启动</div><div style="margin:0px;padding:0px;">  安装Hive之前我们要先安装mysql，</div><div style="margin:0px;padding:0px;">  查看是否安装过mysql：rpm -qa|grep mysql*  </div><div style="margin:0px;padding:0px;">  查看有没有安装包：yum list mysql* </div><div style="margin:0px;padding:0px;">  安装mysql客户端：yum install -y mysql</div><div style="margin:0px;padding:0px;">  安装服务器端：yum install -y mysql-server </div><div style="margin:0px;padding:0px;">                yum install -y mysql-devel</div><div style="margin:0px;padding:0px;">  启动数据库 service mysqld start或者/etc/init.d/mysqld start</div><div style="margin:0px;padding:0px;">  创建hadoop用户并赋予权限：</div><div style="margin:0px;padding:0px;">  mysql&gt;grant all on *.* to hadoop@'%' identified by 'hadoop';</div><div style="margin:0px;padding:0px;">  mysql&gt;grant all on *.* to hadoop@'localhost' identified by 'hadoop';</div><div style="margin:0px;padding:0px;">  mysql&gt;grant all on *.* to hadoop@'master' identified by 'hadoop';</div><div style="margin:0px;padding:0px;">  mysql&gt;flush privileges;</div><div style="margin:0px;padding:0px;">  然后在Hive官网上下载需要的版本，hive.apache.org  <a href="http://archiv.apache.org/" rel="nofollow" style="margin:0px;padding:0px;color:rgb(0,0,0);text-decoration:underline;">archive.apache.org</a></div><div style="margin:0px;padding:0px;">解压：tar -zxvf apache-hive-1.2.1-bin.tar.gz</div><div style="margin:0px;padding:0px;">配置：cd /apache-hive-1.2.1-bin/conf/  vim hive-site.xml</div><div style="margin:0px;padding:0px;">    &lt;?xml version="1.0"?&gt;</div><div style="margin:0px;padding:0px;">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</div><div style="margin:0px;padding:0px;">&lt;configuration&gt;</div><div style="margin:0px;padding:0px;">    &lt;property&gt;</div><div style="margin:0px;padding:0px;">        &lt;name&gt;hive.metastore.local&lt;/name&gt;</div><div style="margin:0px;padding:0px;">        &lt;value&gt;true&lt;/value&gt;</div><div style="margin:0px;padding:0px;">    &lt;/property&gt;</div><div style="margin:0px;padding:0px;">    &lt;property&gt;</div><div style="margin:0px;padding:0px;">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</div><div style="margin:0px;padding:0px;">         &lt;value&gt;jdbc:mysql://master:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</div><div style="margin:0px;padding:0px;">    &lt;/property&gt;</div><div style="margin:0px;padding:0px;">    &lt;property&gt;</div><div style="margin:0px;padding:0px;">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</div><div style="margin:0px;padding:0px;">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</div><div style="margin:0px;padding:0px;">    &lt;/property&gt;</div><div style="margin:0px;padding:0px;">    &lt;property&gt;</div><div style="margin:0px;padding:0px;">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</div><div style="margin:0px;padding:0px;">        &lt;value&gt;hadoop&lt;/value&gt;</div><div style="margin:0px;padding:0px;">    &lt;/property&gt;</div><div style="margin:0px;padding:0px;">    &lt;property&gt;</div><div style="margin:0px;padding:0px;">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</div><div style="margin:0px;padding:0px;">        &lt;value&gt;hadoop&lt;/value&gt;</div><div style="margin:0px;padding:0px;">    &lt;/property&gt;</div><div style="margin:0px;padding:0px;">&lt;/configuration&gt;</div><div style="margin:0px;padding:0px;">复制依赖包：cp mysql-connector-java-5.1.43-bin.jar apache-hive-1.2.1-bin/lib/</div><div style="margin:0px;padding:0px;">配置环境变量：</div><div style="margin:0px;padding:0px;">export HIVE_HOME=$PWD/apache-hive-1.2.1-bin</div><div style="margin:0px;padding:0px;">export PATH=$PATH:$HIVE_HOME/bin</div><div style="margin:0px;padding:0px;">启动hive：hive</div><div style="margin:0px;padding:0px;">hive中可以运行shell命令:! shell命令</div><div style="margin:0px;padding:0px;"></div><div style="margin:0px;padding:0px;"></div><div style="margin:0px;padding:0px;"> <img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205233131-1045638578.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"><p style="margin:10px auto;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205328037-247600045.png" alt="" style="padding:0px;border:none;max-width:800px;"></p><p style="margin:10px auto;"> </p><p style="margin:10px auto;"> </p></div><div style="margin:0px;padding:0px;">hive中可以运行hadoop命令：</div><div style="margin:0px;padding:0px;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205341256-973417976.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"><p style="margin:10px auto;"> </p></div><div style="margin:0px;padding:0px;"></div><div style="margin:0px;padding:0px;">hive中的数据类型：</div><div style="margin:0px;padding:0px;">原子数据类型：TINYINT SMALLINT INT BIGINT FLOAT DOUBLE BOOLEAN STRING </div><div style="margin:0px;padding:0px;">复杂数据类型：STRUCT MAP ARRAY </div><div style="margin:0px;padding:0px;">hive的使用：</div><div style="margin:0px;padding:0px;">建表语句：</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:16px;"><span style="margin:0px;padding:0px;">DDL：</span></span></div><div style="margin:0px;padding:0px;">创建内部表：</div><div style="margin:0px;padding:0px;">create table mytable(</div><div style="margin:0px;padding:0px;">id int, </div><div style="margin:0px;padding:0px;">name string) </div><div style="margin:0px;padding:0px;">row format delimited fields terminated by '\t' stored as textfile;</div><div style="margin:0px;padding:0px;">常见外部表：关键字 external</div><div style="margin:0px;padding:0px;"><p style="margin:10px auto;">create external table mytable2(<br style="margin:0px;padding:0px;">id int, <br style="margin:0px;padding:0px;">name string)<br style="margin:0px;padding:0px;">row format delimited fields terminated by '\t' location '/user/hive/warehouse/mytable2';</p></div><div style="margin:0px;padding:0px;">创建分区表：分区字段要写在partiton by（）</div><div style="margin:0px;padding:0px;"><p style="margin:10px auto;">create table mytable3(<br style="margin:0px;padding:0px;">id int, <br style="margin:0px;padding:0px;">name string)<br style="margin:0px;padding:0px;">partitioned by(sex string) row format delimited fields terminated by '\t'stored as textfile;</p><p style="margin:10px auto;">静态分区插入数据</p><p style="margin:10px auto;">load data local inpath '/root/hivedata/boy.txt' overwrite into table mytable3 partition(sex='boy');</p><p style="margin:10px auto;">增加分区：</p><p style="margin:10px auto;">alter table mytable3 add partition (sex='unknown') location '/user/hive/warehouse/mytable3/sex=unknown';</p><p style="margin:10px auto;">删除分区：alter table mytable3 drop if exists partition(sex='unknown');</p><p style="margin:10px auto;">分区表默认为静态分区，可转换为自动套分区</p><p style="margin:10px auto;">set hive.exec.dynamic.partition=true;</p><p style="margin:10px auto;">set hive.exec.dynamic.partition.mode=nonstrict;</p></div><div style="margin:0px;padding:0px;">给分区表灌入数据：</div><div style="margin:0px;padding:0px;">insert into table mytable3 partition (sex) select id,name,'boy' from student_mdf;</div><div style="margin:0px;padding:0px;">查询表分区：show partitions mytable3;</div><div style="margin:0px;padding:0px;">查询分区表数据：select * from mytable3;</div><div style="margin:0px;padding:0px;">查询表结构：desc mytable3;</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:16px;">DML:</span></span></div><div style="margin:0px;padding:0px;">重命名表：alter table student rename to student_mdf</div><div style="margin:0px;padding:0px;">增加列：alter table student_mdf add columns (sex string);</div><div style="margin:0px;padding:0px;">修改列名：alter table student_mdf change sex gender string;</div><div style="margin:0px;padding:0px;">替换列结构：alter table student_mdf replace columns (id string, name string);</div><div style="margin:0px;padding:0px;">装载数据：（本地数据）load data local inpath '/home/lym/zs.txt' overwrite into student_mdf;</div><div style="margin:0px;padding:0px;">                   （HDFS数据）load data inpath '/zs.txt' into table student_mdf;</div><div style="margin:0px;padding:0px;">插入一条数据：insert into table student_mdf values('1','zhangsan');</div><div style="margin:0px;padding:0px;">创建表接收查询结果：create table mytable5 as select id, name from mytable3;</div><div style="margin:0px;padding:0px;">导出数据：（导出到本地）insert overwrite local directory '/root/hivedata/mytable5.txt' select * from mytable5;</div><div style="margin:0px;padding:0px;">                   （导出到HDFS）</div><div style="margin:0px;padding:0px;">insert overwrite directory 'hdfs://master:9000/user/hive/warehouse/mytable5_load' select * from mytable5;</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:16px;"><span style="margin:0px;padding:0px;">数据查询：</span></span></div><div style="margin:0px;padding:0px;"> select * from mytable3;  查询全表</div><div style="margin:0px;padding:0px;">select uid,uname from student; 查询学生表中的学生姓名与学号字段</div><div style="margin:0px;padding:0px;">select  uname,count(*) from student group by uname; 统计学生表中每个名字的个数</div><div style="margin:0px;padding:0px;">常用的功能还有 having、order by、sort by、distribute by、cluster by；等等</div><div style="margin:0px;padding:0px;">关联查询中有</div><div style="margin:0px;padding:0px;">内连接：将符合两边连接条件的数据查询出来</div><div style="margin:0px;padding:0px;">select * from t_a a inner join t_b b on a.id=b.id;</div><div style="margin:0px;padding:0px;">左外连接：以左表数据为匹配标准，右边若匹配不上则数据显示null</div><div style="margin:0px;padding:0px;">select * from t_a a left join t_b b on a.id=b.id;</div><div style="margin:0px;padding:0px;">右外连接：与左外连接相反</div><div style="margin:0px;padding:0px;">select * from t_a a right join t_b b on a.id=b.id;</div><div style="margin:0px;padding:0px;">左半连接：左半连接会返回左边表的记录，前提是其记录对于右边表满足on语句中的判定条件。</div><div style="margin:0px;padding:0px;">select * from t_a a left semi join t_b b on a.id=b.id;</div><div style="margin:0px;padding:0px;">全连接(full outer join)：</div><div style="margin:0px;padding:0px;">select * from t_a a full join t_b b on a.id=b.id;</div><div style="margin:0px;padding:0px;">in/exists关键字(1.2.1之后新特性)：效果等同于left semi join</div><div style="margin:0px;padding:0px;">select * from t_a a where a.id in (select id from t_b);</div><div style="margin:0px;padding:0px;">select * from t_a a where exists (select * from t_b b where a.id = b.id);</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:16px;">shell操作Hive指令：</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">-e：从命令行执行指定的HQL:</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017212953162-438605194.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"></span><p style="margin:10px auto;"> </p></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">-f：执行HQL脚本</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">-v：输出执行的HQL语句到控制台</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:16px;"> <img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017212813537-1403384921.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"></span></span></span><p style="margin:10px auto;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;"> 内置函数<br style="margin:0px;padding:0px;"></span></span></p></div><div style="margin:0px;padding:0px;"> 查看内置函数：show functions;</div><div style="margin:0px;padding:0px;">显示函数的详细信息：DESC FUNCTION abs;</div><div style="margin:0px;padding:0px;"> 重要常用内置函数：sum()--求和        count()--求数据量             avg()--求平均值</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">                                  distinct--去重       min--求最小值               max--求最大值</span></div><div style="margin:0px;padding:0px;"> 自定义函数：</div><div style="margin:0px;padding:0px;">1.先开发一个简单的Java类，org.apache.hadoop.hive.ql.exec.UDF，重载evaluate方法</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">import org.apache.hadoop.hive.ql.exec.UDF;</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">public final class AddUdf extends UDF {</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">public Integer evaluate(Integer a, Integer b) {</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">if (null == a || null == b) {</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">return null;</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">} return a + b;</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">}</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">public Double evaluate(Double a, Double b) {</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">if (a == null || b == null)</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">return null;</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">return a + b;}</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"></span>}</span></div><div style="margin:0px;padding:0px;"> 2.打成jar包上传到服务器</div><div style="margin:0px;padding:0px;"> 3.将jar包添加到hive  add jar /home/lan/jar/addudf.jar;</div><div style="margin:0px;padding:0px;"> 4.创建临时函数与开发好的class关联起来 </div><div style="margin:0px;padding:0px;">     CREATE TEMPORARY FUNCTION add_example AS 'org.day0914.AddUdf';</div><div style="margin:0px;padding:0px;"> 5.使用自定义函数  SELECT add_example(scores.math, scores.art) FROM scores;</div><div style="margin:0px;padding:0px;"> 销毁临时函数：DROP TEMPORARY FUNCTION add_example;</div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"> Hive相关工具：Sqoop Azkaban Flume</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;">Sqoop</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">Sqoop是一个开源工具，它允许用户将数据从关系型数据库抽取到Hadoop中，用于进一步的处理。抽取出的数据可以被MapReduce程序使用，也可以被其他类似于Hive的工具使用。一旦形成分析结果，Sqoop便可以将这些结果导回数据库，供其他客户端使用</span><br style="margin:0px;padding:0px;"></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">用户向 Sqoop 发起一个命令之后，这个命令会转换为一个基于 Map Task 的 MapReduce 作业。Map Task 会访问数据库的元数据信息，通过并行的 Map Task 将数据库的数据读取出来，然后导入 Hadoop 中。 将 Hadoop 中的数据，导入传统的关系型数据库中。它的核心思想就是通过基于 Map Task （只有 map）的 MapReduce 作业，实现数据的并发拷贝和传输，这样可以大大提高效率</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">数据导入：首先用户输入一个 Sqoop import 命令，Sqoop 会从关系型数据库中获取元数据信息，比如要操作数据库表的 schema是什么样子，这个表有哪些字段，这些字段都是什么数据类型等。它获取这些信息之后，会将输入命令转化为基于 Map 的 MapReduce作业。这样 MapReduce作业中有很多 Map 任务，每个 Map 任务从数据库中读取一片数据，这样多个 Map 任务实现并发的拷贝，把整个数据快速的拷贝到 HDFS 上</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">数据导出：首先用户输入一个 Sqoop export 命令，它会获取关系型数据库的 schema，建立 Hadoop 字段与数据库表字段的映射关系。 然后会将输入命令转化为基于 Map 的 MapReduce作业，这样 MapReduce作业中有很多 Map 任务，它们并行的从 HDFS 读取数据，并将整个数据拷贝到数据库中</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">sqoop查询语句</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">进入sqoop安装主目录：cd /home/lanou/sqoop-1.4.5.bin__hadoop-2.0.4-alpha</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">将HDFS上的数据导入到MySQL中：bin/sqoop export --connect jdbc:mysql://192.168.2.136:3306/test --username hadoop --password hadoop --table name_cnt --export-dir '/user/hive/warehouse/mydb.db/name_cnt' --fields-terminated-by '\t'</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">将MySQL中的数据导入到HDFS上：</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;"><span style="margin:0px;padding:0px;font-size:14px;">bin/sqoop import --connect jdbc:mysql://192.168.2.136:3306/test --username hadoop --password hadoop --table name_cnt -m 1</span></span></div><div style="margin:0px;padding:0px;"><p style="margin:10px auto;">sqoop：表示sqoop命令 <br style="margin:0px;padding:0px;">export：表示导出 <br style="margin:0px;padding:0px;">--connect jdbc:mysql://192.168.2.136:3306/test ：表示告诉jdbc，连接mysql的url。<br style="margin:0px;padding:0px;">--username hadoop: 连接mysql的用户名 <br style="margin:0px;padding:0px;">--password hadoop: 连接mysql的密码<br style="margin:0px;padding:0px;">--table name_cnt: 从mysql要导出数据的表名称 <br style="margin:0px;padding:0px;">--export-dir '/user/hive/warehouse/mydb.db/name_cnt' hive中被导出的文件 <br style="margin:0px;padding:0px;">--fields-terminated-by '\t': 指定输出文件中的行的字段分隔符   </p></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;">Azkaban</span></span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">Azkaban是由Linkedin公司推出的一个批量工作流任务调度器，用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban使用job配置文件建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">Azkaban功能特点：1.兼容所有Hadoop版本</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">                               2.可以通过WebUI来进行管理配置，操作方便</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">                               3.更容易设置job的依赖关系</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">                               4.可以配置定时任务调度</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">                               5.邮件警告失败或成功 等等</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;">Azkaban架构：</span></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171018111109677-187092210.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"></span><p style="margin:10px auto;"> </p></div><div style="margin:0px;padding:0px;">Azkaban使用MySQL去做一些状态的存储</div><div style="margin:0px;padding:0px;">Azkaban Web服务：Azkaban使用Jetty作为Web服务器，用作控制器以及提供Web界面</div><div style="margin:0px;padding:0px;">AzkabanWebServer对数据库的使用：项目管理：对项目权限和上传文件的管理；执行流程状态：对正在执行的程序进行跟踪；查看任务执行结果以及历史日志；调度程序：保持预定的工作状态。</div><div style="margin:0px;padding:0px;"> </div><div style="margin:0px;padding:0px;">Azkaban执行服务器，执行提交的工作流</div><div style="margin:0px;padding:0px;">AzkabanExecutorServer 对数据库的使用：获取项目：从数据库中检索项目文件；执行工作流或Jobs：从数据库获取要执行的任务流；Logs：存储作业的输出日志，并将其流入数据库；不同的依赖进行交流：如果一个流在不同的执行器上运行，它将从数据库中获取状态</div><div style="margin:0px;padding:0px;">安装Azkaban：</div><div style="margin:0px;padding:0px;">1.要先配置mysql。</div><div style="margin:0px;padding:0px;">   1.1修改mysql的编码，vim /etc/my.cnf</div><div style="margin:0px;padding:0px;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171018112232099-1634618432.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"><p style="margin:10px auto;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171018112241631-2020669110.png" alt="" style="padding:0px;border:none;max-width:800px;"></p><p style="margin:10px auto;">  1.2重启mysql，service mysqld restart 然后进入mysql，创建azkaban数据库并授权，刷新权限。与创建hive数据库相同。</p><p style="margin:10px auto;">2.配置Azkaban Web Server</p><p style="margin:10px auto;"> 2.1解压Azkaban压缩包 unzip azkaban-web-2.5.0.zip</p><p style="margin:10px auto;"> 2.2上传mysql驱动包 </p><p style="margin:10px auto;">    cp mysql-connector-java-5.1.43/mysql-connector-java-5.1.43-bin.jar azkaban-web-2.5.0/extlib/</p><p style="margin:10px auto;"> 2.3修改配置文件</p><p style="margin:10px auto;"> 修改azkaban.properties文件 cd azkaban-web-2.5.0/conf/   vim azkaban.properties </p><p style="margin:10px auto;">#默认时区改为亚洲/上海 默认为美国 <br style="margin:0px;padding:0px;">default.timezone.id=Asia/Shanghai  <br style="margin:0px;padding:0px;">#数据库连接IP  <br style="margin:0px;padding:0px;">mysql.host=master</p><p style="margin:10px auto;">修改文件权限：  chmod 755 /home/lanou/azkaban/azkaban-web-2.5.0/bin/*</p><p style="margin:10px auto;">配置jetty ssl：keytool -keystore keystore -alias jetty -genkey -keyalg RSA</p><p style="margin:10px auto;">Enter keystore password: password<br style="margin:0px;padding:0px;">What is your first and last name? 您的名字与姓氏是什么？<br style="margin:0px;padding:0px;">[Unknown]: jetty.mortbay.org<br style="margin:0px;padding:0px;">What is the name of your organizational unit?您的组织单位名称是什么？<br style="margin:0px;padding:0px;">[Unknown]: Jetty<br style="margin:0px;padding:0px;">What is the name of your organization?您的组织名称是什么？<br style="margin:0px;padding:0px;">[Unknown]: Mort Bay Consulting Pty. Ltd.<br style="margin:0px;padding:0px;">What is the name of your City or Locality?您所在的城市或区域名称是什么？<br style="margin:0px;padding:0px;">[Unknown]:<br style="margin:0px;padding:0px;">What is the name of your State or Province?您所在的州或省份名称是什么？<br style="margin:0px;padding:0px;">[Unknown]:<br style="margin:0px;padding:0px;">What is the two-letter country code for this unit?该单位的两字母国家代码是什么<br style="margin:0px;padding:0px;">[Unknown]:<br style="margin:0px;padding:0px;">Is CN=jetty.mortbay.org, OU=Jetty, O=Mort Bay Consulting Pty. Ltd.,<br style="margin:0px;padding:0px;">L=Unknown, ST=Unknown, C=Unknown correct?正确吗？<br style="margin:0px;padding:0px;">[no]: yes<br style="margin:0px;padding:0px;">Enter key password for &lt;jetty&gt;<br style="margin:0px;padding:0px;">(RETURN if same as keystore password): password</p><p style="margin:10px auto;">     这里的密码要与 azkaban-web-2.5.0/conf/azkaban.properties  中设置的密码相同，否则会报错Keystore was tampered with, or password was incorrect。</p><p style="margin:10px auto;"> </p><p style="margin:10px auto;">1.job：</p><p style="margin:10px auto;">type=command<br style="margin:0px;padding:0px;">command=hadoop fs -mkdir /test<br style="margin:0px;padding:0px;">command.1=hadoop fs -put /home/lym /hadoop-2.7.1/README.txt /test</p><p style="margin:10px auto;">2.job：</p><p style="margin:10px auto;">type=command<br style="margin:0px;padding:0px;">command=hadoop jar /home/lym/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test /test-out<br style="margin:0px;padding:0px;">dependencies=1</p><p style="margin:10px auto;">实现上传/README.txt并且用wordcount计算</p><p style="margin:10px auto;">把需要运行的job放在同一个文件下面打成.zip格式的包，注意 Azkaban目前只支持.zip格式</p><p style="margin:10px auto;">页面操作：首先是登陆azkaban；创建一个工程；上传job；执行job；查看job执行情况</p><p style="margin:10px auto;">绿色代表成功，蓝色是运行，红色是失败。可以查看job运行时间，依赖和日志，点击details可以查看各个job运行情况</p><p style="margin:10px auto;"> </p></div><div style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;"><span style="margin:0px;padding:0px;font-size:18px;">Flume</span></span></div><div style="margin:0px;padding:0px;"> Flume是Cloudera提供的日志收集系统，具有分布式、高可靠、高可用性等特点，对海量日志采集、聚合和传输，Flume支持在日志系统中制定各类数据发送，同时，Flume提供对数据进行简单处理，并写到各种数接受方的能力。其设计的原理也是基于将数据流，如日志数据从各种网站服务器上汇集起来存储到HDFS，HBase等集中存储器中。</div><div style="margin:0px;padding:0px;"> <img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171018133654865-2115825649.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"><p style="margin:10px auto;">Flume的核心是把数据从数据源收集过来，在送到目的地，为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据<br style="margin:0px;padding:0px;">Flume传输的数据基本单位是Event，如果是文本文件，通常是一行记录，这也是事务的基本单位。Event从Source，流向Channel，再到Sink，本身为一个byte数组，并可携带headers信息。Event代表着一个数据流的最小完整单元，从外部数据源来，向外部的目的地去<br style="margin:0px;padding:0px;">Flume运行的核心是Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是source、channel、sink。通过这些组件，Event可以从一个地方流向另外一个地方</p></div><div style="margin:0px;padding:0px;"> Flume允许多个agent连在一起，形成前后相连的多级跳：</div><div style="margin:0px;padding:0px;"><img src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171018133725459-597130937.png" alt="" style="margin-bottom:0px;padding:0px;border:none;max-width:800px;"><p style="margin:10px auto;">核心组件：source channel sink</p><p style="margin:10px auto;">source：source负责接收event或通过特殊机制产生event，并将events批量的放到一个或多个channel；source必须至少和一个channel关联</p><p style="margin:10px auto;">channel：channel位于source和sink之间，用于缓存进来的event；当Sink成功的将event发送到下一跳的channel或最终目的时候，event从Channel移除</p><p style="margin:10px auto;">sink：Sink负责将event传输到下一跳或最终目的；sink在设置存储数据时，可以向文件系统、数据库、Hadoop存数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到hadoop中，便于日后进行相应的数据分析；必须作用于一个确切的channel</p><p style="margin:10px auto;">安装部署：</p><p style="margin:10px auto;">1.下载 http://mirror.bit.edu.cn/apache/flume/1.6.0/</p><p style="margin:10px auto;">2.解压   tar -xvf apache-flume-1.6.0-bin.tar.gz   tar -xvf apache-flume-1.6.0-src.tar.gz</p><p style="margin:10px auto;">3.将源码合并至安装目录apache-flume-1.6.0-bin下：</p><p style="margin:10px auto;"> cp -r apache-flume-1.6.0-src apache-flume-1.6.0-bin/</p><p style="margin:10px auto;">3.配置环境变量 vim ~/.bash_profile </p><p style="margin:10px auto;">export FLUME_HOME=/home/lan/apache-flume-1.6.0-bin/</p><p style="margin:10px auto;"><span style="margin:0px;padding:0px;">export PATH=$PATH:$FLUME_HOME/bin</span></p><p style="margin:10px auto;">4.测试flume-ng是否安装成功：flume-ng version</p><p style="margin:10px auto;">测试flume-ng功能：将收集到的日志输出到hdfs上为</p><p style="margin:10px auto;">新建一个flume代理agent1的配置文件example.conf：</p><p style="margin:10px auto;">cd apache-flume-1.6.0-bin/conf/   vim example.conf</p><p style="margin:10px auto;">#agent1<br style="margin:0px;padding:0px;">agent1.sources = source1<br style="margin:0px;padding:0px;">agent1.sinks = sink1<br style="margin:0px;padding:0px;">agent1.channels = c1</p><p style="margin:10px auto;">#source1<br style="margin:0px;padding:0px;">agent1.sources.source1.type = spooldir<br style="margin:0px;padding:0px;">agent1.sources.source1.spoolDir = /home/lan/agent1log<br style="margin:0px;padding:0px;">agent1.sources.source1.channels = c1<br style="margin:0px;padding:0px;">agent1.sources.source1.fileHeader = false</p><p style="margin:10px auto;">#sink1<br style="margin:0px;padding:0px;">agent1.sinks.sink1.type = hdfs<br style="margin:0px;padding:0px;">agent1.sinks.sink1.hdfs.path = hdfs://master:9000/agentlog<br style="margin:0px;padding:0px;">agent1.sinks.sink1.hdfs.fileType = DataStream<br style="margin:0px;padding:0px;">agent1.sinks.sink1.hdfs.writeFormat = TEXT<br style="margin:0px;padding:0px;">agent1.sinks.sink1.hdfs.rollInteval = 4<br style="margin:0px;padding:0px;">agent1.sinks.sink1.channel = c1</p><p style="margin:10px auto;">#channel1<br style="margin:0px;padding:0px;">agent1.channels.c1.type = file<br style="margin:0px;padding:0px;">agent1.channels.c1.checkpointDir = /home/lan/agent1_tmp1<br style="margin:0px;padding:0px;">agent1.channels.c1.dataDirs = /home/lan/agent1_tmpdata<br style="margin:0px;padding:0px;">#agent1.channels.channel1.capacity = 10000<br style="margin:0px;padding:0px;">#agent1.channels.channel.transactionCapacity = 1000</p><p style="margin:10px auto;">新建agent1log ：mkdir agent1log</p><p style="margin:10px auto;">启动flume-ng：</p><p style="margin:10px auto;"> cd apache-flume-1.6.0-bin</p><p style="margin:10px auto;">flume-ng agent -n agent1 -c conf -f /home/lan/apache-flume-1.6.0-bin/conf/example.conf -Dflume.root.logger=DEBUG,console</p><p style="margin:10px auto;">另启一个terminal,在监测目录下创建新的文件test2.txt<br style="margin:0px;padding:0px;">cd ~/agent1log<br style="margin:0px;padding:0px;">vim test2.txt</p><p style="margin:10px auto;">查看sink1的输出，发现目标路径下有一个以FlumeData开始，产生文件的时间戳为后缀的文件，说明flume能监测到目标目录变化，将产生变化的部分实时地收集到sink的输出中。</p></div></div></div></div></div></div></div></div></div>            </div>
                </div>