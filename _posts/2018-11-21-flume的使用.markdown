---
layout:     post
title:      flume的使用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>文本数据：软件、硬件打印信息。<br>
流媒体：音视频、图片</p>

<p>flume是什么？？<br>
flume是一个高效的可靠、可用的、分布式的海量日志数据收集、聚合、传输工具。<br>
Flume is a distributed, reliable, and available service for efficiently <br>
collecting, aggregating, and moving large amounts of log data</p>

<p>flume中组件及作用？？<br>
client：客户端(运行agent的地方)<br>
source:数据源，负责接收数据<br>
channel:管道，负责接收source的数据，然后并将数据推送到sink<br>
sink:负责拉去channel中的数据，将其推送到持久化系统。<br>
interceptor：拦截器，flume允许使用拦截器拦截数据，它作用于source、sink阶段，flume还允许拦截器链。<br>
selector:选择器，作用于source阶段，然后决定数据发送的方式。<br>
event：flume的事件，相当于一条数据。<br>
agent：flume的客户端，一个agent运行在一个jvm里面。它是flume的最小运行单元。</p>

<p><br>
source的种类：<br>
avro 、 exec 、 spooling dir 、 syslogtcp 、 httpsource 、 avro sink() 、kafka等。<br>
channel的种类：<br>
file:<br>
memory:<br>
jdbc：<br>
kafka：</p>

<p>sinks的种类：<br>
logger 、 avro 、 hdfs、kafka 等。</p>

<p><br>
数据模型：<br>
单一数据流模型<br>
多数据流模型：</p>

<p><br>
flume的安装：</p>

<p>flume 0.9 和 1.x的版本的区别？<br>
1、0.9以前的叫flume-og，而1.x的flume-ng<br>
2、0.9区分逻辑和物理的节点，而1.x不在区分逻辑的和物理的node节点，每一个agent就是一个服务。<br>
3、0.9需要master和zookeeper的支持，而1.x不在需要其支持。<br>
4、0.9开发并不是很灵活，而1.x较为灵活，可以支持很多功能模块的自定义(source、sink、channel、interceptor、selector等)。</p>

<p><br>
flume案例？？<br>
案例1、 avro + memory + logger</p>

<p><br>
vi ./conf/avro<br>
#定义agent必须三个组件<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>#配置sources<br>
a1.sources.r1.type=avro<br>
a1.sources.r1.bind=hadoop02<br>
a1.sources.r1.port=6666</p>

<p>#配置channels<br>
a1.channels.c1.type=memory</p>

<p>#配置sinks<br>
a1.sinks.s1.type=logger</p>

<p>#将source和sink通过channel连接上<br>
a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p><br>
启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/avro1.conf -n a1 -Dflume.root.logger=INFO,console<br>
测试：<br>
flume-ng avro-client -c /usr/local/flume-1.6.0/conf/ -H hadoop02 -p 6666 -F /home/flumedata/avrodata</p>

<p><br>
案例2、 exec + memory + logger<br>
vi ./conf/exec<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/exedata</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p><br>
a1.sinks.s1.type = hdfs<br>
a1.sinks.s1.hdfs.path = /flume/events/exec<br>
a1.sinks.s1.hdfs.filePrefix = events-<br>
a1.sinks.s1.hdfs.round = true<br>
a1.sinks.s1.hdfs.roundValue = 10<br>
a1.sinks.s1.hdfs.roundUnit = minute</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/exec -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p>案例3、 spooldir + memory + logger<br>
vi ./conf/spooldir<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=spoolDir<br>
a1.sources.r1.spoolDir=/home/flumedata/spool1<br>
a1.sources.r1.fileHeader=true<br>
a1.sources.r1.fileHeaderKey=file</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p><br>
a1.sinks.s1.type = logger</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/exec -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p><br>
案例4、 syslogtcp + memory + logger<br>
vi ./conf/syslogtcp<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=syslogtcp<br>
a1.sources.r1.port=6666<br>
a1.sources.r1.host=hadoop01</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = logger</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/syslogtcp -n a1 -Dflume.root.logger=INFO,console<br>
测试：<br>
echo "hello qianfeng" | nc hadoop01 6666</p>

<p>案例5、 http + memory + logger<br>
vi ./conf/http<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=org.apache.flume.source.http.HTTPSource<br>
a1.sources.r1.port=6666<br>
a1.sources.r1.bind=hadoop01</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = logger</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/http -n a1 -Dflume.root.logger=INFO,console<br>
测试：<br>
curl -X POST -d '[{"headers":{"time":"2017-06-13"},"body":"this is http"}]' http://hadoop01:6666</p>

<p><br>
案例6、 exec + memory + hdfs<br>
vi ./conf/hdfs<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/exedata</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = hdfs<br>
a1.sinks.s1.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br>
a1.sinks.s1.hdfs.filePrefix = qianfeng-<br>
a1.sinks.s1.hdfs.fileSuffix=.log<br>
a1.sinks.s1.hdfs.inUseSuffix=.tmp<br>
a1.sinks.s1.hdfs.rollInterval=2<br>
a1.sinks.s1.hdfs.rollSize=1024<br>
a1.sinks.s1.hdfs.fileType=DataStream<br>
a1.sinks.s1.hdfs.writeFormat=Text<br>
a1.sinks.s1.hdfs.round = true<br>
a1.sinks.s1.hdfs.roundValue = 1<br>
a1.sinks.s1.hdfs.roundUnit = second<br>
a1.sinks.s1.hdfs.useLocalTimeStamp=false</p>

<p><br>
a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/hdfs -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p>案例7、 exec + file + hdfs<br>
vi ./conf/file<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/exedata</p>

<p>a1.channels.c1.type=file<br>
a1.channels.c1.checkpointDir=/home/flumedata/checkpoint<br>
a1.channels.c1.dataDirs=/home/flumedata/data</p>

<p>a1.sinks.s1.type = hdfs<br>
a1.sinks.s1.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br>
a1.sinks.s1.hdfs.filePrefix = qianfeng-<br>
a1.sinks.s1.hdfs.fileSuffix=.log<br>
a1.sinks.s1.hdfs.inUseSuffix=.tmp<br>
a1.sinks.s1.hdfs.rollInterval=2<br>
a1.sinks.s1.hdfs.rollSize=1024<br>
a1.sinks.s1.hdfs.fileType=DataStream<br>
a1.sinks.s1.hdfs.writeFormat=Text<br>
a1.sinks.s1.hdfs.round = true<br>
a1.sinks.s1.hdfs.roundValue = 1<br>
a1.sinks.s1.hdfs.roundUnit = second<br>
a1.sinks.s1.hdfs.useLocalTimeStamp=false</p>

<p><br>
a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/file -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p>---------------------#############拦截器--------------------<br>
案例1、<br>
vi ./conf/ts1<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/test.dat<br>
a1.sources.r1.interceptors = i1 i2 i3<br>
a1.sources.r1.interceptors.i1.type = timestamp<br>
a1.sources.r1.interceptors.i1.preserveExisting=true<br>
a1.sources.r1.interceptors.i2.type = host<br>
a1.sources.r1.interceptors.i2.hostHeader = hostname<br>
a1.sources.r1.interceptors.i2.preserveExisting=true<br>
a1.sources.r1.interceptors.i3.type = static<br>
a1.sources.r1.interceptors.i3.key = city<br>
a1.sources.r1.interceptors.i3.value = NEW_YORK</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = hdfs<br>
a1.sinks.s1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S<br>
a1.sinks.s1.hdfs.filePrefix = %{hostname}-<br>
a1.sinks.s1.hdfs.fileSuffix=.log<br>
a1.sinks.s1.hdfs.inUseSuffix=.tmp<br>
a1.sinks.s1.hdfs.rollInterval=2<br>
a1.sinks.s1.hdfs.rollSize=1024<br>
a1.sinks.s1.hdfs.fileType=DataStream<br>
a1.sinks.s1.hdfs.writeFormat=Text<br>
a1.sinks.s1.hdfs.round = true<br>
a1.sinks.s1.hdfs.roundValue = 1<br>
a1.sinks.s1.hdfs.roundUnit = second<br>
a1.sinks.s1.hdfs.useLocalTimeStamp=false</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/ts -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p><br>
案例2、<br>
vi ./conf/ts3<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/exedata<br>
a1.sources.r1.interceptors = i1 i2 i3<br>
a1.sources.r1.interceptors.i1.type = timestamp<br>
a1.sources.r1.interceptors.i1.preserveExisting=true<br>
a1.sources.r1.interceptors.i2.type = host<br>
a1.sources.r1.interceptors.i2.hostHeader = hostname<br>
a1.sources.r1.interceptors.i2.preserveExisting=true<br>
a1.sources.r1.interceptors.i3.type = static<br>
a1.sources.r1.interceptors.i3.key = city<br>
a1.sources.r1.interceptors.i3.value = NEW_YORK</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = logger</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/ts -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p><br>
案例3、正则拦截器<br>
vi ./conf/rex<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/text.log<br>
a1.sources.r1.interceptors = i1<br>
a1.sources.r1.interceptors.i1.type = regex_filter<br>
a1.sources.r1.interceptors.i1.regex=^[0-9].*$<br>
a1.sources.r1.interceptors.i1.excludeEvents=false</p>

<p><br>
a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = logger</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p>启动agent:<br>
flume-ng agent -c ./conf/ -f ./conf/rex -n a1 -Dflume.root.logger=INFO,console<br>
测试：</p>

<p>#####案例3、复制选择器<br>
vi ./conf/rep<br>
a1.sources=r1<br>
a1.channels=c1 c2<br>
a1.sinks=s1 s2</p>

<p>a1.sources.r1.type=exec<br>
a1.sources.r1.command= tail -f /home/flumedata/test.dat<br>
a1.sources.r1.selector.type = replicating<br>
a1.sources.r1.selector.optional = c2</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.channels.c2.type=memory<br>
a1.channels.c2.capacity=1000<br>
a1.channels.c2.transactionCapacity=100<br>
a1.channels.c2.keep-alive=3<br>
a1.channels.c2.byteCapacityBufferPercentage = 20<br>
a1.channels.c2.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = logger</p>

<p>a1.sinks.s2.type = hdfs<br>
a1.sinks.s2.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S<br>
a1.sinks.s2.hdfs.filePrefix = event-<br>
a1.sinks.s2.hdfs.fileSuffix=.log<br>
a1.sinks.s2.hdfs.inUseSuffix=.tmp<br>
a1.sinks.s2.hdfs.rollInterval=2<br>
a1.sinks.s2.hdfs.rollSize=1024<br>
a1.sinks.s2.hdfs.fileType=DataStream<br>
a1.sinks.s2.hdfs.writeFormat=Text<br>
a1.sinks.s2.hdfs.round = true<br>
a1.sinks.s2.hdfs.roundValue = 1<br>
a1.sinks.s2.hdfs.roundUnit = second<br>
a1.sinks.s2.hdfs.useLocalTimeStamp=true</p>

<p>a1.sources.r1.channels=c1 c2<br>
a1.sinks.s1.channel=c1<br>
a1.sinks.s2.channel=c2</p>

<p><br>
#####案例4、复分选择器<br>
vi ./conf/mul<br>
a1.sources=r1<br>
a1.channels=c1 c2<br>
a1.sinks=s1 s2</p>

<p>a1.sources.r1.type=org.apache.flume.source.http.HTTPSource<br>
a1.sources.r1.port=6666<br>
a1.sources.r1.bind=hdp01<br>
a1.sources.r1.selector.type = multiplexing<br>
a1.sources.r1.selector.header = status<br>
a1.sources.r1.selector.mapping.CZ = c1<br>
a1.sources.r1.selector.mapping.US = c2<br>
a1.sources.r1.selector.default = c1</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.channels.c2.type=memory<br>
a1.channels.c2.capacity=1000<br>
a1.channels.c2.transactionCapacity=100<br>
a1.channels.c2.keep-alive=3<br>
a1.channels.c2.byteCapacityBufferPercentage = 20<br>
a1.channels.c2.byteCapacity = 800000</p>

<p>a1.sinks.s1.type = logger</p>

<p>a1.sinks.s2.type = hdfs<br>
a1.sinks.s2.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S<br>
a1.sinks.s2.hdfs.filePrefix = event-<br>
a1.sinks.s2.hdfs.fileSuffix=.log<br>
a1.sinks.s2.hdfs.inUseSuffix=.tmp<br>
a1.sinks.s2.hdfs.rollInterval=2<br>
a1.sinks.s2.hdfs.rollSize=1024<br>
a1.sinks.s2.hdfs.fileType=DataStream<br>
a1.sinks.s2.hdfs.writeFormat=Text<br>
a1.sinks.s2.hdfs.round = true<br>
a1.sinks.s2.hdfs.roundValue = 1<br>
a1.sinks.s2.hdfs.roundUnit = second<br>
a1.sinks.s2.hdfs.useLocalTimeStamp=true</p>

<p>a1.sources.r1.channels=c1 c2<br>
a1.sinks.s1.channel=c1<br>
a1.sinks.s2.channel=c2</p>

<p>测试数据：<br>
curl -X POST -d '[{"headers":{"status":"2017-06-13"},"body":"this is default"}]' http://hdp01:6666<br>
curl -X POST -d '[{"headers":{"status":"CZ"},"body":"this is CZ"}]' http://hadoop01:6666<br>
curl -X POST -d '[{"headers":{"status":"US"},"body":"this is US"}]' http://hadoop01:6666</p>

<p><br>
案例5、flume集群搭建：<br>
hadoop01的配置：<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=syslogtcp<br>
a1.sources.r1.port=6666<br>
a1.sources.r1.host=hadoop01</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type =avro<br>
a1.sinks.s1.hostname=hadoop03<br>
a1.sinks.s1.port=6666</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p><br>
hadoop02的配置：<br>
a1.sources=r1<br>
a1.channels=c1<br>
a1.sinks=s1</p>

<p>a1.sources.r1.type=syslogtcp<br>
a1.sources.r1.port=6666<br>
a1.sources.r1.host=hadoop02</p>

<p>a1.channels.c1.type=memory<br>
a1.channels.c1.capacity=1000<br>
a1.channels.c1.transactionCapacity=100<br>
a1.channels.c1.keep-alive=3<br>
a1.channels.c1.byteCapacityBufferPercentage = 20<br>
a1.channels.c1.byteCapacity = 800000</p>

<p>a1.sinks.s1.type =avro<br>
a1.sinks.s1.hostname=hadoop03<br>
a1.sinks.s1.port=6666</p>

<p>a1.sources.r1.channels=c1<br>
a1.sinks.s1.channel=c1</p>

<p><br>
hadoop03的配置：<br>
agent.sources=r1<br>
agent.channels=c1<br>
agent.sinks=s1</p>

<p>agent.sources.r1.type=avro<br>
agent.sources.r1.port=6666<br>
agent.sources.r1.bind=hadoop03</p>

<p>agent.channels.c1.type=memory<br>
agent.channels.c1.capacity=1000<br>
agent.channels.c1.transactionCapacity=100<br>
agent.channels.c1.keep-alive=3<br>
agent.channels.c1.byteCapacityBufferPercentage = 20<br>
agent.channels.c1.byteCapacity = 800000</p>

<p>agent.sinks.s1.type =logger</p>

<p>agent.sources.r1.channels=c1<br>
agent.sinks.s1.channel=c1<br>
 <br>
####然后测试：<br>
先启动master的agent:<br>
flume-ng agent -c ./conf/ -f ./conf/master -n agent -Dflume.root.logger=INFO,console &amp;<br>
然后再启动slave的agent：<br>
flume-ng agent -c ./conf/ -f ./conf/slave1 -n a1 -Dflume.root.logger=INFO,console &amp;<br>
flume-ng agent -c ./conf/ -f ./conf/slave2 -n a1 -Dflume.root.logger=INFO,console &amp;</p>

<p>flume的缺点：？？<br>
同步部署较难</p>

<p>tail -f /vat/access.log <br>
cat /vat/---access.log</p>

<p><br>
#####？？？？<br>
将合并的这台flume的sink写成hdfs sink？<br>
将sink写成hive sink？</p>

<p>flume的高可用</p>

<p><img alt="" class="has" height="418" src="https://img-blog.csdn.net/20180731235837679?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NDYwMjI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="921"></p>

<p>flume框架的设计思想----flume的内部结构</p>

<p><img alt="" class="has" height="558" src="https://img-blog.csdn.net/20180801000005876?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NDYwMjI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="923"></p>

<p> </p>

<p>多agent</p>

<p><img alt="" class="has" height="886" src="https://img-blog.csdn.net/20180801000202181?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NDYwMjI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="963"></p>

<p><img alt="" class="has" height="310" src="https://img-blog.csdn.net/20180801000300884?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NDYwMjI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="492"></p>            </div>
                </div>