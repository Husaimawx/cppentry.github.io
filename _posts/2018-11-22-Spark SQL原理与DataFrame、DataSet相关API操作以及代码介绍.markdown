---
layout:     post
title:      Spark SQL原理与DataFrame、DataSet相关API操作以及代码介绍
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/qq_34822916/article/details/78112383				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1 align="center"><strong>Spark SQL and DataFrame、DataSet</strong></h1>
<h1>1. <strong>课程目标</strong></h1>
<h2>1.1. <strong><span style="font-family:'宋体';">掌握</span>Spark SQL<span style="font-family:'宋体';">的原理</span></strong></h2>
<h2>1.2. <strong><span style="font-family:'宋体';">掌握</span>DataFrame<span style="font-family:'宋体';">数据结构和使用方式</span></strong></h2>
<h2>1.3. <strong><span style="font-family:'宋体';">熟练使用</span>Spark SQL<span style="font-family:'宋体';">完成计算任务</span></strong></h2>
<h1>2. <strong>Spark SQL</strong></h1>
<h2>2.1. <strong>Spark SQL<span style="font-family:'宋体';">概述</span></strong></h2>
<h3>2.1.1. <strong><span style="font-family:'宋体';">什么是</span>Spark SQL</strong></h3>
<div><strong><img src="https://img-blog.csdn.net/20170927142434014" alt=""><br></strong></div>
<p>Spark SQL<span style="font-family:'宋体';">是</span><span style="font-family:Calibri;">Spark</span><span style="font-family:'宋体';">用来处理结构化数据的一个模块，它提供了一个编程抽象叫做</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">并且作为分布式</span><span style="font-family:Calibri;">SQL</span><span style="font-family:'宋体';">查询引擎的作用。</span></p>
<h3>2.1.2. <strong><span style="font-family:'宋体';">为什么要学习</span>Spark SQL</strong></h3>
<p><span style="font-family:'宋体';">我们已经学习了</span>Hive<span style="font-family:'宋体';">，它是将</span><span style="font-family:Calibri;">Hive SQL</span><span style="font-family:'宋体';">转换成</span><span style="font-family:Calibri;">MapReduce</span><span style="font-family:'宋体';">然后提交到集群上执行，大大简化了编写</span><span style="font-family:Calibri;">MapReduce</span><span style="font-family:'宋体';">的程序的复杂性，由于</span><span style="font-family:Calibri;">MapReduce</span><span style="font-family:'宋体';">这种计算模型执行效率比较慢。所有</span><span style="font-family:Calibri;">Spark
 SQL</span><span style="font-family:'宋体';">的应运而生，它是将</span><span style="font-family:Calibri;">Spark SQL</span><span style="font-family:'宋体';">转换成</span><span style="font-family:Calibri;">RDD</span><span style="font-family:'宋体';">，然后提交到集群执行，执行效率非常快！</span></p>
<p>1.<span style="font-family:'宋体';">易整合</span></p>
<p>2.统一的数据访问方式</p>
<p>3.<span style="font-family:'宋体';">兼容</span>Hive </p>
<p>4.标准的数据连接 </p>
<h2>2.2. DataFrames</h2>
<h3>2.2.1. <span style="font-family:'宋体';">什么是</span>DataFrames</h3>
<p><span style="color:rgb(51,51,51);"><span style="font-family:Helvetica;">与</span>RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还</span><span style="color:rgb(51,51,51);"><span style="font-family:'宋体';">记录</span></span><span style="color:rgb(51,51,51);"><span style="font-family:Helvetica;">数据的结构信息，即</span>schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上
 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。</span></p>
<p> <img src="https://img-blog.csdn.net/20170927142538349" alt=""></p>
<h3>2.2.2. <strong><span style="font-family:'宋体';">创建</span>DataFrames</strong></h3>
<p> 在Spark SQL<span style="font-family:'宋体';">中</span>SQLContext是创建DataFrames<span style="font-family:'宋体';">和执行</span><span style="font-family:Calibri;">SQL</span><span style="font-family:'宋体';">的入口，在</span><span style="font-family:Calibri;">spark</span><span style="font-family:'宋体';">中已经内置了一个</span><strong>sqlContext</strong></p>
<p>1.<span style="font-family:'宋体';">在本地创建一个文件，有三列，分别是</span>id<span style="font-family:'宋体';">、</span><span style="font-family:Calibri;">name</span><span style="font-family:'宋体';">、</span><span style="font-family:Calibri;">age</span><span style="font-family:'宋体';">，用空格分隔，然后上传到</span><span style="font-family:Calibri;">hdfs</span><span style="font-family:'宋体';">上</span></p>
<p><span style="color:rgb(51,51,51);">hdfs dfs -put person.txt /</span></p>
<p>2.<span style="font-family:'宋体';">在</span><span style="font-family:Calibri;">spark shell</span><span style="font-family:'宋体';">执行下面命令，读取数据，将每一行的数据使用列分隔符分割</span></p>
<p><span style="color:rgb(51,51,51);">val lineRDD = sc.textFile("hdfs://node1.itcast.cn:9000/person.txt").map(_.split(" "))</span></p>
<p>3.<span style="font-family:'宋体';">定义</span><span style="font-family:Calibri;">case class</span><span style="font-family:'宋体';">（相当于表的</span><span style="font-family:Calibri;">schema</span><span style="font-family:'宋体';">）</span></p>
<p><span style="color:rgb(51,51,51);">case class Person(id:Int, name:String, age:Int)</span></p>
<p>4.<span style="font-family:'宋体';">将</span>RDD<span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">case class</span><span style="font-family:'宋体';">关联</span></p>
<p><span style="color:rgb(51,51,51);">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span></p>
<p>5.<span style="font-family:'宋体';">将</span>RDD<span style="font-family:'宋体';">转换成</span><span style="font-family:Calibri;">DataFrame</span></p>
<p><span style="color:rgb(51,51,51);">val personDF = personRDD.toDF</span></p>
<p>6.<span style="font-family:'宋体';">对</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">进行处理</span></p>
<p><span style="color:rgb(51,51,51);">personDF.show</span></p>
<p><span style="color:rgb(51,51,51);"> <img src="https://img-blog.csdn.net/20170927142820774" alt=""></span></p>
<h2>2.3. <strong>DataFrame<span style="font-family:'宋体';">常用操作</span></strong></h2>
<h3>2.3.1. <strong>DSL<span style="font-family:'宋体';">风格语法</span></strong></h3>
<p>//<span style="font-family:'宋体';">查看</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">中的内容</span></p>
<p><span style="color:rgb(51,51,51);">personDF.show</span></p>
<p>//<span style="font-family:'宋体';">查看</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">部分列中的内容</span></p>
<p><span style="color:rgb(51,51,51);">personDF.select(personDF.col("name")).show</span></p>
<p><span style="color:rgb(51,51,51);">personDF.select(col("name"), col</span><span style="color:rgb(51,51,51);">(</span><span style="color:rgb(51,51,51);">"age"</span><span style="color:rgb(51,51,51);">)</span><span style="color:rgb(51,51,51);">).show</span></p>
<p><span style="color:rgb(51,51,51);">personDF.select("name").show</span><span style="color:rgb(51,51,51);"> </span></p>
<p>//<span style="font-family:'宋体';">打印</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">的</span><span style="font-family:Calibri;">Schema</span><span style="font-family:'宋体';">信息</span></p>
<p><span style="color:rgb(51,51,51);">personDF.printSchema</span></p>
<p>//<span style="font-family:'宋体';">查询所有的</span><span style="font-family:Calibri;">name</span><span style="font-family:'宋体';">和</span><span style="font-family:Calibri;">age</span><span style="font-family:'宋体';">，并将</span><span style="font-family:Calibri;">age+1</span></p>
<p><span style="color:rgb(51,51,51);">personDF.select(col("id"), col("name"), col("age") + 1).show</span></p>
<p><span style="color:rgb(51,51,51);">personDF.select(personDF("id"), personDF("name"), personDF("age") + 1).show</span></p>
<p><span style="color:rgb(51,51,51);"> <img src="https://img-blog.csdn.net/20170927142935053" alt=""></span></p>
<p>//<span style="font-family:'宋体';">过滤</span><span style="font-family:Calibri;">age</span><span style="font-family:'宋体';">大于等于</span><span style="font-family:Calibri;">18</span><span style="font-family:'宋体';">的</span></p>
<p><span style="color:rgb(51,51,51);">personDF.filter(col("age") &gt;= 18).show</span></p>
<p><span style="color:rgb(51,51,51);"> <img src="https://img-blog.csdn.net/20170927143010796" alt=""></span></p>
<p>//<span style="font-family:'宋体';">按年龄进行分组并统计相同年龄的人数</span></p>
<p><span style="color:rgb(51,51,51);">personDF.groupBy("age").count().show()</span></p>
<p> <img src="https://img-blog.csdn.net/20170927143034522" alt=""></p>
<h3>2.3.2. <strong>SQL<span style="font-family:'宋体';">风格语法</span></strong></h3>
<p><span style="font-family:'宋体';">如果想使用</span>SQL<span style="font-family:'宋体';">风格的语法，需要将</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">注册成表</span></p>
<p><span style="color:rgb(51,51,51);">personDF.registerTempTable("t_person")</span></p>
<p>//<span style="font-family:'宋体';">查询年龄最大的前两名</span></p>
<p><span style="color:rgb(51,51,51);">sqlContext.sql("select * from t_person order by age desc limit 2").show</span></p>
<p><span style="color:rgb(51,51,51);"> <img src="https://img-blog.csdn.net/20170927143110740" alt=""></span><span style="color:rgb(51,51,51);"> </span></p>
<p>//<span style="font-family:'宋体';">显示表的</span><span style="font-family:Calibri;">Schema</span><span style="font-family:'宋体';">信息</span></p>
<p><span style="color:rgb(51,51,51);">sqlContext.sql("desc t_person").show</span></p>
<p><span style="color:rgb(51,51,51);"> <img src="https://img-blog.csdn.net/20170927143134805" alt=""></span></p>
<h1>3. <strong><span style="font-family:'宋体';">以编程方式执行</span>Spark SQL<span style="font-family:'宋体';">查询</span></strong></h1>
<h2>3.1. <strong><span style="font-family:'宋体';">编写</span>Spark SQL<span style="font-family:'宋体';">查询程序</span></strong></h2>
<p><span style="font-family:'宋体';">前面我们学习了如何在</span>Spark Shell<span style="font-family:'宋体';">中使用</span><span style="font-family:Calibri;">SQL</span><span style="font-family:'宋体';">完成查询，现在我们来实现在自定义的程序中编写</span><span style="font-family:Calibri;">Spark SQL</span><span style="font-family:'宋体';">查询程序。首先在</span><span style="font-family:Calibri;">maven</span><span style="font-family:'宋体';">项目的</span><span style="font-family:Calibri;">pom.xml</span><span style="font-family:'宋体';">中添加</span><span style="font-family:Calibri;">Spark
 SQL</span><span style="font-family:'宋体';">的依赖</span></p>
<p><span style="font-family:'宋体';"></span>
</p><table><tbody><tr><td valign="top">
<p><span style="background:rgb(239,239,239);">&lt;</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">dependency</span></strong><span style="background:rgb(239,239,239);">&gt;</span><br>
    <span style="background:rgb(239,239,239);">&lt;</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">groupId</span></strong><span style="background:rgb(239,239,239);">&gt;</span>org.apache.spark<span style="background:rgb(239,239,239);">&lt;/</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">groupId</span></strong><span style="background:rgb(239,239,239);">&gt;</span><br>
    <span style="background:rgb(239,239,239);">&lt;</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">artifactId</span></strong><span style="background:rgb(239,239,239);">&gt;</span>spark-sql_2.11<span style="background:rgb(239,239,239);">&lt;/</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">artifactId</span></strong><span style="background:rgb(239,239,239);">&gt;</span><br>
    <span style="background:rgb(239,239,239);">&lt;</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">version</span></strong><span style="background:rgb(239,239,239);">&gt;</span><span style="background-color:rgb(255,255,255);">2.2.0</span><span style="background:rgb(239,239,239);">&lt;/</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">version</span></strong><span style="background:rgb(239,239,239);">&gt;</span></p>
<p><span style="background:rgb(239,239,239);">&lt;/</span><strong><span style="color:rgb(0,0,128);background:rgb(239,239,239);">dependency</span></strong><span style="background:rgb(239,239,239);">&gt;</span></p>
</td>
</tr></tbody></table><h3>3.1.1. <strong><span style="font-family:'宋体';">通过反射推断</span>Schema</strong></h3>
<p></p>
<p><span style="font-family:'宋体';">创建一个</span>object<span style="font-family:'宋体';">为org</span><span style="font-family:Calibri;">.spark.sql.InferringSchema</span></p>
<table><tbody><tr><td valign="top">
<p><strong><span style="color:rgb(0,0,128);">package org</span></strong>.spark.sql<br><br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.{SparkConf, SparkContext}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.SQLContext<br><em><span style="color:rgb(128,128,128);"><br></span></em><strong><span style="color:rgb(0,0,128);">object </span></strong>InferringSchema {<br>
  <strong><span style="color:rgb(0,0,128);">def </span></strong>main(args: Array[<span style="color:rgb(32,153,157);">String</span>]) {<br><br>
    <em><span style="color:rgb(128,128,128);">//创建SparkConf()并设置App名称</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>conf =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SparkConf().setAppName(<strong><span style="color:rgb(0,128,0);">"SQL-1"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//SQLContext要依赖SparkContext</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>sc =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SparkContext(conf)<br>
    <em><span style="color:rgb(128,128,128);">//创建SQLContext</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>sqlContext =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SQLContext(sc)<br><br>
    <em><span style="color:rgb(128,128,128);">//从指定的地址创建RDD</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>lineRDD = sc.textFile(args(<span style="color:rgb(0,0,255);">0</span>)).map(_.split(<strong><span style="color:rgb(0,128,0);">" "</span></strong>))<br><br>
    <em><span style="color:rgb(128,128,128);">//创建case class</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //将RDD和case class关联</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>personRDD = lineRDD.map(x =&gt;
<em>Person</em>(x(<span style="color:rgb(0,0,255);">0</span>).toInt, x(<span style="color:rgb(0,0,255);">1</span>), x(<span style="color:rgb(0,0,255);">2</span>).toInt))<br>
    <em><span style="color:rgb(128,128,128);">//导入隐式转换，如果不到人无法将RDD转换成DataFrame</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //将RDD转换成DataFrame</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">import</span></strong>sqlContext.implicits._<br>
    <strong><span style="color:rgb(0,0,128);">val </span></strong>personDF = personRDD.toDF<br>
    <em><span style="color:rgb(128,128,128);">//注册表</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>personDF.registerTempTable(<strong><span style="color:rgb(0,128,0);">"t_person"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//传入SQL</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>df = sqlContext.sql(<strong><span style="color:rgb(0,128,0);">"select * from t_person order by age desc limit 2"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//将结果以JSON的方式存储到指定位置</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>df.write.json(args(<span style="color:rgb(0,0,255);">1</span>))<br>
    <em><span style="color:rgb(128,128,128);">//停止Spark Context</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>sc.stop()<br>
  }<br>
}<br><em><span style="color:rgb(128,128,128);">//case class一定要放到外面</span><span style="color:rgb(128,128,128);"><br></span></em><strong><span style="color:rgb(0,0,128);">case class </span></strong>Person(id: Int, name:<span style="color:rgb(32,153,157);">String</span>, age: Int)</p>
<p> </p>
</td>
</tr></tbody></table><p><span style="font-family:'宋体';">将程序打成</span>jar<span style="font-family:'宋体';">包，上传到</span><span style="font-family:Calibri;">spark</span><span style="font-family:'宋体';">集群，提交</span><span style="font-family:Calibri;">Spark</span><span style="font-family:'宋体';">任务</span></p>
<p><span style="color:rgb(51,51,51);">.bin/spark-submit \</span></p>
<p><span style="color:rgb(51,51,51);">--class org.spark.sql.InferringSchema \</span></p>
<p><span style="color:rgb(51,51,51);">--master local(本地方式) \或者（-- master spark://masterIP:port）</span></p>
<p><span style="color:rgb(51,51,51);">/root/spark-mvn-1.0-SNAPSHOT.jar \（用maven打的jar包）</span></p>
<p><span style="color:rgb(51,51,51);">hdfs://mater（localhost）:9000/person.txt \(hdfs地址)</span></p>
<p><span style="color:rgb(51,51,51);">hdfs://master(localhost):9000/out</span><span style="color:rgb(51,51,51);"> </span></p>
<p>查看运行结果</p>
<p><span style="color:rgb(51,51,51);">hdfs dfs -cat  hdfs://master(localhost):9000/out/part-r-*</span></p>
<p><img src="https://img-blog.csdn.net/20170927145703472" alt=""><br></p>
<h3>3.1.2. <strong><span style="font-family:'宋体';">通过</span>StructType<span style="font-family:'宋体';">直接指定</span><span style="font-family:Calibri;">Schema</span></strong></h3>
<p><span style="font-family:'宋体';">创建一个</span>object<span style="font-family:'宋体';">为org</span><span style="font-family:Calibri;">.spark.sql.SpecifyingSchema</span></p>
<table><tbody><tr><td valign="top">
<p><strong><span style="color:rgb(0,0,128);">package org</span></strong>.spark.sql<br><br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.{Row, SQLContext}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.types._<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.{SparkContext, SparkConf}<br><em><span style="color:rgb(128,128,128);"><br></span></em><strong><span style="color:rgb(0,0,128);">object </span></strong>SpecifyingSchema {<br>
  <strong><span style="color:rgb(0,0,128);">def </span></strong>main(args: Array[<span style="color:rgb(32,153,157);">String</span>]) {<br>
    <em><span style="color:rgb(128,128,128);">//创建SparkConf()并设置App名称</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>conf =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SparkConf().setAppName(<strong><span style="color:rgb(0,128,0);">"SQL-2"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//SQLContext要依赖SparkContext</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>sc =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SparkContext(conf)<br>
    <em><span style="color:rgb(128,128,128);">//创建SQLContext</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>sqlContext =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SQLContext(sc)<br>
    <em><span style="color:rgb(128,128,128);">//从指定的地址创建RDD</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>personRDD = sc.textFile(args(<span style="color:rgb(0,0,255);">0</span>)).map(_.split(<strong><span style="color:rgb(0,128,0);">" "</span></strong>))<br>
    <em><span style="color:rgb(128,128,128);">//通过StructType直接指定每个字段的schema</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>schema =
<em>StructType</em>(<br>
      <em><span style="color:rgb(102,14,122);">List</span></em>(<br>
        <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"id"</span></strong>, IntegerType,<strong><span style="color:rgb(0,0,128);">true</span></strong>),<br>
        <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"name"</span></strong>, StringType,<strong><span style="color:rgb(0,0,128);">true</span></strong>),<br>
        <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"age"</span></strong>, IntegerType,<strong><span style="color:rgb(0,0,128);">true</span></strong>)<br>
      )<br>
    )<br>
    <em><span style="color:rgb(128,128,128);">//将RDD映射到rowRDD</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>rowRDD = personRDD.map(p =&gt;
<em>Row</em>(p(<span style="color:rgb(0,0,255);">0</span>).toInt, p(<span style="color:rgb(0,0,255);">1</span>).trim, p(<span style="color:rgb(0,0,255);">2</span>).toInt))<br>
    <em><span style="color:rgb(128,128,128);">//将schema信息应用到rowRDD上</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>personDataFrame = sqlContext.createDataFrame(rowRDD, schema)<br>
    <em><span style="color:rgb(128,128,128);">//注册表</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>personDataFrame.registerTempTable(<strong><span style="color:rgb(0,128,0);">"t_person"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//执行SQL</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>df = sqlContext.sql(<strong><span style="color:rgb(0,128,0);">"select * from t_person order by age desc limit 4"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//将结果以JSON的方式存储到指定位置</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>df.write.json(args(<span style="color:rgb(0,0,255);">1</span>))<br>
    <em><span style="color:rgb(128,128,128);">//停止Spark Context</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>sc.stop()<br>
  }<br>
}</p>
<p> </p>
</td>
</tr></tbody></table><p><span style="font-family:'宋体';">将程序打成</span>jar<span style="font-family:'宋体';">包，上传到</span><span style="font-family:Calibri;">spark</span><span style="font-family:'宋体';">集群，提交</span><span style="font-family:Calibri;">Spark</span><span style="font-family:'宋体';">任务</span></p>
<p></p>
<p style="font-family:'宋体';"><span style="color:rgb(51,51,51);">.bin/spark-submit \</span></p>
<p style="font-family:'宋体';"><span style="color:rgb(51,51,51);">--class org.spark.sql.InferringSchema \</span></p>
<p style="font-family:'宋体';"><span style="color:rgb(51,51,51);">--master local(本地方式) \或者（-- master spark://masterIP:port）</span></p>
<p style="font-family:'宋体';"><span style="color:rgb(51,51,51);">/root/spark-mvn-1.0-SNAPSHOT.jar \（用maven打的jar包）</span></p>
<p style="font-family:'宋体';"><span style="color:rgb(51,51,51);">hdfs://mater（localhost）:9000/person.txt \(hdfs地址)</span></p>
<p style="font-family:'宋体';"><span style="color:rgb(51,51,51);">hdfs://master(localhost):9000/out</span><span style="color:rgb(51,51,51);"> </span></p>
<p>查看结果</p>
<p><span style="color:rgb(51,51,51);">hdfs dfs -cat  hdfs://master(localhost):9000/out1/part-r-*</span></p>
<p><span style="color:rgb(51,51,51);"><img src="https://img-blog.csdn.net/20170927150132906" alt=""><br></span></p>
<h1>3.2 <strong>数据源</strong></h1>
<h2>3.2.1. <strong>JDBC</strong></h2>
<p>Spark SQL<span style="font-family:'宋体';">可以通过</span><span style="font-family:Calibri;">JDBC</span><span style="font-family:'宋体';">从关系型数据库中读取数据的方式创建</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">，通过对</span><span style="font-family:Calibri;">DataFrame</span><span style="font-family:'宋体';">一系列的计算后，还可以将数据再写回关系型数据库中。</span></p>
<h3>3.2.1.1. <strong><span style="font-family:'宋体';">从</span>MySQL<span style="font-family:'宋体';">中加载数据（</span><span style="font-family:Calibri;">Spark Shell</span><span style="font-family:'宋体';">方式）</span></strong></h3>
<p>1.<span style="font-family:'宋体';">启动</span>Spark Shell<span style="font-family:'宋体';">，必须指定</span><span style="font-family:Calibri;">mysql</span><span style="font-family:'宋体';">连接驱动</span><span style="font-family:Calibri;">jar</span><span style="font-family:'宋体';">包</span></p>
<p><span style="color:rgb(51,51,51);">.bin/spark-shell \</span></p>
<p><span style="color:rgb(51,51,51);">--master spark://masterIP:Port(或者用local) \</span></p>
<p><span style="color:rgb(51,51,51);">--jars /usr/mysql-connector-java-5.1.35-bin.jar \</span></p>
<p><span style="color:rgb(51,51,51);">--driver-class-path /usr/mysql-connector-java-5.1.35-bin.jar</span>  </p>
<p>2.从mysql<span style="font-family:'宋体';">中加载数据</span></p>
<p><span style="color:rgb(51,51,51);">val jdbcDF = sqlContext.read.format("jdbc").options(Map("url" -&gt; "jdbc:mysql://192.168.111.20:3306/bigdata", "driver" -&gt; "com.mysql.jdbc.Driver", "dbtable" -&gt; "person", "user" -&gt; "root", "password" -&gt; "123456")).load()</span></p>
<p>3.执行查询</p>
<p><span style="color:rgb(51,51,51);">jdbcDF</span><span style="color:rgb(51,51,51);">.show()</span></p>
<p><span style="color:rgb(51,51,51);"><img src="https://img-blog.csdn.net/20170927150212388" alt=""><br></span></p>
<h3>3.2.1.2. <strong><span style="font-family:'宋体';">将数据写入到</span>MySQL<span style="font-family:'宋体';">中（打</span><span style="font-family:Calibri;">jar</span><span style="font-family:'宋体';">包方式）</span></strong></h3>
<p>1.<span style="font-family:'宋体';">编写</span>Spark SQL<span style="font-family:'宋体';">程序</span></p>
<table><tbody><tr><td valign="top">
<p><strong><span style="color:rgb(0,0,128);">package org</span></strong>.spark.sql</p>
<p><br><strong><span style="color:rgb(0,0,128);">import </span></strong>java.util.Properties<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.{SQLContext, Row}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.{SparkConf, SparkContext}<br><br><strong><span style="color:rgb(0,0,128);">object </span></strong>JdbcRDD {<br>
  <strong><span style="color:rgb(0,0,128);">def </span></strong>main(args: Array[<span style="color:rgb(32,153,157);">String</span>]) {<br>
    <strong><span style="color:rgb(0,0,128);">val </span></strong>conf = <strong><span style="color:rgb(0,0,128);">new</span></strong>SparkConf().setAppName(<strong><span style="color:rgb(0,128,0);">"MySQL-Demo"</span></strong>)<br>
    <strong><span style="color:rgb(0,0,128);">val </span></strong>sc = <strong><span style="color:rgb(0,0,128);">new</span></strong>SparkContext(conf)<br>
    <strong><span style="color:rgb(0,0,128);">val </span></strong>sqlContext = <strong>
<span style="color:rgb(0,0,128);">new </span></strong>SQLContext(sc)<br>
    <em><span style="color:rgb(128,128,128);">//通过并行化创建RDD</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>personRDD = sc.parallelize(<em>Array</em>(<strong><span style="color:rgb(0,128,0);">"1 tom 5"</span></strong>,<strong><span style="color:rgb(0,128,0);">"2
 jerry 3"</span></strong>, <strong><span style="color:rgb(0,128,0);">"3 kitty 6"</span></strong>)).map(_.split(<strong><span style="color:rgb(0,128,0);">" "</span></strong>))<br>
    <em><span style="color:rgb(128,128,128);">//通过StructType直接指定每个字段的schema</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>schema =
<em>StructType</em>(<br>
      <em><span style="color:rgb(102,14,122);">List</span></em>(<br>
        <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"id"</span></strong>, IntegerType,<strong><span style="color:rgb(0,0,128);">true</span></strong>),<br>
        <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"name"</span></strong>, StringType,<strong><span style="color:rgb(0,0,128);">true</span></strong>),<br>
        <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"age"</span></strong>, IntegerType,<strong><span style="color:rgb(0,0,128);">true</span></strong>)<br>
      )<br>
    )<br>
    <em><span style="color:rgb(128,128,128);">//将RDD映射到rowRDD</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>rowRDD = personRDD.map(p =&gt;
<em>Row</em>(p(<span style="color:rgb(0,0,255);">0</span>).toInt, p(<span style="color:rgb(0,0,255);">1</span>).trim, p(<span style="color:rgb(0,0,255);">2</span>).toInt))<br>
    <em><span style="color:rgb(128,128,128);">//将schema信息应用到rowRDD上</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>personDataFrame = sqlContext.createDataFrame(rowRDD, schema)<br>
    <em><span style="color:rgb(128,128,128);">//创建Properties存储数据库相关属性</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>prop =
<strong><span style="color:rgb(0,0,128);">new </span></strong>Properties()<br>
    prop.put(<strong><span style="color:rgb(0,128,0);">"user"</span></strong>, <strong>
<span style="color:rgb(0,128,0);">"root"</span></strong>)<br>
    prop.put(<strong><span style="color:rgb(0,128,0);">"password"</span></strong>,<strong><span style="color:rgb(0,128,0);">"123456"</span></strong>)<br>
    <em><span style="color:rgb(128,128,128);">//将数据追加到数据库</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>personDataFrame.write.mode(<strong><span style="color:rgb(0,128,0);">"append"</span></strong>).jdbc(<strong><span style="color:rgb(0,128,0);">"jdbc:mysql://192.168.10.1:3306/bigdata"</span></strong>,<strong><span style="color:rgb(0,128,0);">"bigdata.person"</span></strong>,
 prop)<br>
    <em><span style="color:rgb(128,128,128);">//停止SparkContext</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>sc.stop()<br>
  }<br>
}</p>
<p> </p>
</td>
</tr></tbody></table><p>2.<span style="font-family:'宋体';">用</span>maven<span style="font-family:'宋体';">将程序打包</span> </p>
<p>3.将Jar<span style="font-family:'宋体';">包提交到</span><span style="font-family:Calibri;">spark</span><span style="font-family:'宋体';">集群</span></p>
<p><span style="font-family:'宋体';">.</span><span style="color:rgb(51,51,51);">bin/spark-submit \</span></p>
<p><span style="color:rgb(51,51,51);">--class org.spark.sql.JdbcRDD \</span></p>
<p><span style="color:rgb(51,51,51);">--master spark://masterIP:Port \或者（local）</span></p>
<p><span style="color:rgb(51,51,51);">--jars /usr/mysql-connector-java-5.1.35-bin.jar \</span></p>
<p><span style="color:rgb(51,51,51);">--driver-class-path /usr/mysql-connector-java-5.1.35-bin.jar \</span></p>
<p><span style="color:rgb(51,51,51);">/root/spark-mvn-1.0-SNAPSHOT.jar </span></p>
<h1>4. Spark SQL+DataFrame创建代码</h1>
<p><strong><span style="color:rgb(0,0,128);">import </span></strong>java.util.Properties<br><br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.log4j.{Level, Logger}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.rdd.RDD<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.{DataFrame, Row, SQLContext}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.types.{DataType, DataTypes, StructField, StructType}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.{SparkConf, SparkContext}<br><br><em><span style="color:rgb(128,128,128);">/**</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">  * Created by zys on 2017/9/18 0025.</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">  */</span><span style="color:rgb(128,128,128);"><br></span></em><strong><span style="color:rgb(0,0,128);">case class </span></strong>Person(name:<span style="color:rgb(32,153,157);">String</span>, age:Int)<br><br><strong><span style="color:rgb(0,0,128);">object </span></strong>CreateDF {<br>
  <strong><span style="color:rgb(0,0,128);">def </span></strong>main(args: Array[<span style="color:rgb(32,153,157);">String</span>]): Unit = {<br>
    Logger.<em>getLogger</em>(<strong><span style="color:rgb(0,128,0);">"org.apache.spark"</span></strong>).setLevel(Level.<em><span style="color:rgb(102,14,122);">WARN</span></em>)<br>
    Logger.<em>getLogger</em>(<strong><span style="color:rgb(0,128,0);">"org.eclipse.jetty.server"</span></strong>).setLevel(Level.<em><span style="color:rgb(102,14,122);">OFF</span></em>)<br>
    <em><span style="color:rgb(128,128,128);">//spark入口</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //spark conf配置对象</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>conf =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SparkConf().setAppName(<strong><span style="color:rgb(0,128,0);">"CreateDF"</span></strong>).setMaster(<strong><span style="color:rgb(0,128,0);">"local[2]"</span></strong>)<br>
    <strong><span style="color:rgb(0,0,128);">val </span></strong>sc = <strong><span style="color:rgb(0,0,128);">new</span></strong>SparkContext(conf)<br>
    <em><span style="color:rgb(128,128,128);">//Spark SQL的入口</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>sqlContext =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SQLContext(sc)<br>
    <em><span style="color:rgb(128,128,128);">//一. DataFrame创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    1.json文件</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = sqlContext.read.json("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.json")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      2.parquet文件</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      val df = sqlContext.read.parquet("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\users.parquet")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      3.jdbc方式创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      val props = new Properties()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      props.put("user","root")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      props.put("password","123456")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//     val df = sqlContext.read.jdbc("jdbc:mysql://hdp1:3306/spark","student",props)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      4.通过表创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    df.registerTempTable("student")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    var sql = sqlContext.sql("select * from student")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    sql.printSchema()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    sql.show()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    5.avro文件创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    import com.databricks.spark.avro._</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = sqlContext.read.avro("D:/code/spark_code/course/data/users.avro")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    6.通过RDD的方式</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    6.1反射方式创建DataFrame</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    import sqlContext.implicits._</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    var rdd = sc.textFile("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.txt")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      .map { line =&gt;</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//        val strs = line.split(",")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//        Person(strs(0), strs(1).trim.toInt)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = rdd.toDF()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    6.2注册元数据方法</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">      </span></em><strong><span style="color:rgb(0,0,128);">var</span></strong>rdd = sc.textFile(<strong><span style="color:rgb(0,128,0);">"file:</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">G:</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">code</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">source_code</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">spark</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">examples</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">src</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">main</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">resources</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">people.txt"</span></strong>)<br>
          .map { line =&gt;<br>
            <strong><span style="color:rgb(0,0,128);">val </span></strong>strs = line.split(<strong><span style="color:rgb(0,128,0);">","</span></strong>)<br>
            <em>Row</em>(strs(<span style="color:rgb(0,0,255);">0</span>), strs(<span style="color:rgb(0,0,255);">1</span>).trim.toInt)<br>
          }<br>
    <strong><span style="color:rgb(0,0,128);">var </span></strong>structType = <em>
StructType</em>(<em>Array</em>(<br>
      <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"name"</span></strong>, DataTypes.<em><span style="color:rgb(102,14,122);">StringType</span></em>),<br>
      <em>StructField</em>(<strong><span style="color:rgb(0,128,0);">"age"</span></strong>, DataTypes.<em><span style="color:rgb(102,14,122);">IntegerType</span></em>)<br>
    ))<br>
      <strong><span style="color:rgb(0,0,128);">val </span></strong>df = sqlContext.createDataFrame(rdd,structType)<br><br>
    <em><span style="color:rgb(128,128,128);">//df操作</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em>df.printSchema()<br>
    df.show()<br><br>
  }<br>
}</p>
<h1 style="font-family:'宋体';">5. Spark SQL+DataFrame操作和DataSet代码</h1>
<p><strong><span style="color:rgb(0,0,128);">import </span></strong>java.util.Properties<br><br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.log4j.{Level, Logger}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.rdd.RDD<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.{DataFrame, Row, SQLContext}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.sql.types.{DataType, DataTypes, StructField, StructType}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>org.apache.spark.{SparkConf, SparkContext}<br><strong><span style="color:rgb(0,0,128);">import </span></strong>scala.collection.JavaConverters._<br><br><em><span style="color:rgb(128,128,128);">/**</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">  * Created by zys on 2017/9/25 0025.</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">  */</span><span style="color:rgb(128,128,128);"><br></span></em><strong><span style="color:rgb(0,0,128);">case class </span></strong>Person(name:<span style="color:rgb(32,153,157);">String</span>, age:java.lang.Long)<br><br><strong><span style="color:rgb(0,0,128);">object </span></strong>CreateDF {<br>
  <strong><span style="color:rgb(0,0,128);">def </span></strong>main(args: Array[<span style="color:rgb(32,153,157);">String</span>]): Unit = {<br>
    Logger.<em>getLogger</em>(<strong><span style="color:rgb(0,128,0);">"org.apache.spark"</span></strong>).setLevel(Level.<em><span style="color:rgb(102,14,122);">WARN</span></em>)<br>
    Logger.<em>getLogger</em>(<strong><span style="color:rgb(0,128,0);">"org.eclipse.jetty.server"</span></strong>).setLevel(Level.<em><span style="color:rgb(102,14,122);">OFF</span></em>)<br>
    <em><span style="color:rgb(128,128,128);">//spark入口</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //spark conf配置对象</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>conf =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SparkConf().setAppName(<strong><span style="color:rgb(0,128,0);">"CreateDF"</span></strong>).setMaster(<strong><span style="color:rgb(0,128,0);">"local[2]"</span></strong>)<br>
    <strong><span style="color:rgb(0,0,128);">val </span></strong>sc = <strong><span style="color:rgb(0,0,128);">new</span></strong>SparkContext(conf)<br>
    <em><span style="color:rgb(128,128,128);">//Spark SQL的入口</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">val</span></strong>sqlContext =
<strong><span style="color:rgb(0,0,128);">new </span></strong>SQLContext(sc)<br>
    <em><span style="color:rgb(128,128,128);">//一. DataFrame创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    1.json文件</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = sqlContext.read.json("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.json")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = sqlContext.read.format("json").load("G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.json")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //    sqlContext.read.format("json").load("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.json")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //  2.parquet文件</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      val df = sqlContext.read.parquet("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\users.parquet")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      3.jdbc方式创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      val props = new Properties()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      props.put("user","root")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      props.put("password","123456")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//     val df = sqlContext.read.jdbc("jdbc:mysql://hdp1:3306/spark","student",props)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      4.通过表创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    df.registerTempTable("student")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    var sql = sqlContext.sql("select * from student")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    sql.printSchema()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    sql.show()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    5.avro文件创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    import com.databricks.spark.avro._</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = sqlContext.read.avro("D:/code/spark_code/course/data/users.avro")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    6.通过RDD的方式</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    6.1反射方式创建DataFrame</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    import sqlContext.implicits._</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    var rdd = sc.textFile("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.txt")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      .map { line =&gt;</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//        val strs = line.split(",")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//        Person(strs(0), strs(1).trim.toInt)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val df = rdd.toDF()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    6.2注册元数据方法</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      var rdd = sc.textFile("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.txt")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//          .map { line =&gt;</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//            val strs = line.split(",")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//            Row(strs(0), strs(1).trim.toInt)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//          }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    var structType = StructType(Array(</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      StructField("name", DataTypes.StringType),</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      StructField("age", DataTypes.IntegerType)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    ))</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      val df = sqlContext.createDataFrame(rdd,structType)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //df操作</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    df.printSchema()   //打印对应的约束信息</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    df.show()         //小数据量时候，客户端显示数据</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val arrs = df.collect()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val list = df.collectAsList()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    for(i &lt;- 0 until list.size()){</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      println(list.get(i))</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    for(ele &lt;- list){</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      println(ele)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//  println(df.count())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.describe("name","age"))</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.first())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    for(ele &lt;- df.head(2) ){</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      println(ele)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    for(ele &lt;- df.take(1)){</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      println(ele)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    for(ele &lt;- df.columns){</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //      println(ele)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    //    }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.schema)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.select("age").explain())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    条件过滤</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.filter(df.col("age").gt(20)).first())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.filter(df.col("age") &gt; 20).first())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(df.agg(("name" -&gt; "count")).first())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      println(df.groupBy("name").count())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    df.registerTempTable("people")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    println(sqlContext.sql("select * from people where age &gt; 20").first())</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    第二部分：Dataset</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      1.Dataset的创建</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    import sqlContext.implicits._</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    var dS = List(Person("Kevin",24),Person("Jhon",20)).toDS()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val list = List(Person("Kevin",24),Person("Jhon",20))</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    val frame = sqlContext.createDataFrame(list)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    frame.printSchema()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      var rdd = sc.textFile("file:\\G:\\code\\source_code\\spark\\examples\\src\\main\\resources\\people.txt")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//        .map { line =&gt;</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//          val strs = line.split(",")</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//          Person(strs(0), strs(1).trim.toInt)</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//        }</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      var dS = rdd.toDS()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//      dS.printSchema()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">import</span></strong>sqlContext.implicits._<br>
    <strong><span style="color:rgb(0,0,128);">var </span></strong>frame = sqlContext.read.json(<strong><span style="color:rgb(0,128,0);">"file:</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">G:</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">code</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">source_code</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">spark</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">examples</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">src</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">main</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">resources</span><span style="color:rgb(0,0,128);">\\</span><span style="color:rgb(0,128,0);">people.json"</span></strong>)<br><em><span style="color:rgb(128,128,128);">//    frame.printSchema()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span></em><strong><span style="color:rgb(0,0,128);">var</span></strong>dataset = frame.as[Person]<br><em><span style="color:rgb(128,128,128);">//    dataset.printSchema()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    dataset.show()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">//    dataset.filter(person =&gt; person.age &gt; 21).show()</span><span style="color:rgb(128,128,128);"><br></span><span style="color:rgb(128,128,128);">    </span>println</em>(dataset.groupBy(person =&gt; person.name).count().show())<br><br>
  }<br>
}</p>
<p></p>
<p></p>
<h1 style="font-family:'宋体';">6. 相关文章</h1>
<div><span style="font-size:14px;"><strong>1.spark2.2.0 http://spark.apachecn.org/docs/cn/2.2.0/sql-programming-guide.html#datasets-and-dataframes</strong></span></div>
<div><span style="font-size:14px;"><strong>2.spark2.2.0 http://spark.apache.org/sql/</strong></span></div>
<div><span style="font-size:14px;"><strong>3.spark2.2.0 http://spark.apache.org/docs/latest/sql-programming-guide.html</strong></span></div>
<div><span style="font-size:14px;"><strong>4.</strong></span><span style="color:rgb(102,102,102);line-height:28px;"><span style="font-size:14px;"><strong>《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南 http://ifeve.com/spark-sql-dataframes/</strong></span></span></div>
<br><p></p>
<br><br>            </div>
                </div>