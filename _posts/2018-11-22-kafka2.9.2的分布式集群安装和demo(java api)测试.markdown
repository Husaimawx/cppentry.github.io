---
layout:     post
title:      kafka2.9.2的分布式集群安装和demo(java api)测试
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<strong>问题导读<br><span style="color:#ff0000;">1、什么是kafka?</span><br><span style="color:#ff0000;">2、kafka的官方网站在哪里？</span><br><span style="color:#ff0000;">3、在哪里下载？需要哪些组件的支持？</span><br><span style="color:#ff0000;">4、如何安装？</span></strong><br><span style="color:#000000;"><br><img id="aimg_H5R1A" class="zoom" border="0" alt="" src="http://www.aboutyun.com/static/image/hrline/4.gif" width="500" height="35"><br><br><br></span><br><br><span style="color:#000000;"><strong>　　一、什么是kafka?</strong></span><br><span style="color:#000000;">　　kafka是LinkedIn开发并开源的一个分布式MQ系统，现在是Apache的一个孵化项目。在它的主页描述kafka为一个高吞吐量的分布式（能将消息分散到不同的节点上）MQ。Kafka仅仅由7000行Scala编写，据了解，Kafka每秒可以生产约25万消息（50 MB），每秒处理55万消息（110 MB）。</span><br><span style="color:#000000;">　　kafka目前支持多种客户端语言：java，python，c++，php等等。</span><br><span style="color:#000000;">　　kafka集群的简要图解如下，producer写入消息，consumer读取消息：</span><br><span style="color:#000000;"><img id="aimg_6662" class="zoom" alt="" src="http://www.aboutyun.com/data/attachment/forum/201408/26/022803cpp4x6r1pmur64ac.png" width="258"></span>
<div id="aimg_6662_menu" class="tip tip_4 aimg_tip">
<div class="xs0">
<p><strong>145455f6bg6fggektmo4ko.png</strong> <em class="xg1">(15.89 KB, 下载次数: 4)</em></p>
<p><a href="http://www.aboutyun.com/forum.php?mod=attachment&amp;aid=NjY2MnxiZGZiNzY1ZnwxNDU0NjM3MTAxfDB8ODkxOQ%3D%3D&amp;nothumb=yes" rel="nofollow">下载附件</a>  <a id="savephoto_6662" href="" rel="nofollow">保存到相册</a></p>
<p class="xg1 y">2014-8-26 02:28 上传</p>
</div>
<div class="tip_horn"></div>
</div>
<br><br><br><span style="color:#000000;">　　kafka设计目标<br></span>
<ul><li>高吞吐量是其核心设计之一。 </li><li>数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能。 </li><li>zero-copy：减少IO操作步骤。 </li><li>支持数据批量发送和拉取。 </li><li>支持数据压缩。 </li><li>Topic划分为多个partition，提高并行处理能力。<br></li></ul><br><br><strong>　　kafka名词解释和工作方式：</strong><br><ul><li>Producer ：消息生产者，就是向kafka broker发消息的客户端。 </li><li>Consumer ：消息消费者，向kafka broker取消息的客户端 </li><li>Topic ：可以理解为一个队列。 </li><li>Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。</li><li>Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。 </li><li>Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。</li><li>Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka<br></li></ul><br><br><strong>　　kafak系统扩展性：</strong><br><ul><li>kafka使用zookeeper来实现动态的集群扩展，不需要更改客户端（producer和consumer）的配置。broker会在zookeeper注册并保持相关的元数据（topic，partition信息等）更新。</li><li>而客户端会在zookeeper上注册相关的watcher。一旦zookeeper发生变化，客户端能及时感知并作出相应调整。这样就保证了添加或去除broker时，各broker间仍能自动实现负载均衡。<br></li></ul><br><br><strong>　　kafak和zookeeper的关系：</strong><br><ul><li>Producer端使用zookeeper用来"发现"broker列表,以及和Topic下每个partition leader建立socket连接并发送消息.</li><li>Broker端使用zookeeper用来注册broker信息,已经监测partition leader存活性. </li><li>Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partition leader建立socket连接,并获取消息.<br></li></ul><br><br><br><span style="color:#000000;"><strong>　　二、kafka的官方网站在哪里？</strong><br>
　　<span><a href="http://kafka.apache.org/" rel="nofollow">http://kafka.apache.org/</a></span><br><br><strong>　　三、在哪里下载？需要哪些组件的支持？</strong><br>
　　kafka2.9.2在下面的地址可以下载：<br>
　　<span><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz" rel="nofollow">https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz</a></span><br></span><br><span style="color:#000000;"><strong>　四、如何安装？</strong><br>
　　1、解压kafka_2.9.2-0.8.1.1.tgz，本文中解压到/home/hadoop目录下<br></span><br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_web">
<ol><li>root@m1:/home/hadoop/kafka_2.9.2-0.8.1.1# pwd<br></li><li>/home/hadoop/kafka_2.9.2-0.8.1.1</li></ol></div>
<em>复制代码</em></div>
<br>
2、修改server.properties配置文件。这里使用zookeeper的部分，见下方第123行：<br><br><br><div class="quote">
<blockquote><span style="color:#000000;">root@m1:/home/hadoop/kafka_2.9.2-0.8.1.1# cat config/server.properties</span><br><span style="color:#000000;"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span style="color:#000000;"># contributor license agreements.  See the NOTICE file distributed with</span><br><span style="color:#000000;"># this work for additional information regarding copyright ownership.</span><br><span style="color:#000000;"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span style="color:#000000;"># (the "License"); you may not use this file except in compliance with</span><br><span style="color:#000000;"># the License.  You may obtain a copy of the License at</span><br><span style="color:#000000;"># </span><br><span style="color:#000000;">#    <a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">
http://www.apache.org/licenses/LICENSE-2.0</a></span><br><span style="color:#000000;"># </span><br><span style="color:#000000;"># Unless required by applicable law or agreed to in writing, software</span><br><span style="color:#000000;"># distributed under the License is distributed on an "AS IS" BASIS,</span><br><span style="color:#000000;"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span style="color:#000000;"># See the License for the specific language governing permissions and</span><br><span style="color:#000000;"># limitations under the License.</span><br><span style="color:#000000;"># see kafka.server.KafkaConfig for additional details and defaults</span><br><br><span style="color:#000000;">############################# Server Basics #############################</span><br><br><span style="color:#000000;"># The id of the broker. This must be set to a unique integer for each broker.</span><br><span style="color:#000000;">#整数，建议根据ip区分,这里我是使用zookeeper中的id来设置</span><br><span style="color:#000000;">broker.id=1</span><br><br><span style="color:#000000;">############################# Socket Server Settings #############################</span><br><br><span style="color:#000000;"># The port the socket server listens on</span><br><span style="color:#000000;">#broker用于接收producer消息的端口</span><br><span style="color:#000000;">port=9092</span><br><span style="color:#000000;">#port=44444</span><br><br><span style="color:#000000;"># Hostname the broker will bind to. If not set, the server will bind to all interfaces</span><br><span style="color:#000000;">#broker的hostname</span><br><span style="color:#000000;">host.name=m1</span><br><br><span style="color:#000000;"># Hostname the broker will advertise to producers and consumers. If not set, it uses the</span><br><span style="color:#000000;"># value for "host.name" if configured.  Otherwise, it will use the value returned from</span><br><span style="color:#000000;"># java.net.InetAddress.getCanonicalHostName().</span><br><span style="color:#000000;">#这个是配置PRODUCER/CONSUMER连上来的时候使用的地址</span><br><span style="color:#000000;">advertised.host.name=m1</span><br><br><span style="color:#000000;"># The port to publish to ZooKeeper for clients to use. If this is not set,</span><br><span style="color:#000000;"># it will publish the same port that the broker binds to.</span><br><span style="color:#000000;">#advertised.port=&lt;port accessible by clients&gt;</span><br><br><span style="color:#000000;"># The number of threads handling network requests</span><br><span style="color:#000000;">num.network.threads=2</span><br>
  <br><span style="color:#000000;"># The number of threads doing disk I/O</span><br><span style="color:#000000;">num.io.threads=8</span><br><br><span style="color:#000000;"># The send buffer (SO_SNDBUF) used by the socket server</span><br><span style="color:#000000;">socket.send.buffer.bytes=1048576</span><br><br><span style="color:#000000;"># The receive buffer (SO_RCVBUF) used by the socket server</span><br><span style="color:#000000;">socket.receive.buffer.bytes=1048576</span><br><br><span style="color:#000000;"># The maximum size of a request that the socket server will accept (protection against OOM)</span><br><span style="color:#000000;">socket.request.max.bytes=104857600</span><br><br><br><span style="color:#000000;">############################# Log Basics #############################</span><br><br><span style="color:#000000;"># A comma seperated list of directories under which to store log files</span><br><span style="color:#000000;">#kafka存放消息文件的路径</span><br><span style="color:#000000;">log.dirs=/home/hadoop/kafka_2.9.2-0.8.1.1/kafka-logs</span><br><br><span style="color:#000000;"># The default number of log partitions per topic. More partitions allow greater</span><br><span style="color:#000000;"># parallelism for consumption, but this will also result in more files across</span><br><span style="color:#000000;"># the brokers.</span><br><span style="color:#000000;">#topic的默认分区数</span><br><span style="color:#000000;">num.partitions=2</span><br><br><span style="color:#000000;">############################# Log Flush Policy #############################</span><br><br><span style="color:#000000;"># Messages are immediately written to the filesystem but by default we only fsync() to sync</span><br><span style="color:#000000;"># the OS cache lazily. The following configurations control the flush of data to disk.</span><br><span style="color:#000000;"># There are a few important trade-offs here:</span><br><span style="color:#000000;">#    1. Durability: Unflushed data may be lost if you are not using replication.</span><br><span style="color:#000000;">#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</span><br><span style="color:#000000;">#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.</span><br><span style="color:#000000;"># The settings below allow one to configure the flush policy to flush data after a period of time or</span><br><span style="color:#000000;"># every N messages (or both). This can be done globally and overridden on a per-topic basis.</span><br><br><span style="color:#000000;"># The number of messages to accept before forcing a flush of data to disk</span><br><span style="color:#000000;">#log.flush.interval.messages=10000</span><br><br><span style="color:#000000;"># The maximum amount of time a message can sit in a log before we force a flush</span><br><span style="color:#000000;">#log.flush.interval.ms=1000</span><br><br><span style="color:#000000;">############################# Log Retention Policy #############################</span><br><br><span style="color:#000000;"># The following configurations control the disposal of log segments. The policy can</span><br><span style="color:#000000;"># be set to delete segments after a period of time, or after a given size has accumulated.</span><br><span style="color:#000000;"># A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span><br><span style="color:#000000;"># from the end of the log.</span><br><br><span style="color:#000000;"># The minimum age of a log file to be eligible for deletion</span><br><span style="color:#000000;">#kafka接收日志的存储目录(目前我们保存7天数据log.retention.hours=168)</span><br><span style="color:#000000;">log.retention.hours=168</span><br><br><span style="color:#000000;"># A size-based retention policy for logs. Segments are pruned from the log as long as the remaining</span><br><span style="color:#000000;"># segments don't drop below log.retention.bytes.</span><br><span style="color:#000000;">#log.retention.bytes=1073741824</span><br><br><span style="color:#000000;"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span><br><span style="color:#000000;">log.segment.bytes=536870912</span><br><br><span style="color:#000000;"># The interval at which log segments are checked to see if they can be deleted according</span><br><span style="color:#000000;"># to the retention policies</span><br><span style="color:#000000;">log.retention.check.interval.ms=60000</span><br><br><span style="color:#000000;"># By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.</span><br><span style="color:#000000;"># If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.</span><br><span style="color:#000000;">log.cleaner.enable=false</span><br><br><span style="color:#000000;">############################# Zookeeper #############################</span><br><br><span style="color:#000000;"># Zookeeper connection string (see zookeeper docs for details).</span><br><span style="color:#000000;"># This is a comma separated host:port pairs, each corresponding to a zk</span><br><span style="color:#000000;"># server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".</span><br><span style="color:#000000;"># You can also append an optional chroot string to the urls to specify the</span><br><span style="color:#000000;"># root directory for all kafka znodes.</span><br><span style="color:#ff0000;">zookeeper.connect=m1:2181,m2:2181,s1:2181,s2:2181</span><br><br><span style="color:#000000;"># Timeout in ms for connecting to zookeeper</span><br><span style="color:#000000;">zookeeper.connection.timeout.ms=1000000</span></blockquote>
</div>
<br><span style="color:#000000;">3、启动zookeeper和kafka</span><br><span style="color:#000000;">　　　　1)zookeeper的启动</span><br><span style="color:#000000;">　　　　启动后可以用以下命令在每台机器上查看状态：</span><br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_ZyI">
<ol><li>root@m1:/home/hadoop# /home/hadoop/zookeeper-3.4.5/bin/zkServer.sh status<br></li><li>JMX enabled by default<br></li><li>Using config: /home/hadoop/zookeeper-3.4.5/bin/../conf/zoo.cfg<br></li><li>Mode: leader</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　2)在m1,m2,s1,s2的机器上启动kafka，在这之前请先将m1上的kafka复制到另外三台机器上，复制后，记得更改server.properties配置文件中的host名称为当前所在机器。以下代码是在m1上执行后的效果：<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_T00">
<ol><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-server-start.sh /home/hadoop/kafka_2.9.2-0.8.1.1/config/server.properties &amp;<br></li><li>[1] 31823<br></li><li>root@m1:/home/hadoop# [2014-08-05 10:03:11,210] INFO Verifying properties (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,261] INFO Property advertised.host.name is overridden to m1 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,261] INFO Property broker.id is overridden to 1 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,264] INFO Property host.name is overridden to m1 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,264] INFO Property log.cleaner.enable is overridden to false (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,264] INFO Property log.dirs is overridden to /home/hadoop/kafka_2.9.2-0.8.1.1/kafka-logs (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,265] INFO Property log.retention.check.interval.ms is overridden to 60000 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,265] INFO Property log.retention.hours is overridden to 168 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,265] INFO Property log.segment.bytes is overridden to 536870912 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,265] INFO Property num.io.threads is overridden to 8 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,266] INFO Property num.network.threads is overridden to 2 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,266] INFO Property num.partitions is overridden to 2 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,267] INFO Property port is overridden to 9092 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,267] INFO Property socket.receive.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,268] INFO Property socket.request.max.bytes is overridden to 104857600 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,268] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,268] INFO Property zookeeper.connect is overridden to m1:2181,m2:2181,s1:2181,s2:2181 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,269] INFO Property zookeeper.connection.timeout.ms is overridden to 1000000 (kafka.utils.VerifiableProperties)<br></li><li>[2014-08-05 10:03:11,302] INFO [Kafka Server 1], starting (kafka.server.KafkaServer)<br></li><li>[2014-08-05 10:03:11,303] INFO [Kafka Server 1], Connecting to zookeeper on m1:2181,m2:2181,s1:2181,s2:2181 (kafka.server.KafkaServer)<br></li><li>[2014-08-05 10:03:11,335] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)<br></li><li>[2014-08-05 10:03:11,348] INFO Client environment:zookeeper.version=3.3.3-1203054, built on 11/17/2011 05:47 GMT (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,348] INFO Client environment:host.name=m1 (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,349] INFO Client environment:java.version=1.7.0_65 (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,349] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,349] INFO Client environment:java.home=/usr/lib/jvm/java-7-oracle/jre (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,349] INFO Client environment:java.class.path=.:/usr/lib/jvm/java-7-oracle/lib/tools.jar:/usr/lib/jvm/java-7-oracle/lib/dt.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../core/build/dependant-libs-2.8.0/*.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../perf/build/libs//kafka-perf_2.8.0*.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../clients/build/libs//kafka-clients*.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../examples/build/libs//kafka-examples*.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../contrib/hadoop-consumer/build/libs//kafka-hadoop-consumer*.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../contrib/hadoop-producer/build/libs//kafka-hadoop-producer*.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/jopt-simple-3.2.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/kafka_2.9.2-0.8.1.1.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/kafka_2.9.2-0.8.1.1-javadoc.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/kafka_2.9.2-0.8.1.1-scaladoc.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/kafka_2.9.2-0.8.1.1-sources.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/log4j-1.2.15.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/metrics-core-2.2.0.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/scala-library-2.9.2.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/slf4j-api-1.7.2.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/snappy-java-1.0.5.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/zkclient-0.3.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../libs/zookeeper-3.3.4.jar:/home/hadoop/kafka_2.9.2-0.8.1.1/bin/../core/build/libs/kafka_2.8.0*.jar
 (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,350] INFO Client environment:java.library.path=:/usr/local/lib:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,350] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,350] INFO Client environment:java.compiler=&lt;NA&gt; (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,350] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,350] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,351] INFO Client environment:os.version=3.11.0-15-generic (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,351] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,351] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,351] INFO Client environment:user.dir=/home/hadoop (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,352] INFO Initiating client connection, connectString=m1:2181,m2:2181,s1:2181,s2:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@51f782b8 (org.apache.zookeeper.ZooKeeper)<br></li><li>[2014-08-05 10:03:11,380] INFO Opening socket connection to server m2/192.168.1.51:2181 (org.apache.zookeeper.ClientCnxn)<br></li><li>[2014-08-05 10:03:11,386] INFO Socket connection established to m2/192.168.1.51:2181, initiating session (org.apache.zookeeper.ClientCnxn)<br></li><li>[2014-08-05 10:03:11,398] INFO Session establishment complete on server m2/192.168.1.51:2181, sessionid = 0x247a3e09b460000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)<br></li><li>[2014-08-05 10:03:11,400] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)<br></li><li>[2014-08-05 10:03:11,652] INFO Loading log 'test-1' (kafka.log.LogManager)<br></li><li>[2014-08-05 10:03:11,681] INFO Recovering unflushed segment 0 in log test-1. (kafka.log.Log)<br></li><li>[2014-08-05 10:03:11,711] INFO Completed load of log test-1 with log end offset 137 (kafka.log.Log)<br></li><li>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".<br></li><li>SLF4J: Defaulting to no-operation (NOP) logger implementation<br></li><li>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.<br></li><li>[2014-08-05 10:03:11,747] INFO Loading log 'idoall.org-0' (kafka.log.LogManager)<br></li><li>[2014-08-05 10:03:11,748] INFO Recovering unflushed segment 0 in log idoall.org-0. (kafka.log.Log)<br></li><li>[2014-08-05 10:03:11,754] INFO Completed load of log idoall.org-0 with log end offset 5 (kafka.log.Log)<br></li><li>[2014-08-05 10:03:11,760] INFO Loading log 'test-0' (kafka.log.LogManager)<br></li><li>[2014-08-05 10:03:11,765] INFO Recovering unflushed segment 0 in log test-0. (kafka.log.Log)<br></li><li>[2014-08-05 10:03:11,777] INFO Completed load of log test-0 with log end offset 151 (kafka.log.Log)<br></li><li>[2014-08-05 10:03:11,779] INFO Starting log cleanup with a period of 60000 ms. (kafka.log.LogManager)<br></li><li>[2014-08-05 10:03:11,782] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)<br></li><li>[2014-08-05 10:03:11,800] INFO Awaiting socket connections on m1:9092. (kafka.network.Acceptor)<br></li><li>[2014-08-05 10:03:11,802] INFO [Socket Server on Broker 1], Started (kafka.network.SocketServer)<br></li><li>[2014-08-05 10:03:11,890] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)<br></li><li>[2014-08-05 10:03:11,919] INFO 1 successfully elected as leader (kafka.server.ZookeeperLeaderElector)<br></li><li>[2014-08-05 10:03:12,359] INFO New leader is 1 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)<br></li><li>[2014-08-05 10:03:12,387] INFO Registered broker 1 at path /brokers/ids/1 with address m1:9092. (kafka.utils.ZkUtils$)<br></li><li>[2014-08-05 10:03:12,392] INFO [Kafka Server 1], started (kafka.server.KafkaServer)<br></li><li>[2014-08-05 10:03:12,671] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions [idoall.org,0],[test,0],[test,1] (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:03:12,741] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions [idoall.org,0],[test,0],[test,1] (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:03:25,327] INFO Partition [test,0] on broker 1: Expanding ISR for partition [test,0] from 1 to 1,2 (kafka.cluster.Partition)<br></li><li>[2014-08-05 10:03:25,334] INFO Partition [test,1] on broker 1: Expanding ISR for partition [test,1] from 1 to 1,2 (kafka.cluster.Partition)<br></li><li>[2014-08-05 10:03:26,905] INFO Partition [test,1] on broker 1: Expanding ISR for partition [test,1] from 1,2 to 1,2,3 (kafka.cluster.Partition)</li></ol></div>
<em>复制代码</em></div>
<br>
　4、测试kafka的状态<br>
　　　　1)在m1上创建一个idoall_testTopic主题<br><br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_G6X">
<ol><li>#KAFKA有几个，replication-factor就填几个<br></li><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-topics.sh --create --topic idoall_testTopic --replication-factor 4 --partitions 2 --zookeeper m1:2181<br></li><li>Created topic "idoall_testTopic".<br></li><li>[2014-08-05 10:08:29,315] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions [idoall_testTopic,0] (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:08:29,334] INFO Completed load of log idoall_testTopic-0 with log end offset 0 (kafka.log.Log)<br></li><li>[2014-08-05 10:08:29,373] INFO Created log for partition [idoall_testTopic,0] in /home/hadoop/kafka_2.9.2-0.8.1.1/kafka-logs with properties {segment.index.bytes -&gt; 10485760, file.delete.delay.ms -&gt; 60000, segment.bytes -&gt; 536870912, flush.ms -&gt; 9223372036854775807,
 delete.retention.ms -&gt; 86400000, index.interval.bytes -&gt; 4096, retention.bytes -&gt; -1, cleanup.policy -&gt; delete, segment.ms -&gt; 604800000, max.message.bytes -&gt; 1000012, flush.messages -&gt; 9223372036854775807, min.cleanable.dirty.ratio -&gt; 0.5, retention.ms -&gt;
 604800000}. (kafka.log.LogManager)<br></li><li>[2014-08-05 10:08:29,384] WARN Partition [idoall_testTopic,0] on broker 1: No checkpointed highwatermark is found for partition [idoall_testTopic,0] (kafka.cluster.Partition)<br></li><li>[2014-08-05 10:08:29,415] INFO Completed load of log idoall_testTopic-1 with log end offset 0 (kafka.log.Log)<br></li><li>[2014-08-05 10:08:29,416] INFO Created log for partition [idoall_testTopic,1] in /home/hadoop/kafka_2.9.2-0.8.1.1/kafka-logs with properties {segment.index.bytes -&gt; 10485760, file.delete.delay.ms -&gt; 60000, segment.bytes -&gt; 536870912, flush.ms -&gt; 9223372036854775807,
 delete.retention.ms -&gt; 86400000, index.interval.bytes -&gt; 4096, retention.bytes -&gt; -1, cleanup.policy -&gt; delete, segment.ms -&gt; 604800000, max.message.bytes -&gt; 1000012, flush.messages -&gt; 9223372036854775807, min.cleanable.dirty.ratio -&gt; 0.5, retention.ms -&gt;
 604800000}. (kafka.log.LogManager)<br></li><li>[2014-08-05 10:08:29,422] WARN Partition [idoall_testTopic,1] on broker 1: No checkpointed highwatermark is found for partition [idoall_testTopic,1] (kafka.cluster.Partition)<br></li><li>[2014-08-05 10:08:29,430] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions [idoall_testTopic,1] (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:08:29,438] INFO Truncating log idoall_testTopic-1 to offset 0. (kafka.log.Log)<br></li><li>[2014-08-05 10:08:29,473] INFO [ReplicaFetcherManager on broker 1] Added fetcher for partitions ArrayBuffer([[idoall_testTopic,1], initOffset 0 to broker id:2,host:m2,port:9092] ) (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:08:29,475] INFO [ReplicaFetcherThread-0-2], Starting  (kafka.server.ReplicaFetcherThread)</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　2)在m1上查看刚才创建的idoall_testTopic主题<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_WGg">
<ol><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-topics.sh --list --zookeeper m1:2181   <br></li><li>idoall_testTopic</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　3)在m2上发送消息至kafka（m2模拟producer），发送消息“hello idoall.org”<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_DdT">
<ol><li>root@m2:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-console-producer.sh --broker-list m1:9092 --sync --topic idoall_testTopic<br></li><li>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".<br></li><li>SLF4J: Defaulting to no-operation (NOP) logger implementation<br></li><li>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.<br></li><li>hello idoall.org</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　4)在s1上开启一个消费者（s1模拟consumer），可以看到刚才发送的消息<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_LiQ">
<ol><li>root@s1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-console-consumer.sh --zookeeper m1:2181 --topic idoall_testTopic --from-beginning<br></li><li>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".<br></li><li>SLF4J: Defaulting to no-operation (NOP) logger implementation<br></li><li>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.<br></li><li>hello idoall.org </li></ol></div>
<em>复制代码</em></div>
<br>
　　　　5)删除掉一个Topic，这里我们测试创建一个idoall的主题，再删除掉<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_uE1">
<ol><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-topics.sh --create --topic idoall --replication-factor 4 --partitions 2 --zookeeper m1:2181         <br></li><li>Created topic "idoall".<br></li><li>[2014-08-05 10:38:30,862] INFO Completed load of log idoall-1 with log end offset 0 (kafka.log.Log)<br></li><li>[2014-08-05 10:38:30,864] INFO Created log for partition [idoall,1] in /home/hadoop/kafka_2.9.2-0.8.1.1/kafka-logs with properties {segment.index.bytes -&gt; 10485760, file.delete.delay.ms -&gt; 60000, segment.bytes -&gt; 536870912, flush.ms -&gt; 9223372036854775807,
 delete.retention.ms -&gt; 86400000, index.interval.bytes -&gt; 4096, retention.bytes -&gt; -1, cleanup.policy -&gt; delete, segment.ms -&gt; 604800000, max.message.bytes -&gt; 1000012, flush.messages -&gt; 9223372036854775807, min.cleanable.dirty.ratio -&gt; 0.5, retention.ms -&gt;
 604800000}. (kafka.log.LogManager)<br></li><li>[2014-08-05 10:38:30,870] WARN Partition [idoall,1] on broker 1: No checkpointed highwatermark is found for partition [idoall,1] (kafka.cluster.Partition)<br></li><li>[2014-08-05 10:38:30,878] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions [idoall,1] (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:38:30,880] INFO Truncating log idoall-1 to offset 0. (kafka.log.Log)<br></li><li>[2014-08-05 10:38:30,885] INFO [ReplicaFetcherManager on broker 1] Added fetcher for partitions ArrayBuffer([[idoall,1], initOffset 0 to broker id:3,host:s1,port:9092] ) (kafka.server.ReplicaFetcherManager)<br></li><li>[2014-08-05 10:38:30,887] INFO [ReplicaFetcherThread-0-3], Starting  (kafka.server.ReplicaFetcherThread)<br></li><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-topics.sh --list --zookeeper m1:2181<br></li><li>idoall<br></li><li>idoall_testTopic<br></li><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-run-class.sh kafka.admin.DeleteTopicCommand --topic idoall --zookeeper m2:2181   <br></li><li>deletion succeeded!<br></li><li>root@m1:/home/hadoop# /home/hadoop/kafka_2.9.2-0.8.1.1/bin/kafka-topics.sh --list --zookeeper m1:2181                  idoall_testTopic<br></li><li>root@m1:/home/hadoop#</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　同样也可以进入到zookeeper中查看主题是否已经删除掉。<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_LJ5">
<ol><li>root@m1:/home/hadoop# /home/hadoop/zookeeper-3.4.5/bin/zkCli.sh<br></li><li>Connecting to localhost:2181<br></li><li>2014-08-05 10:15:21,863 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT<br></li><li>2014-08-05 10:15:21,871 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=m1<br></li><li>2014-08-05 10:15:21,871 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_65<br></li><li>2014-08-05 10:15:21,872 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation<br></li><li>2014-08-05 10:15:21,872 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-7-oracle/jre<br></li><li>2014-08-05 10:15:21,873 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/home/hadoop/zookeeper-3.4.5/bin/../build/classes:/home/hadoop/zookeeper-3.4.5/bin/../build/lib/*.jar:/home/hadoop/zookeeper-3.4.5/bin/../lib/slf4j-log4j12-1.6.1.jar:/home/hadoop/zookeeper-3.4.5/bin/../lib/slf4j-api-1.6.1.jar:/home/hadoop/zookeeper-3.4.5/bin/../lib/netty-3.2.2.Final.jar:/home/hadoop/zookeeper-3.4.5/bin/../lib/log4j-1.2.15.jar:/home/hadoop/zookeeper-3.4.5/bin/../lib/jline-0.9.94.jar:/home/hadoop/zookeeper-3.4.5/bin/../zookeeper-3.4.5.jar:/home/hadoop/zookeeper-3.4.5/bin/../src/java/lib/*.jar:/home/hadoop/zookeeper-3.4.5/bin/../conf:.:/usr/lib/jvm/java-7-oracle/lib/tools.jar:/usr/lib/jvm/java-7-oracle/lib/dt.jar<br></li><li>2014-08-05 10:15:21,874 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=:/usr/local/lib:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib<br></li><li>2014-08-05 10:15:21,874 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp<br></li><li>2014-08-05 10:15:21,874 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;<br></li><li>2014-08-05 10:15:21,875 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux<br></li><li>2014-08-05 10:15:21,875 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64<br></li><li>2014-08-05 10:15:21,876 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=3.11.0-15-generic<br></li><li>2014-08-05 10:15:21,876 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root<br></li><li>2014-08-05 10:15:21,877 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root<br></li><li>2014-08-05 10:15:21,878 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/home/hadoop<br></li><li>2014-08-05 10:15:21,879 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@666c211a<br></li><li>Welcome to ZooKeeper!<br></li><li>2014-08-05 10:15:21,920 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@966] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)<br></li><li>2014-08-05 10:15:21,934 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@849] - Socket connection established to localhost/127.0.0.1:2181, initiating session<br></li><li>JLine support is enabled<br></li><li>2014-08-05 10:15:21,966 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1207] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x147a3e1246b0007, negotiated timeout = 30000<br></li><li><br></li><li>WATCHER::<br></li><li><br></li><li>WatchedEvent state:SyncConnected type:None path:null<br></li><li>[zk: localhost:2181(CONNECTED) 0] ls /<br></li><li>[hbase, hadoop-ha, admin, zookeeper, consumers, config, controller, storm, brokers, controller_epoch]<br></li><li>[zk: localhost:2181(CONNECTED) 1] ls /brokers<br></li><li>[topics, ids]<br></li><li>[zk: localhost:2181(CONNECTED) 2] ls /brokers/topics<br></li><li>[idoall_testTopic]</li></ol></div>
<em>复制代码</em></div>
<br>
　5、使用Eclipse来调用kafka的JAVA API来测试kafka的集群状态<br>
　　　　1)消息生产端：Producertest.java<br><br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_U48">
<ol><li>package idoall.testkafka;<br></li><li><br></li><li>import java.util.Date;<br></li><li>import java.util.Properties;<br></li><li>import java.text.SimpleDateFormat;   <br></li><li><br></li><li>import kafka.javaapi.producer.Producer;<br></li><li>import kafka.producer.KeyedMessage;<br></li><li>import kafka.producer.ProducerConfig;<br></li><li><br></li><li><br></li><li>/**<br></li><li>* 消息生产端<br></li><li>* @author 迦壹<br></li><li>* @Time 2014-08-05<br></li><li>*/<br></li><li>public class Producertest {<br></li><li>     <br></li><li>     public static void main(String[] args) {<br></li><li>         Properties props = new Properties();<br></li><li>         props.put("zk.connect", "m1:2181,m2:2181,s1:2181,s2:2181");<br></li><li>         // serializer.class为消息的序列化类<br></li><li>         props.put("serializer.class", "kafka.serializer.StringEncoder");<br></li><li>         // 配置metadata.broker.list, 为了高可用, 最好配两个broker实例<br></li><li>         props.put("metadata.broker.list", "m1:9092,m2:9092,s1:9092,s2:9092");<br></li><li>         // 设置Partition类, 对队列进行合理的划分<br></li><li>         //props.put("partitioner.class", "idoall.testkafka.Partitionertest");<br></li><li>         // ACK机制, 消息发送需要kafka服务端确认<br></li><li>         props.put("request.required.acks", "1");<br></li><li><br></li><li>          props.put("num.partitions", "4");<br></li><li>         ProducerConfig config = new ProducerConfig(props);<br></li><li>         Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config);<br></li><li>         for (int i = 0; i &lt; 10; i++)<br></li><li>         {<br></li><li>           // KeyedMessage&lt;K, V&gt;<br></li><li>           // 　　K对应Partition Key的类型<br></li><li>           // 　　V对应消息本身的类型<br></li><li>//　　 topic: "test", key: "key", message: "message"<br></li><li>           SimpleDateFormat formatter = new SimpleDateFormat   ("yyyy年MM月dd日 HH:mm:ss SSS");      <br></li><li>           Date curDate = new Date(System.currentTimeMillis());//获取当前时间      <br></li><li>           String str = formatter.format(curDate);   <br></li><li>            <br></li><li>           String msg = "idoall.org" + i+"="+str;<br></li><li>           String key = i+"";<br></li><li>           producer.send(new KeyedMessage&lt;String, String&gt;("idoall_testTopic",key, msg));<br></li><li>         }<br></li><li>       }<br></li><li>}</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　2)消息消费端：Consumertest.java<br><span style="color:#000000;"></span>
<div class="blockcode">
<div id="code_CXX">
<ol><li>package idoall.testkafka;<br></li><li><br></li><li>import java.util.HashMap; <br></li><li>import java.util.List;  <br></li><li>import java.util.Map;  <br></li><li>import java.util.Properties;  <br></li><li>   <br></li><li>import kafka.consumer.ConsumerConfig;  <br></li><li>import kafka.consumer.ConsumerIterator;  <br></li><li>import kafka.consumer.KafkaStream;  <br></li><li>import kafka.javaapi.consumer.ConsumerConnector;<br></li><li><br></li><li><br></li><li>/**<br></li><li>* 消息消费端<br></li><li>* @author 迦壹<br></li><li>* @Time 2014-08-05<br></li><li>*/<br></li><li>public class Consumertest extends Thread{<br></li><li>      <br></li><li>     private final ConsumerConnector consumer;  <br></li><li>    private final String topic;  <br></li><li><br></li><li>    public static void main(String[] args) {  <br></li><li>      Consumertest consumerThread = new Consumertest("idoall_testTopic");  <br></li><li>        consumerThread.start();  <br></li><li>    }  <br></li><li>    public Consumertest(String topic) {  <br></li><li>        consumer =kafka.consumer.Consumer.createJavaConsumerConnector(createConsumerConfig());  <br></li><li>        this.topic =topic;  <br></li><li>    }  <br></li><li><br></li><li>private static ConsumerConfig createConsumerConfig() {  <br></li><li>    Properties props = new Properties();  <br></li><li>    // 设置zookeeper的链接地址<br></li><li>    props.put("zookeeper.connect","m1:2181,m2:2181,s1:2181,s2:2181");  <br></li><li>    // 设置group id<br></li><li>    props.put("group.id", "1");  <br></li><li>    // kafka的group 消费记录是保存在zookeeper上的, 但这个信息在zookeeper上不是实时更新的, 需要有个间隔时间更新<br></li><li>    props.put("auto.commit.interval.ms", "1000");<br></li><li>    props.put("zookeeper.session.timeout.ms","10000");  <br></li><li>    return new ConsumerConfig(props);  <br></li><li>}  <br></li><li><br></li><li>public void run(){  <br></li><li>     //设置Topic=&gt;Thread Num映射关系, 构建具体的流<br></li><li>    Map&lt;String,Integer&gt; topickMap = new HashMap&lt;String, Integer&gt;();  <br></li><li>    topickMap.put(topic, 1);  <br></li><li>    Map&lt;String, List&lt;KafkaStream&lt;byte[],byte[]&gt;&gt;&gt;  streamMap=consumer.createMessageStreams(topickMap);  <br></li><li>    KafkaStream&lt;byte[],byte[]&gt;stream = streamMap.get(topic).get(0);  <br></li><li>    ConsumerIterator&lt;byte[],byte[]&gt; it =stream.iterator();  <br></li><li>    System.out.println("*********Results********");  <br></li><li>    while(it.hasNext()){  <br></li><li>        System.err.println("get data:" +new String(it.next().message()));  <br></li><li>        try {  <br></li><li>            Thread.sleep(1000);  <br></li><li>        } catch (InterruptedException e) {  <br></li><li>            e.printStackTrace();  <br></li><li>        }  <br></li><li>    }  <br></li><li>}  <br></li><li>}</li></ol></div>
<em>复制代码</em></div>
<br><br><br><span style="color:#000000;">　　　　3)在Eclipse查看java代码效果，在这之前先在其中一台机器(我使用的s1)，开启消费者，同时观察eclipse和s1上的消费者是否都收到了消息。最后结果如下图：<br><img id="aimg_6658" class="zoom" alt="" src="http://www.aboutyun.com/data/attachment/forum/201408/26/022751abp81pz6o9mm21k2.png" width="600"></span>
<div id="aimg_6658_menu" class="tip tip_4 aimg_tip">
<div class="xs0">
<p><strong>2.png</strong> <em class="xg1">(109.75 KB, 下载次数: 3)</em></p>
<p><a href="http://www.aboutyun.com/forum.php?mod=attachment&amp;aid=NjY1OHw1NDE0NWIxOHwxNDU0NjM3MTAxfDB8ODkxOQ%3D%3D&amp;nothumb=yes" rel="nofollow">下载附件</a>  <a id="savephoto_6658" href="" rel="nofollow">保存到相册</a></p>
<p class="xg1 y">2014-8-26 02:27 上传</p>
</div>
<div class="tip_horn"></div>
</div>
<br><br>
----------两张图片之间的分隔线----------<br><img id="aimg_6659" class="zoom" alt="" src="http://www.aboutyun.com/data/attachment/forum/201408/26/022755d40q55kbyg1xy0q0.png" width="600"><div id="aimg_6659_menu" class="tip tip_4 aimg_tip">
<div class="xs0">
<p><strong>3.png</strong> <em class="xg1">(177.59 KB, 下载次数: 2)</em></p>
<p><a href="http://www.aboutyun.com/forum.php?mod=attachment&amp;aid=NjY1OXw3YTQ3OGI2YXwxNDU0NjM3MTAxfDB8ODkxOQ%3D%3D&amp;nothumb=yes" rel="nofollow">下载附件</a>  <a id="savephoto_6659" href="" rel="nofollow">保存到相册</a></p>
<p class="xg1 y">2014-8-26 02:27 上传</p>
</div>
<div class="tip_horn"></div>
</div>
<br><br><br>
　　　　<span style="color:#f00000;">可以看到，刚好10条信息，没有丢失。不过消息因为均衡的原因，并非是有序的，在Kafka只提供了分区内部的有序性，不能跨partition. 每个分区的有序性，结合按Key分partition的能力对大多应用都够用了。（如何按key进行分partition，在文章末尾提供的Eclpise代码中有个Partitionertest.java提供了一个Demo）</span><br><br>
　　6、在命令行下打包java文件，测试kafka<br>
　　　　1)修改工程目录中的pom.xml文件<br><br><div class="blockcode">
<div id="code_GB9">
<ol><li>&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"<br></li><li>  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;<br></li><li>  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;<br></li><li>  &lt;groupId&gt;idoall.testkafka&lt;/groupId&gt;<br></li><li>  &lt;artifactId&gt;idoall.testkafka&lt;/artifactId&gt;<br></li><li>  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;<br></li><li>  &lt;packaging&gt;jar&lt;/packaging&gt;<br></li><li>  &lt;name&gt;idoall.testkafka&lt;/name&gt;<br></li><li>  &lt;url&gt;http://maven.apache.org&lt;/url&gt;<br></li><li>  &lt;properties&gt;<br></li><li>    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br></li><li>  &lt;/properties&gt;<br></li><li>  &lt;dependencies&gt;<br></li><li>    &lt;dependency&gt;<br></li><li>      &lt;groupId&gt;junit&lt;/groupId&gt;<br></li><li>      &lt;artifactId&gt;junit&lt;/artifactId&gt;<br></li><li>      &lt;version&gt;3.8.1&lt;/version&gt;<br></li><li>      &lt;scope&gt;test&lt;/scope&gt;<br></li><li>    &lt;/dependency&gt;<br></li><li>    &lt;dependency&gt;<br></li><li>      &lt;groupId&gt;log4j&lt;/groupId&gt;<br></li><li>      &lt;artifactId&gt;log4j&lt;/artifactId&gt;<br></li><li>      &lt;version&gt;1.2.14&lt;/version&gt;<br></li><li>    &lt;/dependency&gt;<br></li><li>    &lt;dependency&gt;<br></li><li>      &lt;groupId&gt;com.sksamuel.kafka&lt;/groupId&gt;<br></li><li>      &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;<br></li><li>      &lt;version&gt;0.8.0-beta1&lt;/version&gt;<br></li><li>    &lt;/dependency&gt;<br></li><li>  &lt;/dependencies&gt;<br></li><li>  &lt;build&gt;  <br></li><li>        &lt;finalName&gt;idoall.testkafka&lt;/finalName&gt;  <br></li><li>        &lt;plugins&gt;  <br></li><li>            &lt;plugin&gt;  <br></li><li>                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  <br></li><li>                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  <br></li><li>                &lt;version&gt;2.0.2&lt;/version&gt;  <br></li><li>                &lt;configuration&gt;  <br></li><li>                    &lt;source&gt;1.5&lt;/source&gt;  <br></li><li>                    &lt;target&gt;1.5&lt;/target&gt;  <br></li><li>                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;  <br></li><li>                &lt;/configuration&gt;  <br></li><li>            &lt;/plugin&gt;  <br></li><li>            &lt;plugin&gt;  <br></li><li>                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;  <br></li><li>                &lt;version&gt;2.4&lt;/version&gt;  <br></li><li>                &lt;configuration&gt;  <br></li><li>                    &lt;descriptors&gt;  <br></li><li>                        &lt;descriptor&gt;src/main/src.xml&lt;/descriptor&gt;  <br></li><li>                    &lt;/descriptors&gt;  <br></li><li>                    &lt;descriptorRefs&gt;  <br></li><li>                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;  <br></li><li>                    &lt;/descriptorRefs&gt;  <br></li><li>                &lt;/configuration&gt;  <br></li><li>                &lt;executions&gt;  <br></li><li>                    &lt;execution&gt;  <br></li><li>                        &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- this is used for inheritance merges --&gt;<br></li><li>                        &lt;phase&gt;package&lt;/phase&gt; &lt;!-- bind to the packaging phase --&gt;<br></li><li>                        &lt;goals&gt;  <br></li><li>                            &lt;goal&gt;single&lt;/goal&gt;  <br></li><li>                        &lt;/goals&gt;  <br></li><li>                    &lt;/execution&gt;  <br></li><li>                &lt;/executions&gt;  <br></li><li>            &lt;/plugin&gt;  <br></li><li>        &lt;/plugins&gt;  <br></li><li>    &lt;/build&gt;<br></li><li>&lt;/project&gt;</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　2)修改工程目录中的src/main/src.xml文件<br><br><div class="blockcode">
<div id="code_i9J">
<ol><li>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br></li><li>&lt;assembly xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"<br></li><li>          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"<br></li><li>          xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd "&gt;<br></li><li>    &lt;id&gt;jar-with-dependencies&lt;/id&gt;<br></li><li>    &lt;formats&gt;<br></li><li>        &lt;format&gt;jar&lt;/format&gt;<br></li><li>    &lt;/formats&gt;<br></li><li>    &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt;<br></li><li>    &lt;dependencySets&gt;<br></li><li>        &lt;dependencySet&gt;<br></li><li>            &lt;unpack&gt;false&lt;/unpack&gt;<br></li><li>            &lt;scope&gt;runtime&lt;/scope&gt;<br></li><li>        &lt;/dependencySet&gt;<br></li><li>    &lt;/dependencySets&gt;<br></li><li>    &lt;fileSets&gt;<br></li><li>        &lt;fileSet&gt;<br></li><li>            &lt;directory&gt;/lib&lt;/directory&gt;<br></li><li>        &lt;/fileSet&gt;<br></li><li>    &lt;/fileSets&gt;<br></li><li>&lt;/assembly&gt;</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　3)制作依赖包，在工程目录执行mvn package，得到idoall.testkafka-jar-with-dependencies.jar，下面是部分执行后的结果：<br><div class="blockcode">
<div id="code_ssO">
<ol><li>Running idoall.testkafka.idoall.testkafka.AppTest<br></li><li>Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec<br></li><li><br></li><li>Results :<br></li><li><br></li><li>Tests run: 1, Failures: 0, Errors: 0, Skipped: 0<br></li><li><br></li><li>[INFO] <br></li><li>[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ idoall.testkafka ---<br></li><li>[INFO] Building jar: /Users/lion/Documents/_my_project/java/idoall.testkafka/target/idoall.testkafka.jar<br></li><li>[INFO] <br></li><li>[INFO] --- maven-assembly-plugin:2.4:single (make-assembly) @ idoall.testkafka ---<br></li><li>[INFO] Reading assembly descriptor: src/main/src.xml<br></li><li>[WARNING] The assembly id jar-with-dependencies is used more than once.<br></li><li>[INFO] Building jar: /Users/lion/Documents/_my_project/java/idoall.testkafka/target/idoall.testkafka-jar-with-dependencies.jar<br></li><li>[INFO] Building jar: /Users/lion/Documents/_my_project/java/idoall.testkafka/target/idoall.testkafka-jar-with-dependencies.jar<br></li><li>[INFO] ------------------------------------------------------------------------<br></li><li>[INFO] BUILD SUCCESS<br></li><li>[INFO] ------------------------------------------------------------------------<br></li><li>[INFO] Total time: 9.074 s<br></li><li>[INFO] Finished at: 2014-08-05T12:22:47+08:00<br></li><li>[INFO] Final Memory: 63M/836M<br></li><li>[INFO] ------------------------------------------------------------------------</li></ol></div>
<em>复制代码</em></div>
<br>
　　　　4)编译文件，进入到工程目录，执行命令<br><div class="blockcode">
<div id="code_mqf">
<ol><li>liondeMacBook-Pro:idoall.testkafka lion$ pwd<br></li><li>/Users/lion/Documents/_my_project/java/idoall.testkafka<br></li><li>liondeMacBook-Pro:idoall.testkafka lion$ javac -classpath target/idoall.testkafka-jar-with-dependencies.jar -d . src/main/java/idoall/testkafka/*.java<br></li><li></ol></div>
<em>复制代码</em></div>
<br><br><br>
　　　　5)执行编译后的文件。分别打开两个窗口，一个用来消费，一个用来生产。可以看到消费窗口可以正常显示消息。<br><div class="blockcode">
<div id="code_I9Q">
<ol><li>java -classpath .:target/idoall.testkafka-jar-with-dependencies.jar idoall.testkafka.Producertest<br></li><li><br></li><li>java -classpath .:target/idoall.testkafka-jar-with-dependencies.jar idoall.testkafka.Consumertest<br></li><li></ol></div>
<em>复制代码</em></div>
<br><img id="aimg_6660" class="zoom" alt="" src="http://www.aboutyun.com/data/attachment/forum/201408/26/022758rzq2pwdzowoyyr2e.png" width="600"><div id="aimg_6660_menu" class="tip tip_4 aimg_tip">
<div class="xs0">
<p><strong>4.png</strong> <em class="xg1">(178.55 KB, 下载次数: 2)</em></p>
<p><a href="http://www.aboutyun.com/forum.php?mod=attachment&amp;aid=NjY2MHxiZjc3ZmI3ZHwxNDU0NjM3MTAxfDB8ODkxOQ%3D%3D&amp;nothumb=yes" rel="nofollow">下载附件</a>  <a id="savephoto_6660" href="" rel="nofollow">保存到相册</a></p>
<p class="xg1 y">2014-8-26 02:27 上传</p>
</div>
<div class="tip_horn"></div>
</div>
<br><br>
----------两张图片之间的分隔线----------<br><img id="aimg_6661" class="zoom" alt="" src="http://www.aboutyun.com/data/attachment/forum/201408/26/022803j3oti0v308gi3g1k.png" width="600"><div id="aimg_6661_menu" class="tip tip_4 aimg_tip" style="z-index:301;">
<div class="xs0">
<p><strong>5.png</strong> <em class="xg1">(253.37 KB, 下载次数: 2)</em></p>
<p><a href="http://www.aboutyun.com/forum.php?mod=attachment&amp;aid=NjY2MXxlNWY3OWIyOXwxNDU0NjM3MTAxfDB8ODkxOQ%3D%3D&amp;nothumb=yes" rel="nofollow">下载附件</a>  <a id="savephoto_6661" href="" rel="nofollow">保存到相册</a></p>
<p class="xg1 y">2014-8-26 02:28 上传</p>
</div>
<div class="tip_horn"></div>
</div>
<br><br><br><strong>　　五、FAQ</strong><br>
　　1、如果在创建主题时出现下面的错误 ，那就是启动的brokers的个数达不到你所指定的--replication-factor值：<br><br><br><div class="blockcode">
<div id="code_I5P">
<ol><li>Error while executing topic command replication factor: 3 larger than available brokers: 1<br></li><li>kafka.admin.AdminOperationException: replication factor: 3 larger than available brokers: 1<br></li><li>        at kafka.admin.AdminUtils$.assignReplicasToBrokers(AdminUtils.scala:70)<br></li><li>        at kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:155)<br></li><li>        at kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:86)<br></li><li>        at kafka.admin.TopicCommand$.main(TopicCommand.scala:50)<br></li><li>        at kafka.admin.TopicCommand.main(TopicCommand.scala)</li></ol></div>
<em>复制代码</em></div>
<br>
　2、如果出现下面的错误，可以先启动kafka，再启动hadoop中的zkfc(DFSZKFailoverController)：<br><div class="blockcode">
<div id="code_RMX">
<ol><li>Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0130000, 986513408, 0) failed; error='Cannot allocate memory' (errno=12)<br></li><li>#<br></li><li># There is insufficient memory for the Java Runtime Environment to continue.<br></li><li># Native memory allocation (malloc) failed to allocate 986513408 bytes for committing reserved memory.<br></li><li># An error report file with more information is saved as:<br></li><li># /home/hadoop/hs_err_pid13558.log</li></ol></div>
<em>复制代码</em></div>
<br><br>
代码下载：<br>
链接: <a href="http://pan.baidu.com/s/1bnriYK3" rel="nofollow">http://pan.baidu.com/s/1bnriYK3</a> 密码: x20l<br><br>            </div>
                </div>