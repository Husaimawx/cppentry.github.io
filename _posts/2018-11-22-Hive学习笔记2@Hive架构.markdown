---
layout:     post
title:      Hive学习笔记2@Hive架构
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/hqwang4/article/details/60141364				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>Hive架构分为四部分</h1>
<div><img src="https://img-blog.csdn.net/20170303155315637?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHF3YW5nNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></div>
<h2>接口</h2>
<p>Hive 对外提供了三种服务模式，即 Hive 命令行模式（CLI），Hive 的 Web模式（WUI），Hive的远程服务（Client）。、</p>
<h3>命令行模式</h3>
<p>Hive 命令行模式启动有两种方式。执行这条命令的前提是要配置 Hive 的环境变量。 </p>
<p>1、cd $HIVE_HOME/bin，执行如下命令：</p>
<p>         ./hive</p>
<p>2、直接执行命令。</p>
<p>　hive - -service cli</p>
<h3>Hive Web模式</h3>
<p>Hive Web的启动命令如下：</p>
<p>         hive–service hwi</p>
<p>通过浏览器访问Hive，默认端口为9999，配置如下：</p>
<p>&lt;property&gt;  </p>
<p>         &lt;name&gt;hive.hwi.war.file&lt;/name&gt;  </p>
<p>         &lt;value&gt;lib/hive-hwi-0.12.0-SNAPSHOT.war&lt;/value&gt;  </p>
<p>         &lt;description&gt;Thissets the path to the HWI war file, relative to ${HIVE_HOME}.&lt;/description&gt; 
</p>
<p>&lt;/property&gt;  </p>
<p>&lt;property&gt;  </p>
<p>         &lt;name&gt;hive.hwi.listen.host&lt;/name&gt;  </p>
<p>         &lt;value&gt;0.0.0.0&lt;/value&gt;  </p>
<p>         &lt;description&gt;Thisis the host address the Hive Web Interface will listen on&lt;/description&gt; 
</p>
<p>&lt;/property&gt;  </p>
<p>&lt;property&gt;  </p>
<p>         &lt;name&gt;hive.hwi.listen.port&lt;/name&gt;  </p>
<p>         &lt;value&gt;9999&lt;/value&gt;  </p>
<p>         &lt;description&gt;Thisis the port the Hive Web Interface will listen on&lt;/description&gt; 
</p>
<p>&lt;/property&gt;</p>
<h3>Hive远程服务</h3>
<p>远程服务（默认端口号 10000）启动方式命令如下：</p>
<p>nohup hive - -servicehiveserver2 &amp;</p>
<p>配置文件默认配置如下：</p>
<p>&lt;property&gt;</p>
<p>       &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</p>
<p>       &lt;value&gt;10000&lt;/value&gt;</p>
<p>    &lt;description&gt;Port number ofHiveServer2 Thrift interface when hive.server2.transport.mode is'binary'.&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<h2>元数据存储</h2>
<p>         Hive将元数据存储在RDBMS中，一般常用Derby和Mysql。默认情况下，Hive元数据保存在Derby数据库中，但只能允许一个会话连接，只用于简单的测试。实际生产中则需要一个独立的元数据库，使用Mysql作为元数据库。</p>
<p>         Metastore存储表的名字、表的列、分区、属性（是否为外部表）、表的数据所在目录等。</p>
<h3>hive-site.xml配置如下：</h3>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</p>
<p>   &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</p>
<p>   &lt;description&gt;Driver class name for a JDBCmetastore&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</p>
<p>   &lt;value&gt;jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</p>
<p>   &lt;description&gt;JDBC connect string for a JDBCmetastore&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</p>
<p>   &lt;value&gt;hive&lt;/value&gt;</p>
<p>   &lt;description&gt;Username to use against metastoredatabase&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</p>
<p>   &lt;value&gt;hive&lt;/value&gt;</p>
<p>   &lt;description&gt;password to use against metastoredatabase&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;hive.querylog.location&lt;/name&gt;</p>
<p>   &lt;value&gt;/home/hadoop/app/hive/iotmp&lt;/value&gt;</p>
<p>   &lt;description&gt;Location of Hive run time structured logfile&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</p>
<p>   &lt;value&gt;/home/hadoop/app/hive/iotmp&lt;/value&gt;</p>
<p>   &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<p>&lt;property&gt;</p>
<p>   &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</p>
<p>   &lt;value&gt;/home/hadoop/app/hive/iotmp&lt;/value&gt;</p>
<p>   &lt;description&gt;Temporary local directory for added resources in theremote file system.&lt;/description&gt;</p>
<p>&lt;/property&gt;</p>
<h2>驱动Driver</h2>
<p>用户通过接口提交Hive给Driver，由Driver进行HQL语句解析，此时从Metastore中获取表的信息，先生成逻辑计划，再生成物理计划，再由Executor生成Job交给Hadoop运行，然后由Driver将结果返回给用户。</p>
<p>编译器（Hive的核心）：1，语义解析器（ParseDriver），将查询字符串转换成解析树表达式；2，语法解析器（SemanticAnalyzer），将解析树转换成基于语句块的内部查询表达式；3，逻辑计划生成器（Logical Plan Generator），将内部查询表达式转换为逻辑计划，这些计划由逻辑操作树组成，操作符是Hive的最小处理单元，每个操作符处理代表一道HDFS操作或者是MR作业；4，查询计划生成器（QueryPlan Generator），将逻辑计划转化成物理计划（MR Job）。</p>
<p>优化器：优化器是一个演化<a href="http://www.2cto.com/kf/all/zujian/" rel="nofollow"><span>组件</span></a>，当前它的规则是：列修剪，谓词下压。</p>
<p>执行器：编译器将操作树切分成一个Job链（DAG），执行器会顺序执行其中所有的Job；如果Task链不存在依赖关系，可以采用并发执行的方式进行Job的执行。</p>
<h2>数据存储</h2>
<p>用HDFS进行存储，用MapReduce进行计算。</p>
<p>表中的一个Partition对应表下的一个子目录，每一个Bucket对应一个文件；Hive的默认数据仓库目录是/user/hive/warehouse，在hive-site.xml中由hive.metastore.warehouse.dir项定义；</p>
<h1>运行过程</h1>
<p>由客户端提供查询语句，提交给Hive，Hive再交给Driver处理（1，Compiler先编译，编译时要从Metastore中获取元数据信息，生成逻辑计划；2，生成物理计划；3，由Driver进行优化；4，Executor执行时对物理计划再进行分解成Job，并将这些Job提交给MR的JobTracker运行，提交Job的同时，还需要提取元数据信息关联具体的数据，这些元数据信息送到NN），JT拆分成各个Task进行计算，并将结果返回或写入HDFS。</p>
<p align="left"></p>
            </div>
                </div>