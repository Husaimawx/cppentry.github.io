---
layout:     post
title:      spark初探，官方文档
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="pysparksql">pyspark.sql</h1>

<ul>
<li>pyspark.sql.SparkSession Main entry point for DataFrame and SQL functionality.  SQL功能和DataFrame的主要入口</li>
<li>pyspark.sql.DataFrame A distributed collection of data grouped into named columns.          分布式数据集合，感觉有点像pandas的DF</li>
<li>pyspark.sql.Column A column expression in a DataFrame.</li>
<li>pyspark.sql.Row A row of data in a DataFrame.</li>
<li>pyspark.sql.GroupedData Aggregation methods, returned by DataFrame.groupBy().  不知道这玩意啥用</li>
<li>pyspark.sql.DataFrameNaFunctions Methods for handling missing data (null values).  缺失值的处理</li>
<li>pyspark.sql.DataFrameStatFunctions Methods for statistics functionality.    统计功能</li>
<li>pyspark.sql.functions List of built-in functions available for DataFrame. DataFrame可用的内置函数列表</li>
<li>pyspark.sql.types List of data types available.数据类型的类型列表</li>
<li>pyspark.sql.Window For working with window functions.用于处理窗口函数</li>
</ul>



<h2 id="sparksession">SparkSession</h2>

<p><em>spark编程 DataFrame and SQL的API</em> <br>
能够用来建立DF,将DF注册为表，并对其执行SQL操作，缓存表，读取parquet 文件。SparkSession这样创建</p>



<pre class="prettyprint"><code class=" hljs r">spark = SparkSession.builder \
<span class="hljs-keyword">...</span>     .master(<span class="hljs-string">"local"</span>) \ <span class="hljs-comment">#连接到spark主节点，local[4]四核心 ,     spark://master:7077 运行在独立集群</span>
<span class="hljs-keyword">...</span>     .appName(<span class="hljs-string">"Word Count"</span>) \  
<span class="hljs-keyword">...</span>     .config(<span class="hljs-string">"spark.some.config.option"</span>, <span class="hljs-string">"some-value"</span>) \
<span class="hljs-keyword">...</span>     .getOrCreate()    <span class="hljs-comment">#该方法先检查全局中是否存在sparksession，存在则返回该sparksession,否则新创建一个</span></code></pre>

<p>或者</p>



<pre class="prettyprint"><code class=" hljs python"><span class="hljs-prompt">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> pyspark.conf <span class="hljs-keyword">import</span> SparkConf
<span class="hljs-prompt">&gt;&gt;&gt; </span>conf = SparkConf().setMaster(<span class="hljs-string">"local"</span>).setAppName(<span class="hljs-string">" test "</span>)
<span class="hljs-prompt">&gt;&gt;&gt; </span>SparkSession.builder.config(conf=conf)
</code></pre>

<p><em>SparkSession.catalog</em> <br>
用户可以创建、删除、修改或查询底层数据库、表、函数等的接口</p>

<p><em>SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</em> <br>
 从一个RDD, List 或者pandas.DataFrame建立一个DF <br>
 schema 列名，类似于columns,类型从数据中推断，如果为None,则从数据推断，数据必须为 RDD of Row, or namedtuple, or dict. <br>
 如果为 <em>pyspark.sql.types.DataType or a datatype string</em>必须与数据匹配，否则会抛出异常，如果不是<em>pyspark.sql.types.StructType</em>，则会被包装为<em>pyspark.sql.types.StructType</em>作为唯一字段，字段名将为value, <br>
 如果需要从data推断schema, <em>samplingRatio</em> 确定抽取多少row来确定，默认取第一行 <br>
verifyschema,验证每一行对模式的数据类型</p>

<p><em>SparkSession.newSession()</em> <br>
 返回一个新的session，具有独立的conf</p>

<p><em>SparkSession.range(start, end=None, step=1, numPartitions=None)</em> <br>
 返回一个列名为id的range <br>
 <code>spark.range(1, 7, 2).collect() <br>
[Row(id=1), Row(id=3), Row(id=5)]</code></p>

<p><em>SparkSession.read</em> <br>
 读入数据作为df</p>



<h2 id="class-pysparksqldataframejdf-sqlctx">class pyspark.sql.DataFrame(jdf, sql_ctx)</h2>

<p><em>agg(*exprs)</em> <br>
    无分组的聚合整个df</p>



<pre class="prettyprint"><code class=" hljs avrasm">&gt;&gt;&gt; df<span class="hljs-preprocessor">.agg</span>({<span class="hljs-string">"age"</span>: <span class="hljs-string">"max"</span>})<span class="hljs-preprocessor">.collect</span>()
[Row(max(age)=<span class="hljs-number">5</span>)]
&gt;&gt;&gt; from pyspark<span class="hljs-preprocessor">.sql</span> import functions as F
&gt;&gt;&gt; df<span class="hljs-preprocessor">.agg</span>(F<span class="hljs-preprocessor">.min</span>(df<span class="hljs-preprocessor">.age</span>))<span class="hljs-preprocessor">.collect</span>()
[Row(min(age)=<span class="hljs-number">2</span>)]</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>