---
layout:     post
title:      hive系列（1）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/tianjun2012/article/details/64439605				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2 id="一概述">一、概述</h2>

<p>理解下hive整体流程： <br>
<img src="https://img-blog.csdn.net/20170321114352764?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGlhbmp1bjIwMTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="二hive的数据存储">二、Hive的数据存储</h2>

<p>1、Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等） <br>
2、只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。 <br>
3、Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。 <br>
1）、db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 <br>
2）、table：在hdfs中表现所属db目录下一个文件夹 <br>
3）、external table：外部表, 与table类似，不过其数据存放位置可以在任意指定路径 <br>
普通表: 删除表后, hdfs上的文件都删了 <br>
External外部表删除后, hdfs上的文件没有删除, 只是把文件删除了 <br>
4）、partition：在hdfs中表现为table目录下的子目录 <br>
5）、bucket：桶, 在hdfs中表现为同一个表目录下根据hash散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中。</p>



<h2 id="三hive-thrift服务">三、Hive thrift服务</h2>

<p>原理图 <br>
 <img src="https://img-blog.csdn.net/20170321151000293?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGlhbmp1bjIwMTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
启动方式，（假如是在mini01上）： <br>
启动为前台：bin/hiveserver2 <br>
启动为后台：nohup bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp;</p>

<p>启动成功后，可以在别的节点上用beeline去连接 <br>
方式（1） <br>
hive/bin/beeline  回车，进入beeline的命令界面 <br>
输入命令连接hiveserver2 <br>
beeline&gt; !connect jdbc:hive2//mini1:10000 <br>
（mini01是hiveserver2所启动的那台主机名，端口默认是10000） <br>
方式（2） <br>
或者启动就连接： <br>
bin/beeline -u jdbc:hive2://mini01:10000 -n root</p>

<p>接下来就可以做正常sql查询了</p>



<h2 id="四常用命令">四、常用命令</h2>



<h1 id="ddl操作">DDL操作</h1>

<p>1.创建表</p>



<pre class="prettyprint"><code class=" hljs r">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
   [(col_name data_type [COMMENT col_comment], <span class="hljs-keyword">...</span>)] 
   [COMMENT table_comment] 
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], <span class="hljs-keyword">...</span>)] 
   [CLUSTERED BY (col_name, col_name, <span class="hljs-keyword">...</span>) 
   [SORTED BY (col_name [ASC|DESC], <span class="hljs-keyword">...</span>)] INTO num_buckets BUCKETS] 
   [ROW FORMAT row_format] 
   [STORED AS file_format] 
   [LOCATION hdfs_path]
</code></pre>



<pre class="prettyprint"><code class=" hljs sql">说明：
1、  <span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 <span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> 选项来忽略这个异常。
<span class="hljs-number">2</span>、  <span class="hljs-keyword">EXTERNAL</span>关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。
<span class="hljs-number">3</span>、  <span class="hljs-keyword">LIKE</span> 允许用户复制现有的表结构，但是不复制数据。
<span class="hljs-number">4</span>、  <span class="hljs-keyword">ROW</span> FORMAT 
DELIMITED [FIELDS TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-keyword">char</span>] [COLLECTION ITEMS TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-keyword">char</span>] 
        [MAP KEYS TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-keyword">char</span>] [LINES TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-keyword">char</span>] 
   | SERDE serde_name [<span class="hljs-keyword">WITH</span> SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]
用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 <span class="hljs-keyword">ROW</span> FORMAT 或者 <span class="hljs-keyword">ROW</span> FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。
<span class="hljs-number">5</span>、  STORED <span class="hljs-keyword">AS</span> 
SEQUENCEFILE|TEXTFILE|RCFILE
如果文件数据是纯文本，可以使用 STORED <span class="hljs-keyword">AS</span> TEXTFILE。如果数据需要压缩，使用 STORED <span class="hljs-keyword">AS</span> SEQUENCEFILE。
<span class="hljs-number">6</span>、CLUSTERED <span class="hljs-keyword">BY</span>
对于每一个表（<span class="hljs-keyword">table</span>）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 
把表（或者分区）组织成桶（Bucket）有两个理由：
（<span class="hljs-number">1</span>）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side <span class="hljs-keyword">join</span>）高效的实现。比如<span class="hljs-keyword">JOIN</span>操作。对于<span class="hljs-keyword">JOIN</span>操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行<span class="hljs-keyword">JOIN</span>操作就可以，可以大大较少<span class="hljs-keyword">JOIN</span>的数据量。
（<span class="hljs-number">2</span>）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。</span></code></pre>



<h2 id="表创建实例"> 表创建实例</h2>

<p>1、创建内部表</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> mytable(sid <span class="hljs-keyword">int</span>,sname string)
<span class="hljs-keyword">row</span> format delimited fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> stored <span class="hljs-keyword">as</span> textfile;</span></code></pre>

<p>2、创建外部表</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> pageview(pageid <span class="hljs-keyword">int</span>,page_url string comment <span class="hljs-string">'The page URL'</span>)
<span class="hljs-keyword">row</span> format delimited fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>
location <span class="hljs-string">'hdfs://mini01:9000/user/hive/warehouse/tianjun'</span></span></code></pre>

<p>3、创建分区表</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student p(Sno <span class="hljs-keyword">int</span>,Sname string,Sex string,Sage <span class="hljs-keyword">int</span>,Sdept string) partitioned <span class="hljs-keyword">by</span>(part string) 
<span class="hljs-keyword">row</span> format delimited fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> stored <span class="hljs-keyword">as</span> textfile;</span></code></pre>

<p>4、创建带桶的表</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(id <span class="hljs-keyword">int</span>,name string)
partitioned <span class="hljs-keyword">by</span> (stat_date string)
clustered <span class="hljs-keyword">by</span> (id) sorted <span class="hljs-keyword">by</span>(age) <span class="hljs-keyword">into</span> <span class="hljs-number">2</span> buckets
<span class="hljs-keyword">row</span> format delimited fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>;</span></code></pre>



<h2 id="表修改实例">表修改实例</h2>

<dl>
<dt>增加或修改分区</dt>
<dt>语法结构</dt>
<dt>ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION ‘location1’ ] partition_spec [ LOCATION ‘location2’ ] …</dt>
<dt>partition_spec:</dt>
<dd>PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, …) <br>
ALTER TABLE table_name DROP partition_spec, partition_spec,… <br>
具体实例如下：</dd>
</dl>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> student_p <span class="hljs-keyword">add</span> partition(part=<span class="hljs-string">'a'</span>) partition(part=<span class="hljs-string">'b'</span>);</span></code></pre>

<p>显示某表的分区</p>



<pre class="prettyprint"><code class=" hljs sql"> <span class="hljs-operator"><span class="hljs-keyword">show</span> partitions studens;</span></code></pre>

<p>重命名表 <br>
语法结构 <br>
ALTER TABLE table_name RENAME TO new_table_name</p>

<p>增加/更新列 <br>
语法结构 <br>
ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …)  <br>
注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。 <br>
ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</p>

<p>显示命令 <br>
show tables <br>
show databases <br>
show partitions <br>
show functions <br>
desc extended t_name; <br>
desc formatted table_name;</p>



<h1 id="dml操作">DML操作 </h1>



<h2 id="load">Load</h2>

<pre><code>语法结构
</code></pre>

<p>LOAD DATA [LOCAL] INPATH ‘filepath’ [OVERWRITE] INTO  <br>
TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)]</p>

<p>说明： <br>
1、  Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 <br>
2、  filepath： <br>
相对路径，例如：project/data1  <br>
绝对路径，例如：/user/hive/project/data1  <br>
包含模式的完整 URI，列如： <br>
hdfs://namenode:9000/user/hive/project/data1 <br>
3、  LOCAL关键字 <br>
如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。 <br>
如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件 <br>
4、  OVERWRITE 关键字 <br>
如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。  <br>
如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。</p>



<h2 id="insert">Insert</h2>

<pre><code>将查询结果插入Hive表
</code></pre>

<p>语法结构 <br>
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement <br>
Multiple inserts: <br>
FROM from_statement  <br>
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1  <br>
[INSERT OVERWRITE TABLE tablename2 [PARTITION …] select_statement2] … <br>
Dynamic partition inserts: <br>
INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] …) select_statement FROM from_statement</p>

<p>导出表数据 <br>
语法结构 <br>
INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT … FROM … <br>
实例如下： </p>



<pre class="prettyprint"><code class=" hljs sql"> <span class="hljs-operator"><span class="hljs-keyword">insert</span> overwrite <span class="hljs-keyword">local</span> directory <span class="hljs-string">'/root/tianjun/student'</span>
 <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> student;</span></code></pre>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">insert</span> overwrite directory <span class="hljs-string">'hdfs://mini01:9000/user/tianjun/student'</span>
<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> student;</span></code></pre>



<pre class="prettyprint"><code class=" hljs lasso">hive导出查询文件到本地文件的<span class="hljs-number">2</span>种办法
通过HQL语句
可以将hive  中表的数据生成到指定的目录。
有时候 我们可以利用hive来生成统计的中间文件（比源文件小的多的）
方法有如下<span class="hljs-number">2</span>种：
<span class="hljs-number">1.</span>INSERT OVERWRITE <span class="hljs-built_in">LOCAL</span> DIRECTORY
将结果输出到指定的目录：
生成的文件数 和redurcer的数目的一样的
在hive下面执行
INSERT OVERWRITE <span class="hljs-built_in">LOCAL</span> DIRECTORY <span class="hljs-string">'/hive_dat/package_name'</span>
<span class="hljs-keyword">select</span> package_name,count(<span class="hljs-number">1</span>) from app_list <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> package_name;

<span class="hljs-number">2.</span>直接查询生成结果文件：
在linux下面执行:(也支持后台运行 如果执行时间比较长的话)
hive <span class="hljs-attribute">-S</span> <span class="hljs-attribute">-e</span><span class="hljs-string">"select package_name,count(1) from app_list group by package_name;"</span><span class="hljs-subst">&gt;</span> grp_app_id<span class="hljs-built_in">.</span>dat
这个只生成一个文件 并且输出的文件里面的列 是以空格隔开的。</code></pre>



<h2 id="select">SELECT</h2>

<p>基本的Select操作 <br>
语法结构 <br>
SELECT [ALL | DISTINCT] select_expr, select_expr, …  <br>
FROM table_reference <br>
[WHERE where_condition]  <br>
[GROUP BY col_list [HAVING condition]]  <br>
[CLUSTER BY col_list  <br>
  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]  <br>
]  <br>
[LIMIT number]</p>

<p>注：1、order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 <br>
2、sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。 <br>
3、distribute by根据distribute by指定的内容将数据分到同一个reducer。 <br>
4、Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by</p>

<p>分桶表的作用：最大的作用是用来提高join操作效率； <br>
（思考： <br>
Select a.id ,a.name,b.addr from a join b on a.id = b.id; <br>
如果a表和b表已经是分桶表，而且分桶的字段是id字段，做这个join操作时，还需要全表做笛卡尔积么？）</p>



<pre class="prettyprint"><code class=" hljs sql">#创建分桶表
<span class="hljs-operator"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> stu_buck(sno <span class="hljs-keyword">int</span>,sname string,sex string,sage <span class="hljs-keyword">int</span>,sdept string)
clustered <span class="hljs-keyword">by</span>(sno) 
sorted <span class="hljs-keyword">by</span>(sno <span class="hljs-keyword">DESC</span>)
<span class="hljs-keyword">into</span> <span class="hljs-number">4</span> buckets
<span class="hljs-keyword">row</span> format delimited
fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>;</span>

#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数
<span class="hljs-operator"><span class="hljs-keyword">set</span> hive.enforce.bucketing = <span class="hljs-keyword">true</span>;</span>
<span class="hljs-operator"><span class="hljs-keyword">set</span> mapreduce.job.reduces=<span class="hljs-number">4</span>;</span>

#客户往创建的分桶表插入数据(插入数据需要是已分桶, 且排序的)
#可以使用distribute by(sno) sort by(sno asc)   或是排序和分桶的字段相同的时候使用Cluster by(字段)
#注意使用cluster by  就等同于分桶+排序(sort)
<span class="hljs-operator"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> stu_buck
<span class="hljs-keyword">select</span> sno,sname,sex,sage,sdept <span class="hljs-keyword">from</span> student distribute <span class="hljs-keyword">by</span>(sno) sort <span class="hljs-keyword">by</span>(sno <span class="hljs-keyword">asc</span>);</span></code></pre>

<p>查看表大小 <br>
show create table table_name(表名)——&gt;查看表的hdfs上的位置 <br>
计算表大小，单位G</p>



<pre class="prettyprint"><code class=" hljs rsl">hdfs dfs -du /dhome/<span class="hljs-built_in">trace</span>/di/t_di_trace_event_d|awk <span class="hljs-string">'{SUM += $1} END {print SUM/(1024*1024*1024)}'</span></code></pre>

<p>hive 空值处理</p>



<pre class="prettyprint"><code class=" hljs r">coalesce(a,x1,x2,x3,<span class="hljs-keyword">...</span>)
isnull(a,x)
</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>