---
layout:     post
title:      HDFS环境搭建—伪分布式搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h2>                              本文是慕课网大数据学习的笔记加总结：</h2>

<h2>目录：</h2>

<h2>一、HDFS环境搭建—伪分布式搭建</h2>

<h2>二、HDFS的shell命令</h2>

<h2>三、java操作HDFS开发环境搭建</h2>

<h2>四、java API操作HDFS文件系统</h2>

<h2>一、HDFS环境搭建—伪分布式搭建</h2>

<h3>1.安装java，</h3>

<p>​ 1.配置到系统环境变量中</p>

<p>vim /etc/profile</p>

<pre>
export JAVA_HOME=/home/hadoop/app/jdk1.7.0_79
export PATH=$JAVA_HOME/bin:$PATH</pre>

<p>​ 2.使环境变量生效：</p>

<p>​ source /etc/profile</p>

<p>​ 3.验证java是否配置成功：</p>

<p>​ java —version</p>

<p>​ 4.查找JAVA_HOME中配置的java路径</p>

<p>​ echo $JAVA_HOME</p>

<h3>2.安装ssh</h3>

<p>​ 1.安装ssh：</p>

<p>​ sudo yum install ssh</p>

<p>​ 2.生成公钥和私钥</p>

<p>​ ssh-keygen -t rsa</p>

<p>​ 3.查看生成时候的记录找到 key存放的目录</p>

<p>​ 4.将生成的公钥拷贝到 authorized_keys里</p>

<p>​ 先进入到公钥所在目录</p>

<p>​ ssh-copy-id hadoop000</p>

<h3>2.安装hadoop</h3>

<p>​ hadoop目录结构</p>

<p>​ bin:客户端访问的一些脚本 如 hadoop map hdfs</p>

<p>​ etc-&gt;hadoop:配置文件</p>

<p>​ sbin:启动集群的，启动dfs的，启动yarn</p>

<p>​ share-&gt;hadoop-&gt;mapreduce:example包带有案例，到时候直接使用就好</p>

<p>​ 1.hadoop配置文件的修改</p>

<p>​ 进入到/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop</p>

<p>​ 打开</p>

<p>​ vim hadoop-env.sh 设置JAVA_HOME</p>

<p>​ export JAVA_HOME=/home/hadoop/app/jdk1.7.0_79</p>

<p>​ 2.配置HDFS默认文件地址</p>

<p>etc/hadoop/core-site.xml</p>

<pre>
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;
    &lt;/property&gt;
    &lt;!--设置hadoop的存储位置--&gt;
    &lt;!--如果使用默认的设置将保存在临时文件夹，每次重启都会重置该文件夹--&gt;
     &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</pre>

<p>​</p>

<p>​ 3.设置副本系数：为1 就说每一个block只有一个副本</p>

<p>etc/hadoop/hdfs-site.xml</p>

<pre>
configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</pre>

<p>​ 4.将hadoop的bin目录配置到环境变量中</p>

<p>vim /etc/profile</p>

<pre>
export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0
export PATH=$HADOOP_HOME/bin:$PATH</pre>

<p>让配置生效</p>

<p>​ source /etc/profile</p>

<p>检验是否配对</p>

<p>​ echo $HADOOP_HOME</p>

<h3>3启动HDFS</h3>

<p>​ 1.格式化文件系统（仅第一次执行即可，不要重复执行)</p>

<p>hdfs namenode -format</p>

<p>​ 2.启动namenod和dataNode</p>

<p>​ 在/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/sbin目录下</p>

<p>​ ./start-dfs.sh</p>

<p>​ 3.验证是否启动成功</p>

<p>​ 输入jps</p>

<p>​ 应当有：</p>

<p>​ NameNode SecondaryNameNode</p>

<p>​ DataNode</p>

<p>​ 如果有误，可以查看日志</p>

<p>​ /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop000.log</p>

<p>​ 通过浏览器方式</p>

<p>​ (需要在windows上配置映射关系）<a href="http://hadoop000:50070" rel="nofollow">http://hadoop000:50070</a></p>

<h3>4.停止hdfs</h3>

<p>​ 在/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/sbin目录下</p>

<p>​ ./stop-dfs.sh</p>

<h3>5.HDFS伪分布式搭建出现的问题：</h3>

<pre>
 Connection reset by peer; Host Details : local host is: "hadoop000/192.168.199.111"; destination host is: "hadoop000":8020;</pre>

<p>解决：将core-site.xml中的端口号去掉</p>

<pre>
 &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://hadoop000&lt;/value&gt;
    &lt;/property&gt;</pre>

<h2>二、HDFS SHELL</h2>

<p>​ hdfs dfs 等价 hadoop fs</p>

<p>​ hadoop fs -ls / 查看根目录的所有文件</p>

<p>​ hadoop fs -put 文件名 目标路径 将文件传到hdfs上</p>

<p>​ hadoop fs -cat 文件路径 查看文件中的内容</p>

<p>​ hadoop fs -text 文件路径 查看文件中的内容</p>

<p>​ hadoop fs -mkdir /test 创建目录</p>

<p>​ hadoop fs -mkdir -p /test/a/b 递归创建文件夹</p>

<p>​ hadoop fs -ls -R / 显示文件，文件夹，及文件夹下的文件。</p>

<p>​ hadoop fs -get /test/a/b/h.text 将文件复制到本地</p>

<p>​ hadoop fs -rm /hello.text 删除文件</p>

<p>​ hadoop fs -rm -R /test 删除文件夹</p>

<p>​ hadoop fs -mv /1.text /test/ 将1.text移动到test下</p>

<p> </p>

<h2>三、java操作HDFS开发环境搭建</h2>

<h3>1.创建maven工程</h3>

<h3>2.导入依赖</h3>

<pre>
&lt;!--添加hadoop的依赖--&gt;
&lt;!--添加cdh仓库--&gt;
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;cloudera&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;properties&gt;
    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;
&lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;version&gt;${hadoop.version}&lt;/version&gt;
            &lt;/dependency&gt;
    &lt;/dependencies&gt;</pre>

<h3>问题：</h3>

<p>Missing artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0</p>

<pre>
&lt;!--没有添加cdh仓库--&gt;
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;cloudera&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;</pre>

<p> </p>

<h2>四、java API操作HDFS文件系统</h2>

<h3>1.Hadoop HDFS Java API 操作</h3>

<pre>
​
public class HDFSApp{
    public static final String HDFS_PATH ="hdfs://hadoop000:8020";
    FileSystem fileSystem=null;
    Configuration configuration =null;
    //创建hdfs目录
    @Test
    public void mkdir() throws Exception{
        fileSystem.mkdirs(new Path(pathString:"/hdfsapi/test"));
        
    }
    //创建文件
    @Test
    public void mkFile() throws Exception{
        FSDataOutPutStream output=fileSystem.create(new Path(pathString:"/hdfsapi/test/a.txt"));
        output.write("hello hadoop".getBytes());
        output.flush();
        output.close();
    }
    //查看HDFS文件上的内容
    @Test
    public void cat() throws Exception{
        FSDataInPutStream in=fileSystem.open(newPath(pathString:"/hdfsapi/test/a.txt"));
        IOUtils.copyBytes(in,System.out,buffSize:1024);
        in.close();
    }
    //给文件重命名
    public void rename() throws Exception{
        Path oldPath=new Path(pathString:"/hdfsapi/test/a.txt");
        Path newPath=new Path(pathString:"/hdfsapi/test/b.txt");
        fileSystem.rename(oldPath,newPath);
    }
    //从本地拷贝文件到HDFS
    @Test
    public void copyFromLocal() throws Excpetion{
        Path localPath=new Path(pathString:"d:/1.txt");
        Path hdfsPath=new Path(pathString:"/hdfsapi/test");
        fileSystem.copyFromLocalFile(localPath.hdfsPath).;
    }
      //从本地拷贝文件到HDFS上带有进度条
  @Test
    public void copyFromLocalWithProgress() throws Exception {
        Configuration configuration=new Configuration();
        FileSystem fileSystem2 = FileSystem.get(new URI("hdfs://hadoop000:8020"), configuration, "hadoop");
        Path dst=new Path("/testApp2/feiq.tar");
        BufferedInputStream in=new BufferedInputStream(new FileInputStream(new File("D:\\\\BaiduYunDownload\\\\FeiQ.exe")));
        FSDataOutputStream fsDataOutputStream = fileSystem2.create(dst, new Progressable() {
            
            @Override
            public void progress() {
                System.out.print(".\t");
            }
        });
        IOUtils.copyBytes(in, fsDataOutputStream, 4096);
        fsDataOutputStream.close();
        in.close();
        fileSystem2.close();
    }
    //从HDFS拷贝文件到本地
    @Test
public void copyToLocalFile() throws Exception{
    Path localPath=new Path(pathString:"");
    Path hdfsPath=new Path(pathString:"");
    fileSystem.copyToLocalFile(hdfsPath,localPath);
}
//查看某个目录下的所有文件
@Test
public void listFiles() throws Exception{
    FileStatus[] fileStatuses=fileSystem.listStatus(new Path(pathString:"/hdfsapi/test"));
    for(FileStatus fileStatus:fileStatuses){
        String idDir=fielStatus.isDirectory()?"文件夹":"文件";
        //得到一个文件的副本因子
        short replication=fileStatus.getReplication();
        //得到文件的大小
        long len=fileStatus.getLen();
        //得到文件的路径
        String path=fileStatus.getPath().toString();
        System.out.println(isDir+"\t"+replication+"\t"+len+"t"+path):
    }
    
}
//删除
@Test
public void delete() throws Exception{
    //是否递归删除
    fileSystem.delte(new Path(pathString:"/hdfsapi/test/")recursive:true);
}
​
    @Before
    public void setUp() throws Exception{
        System.out.println("HDFSApp.setUp");
        configuration =new Configuration();
        fileSystem =FileSystem.get(new URI(HDFS_PATH),configuration，user:"hadoop")
    }
    @After
    public void tearDown() throws Exception{
        configuration =null;
        fileSystem =null;
        System.out.println("HDFSApp.tearDown");
    }
}</pre>

<p> </p>

<h3>2.注意：</h3>

<p>​ 如果通过java上传到HDFS上副本将是3，因为在java上并没有设置副本系数</p>

<p>​ 如果同HDFS shell 的put命令将文件上传到HDFS上 副本将是1，因为在linux中已经配置了副本系数了。</p>

<h3>3.HDFS读写数据流程</h3>

<p>​ 1.写数据的流程（重点，需要画图）</p>

<p> <img alt="" class="has" src="https://img-blog.csdn.net/20180716214724862?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5ndGlhbnB1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p></p>

<p>​ 2.读数据流程</p>

<p></p>

<p> <img alt="" class="has" src="https://img-blog.csdn.net/20180716214800291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5ndGlhbnB1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h3>4.HDFS优缺点</h3>

<p>​ 1.优点：</p>

<p>​ 数据冗余，硬件容错：多副本，保证一台机坏掉还有其他的副本在别的机器上。</p>

<p>​ 处理流式的数据访问：一次写入多次读取。</p>

<p>​ 适合存储大文件</p>

<p>​ 可构建在廉价的机器上</p>

<p>​ 2.缺点</p>

<p>​ 低延迟的数据访问</p>

<p>​ 不适合小文件存储：小文件的存储也必须占用namenode来记录元数据，如果小文件过多，就相当于一个小文件就对应一个namenode，性价比不高</p>            </div>
                </div>