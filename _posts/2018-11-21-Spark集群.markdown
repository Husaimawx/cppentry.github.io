---
layout:     post
title:      Spark集群
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2><a id="Spark__0"></a>Spark 运行模式</h2>
<p>Spark 支持四种运行模式：</p>
<blockquote>
<p>Local		使用本地线程模拟，多用于测试<br>
Standalone		spark默认支持的<br>
YARN		最具前景<br>
Mesos</p>
</blockquote>
<h2><a id="Spark__8"></a>Spark 集群提交模式</h2>
<p>Spark 支持两种提交模式：</p>
<blockquote>
<p>client	该提交模式 driver 进程在客户端启动<br>
cluster	该提交模式 driver 进程在任意 worker 节点上启动</p>
</blockquote>
<h2><a id="Spark__14"></a>Spark 集群提交命令</h2>

<table>
<thead>
<tr>
<th>参数</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>- -master</td>
<td>指定 Job 提交的 Master 节点</td>
</tr>
<tr>
<td>- -class</td>
<td>指定运行 main 方法所在的类</td>
</tr>
<tr>
<td>- - deploy-mode</td>
<td>指定提交模式</td>
</tr>
<tr>
<td>- -executor-cores</td>
<td>指定每个 Executor 使用的 CPU 核数</td>
</tr>
<tr>
<td>- -executor-memory</td>
<td>指定每个 Executor 使用的内存</td>
</tr>
<tr>
<td>- -total-executor-cores</td>
<td>指定所有 Executor 一共使用的 CPU 核数</td>
</tr>
<tr>
<td>- -driver-cores</td>
<td>指定 driver 使用的 CPU 核数</td>
</tr>
<tr>
<td>- -driver-memory</td>
<td>指定 driver 使用的内存</td>
</tr>
<tr>
<td>- -driver-class-path</td>
<td>指定 driver 所依赖的 jar 包的位置</td>
</tr>
</tbody>
</table><h2><a id="Spark_standalone__28"></a>Spark standalone 集群</h2>
<ol>
<li>
<p>到官网下载 spark 的安装包，这里使用 spark-1.6.3，hadoop版本是 2.6.5<br>
<img src="https://img-blog.csdnimg.cn/20181106190805174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="spark download"></p>
</li>
<li>
<p>解压，改名为 spark-1.6.3</p>
</li>
<li>
<p>进入 conf 目录下，使用如下命令</p>
</li>
</ol>
<pre><code> cp slaves.template slaves
 cp spark-env.sh.template spark-env.sh
</code></pre>
<ol start="4">
<li>修改 slaves文件，在其中添加从节点</li>
</ol>
<pre><code>node02
node03
node04
</code></pre>
<ol start="5">
<li>修改 spark-env.sh文件</li>
</ol>
<pre><code># export JAVA_HOME=/opt/software/jdk1.8.0_151	# centOS 7 需要引入JAVA_HOME
SPARK_MASTER_IP=node01	master所在节点
SPARK_MASTER_PORT=7077	master资源通信端口
SPARK_WORKER_CORES=2	worker管理的核数
SPARK_WORKER_MEMORY=800m	worker管理的内存
SPARK_WORKER_INSTANCES=1	每个节点启动worker的个数
SPARK_WORKER_DIR=/var/zgl/spark	worker的工作目录
# SPARK_MASTER_WEBUI_PORT=8888	WebUI的端口号，默认为8080，与tomcat冲突
</code></pre>
<ol start="6">
<li>进入 sbin 目录，做如下修改，因为这两个命令与 hadoop 命令冲突</li>
</ol>
<pre><code>mv start-all.sh start-spark.sh 
mv stop-all.sh  stop-spark.sh  
</code></pre>
<ol start="7">
<li>将配置好的spark安装包发送到其他节点和客户端节点</li>
</ol>
<pre><code>[root@node01 zgl]# scp -r spark-1.6.3 node02:`pwd`
[root@node01 zgl]# scp -r spark-1.6.3 node03:`pwd`
[root@node01 zgl]# scp -r spark-1.6.3 node04:`pwd`
[root@node01 zgl]# scp -r spark-1.6.3 client:`pwd`
</code></pre>
<ol start="8">
<li>在 node01 和 client 节点上配置 spark 的环境变量，配置到 ~/.bashrc 中</li>
</ol>
<pre><code>vim ~/.bashrc

export SPARK_HOME=/opt/zgl/spark-1.6.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
</code></pre>
<ol start="9">
<li>在 node01 上启动集群</li>
</ol>
<pre><code>start-spark.sh
</code></pre>
<ol start="10">
<li>jps 检查集群是否启动</li>
</ol>
<pre><code>[root@node01 ~]# jps
1123 Master

[root@node02 ~]# jps
1124 Worker

[root@node03 ~]# jps
1125 Worker

[root@node04 ~]# jps
1129 Worker
</code></pre>
<ol start="11">
<li>在客户端提交 spark 提供的示例程序 SparkPi （该程序通过概率计算 π 的值）<br>
–注意提交命令一行写完，不要换行	<br>
–任务很可能会因为内存不足导致失败</li>
</ol>
<pre><code>spark-submit  master的url  执行器所使用的内存  
运行的程序的全类名  
程序jar包所在的位置  
提供一个参数（参数越大，计算的π值越精准）

spark-submit --master spark://node01:7077 --executor-memory 500m
 --class org.apache.spark.examples.SparkPi 
 /opt/zgl/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 
 50
</code></pre>
<ol start="12">
<li>可以在 node01:8080 WEBUI页面查看任务<br>
<img src="https://img-blog.csdnimg.cn/20181106201042514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="WEB UI"></li>
</ol>
<h2><a id="Spark_standalone_HA_119"></a>Spark standalone HA</h2>
<p>同 HDFS 的 NameNode，YARN 的 ResourceManager 一样，spark standalone 的 master 也存在单点故障问题，于是同样也就有了基于 zookeeper 监控的高可用模式（spark standalone 还有一种高可用模式是基于本地文件系统，主master挂掉后需要手动切换）</p>
<h3><a id="_122"></a>主备切换</h3>
<blockquote>
<ul>
<li>
<p>spark standalone HA 模式中，master(active) 会将元数据同步到 zookeeper 中，元数据中有 worker，driver 和 application 的信息</p>
</li>
<li>
<p>当  master(active) 挂掉时，zookeeper 会选举出一个 master(standby)，被选中的 master(standby) 进入恢复状态 master(recovering)</p>
</li>
<li>
<p>master(recovering) 从 zookeeper 读取元数据，得到元数据后，master(recovering) 会向 worker 节点发送消息，告知主master已经更换</p>
</li>
<li>
<p>正在正常运行的 worker 在收到通知后，会向 master(recovering) 节点发送响应信息</p>
</li>
<li>
<p>master(recovering) 节点收到响应信息后，会调用自身的 completeRecovery()方法，此时未向 master(recovering) 节点发送响应信息的 worker 节点会被认为已经挂掉，从 workers 中删除（workers 是存储 worker 信息的对象）</p>
</li>
<li>
<p>短暂的 completeRecovery 状态一闪而过，master(completeRecovery) 成为 active 状态，开始对外提供服务</p>
</li>
</ul>
</blockquote>
<p>–主备切换的过程大概需要1~2分钟，此期间集群不接受提交任务的请求，但是已经跑在集群上的任务不会受到影响，会正常执行，这得益于 spark 粗粒度的资源调度<br>
–workers 使用 HashSet 数据结构来存储 worker 信息，是为了防止同一台 worker 节点在 master 中注册两次（worker 节点挂掉但是迅速恢复可能会导致此问题）</p>
<h3><a id="_139"></a>集群配置</h3>
<p>在上面 standalone 集群配置的基础上，先关闭之前开启的集群</p>
<pre><code> stop-spark.sh
</code></pre>
<p>在 node01 节点上对 spark 安装包中的 spark-env.sh文件添加如下配置，注意写在一行，不要手动换行，三个参数间使用空格隔开</p>
<pre><code> SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER
 -Dspark.deploy.zookeeper.url=node02:2181,node03:2181,node04:2181 
 -Dspark.deploy.zookeeper.dir=/spark"
</code></pre>
<p>看官网对配置信息的解释<br>
<img src="https://img-blog.csdnimg.cn/20181112191359862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="HA"></p>
<p>将 spark-env.sh发送到其他节点，进入 spark 安装包的 conf 目录下执行</p>
<pre><code>[root@node01 conf]# scp spark-env.sh node02:`pwd`
[root@node01 conf]# scp spark-env.sh node03:`pwd`
[root@node01 conf]# scp spark-env.sh node04:`pwd`
[root@node01 conf]# scp spark-env.sh client:`pwd`
</code></pre>
<p>这里我们选用 node02 节点作为备用的 master 节点，在 node02 节点的 spark 安装包中的 spark-env.sh文件中修改 SPARK_MASTER_IP，令 SPARK_MASTER_IP=node02</p>
<p>开启 zookeeper，在 node02, node03, node04 节点上手动启动</p>
<pre><code> zkServer.sh start
</code></pre>
<p>在 node01 节点上启动集群</p>
<pre><code> start-spark.sh
</code></pre>
<p>在 node02 节点上手动启动 master(standby)</p>
<pre><code> start-master.sh
</code></pre>
<p>可以在 node02:8888 WEBUI页面查看节点状态信息<br>
<img src="https://img-blog.csdnimg.cn/20181113083419838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="standby"></p>
<p>将 nodeo1 上的 master 进程 kill 掉，然后观察 master(standby) 的状态变化<br>
<img src="https://img-blog.csdnimg.cn/20181113091519451.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="Recovering"><br>
<img src="https://img-blog.csdnimg.cn/20181113091419758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="active"></p>
<p>在客户端提交 spark 提供的 SparkPi 程序</p>
<pre><code>spark-submit --master spark://node01:7077,node02:7077 
--class org.apache.spark.examples.SparkPi 
/opt/software/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 
100
</code></pre>
<p>在 WEBUI 页面查看任务<br>
<img src="https://img-blog.csdnimg.cn/20181113092137287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="job"></p>
<h2><a id="Spark_on_YARN_201"></a>Spark on YARN</h2>
<p>spark 应用程序跑在 yarn 集群上很简单，只需要在客户端有 spark 安装包就可以了(用来提交 spark 应用程序)</p>
<p>1&gt; 在客户端 spark 安装包中配置 spark-env.sh文件，添加如下配置信息（hadoop安装包配置文件的位置）</p>
<pre><code>HADOOP_CONF_DIR=/opt/zgl/hadoop-2.6.5/etc/hadoop
</code></pre>
<p>–spark会去这个位置寻找 hadoop 的配置文件，获取 yarn 集群的信息</p>
<p>2&gt; 开启 HDFS集群和 YARN集群，客户端提交的应用程序 jar 包会托管在 hdfs 上</p>
<blockquote>
<p>HDFS集群搭建可以参看这里 <a href="https://blog.csdn.net/xingyao231/article/details/83929060" rel="nofollow">Hadoop HDFS HA高可用的完全分布式</a><br>
YARN集群搭建可以参看这里 <a href="https://blog.csdn.net/xingyao231/article/details/83185643" rel="nofollow">Hadoop YARN</a></p>
</blockquote>
<p>3&gt; 在客户端提交 spark 提供的 SparkPi 程序，这里改用了服务器，内存很豪，所以参数适当的大了些</p>
<pre><code>spark-submit --master yarn
 --class org.apache.spark.examples.SparkPi 
 /opt/zgl/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 
 10000
</code></pre>
<p>4&gt; 在 yarn 的 WebUI 页面查看任务<br>
<img src="https://img-blog.csdnimg.cn/20181109154019968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="yarn"></p>
<p><strong>注意</strong>：提交任务可能会报下面这个异常，主要是因为虚拟内存超限， contrainer 被 kill，从而导致任务结束</p>
<blockquote>
<p>Yarn application has already ended! It might have been killed or unable to launch application master</p>
</blockquote>
<p>可以通过在 hadoop 安装包的 yarn-site.xml 文件中配置如下信息来解决</p>
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;description&gt;是否检查每个任务正使用的物理内存量，如果超过默认值则将其杀死，默认是true &lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;
&lt;/property&gt;
</code></pre>
<hr>
<p><em><strong>可上九天揽月，可下五洋捉鳖，谈笑凯歌还。世上无难事，只要肯攀登。</strong></em></p>
<hr>
<hr>
<h2><a id="Spark__252"></a>Spark 运行模式</h2>
<p>Spark 支持四种运行模式：</p>
<blockquote>
<p>Local		使用本地线程模拟，多用于测试<br>
Standalone		spark默认支持的<br>
YARN		最具前景<br>
Mesos</p>
</blockquote>
<h2><a id="Spark__260"></a>Spark 集群提交模式</h2>
<p>Spark 支持两种提交模式：</p>
<blockquote>
<p>client	该提交模式 driver 进程在客户端启动<br>
cluster	该提交模式 driver 进程在任意 worker 节点上启动</p>
</blockquote>
<h2><a id="Spark__266"></a>Spark 集群提交命令</h2>

<table>
<thead>
<tr>
<th>参数</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>- -master</td>
<td>指定 Job 提交的 Master 节点</td>
</tr>
<tr>
<td>- -class</td>
<td>指定运行 main 方法所在的类</td>
</tr>
<tr>
<td>- - deploy-mode</td>
<td>指定提交模式</td>
</tr>
<tr>
<td>- -executor-cores</td>
<td>指定每个 Executor 使用的 CPU 核数</td>
</tr>
<tr>
<td>- -executor-memory</td>
<td>指定每个 Executor 使用的内存</td>
</tr>
<tr>
<td>- -total-executor-cores</td>
<td>指定所有 Executor 一共使用的 CPU 核数</td>
</tr>
<tr>
<td>- -driver-cores</td>
<td>指定 driver 使用的 CPU 核数</td>
</tr>
<tr>
<td>- -driver-memory</td>
<td>指定 driver 使用的内存</td>
</tr>
<tr>
<td>- -driver-class-path</td>
<td>指定 driver 所依赖的 jar 包的位置</td>
</tr>
</tbody>
</table><h2><a id="Spark_standalone__280"></a>Spark standalone 集群</h2>
<ol>
<li>
<p>到官网下载 spark 的安装包，这里使用 spark-1.6.3，hadoop版本是 2.6.5<br>
<img src="https://img-blog.csdnimg.cn/20181106190805174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="spark download"></p>
</li>
<li>
<p>解压，改名为 spark-1.6.3</p>
</li>
<li>
<p>进入 conf 目录下，使用如下命令</p>
</li>
</ol>
<pre><code> cp slaves.template slaves
 cp spark-env.sh.template spark-env.sh
</code></pre>
<ol start="4">
<li>修改 slaves文件，在其中添加从节点</li>
</ol>
<pre><code>node02
node03
node04
</code></pre>
<ol start="5">
<li>修改 spark-env.sh文件</li>
</ol>
<pre><code># export JAVA_HOME=/opt/software/jdk1.8.0_151	# centOS 7 需要引入JAVA_HOME
SPARK_MASTER_IP=node01	master所在节点
SPARK_MASTER_PORT=7077	master资源通信端口
SPARK_WORKER_CORES=2	worker管理的核数
SPARK_WORKER_MEMORY=800m	worker管理的内存
SPARK_WORKER_INSTANCES=1	每个节点启动worker的个数
SPARK_WORKER_DIR=/var/zgl/spark	worker的工作目录
# SPARK_MASTER_WEBUI_PORT=8888	WebUI的端口号，默认为8080，与tomcat冲突
</code></pre>
<ol start="6">
<li>进入 sbin 目录，做如下修改，因为这两个命令与 hadoop 命令冲突</li>
</ol>
<pre><code>mv start-all.sh start-spark.sh 
mv stop-all.sh  stop-spark.sh  
</code></pre>
<ol start="7">
<li>将配置好的spark安装包发送到其他节点和客户端节点</li>
</ol>
<pre><code>[root@node01 zgl]# scp -r spark-1.6.3 node02:`pwd`
[root@node01 zgl]# scp -r spark-1.6.3 node03:`pwd`
[root@node01 zgl]# scp -r spark-1.6.3 node04:`pwd`
[root@node01 zgl]# scp -r spark-1.6.3 client:`pwd`
</code></pre>
<ol start="8">
<li>在 node01 和 client 节点上配置 spark 的环境变量，配置到 ~/.bashrc 中</li>
</ol>
<pre><code>vim ~/.bashrc

export SPARK_HOME=/opt/zgl/spark-1.6.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
</code></pre>
<ol start="9">
<li>在 node01 上启动集群</li>
</ol>
<pre><code>start-spark.sh
</code></pre>
<ol start="10">
<li>jps 检查集群是否启动</li>
</ol>
<pre><code>[root@node01 ~]# jps
1123 Master

[root@node02 ~]# jps
1124 Worker

[root@node03 ~]# jps
1125 Worker

[root@node04 ~]# jps
1129 Worker
</code></pre>
<ol start="11">
<li>在客户端提交 spark 提供的示例程序 SparkPi （该程序通过概率计算 π 的值）<br>
–注意提交命令一行写完，不要换行	<br>
–任务很可能会因为内存不足导致失败</li>
</ol>
<pre><code>spark-submit  master的url  执行器所使用的内存  
运行的程序的全类名  
程序jar包所在的位置  
提供一个参数（参数越大，计算的π值越精准）

spark-submit --master spark://node01:7077 --executor-memory 500m
 --class org.apache.spark.examples.SparkPi 
 /opt/zgl/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 
 50
</code></pre>
<ol start="12">
<li>可以在 node01:8080 WEBUI页面查看任务<br>
<img src="https://img-blog.csdnimg.cn/20181106201042514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="WEB UI"></li>
</ol>
<h2><a id="Spark_standalone_HA_371"></a>Spark standalone HA</h2>
<p>同 HDFS 的 NameNode，YARN 的 ResourceManager 一样，spark standalone 的 master 也存在单点故障问题，于是同样也就有了基于 zookeeper 监控的高可用模式（spark standalone 还有一种高可用模式是基于本地文件系统，主master挂掉后需要手动切换）</p>
<h3><a id="_374"></a>主备切换</h3>
<blockquote>
<ul>
<li>
<p>spark standalone HA 模式中，master(active) 会将元数据同步到 zookeeper 中，元数据中有 worker，driver 和 application 的信息</p>
</li>
<li>
<p>当  master(active) 挂掉时，zookeeper 会选举出一个 master(standby)，被选中的 master(standby) 进入恢复状态 master(recovering)</p>
</li>
<li>
<p>master(recovering) 从 zookeeper 读取元数据，得到元数据后，master(recovering) 会向 worker 节点发送消息，告知主master已经更换</p>
</li>
<li>
<p>正在正常运行的 worker 在收到通知后，会向 master(recovering) 节点发送响应信息</p>
</li>
<li>
<p>master(recovering) 节点收到响应信息后，会调用自身的 completeRecovery()方法，此时未向 master(recovering) 节点发送响应信息的 worker 节点会被认为已经挂掉，从 workers 中删除（workers 是存储 worker 信息的对象）</p>
</li>
<li>
<p>短暂的 completeRecovery 状态一闪而过，master(completeRecovery) 成为 active 状态，开始对外提供服务</p>
</li>
</ul>
</blockquote>
<p>–主备切换的过程大概需要1~2分钟，此期间集群不接受提交任务的请求，但是已经跑在集群上的任务不会受到影响，会正常执行，这得益于 spark 粗粒度的资源调度<br>
–workers 使用 HashSet 数据结构来存储 worker 信息，是为了防止同一台 worker 节点在 master 中注册两次（worker 节点挂掉但是迅速恢复可能会导致此问题）</p>
<h3><a id="_391"></a>集群配置</h3>
<p>在上面 standalone 集群配置的基础上，先关闭之前开启的集群</p>
<pre><code> stop-spark.sh
</code></pre>
<p>在 node01 节点上对 spark 安装包中的 spark-env.sh文件添加如下配置，注意写在一行，不要手动换行，三个参数间使用空格隔开</p>
<pre><code> SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER
 -Dspark.deploy.zookeeper.url=node02:2181,node03:2181,node04:2181 
 -Dspark.deploy.zookeeper.dir=/spark"
</code></pre>
<p>看官网对配置信息的解释<br>
<img src="https://img-blog.csdnimg.cn/20181112191359862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="HA"></p>
<p>将 spark-env.sh发送到其他节点，进入 spark 安装包的 conf 目录下执行</p>
<pre><code>[root@node01 conf]# scp spark-env.sh node02:`pwd`
[root@node01 conf]# scp spark-env.sh node03:`pwd`
[root@node01 conf]# scp spark-env.sh node04:`pwd`
[root@node01 conf]# scp spark-env.sh client:`pwd`
</code></pre>
<p>这里我们选用 node02 节点作为备用的 master 节点，在 node02 节点的 spark 安装包中的 spark-env.sh文件中修改 SPARK_MASTER_IP，令 SPARK_MASTER_IP=node02</p>
<p>开启 zookeeper，在 node02, node03, node04 节点上手动启动</p>
<pre><code> zkServer.sh start
</code></pre>
<p>在 node01 节点上启动集群</p>
<pre><code> start-spark.sh
</code></pre>
<p>在 node02 节点上手动启动 master(standby)</p>
<pre><code> start-master.sh
</code></pre>
<p>可以在 node02:8888 WEBUI页面查看节点状态信息<br>
<img src="https://img-blog.csdnimg.cn/20181113083419838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="standby"></p>
<p>将 nodeo1 上的 master 进程 kill 掉，然后观察 master(standby) 的状态变化<br>
<img src="https://img-blog.csdnimg.cn/20181113091519451.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="Recovering"><br>
<img src="https://img-blog.csdnimg.cn/20181113091419758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="active"></p>
<p>在客户端提交 spark 提供的 SparkPi 程序</p>
<pre><code>spark-submit --master spark://node01:7077,node02:7077 
--class org.apache.spark.examples.SparkPi 
/opt/software/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 
100
</code></pre>
<p>在 WEBUI 页面查看任务<br>
<img src="https://img-blog.csdnimg.cn/20181113092137287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="job"></p>
<h2><a id="Spark_on_YARN_453"></a>Spark on YARN</h2>
<p>spark 应用程序跑在 yarn 集群上很简单，只需要在客户端有 spark 安装包就可以了(用来提交 spark 应用程序)</p>
<p>1&gt; 在客户端 spark 安装包中配置 spark-env.sh文件，添加如下配置信息（hadoop安装包配置文件的位置）</p>
<pre><code>HADOOP_CONF_DIR=/opt/zgl/hadoop-2.6.5/etc/hadoop
</code></pre>
<p>–spark会去这个位置寻找 hadoop 的配置文件，获取 yarn 集群的信息</p>
<p>2&gt; 开启 HDFS集群和 YARN集群，客户端提交的应用程序 jar 包会托管在 hdfs 上</p>
<blockquote>
<p>HDFS集群搭建可以参看这里 <a href="https://blog.csdn.net/xingyao231/article/details/83929060" rel="nofollow">Hadoop HDFS HA高可用的完全分布式</a><br>
YARN集群搭建可以参看这里 <a href="https://blog.csdn.net/xingyao231/article/details/83185643" rel="nofollow">Hadoop YARN</a></p>
</blockquote>
<p>3&gt; 在客户端提交 spark 提供的 SparkPi 程序，这里改用了服务器，内存很豪，所以参数适当的大了些</p>
<pre><code>spark-submit --master yarn
 --class org.apache.spark.examples.SparkPi 
 /opt/zgl/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 
 10000
</code></pre>
<p>4&gt; 在 yarn 的 WebUI 页面查看任务<br>
<img src="https://img-blog.csdnimg.cn/20181109154019968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbmd5YW8yMzE=,size_16,color_FFFFFF,t_70" alt="yarn"></p>
<p><strong>注意</strong>：提交任务可能会报下面这个异常，主要是因为虚拟内存超限， contrainer 被 kill，从而导致任务结束</p>
<blockquote>
<p>Yarn application has already ended! It might have been killed or unable to launch application master</p>
</blockquote>
<p>可以通过在 hadoop 安装包的 yarn-site.xml 文件中配置如下信息来解决</p>
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;description&gt;是否检查每个任务正使用的物理内存量，如果超过默认值则将其杀死，默认是true &lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;
&lt;/property&gt;
</code></pre>
<hr>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>