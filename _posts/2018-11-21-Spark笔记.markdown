---
layout:     post
title:      Spark笔记
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/nightwish2018/article/details/59109045				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><div class="toc">
<ul>
<li><ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86" rel="nofollow">基础知识</a><ul>
<li><a href="#rdd%E5%8E%9F%E7%90%86" rel="nofollow">RDD原理</a></li>
<li><a href="#spark-%E5%9F%BA%E7%A1%80" rel="nofollow">Spark 基础</a></li>
</ul>
</li>
<li><a href="#spark-%E9%85%8D%E7%BD%AE" rel="nofollow">Spark 配置</a><ul>
<li><a href="#conf-%E5%92%8C-sc" rel="nofollow">conf 和 sc</a></li>
</ul>
</li>
<li><a href="#rdd%E6%93%8D%E4%BD%9C" rel="nofollow">RDD操作</a></li>
<li><a href="#%E5%87%BD%E6%95%B0" rel="nofollow">函数</a></li>
<li><a href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F" rel="nofollow">共享变量</a><ul>
<li><a href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F" rel="nofollow">广播变量</a></li>
<li><a href="#%E7%B4%AF%E5%8A%A0%E5%99%A8" rel="nofollow">累加器</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>




<h2 id="基础知识"><strong>基础知识</strong></h2>



<h3 id="rdd原理"><strong>RDD原理</strong></h3>

<p><a href="http://www-bcf.usc.edu/~minlanyu/teach/csci599-fall12/papers/nsdi_spark.pdf" rel="nofollow">[<strong>RDD原理详解</strong>]</a></p>



<h3 id="spark-基础"><strong>Spark 基础</strong></h3>

<p><a href="http://spark.apache.org/docs/1.6.1/programming-guide.html" rel="nofollow">[<strong>Spark基础教程</strong>]</a></p>



<h2 id="spark-配置"><strong>Spark 配置</strong></h2>



<h3 id="conf-和-sc">conf 和 sc</h3>

<p>开发Spark程序第一件事就是SparkConf对象，这个对象包含应用的一些信息，然后创建SparkContext，SparkContext可以让Spark知道如何访问集群</p>



<pre class="prettyprint"><code class=" hljs avrasm">conf = SparkConf()<span class="hljs-preprocessor">.setAppName</span>(<span class="hljs-string">"&lt;app name&gt;"</span>)
       <span class="hljs-preprocessor">.setMaster</span>(<span class="hljs-string">"&lt;集群地址&gt;"</span>)
sc = SparkContext(conf = conf)</code></pre>



<h2 id="rdd操作"><strong>RDD操作</strong></h2>

<p><a href="http://spark.apache.org/docs/latest/programming-guide.html#transformations" rel="nofollow">[RDD操作全集]</a></p>

<ol>
<li>转换 <br>
将已存在的数据集转换成新的数据集，例如map。转换是惰性的，不会立刻计算结果，仅仅记录转换操作应用的目标数据集，当动作需要一个结果时才计算。</li>
<li>动作 <br>
数据集计算后返回一个值给驱动程序，例如reduce</li>
</ol>



<pre class="prettyprint"><code class=" hljs fsharp"><span class="hljs-comment">// 从protocols文件创建RDD</span>
<span class="hljs-keyword">val</span> distFile = sc.textFile(<span class="hljs-string">"/etc/protocols"</span>)

<span class="hljs-comment">// Map操作，每行的长度</span>
<span class="hljs-keyword">val</span> lineLengths = distFile.map(s =&gt; s.length)

<span class="hljs-comment">// Reduce操作，获得所有行长度的和，即文件总长度，这里才会执行map运算</span>
<span class="hljs-keyword">val</span> totalLength = lineLengths.reduce((a, b) =&gt; a + b)

<span class="hljs-comment">// 可以将转换后的RDD保存到集群内存中</span>
lineLengths.persist()</code></pre>



<h2 id="函数"><strong>函数</strong></h2>



<pre class="prettyprint"><code class=" hljs python">//创建一个单例对象MyFunctions
object MyFunctions {
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func1</span><span class="hljs-params">(s: String)</span>:</span> Int = {s.split(<span class="hljs-string">" "</span>).length}
}

//将函数传入集群运行
val lineLengths = sc.textFile(<span class="hljs-string">"/etc/protocols"</span>).map(MyFunctions.func1)</code></pre>



<h2 id="共享变量"><strong>共享变量</strong></h2>



<h3 id="广播变量"><strong>广播变量</strong></h3>

<p>广播变量可以在每个机器上缓存一个只读的变量，可以通过sc.broadcast(v)方法创建。广播变量能够更高效的进行数据集的分配。</p>

<h3 id="累加器"><strong>累加器</strong></h3>



<pre class="prettyprint"><code class=" hljs php"><span class="hljs-comment">// 创建累加器</span>
val accum = sc.accumulator(<span class="hljs-number">0</span>, <span class="hljs-string">"My Accumulator"</span>)

<span class="hljs-comment">// 对RDD数据集的数值进行累加</span>
sc.parallelize(<span class="hljs-keyword">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)).<span class="hljs-keyword">foreach</span>(x =&gt; accum += x)

<span class="hljs-comment">// 查看累加器的结果</span>
accum.value</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>