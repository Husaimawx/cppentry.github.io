---
layout:     post
title:      Spark2.2.1源码编译企业版
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/tian_qing_lei/article/details/79024162				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
Spark2.2.1源码编译<br>
1、概述<br>
Spark和Haoop的官网提供了写Haoop和Spark的安装版本，但是在生产中，很大部分是CDH和HDP，官方提供的Hadoop和Spark，经常会对源码进行修改，根据本人在企业中做大数据的经验，大数据开发的人员应该会根据Haddop的版本编译出对应的Spark安装包，集成CDH<br><br>
2、准备<br>
参考Spark的官方文档<br>
根据Spark官方文档编译模块的介绍（http://spark.apache.org/docs/2.1.1/building-spark.html）的介绍：<br>
3、环境准备<br>
环境：centos/ubuntu<br><br>
软件准备：spark-2.2.1.tgz源码<br>
                   jdk-8u144-linux-x64.tar.gz   JDK1.8以上  <br>
                   apache-maven-3.3.9-bin.tar.gz<br>
                   scala-2.11.8.tgz<br>
                   hadoop-2.6.0-cdh5.12.0.tar.gz<br><br>
   1.安装jdk<br>
            tar -xf jdk-8u144-linux-x64.tar.gz -C  ~/soft/<br>
        ln -s   dk-8u144-linux-x64.tar.gz jdk<br><br><br>
            vi  /etc/profile       <br>
             export  JAVA_HOME=~/soft/jdk<br><br>
             export  PATH=$JAVA_HOME/bin:$PATH<br>
             source /etc/profile<br>
             java -version    <br>
   2.安装maven<br>
            tar -xf apache-maven-3.3.9-bin.tar.gz -C ~/soft/<br>
            ln -s  apache-maven-3.3.9-bin.tar.gz maven<br>
            nano /etc/profile<br>
            export MAVEN_HOME =~/soft/maven(注意配置环境变量时，=与后面的路径之间不能有空格)<br>
            export PATH=$MAVEN_HOME/bin:$PATH<br>
            source /etc/profile<br>
            mvn -version<br>
            建议修改maven本地仓库的地址：<br>
            cd /usr/local/spark-test/app/apache-maven-3.3.9/conf<br>
            vi  settings.xml<br>
            添加一下   &lt;localRepository&gt;/home/tianlei/soft/m2/repository&lt;/localRepository&gt;<br>
            <br>
   3.安装scala<br>
            tar -xf scala-2.11.8.tgz  -C ~/soft/<br>
            ln -s  scala-2.11.8.tgz scala<br>
            vi /etc/profile<br>
             export SCALA_HOME=~/soft/scala<br>
             export PATH=$SCALA_HOME/bin:$PATH<br>
            source /etc/profile<br>
            scala<br>
    4.建议安装下git<br>
            yum -y install git<br>
    5.spark源码编译,按照官网来(编译spark要内存够大，1g根本后面会出现各种问题)<br>
                vi /etc/profile<br>
        export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"<br>
官网中有这句./build/mvn<br>
 -DskipTests clean package 这是使用的自带的maven，一般我们都是自己下载maven，这句话的意思是跳过测试，打包，一般这个编译出来的hadoop版本是默认的。究竟默认的版本是什么，我们可以在github上面查看https://github.com/apache/spark<br>
找到pom.xml<br>
            查看下pom.xml文件，当中有profile字眼的，就是设置版本，如：<br>
    <br>
       <br><br>
                    &lt;profile&gt;      <br>
      <br>
&lt;id&gt;hadoop-2.7&lt;/id&gt;<br>
      <br>
&lt;properties&gt;<br>
      <br>
&lt;hadoop.version&gt;2.7.3&lt;/hadoop.version&gt;<br>
      <br>
&lt;curator.version&gt;2.7.1&lt;/curator.version&gt;<br>
      <br>
&lt;/properties&gt;<br>
      <br>
&lt;/profile&gt;<br>
      <br>
 <br>
      <br>
&lt;profile&gt;<br>
      <br>
&lt;id&gt;yarn&lt;/id&gt;<br>
      <br>
&lt;modules&gt;<br>
      <br>
&lt;module&gt;resource-managers/yarn&lt;/module&gt;<br>
      <br>
&lt;module&gt;common/network-yarn&lt;/module&gt;<br>
      <br>
&lt;/modules&gt;<br>
      <br>
&lt;/profile&gt;<br>
       <br><br><br><br>
# Apache Hadoop 2.7.X and later（指定hadoop版本）<br>
mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package<br><br>
# With Hive 1.2.1 support（指定hive的版本）<br>
mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package<br><br>
指定scala版本<br>
./dev/change-scala-version.sh 2.10<br>
mvn -Pyarn -Dscala-2.10 -DskipTests clean package<br><br>
由于我们生产上的CDH的版本如下： hadoop-2.6.0-cdh5.12.0.tar.gz<br>
mvn -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.12.0 -DskipTests clean package  （采用这种方式编译出来的不是tar,gz的格式）<br>
./dev/make-distribution.sh --name 2.6.0-cdh5.12.0   --tgz   -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.12.0 -Phive -Phive-thriftserver  -Pyarn   （这种方法编译出来的就是tgz形式，但是速度不快）<br>
要想速度快，修改/dev/make-distribution.sh里面的脚本<br>
1.将VERSION ，SCALA_VERSION ，SPARK_HADOOP_VERSION ，SPARK_HIVE 注释掉，直接写上自己的版本<br>
#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null | grep -v "INFO" | tail -n 1)        指的是spark2.2.0这个版本<br>
#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\    指的是scala 2.11<br>
#    | grep -v "INFO"\<br>
#    | tail -n 1)<br>
#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\  指的是hadoop.version=2.6.0-cdh5.12.0<br>
#    | grep -v "INFO"\<br>
#    | tail -n 1)<br>
#SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\    SPARK_HIVE为1表示支持<br>
#    | grep -v "INFO"\<br>
#    | fgrep --count "&lt;id&gt;hive&lt;/id&gt;";\<br>
#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\<br>
#    # because we use "set -o pipefail"<br>
#    echo -n)<br>
将以下的内容贴在注释掉的那个脚本的后面即可<br>
VERSION=2.2.1<br>
SCALA_VERSION=2.11   大版本<br>
SPARK_HADOOP_VERSION=2.6.0-cdh5.12.0<br>
SPARK_HIVE=1<br>
然后执行<br>
./dev/make-distribution.sh --name 2.6.0-cdh5.12.0   --tgz   -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.7.0 -Phive -Phive-thriftserver  -Pyarn    在spark的主目录执行<br><br>
执行时会遇到各种问题：<br>
问题1：[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.2.0: Failure to find org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 in https://repo1.maven.org/maven2
 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -&gt; [Help 1]<br>
          这是因为默认的是apache的仓库，但是我们hadoop的版本写的是CDH，这时要将CDH的仓库配进来，打开spark目录下的pom.xml文件，将CDH的仓库配进去<br>
          nano  ~/source/spark-2.2.1/pom.xml     添加如下   <br>
               &lt;repository&gt;<br>
                &lt;id&gt;cloudera&lt;/id&gt;<br>
                &lt;name&gt;cloudera Repository&lt;/name&gt;<br>
                &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;<br>
               &lt;/repository&gt;<br>
这个下面就有要的jar包，没事可以看下  https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hadoop/hadoop-client/2.6.0-cdh5.12.0/<br><br>
然后继续执行    ./dev/make-distribution.sh --name 2.6.0-cdh5.12.0   --tgz   -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.12.0 -Phive -Phive-thriftserver  -Pyarn<br>
等待编译完成<br><br>
继续研读make-distribution.sh这个脚本<br>
if [ "$MAKE_TGZ" == "true" ]; then<br>
TARDIR_NAME=spark-$VERSION-bin-$NAME #打包的文件名spark-2.2.1-bin-2.6.0-cdh5.7.0                                                                                    <br>
TARDIR="$SPARK_HOME/$TARDIR_NAME"     <br>
rm -rf "$TARDIR"<br>
cp -r "$DISTDIR" "$TARDIR"<br>
tar czf "spark-$VERSION-bin-$NAME.tgz" -C "$SPARK_HOME" "$TARDIR_NAME"<br>
rm -rf "$TARDIR"<br>
fi<br>
这个编译出来的文件，放在spark主目录下，且是<br>
  spark-2.2.1-bin-2.6.0-cdh5.12.0.tgz   <br><br>
将编译好的spark-2.2.1-bin-2.6.0-cdh5.12.0.tgz这个包进行解压，看到解压后的目录中有这些目录:<br>
drwxrwxr-x 11 tianlei tianlei  4096 1月  10 15:12 ./<br>
drwxrwxr-x 14 tianlei tianlei  4096 1月  10 15:16 ../<br>
drwxrwxr-x  2 tianlei tianlei  4096 1月  10 15:12 bin/<br>
drwxrwxr-x  2 tianlei tianlei  4096 1月  10 15:12 conf/<br>
drwxrwxr-x  5 tianlei tianlei  4096 1月  10 15:12 data/<br>
drwxrwxr-x  4 tianlei tianlei  4096 1月  10 15:12 examples/<br>
drwxrwxr-x  2 tianlei tianlei 16384 1月  10 15:12 jars/<br>
-rw-rw-r--  1 tianlei tianlei 17881 1月  10 15:12 LICENSE<br>
drwxrwxr-x  2 tianlei tianlei  4096 1月  10 15:12 licenses/<br>
-rw-rw-r--  1 tianlei tianlei 24645 1月  10 15:12 NOTICE<br>
drwxrwxr-x  6 tianlei tianlei  4096 1月  10 15:12 python/<br>
-rw-rw-r--  1 tianlei tianlei  3809 1月  10 15:12 README.md<br>
-rw-rw-r--  1 tianlei tianlei   138 1月  10 15:12 RELEASE<br>
drwxrwxr-x  2 tianlei tianlei  4096 1月  10 15:12 sbin/<br>
drwxrwxr-x  2 tianlei tianlei  4096 1月  10 15:12 yarn/<br><br><br><br><br><br>
  <br>            </div>
                </div>