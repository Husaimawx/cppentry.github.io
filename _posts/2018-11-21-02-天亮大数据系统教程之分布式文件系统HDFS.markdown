---
layout:     post
title:      02-天亮大数据系统教程之分布式文件系统HDFS
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>目录 <br>
       1、分布式文件系统产生背景、特点、应用 <br>
       2、HDFS架构设计 <br>
       3、HDFS shell应用 <br>
       4、HDFS Java API应用 <br>
       5、HDFS特征说明 <br>
       6、HDFS项目练习 <br>
详情 <br>
1、分布式文件系统产生背景、特点、应用</p>

<pre><code>* 产生背景

    * 传统的本地（单机式）文件系统，在数据量增长过快、数据备份、数据安全性、操作与使用便捷性上存在严重不足。



* 特点

    * 扩展能力: 灵活支持不同层次的存储量级
    * 高可用性: 包含两层，一是整个文件系统的可用性，二是数据的完整和一致性。不能某一两个节点故障，导致系统不能正常使用。
    * 协议和接口: 提供给应用的接口多种多样，Http RestFul接口、NFS接口、Ftp等等POSIX标准协议，另外通常会有自己的专用接口。
    * 弹性存储: 根据业务需要灵活地增加或缩减数据存储以及增删存储池中的资源，而不需要中断系统运行。弹性存储的最大挑战是减小或增加资源时的数据震荡问题。
    * 压缩、加密、缓存和存储配额: 多样的系统管理功能



* 应用

    * 运营商
    * 中大型互联网公司，如BAT、京东、乐视、美团等
    * 金融银行保险类公司
    * 各大云平台底层存储平台 
    * 其它本地系统无法承载存储能力的应用      
</code></pre>

<p>2、HDFS架构设计 <br>
      2.1  HDFS是什么 <br>
       HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。 <br>
       由论文为GFS（Google File System）Google 文件系统启发后的Java实现。 <br>
     2.2  组成角色说明 <br>
        *   NameNode <br>
        * hdfs元数据管理者，管理namespace(文件系统命名空间)，NameNode维护文件系统命名空间。 <br>
        * namespace或其属性的任何更改都由NameNode记录 <br>
        *   DataNodes <br>
        * Datanode是文件系统的工作节点，他们根据管理指令(客户端或者是namenode)调度存储和检索数据。 <br>
        * 定期向namenode发送他们所存储的块(block)的列表。 <br>
        *   Client <br>
        * 客户端(client)代表用户与namenode和datanode交互来访问整个文件系统。 <br>
        * 开发人员面向client api来编程即可，对namenode、datanode可以无感。 <br>
     2.3  HDFS架构设计 <br>
<img src="https://img-blog.csdn.net/20171018223308673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>3、HDFS shell应用 <br>
           3.1 shell命令分类 <br>
<img src="https://img-blog.csdn.net/20171018223343528?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>3.2 hdfs dfs常用命令 <br>
    * hdfs dfs与hadoop fs对等，只是一个先后推荐使用的区别 <br>
    * 查看所有命令 <br>
                         hdfs dfs <br>
<img src="https://img-blog.csdn.net/20171018223419226?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
    * 查看目录下文件列表 <br>
                    查看hdfs根目录下的文件列表：hdfs dfs  -ls / <br>
<img src="https://img-blog.csdn.net/20171018223511646?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
    * 查看文本文件的内容 <br>
                    查看hdfs的某个文本文件： hdfs dfs -cat /tmp/index.html <br>
<img src="https://img-blog.csdn.net/20171018223551159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>4、HDFS Java API应用 <br>
         从hdfs文件/tmp/tianliangedu/index.html中读取其文本内容，并打印出来：</p>

<pre class="prettyprint"><code class=" hljs java"><span class="hljs-keyword">package</span> com.tianliangedu.utils;

<span class="hljs-keyword">import</span> java.io.BufferedReader;
<span class="hljs-keyword">import</span> java.io.ByteArrayOutputStream;
<span class="hljs-keyword">import</span> java.io.FileNotFoundException;
<span class="hljs-keyword">import</span> java.io.IOException;
<span class="hljs-keyword">import</span> java.io.InputStreamReader;
<span class="hljs-keyword">import</span> java.net.URI;
<span class="hljs-keyword">import</span> java.util.ArrayList;
<span class="hljs-keyword">import</span> java.util.List;

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.log4j.Logger;

<span class="hljs-javadoc">/**
 * hdfs 文件操作工具类
 *
 *<span class="hljs-javadoctag"> @author</span> zel
 *
 */</span>
<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HdfsFileOperatorUtil</span> {</span>
       Logger logger=Logger.getLogger(HdfsFileOperatorUtil.class);

       <span class="hljs-javadoc">/**
        * 从HDFS上指定的文件中读取指定的n级目录 ,作为map的输入文件
        *
        *<span class="hljs-javadoctag"> @throws</span> FileNotFoundException
        *<span class="hljs-javadoctag"> @throws</span> IOException
        */</span>
       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> List&lt;String&gt; <span class="hljs-title">getSubDirs</span>(String rootDir) {
             <span class="hljs-comment">// 存放最后返回的文件夹列表</span>
             List&lt;String&gt; fileList = <span class="hljs-keyword">new</span> ArrayList&lt;String&gt;();
             <span class="hljs-keyword">try</span> {
                    FileSystem fs = FileSystem.get(URI.create(rootDir),
                                 ConfigurationUtil.conf);
                    Path rootPath = <span class="hljs-keyword">new</span> Path(rootDir);

                    FileStatus fileStatus = fs.getFileStatus(rootPath);
                    <span class="hljs-comment">// 如果传入的是个文件则不再遍历之</span>
                    <span class="hljs-keyword">if</span> (!fileStatus.isDir()) {
                          fileList.add(fileStatus.getPath().toString());
                    } <span class="hljs-keyword">else</span> {
                          FileStatus[] fileStatusArray = fs.listStatus(rootPath);
                          <span class="hljs-keyword">for</span> (FileStatus fileState : fileStatusArray) {
                                 fileList.add(fileState.getPath().toString());
                          }
                    }
                    <span class="hljs-comment">// fs.close();</span>
             } <span class="hljs-keyword">catch</span> (Exception e) {
                    e.printStackTrace();
             }
             <span class="hljs-keyword">return</span> fileList;
       }

       <span class="hljs-javadoc">/** 从HDFS上读取文件 */</span>
       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> String <span class="hljs-title">readFromFile</span>(String srcFile) <span class="hljs-keyword">throws</span> Exception {
             <span class="hljs-keyword">if</span> (StringOperatorUtil.isBlank(srcFile)) {
                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception(<span class="hljs-string">"所要读取的源文件"</span> + srcFile + <span class="hljs-string">",不存在，请检查!"</span>);
             }
             <span class="hljs-keyword">byte</span>[] byteArray = readFromFileToByteArray(srcFile);
             <span class="hljs-keyword">if</span> (byteArray == <span class="hljs-keyword">null</span> || byteArray.length == <span class="hljs-number">0</span>) {
                    <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;
             }
             <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> String(byteArray);
       }

       <span class="hljs-javadoc">/**
        * 将指定的文件路径从hdfs读取并转换为byte array.
        *
        *<span class="hljs-javadoctag"> @param</span> srcFile
        *<span class="hljs-javadoctag"> @return</span>
        */</span>
       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">byte</span>[] <span class="hljs-title">readFromFileToByteArray</span>(String srcFile)
                    <span class="hljs-keyword">throws</span> Exception {
             <span class="hljs-keyword">if</span> (StringOperatorUtil.isBlank(srcFile)) {
                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception(<span class="hljs-string">"所要读取的源文件"</span> + srcFile + <span class="hljs-string">",不存在，请检查!"</span>);
             }
             FileSystem fs = FileSystem.get(<span class="hljs-keyword">new</span> Configuration());
             FSDataInputStream hdfsInStream = fs.open(<span class="hljs-keyword">new</span> Path(srcFile));

             <span class="hljs-keyword">byte</span>[] byteArray = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">65536</span>];
             <span class="hljs-comment">// 存放最后的所有字节数组</span>
             ByteArrayOutputStream bos = <span class="hljs-keyword">new</span> ByteArrayOutputStream();

             <span class="hljs-comment">// 实际读过来多少</span>
             <span class="hljs-keyword">int</span> readLen = <span class="hljs-number">0</span>;
             <span class="hljs-keyword">while</span> ((readLen = hdfsInStream.read(byteArray)) &gt; <span class="hljs-number">0</span>) {
                    bos.write(byteArray);
                    byteArray = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">65536</span>];
             }
             hdfsInStream.close();
             <span class="hljs-comment">// fs.close();</span>
             <span class="hljs-keyword">return</span> bos.toByteArray();
       }

       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">writeDataToPath</span>(String filePath, String data) {
             <span class="hljs-keyword">try</span> {
                    FileSystem fs = FileSystem.get(ConfigurationUtil.conf);
                    java.io.OutputStream out = fs.create(<span class="hljs-keyword">new</span> Path(filePath));
                    out.write(data.getBytes(StaticValue.default_encoding));
                    out.flush();
                    out.close();
             } <span class="hljs-keyword">catch</span> (Exception e) {
                    e.printStackTrace();
             }
       }

       <span class="hljs-javadoc">/**
        * 批量读取指定hdfs file中的行记录
        *
        *<span class="hljs-javadoctag"> @param</span> args
        *<span class="hljs-javadoctag"> @throws</span> Exception
        */</span>
       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> List&lt;String&gt; <span class="hljs-title">getHDFSLineArrayFromFile</span>(String path,
                    String encoding, <span class="hljs-keyword">long</span> begin_line_number, <span class="hljs-keyword">int</span> line_length) {
             <span class="hljs-keyword">long</span> end_line_number = begin_line_number + line_length;

             <span class="hljs-keyword">return</span> readHDFSFileToList(path, encoding, begin_line_number,
                          end_line_number);
       }

       <span class="hljs-comment">// 得到hdfs上的流对象</span>
       <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> FSDataInputStream <span class="hljs-title">getFSDataInputStream</span>(String srcFile)
                    <span class="hljs-keyword">throws</span> Exception {
             <span class="hljs-keyword">if</span> (StringOperatorUtil.isBlank(srcFile)) {
                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> Exception(<span class="hljs-string">"所要读取的源文件"</span> + srcFile + <span class="hljs-string">",不存在，请检查!"</span>);
             }
             FileSystem fs = FileSystem.get(ConfigurationUtil.conf);
             FSDataInputStream hdfsInStream = fs.open(<span class="hljs-keyword">new</span> Path(srcFile));

             <span class="hljs-keyword">return</span> hdfsInStream;
       }

       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> List&lt;String&gt; <span class="hljs-title">readHDFSFileToList</span>(String hdfsFilePath,
                    String fileEncoding, <span class="hljs-keyword">long</span> begin_line_number, <span class="hljs-keyword">long</span> end_line_number) {
             <span class="hljs-keyword">if</span> (fileEncoding == <span class="hljs-keyword">null</span>) {
                    fileEncoding = System.getProperty(<span class="hljs-string">"file.encoding"</span>);
             }
             FSDataInputStream fsDataInputStream = <span class="hljs-keyword">null</span>;
             <span class="hljs-keyword">try</span> {
                    fsDataInputStream = getFSDataInputStream(hdfsFilePath);
             } <span class="hljs-keyword">catch</span> (Exception e) {
                    e.printStackTrace();
                    <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;
             }
             BufferedReader br = <span class="hljs-keyword">null</span>;
             String line = <span class="hljs-keyword">null</span>;

             List&lt;String&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;String&gt;();
             <span class="hljs-keyword">try</span> {
                    br = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> InputStreamReader(fsDataInputStream,
                                 fileEncoding));
                    <span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;
                    <span class="hljs-comment">// 空读完前边的行</span>
                    <span class="hljs-keyword">while</span> (i &lt; begin_line_number &amp;&amp; (line = br.readLine()) != <span class="hljs-keyword">null</span>) {
                          i++;
                    }
                    <span class="hljs-keyword">while</span> ((line = br.readLine()) != <span class="hljs-keyword">null</span>
                                 &amp;&amp; begin_line_number &lt; end_line_number) {
                          <span class="hljs-comment">// 提换掉一些特殊字符</span>
                          line = line.replaceAll(<span class="hljs-string">"[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f]"</span>,
                                       <span class="hljs-string">""</span>);
                          <span class="hljs-comment">// 先做掉去空格</span>
                          line = line.trim();
                          list.add(line);
                          begin_line_number++;
                    }
             } <span class="hljs-keyword">catch</span> (Exception e) {
                    <span class="hljs-comment">// logger.info(e.getLocalizedMessage());</span>
                    e.printStackTrace();
             } <span class="hljs-keyword">finally</span> {
                    <span class="hljs-keyword">if</span> (br != <span class="hljs-keyword">null</span>) {
                          <span class="hljs-keyword">try</span> {
                                 br.close();
                          } <span class="hljs-keyword">catch</span> (IOException e) {
                                 <span class="hljs-comment">// logger.info(e.getLocalizedMessage());</span>
                                 <span class="hljs-comment">// logger.info("关闭IOUtil流时出现错误!");</span>
                                 e.printStackTrace();
                          }
                    }
             }
             <span class="hljs-keyword">return</span> list;
       }

       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args) <span class="hljs-keyword">throws</span> Exception {
             String hdfsFilePath=<span class="hljs-string">"/tmp/tianliangedu/index.html"</span>;
             String result=readFromFile(hdfsFilePath);
             System.out.println(result);
       }
}
</code></pre>

<p>5、HDFS特征说明 <br>
    * 流数据访问 <br>
        * 流处理即通过java io stream的方式操作，由于数据成块的连续存储，所以IO效率极高。  <br>
        * HDFS上运行的应用程序需要对其数据集进行流式访问， 不是通用于通用文件系统的通用应用程序 <br>
        * 多用于批处理，重在高吐吞量，不是用户交互和低延迟。 <br>
    * 大数据集存储 <br>
              大数据文件是分block块存储，故可以承受超大数据集的存储。 <br>
    * 简单一致性模型 <br>
          只支持对文件的一次性写入或是尾部追加写入，多次读取的模式。简化了数据一致性，实现了高吞吐量访问。 <br>
    * 移动计算比移动数据便宜 <br>
          处理数据的级别往往很大，把计算代码移到数据所在的节点上比把数据移动到计算的节点上的成本要低的多。 <br>
    * 硬件故障 <br>
          有了数据的冗余存储和丢失数据块补齐，个别机器坏掉引起的数据块丢失，不影响整体的数据完整性和计算任务的正常执行。 <br>
    * 数据流水线复制(副本为3为例) <br>
<img src="https://img-blog.csdn.net/20171018223727673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p><br><br><br><br> <br>
欢迎关注天亮教育公从号，大数据技术资料与课程、招生就业动态、教育资讯动态、创业历程分享一站式分享，官方微信公众号二维码： <br>
<img src="https://img-blog.csdn.net/20171017184016853?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZXJsaWFuZzIwMDg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title="">  <br>
更多学习讨论， 请加入 <br>
官方爬虫、nlp技术qq群320349384  <br>
天亮教育官方群318971238， <br>
hadoop &amp; spark &amp; hive技术群297585251，  <br>
官网：myhope365.com <br>
官方天亮论坛:<a href="http://bbs.yuqing36524.com/" rel="nofollow">http://bbs.yuqing36524.com/</a>  <br>
天亮教育视频链接：<a href="http://pan.baidu.com/s/1jIxI4IU" rel="nofollow">http://pan.baidu.com/s/1jIxI4IU</a> 密码：zqa7</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>