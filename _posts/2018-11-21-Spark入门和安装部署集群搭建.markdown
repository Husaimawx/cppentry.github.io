---
layout:     post
title:      Spark入门和安装部署集群搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>一、Spark的概述：</p><p>   spark是什么<br>     spark是基于内存的计算框架，计算速度非常快。如果想要对接外部的数据，比如HDFS读取数据，需要事先搭建一个   hadoop   集群。<br> 为什么要学习spark<br>    * 1、spark运行速度比mapreduce快很多<br>    * spark的job中间处理结果可以保存在内存中。mapreduce每次任务的结果输出都会保存在磁盘<br>    * mapreduce中，最终任务是进程中方式去计算任务。比如有100个map任务，1个reduce任务。至少会使用到101个进程。<br></p><p>    * spark中，最终任务是以线程的方式去计算任务，比如说有100个task,它就会在进程中运行100个线程。大大节省进程(资源)的开销，速度得到提升。</p><p>二、Spark的特点</p><p>        </p><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"> * spark是什么</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * spark是<span style="background-color:rgb(247,117,103);">基于内存的计算框架，计算速度非常快</span>。如果想要对接外部的数据，比如HDFS读取数据，需要事先搭建一个hadoop集群。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">*<span style="color:rgb(223,64,42);"> 为什么要学习spark</span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * 1、<span style="background-color:rgb(119,201,75);">spark运行速度比mapreduce快很多</span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    * <span style="background-color:rgb(250,226,32);">spark的job中间处理结果可以保存在内存中。mapreduce每次任务的结果输出都会保存在磁盘</span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    * <span style="background-color:rgb(250,226,32);">mapreduce中，最终任务是进程中方式去计算任务</span>。比如有100个map任务，1个reduce任务。至少会使用到101个进程。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    * spark中，最终任务是以线程的方式去计算任务，比如说有100个task,它就会在进程中运行100个线程。大大节省进程(资源)的开销，速度得到提升。</div><p>三、Spark集群的搭建</p><p>  </p><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 1、下载spark安装包http://spark.apache.org/downloads.html</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 2、上传安装包到服务中</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 3、规划安装目录</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * /export/servers</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 4、解压安装包到指定的安装目录</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * tar -zxvf   spark-2.0.2-bin-hadoop2.7.tgz -C /export/servers</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 5、重命名安装目录</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * mv  spark-2.0.2-bin-hadoop2.7 spark</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 6、修改配置文件</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * $SPARK_HOME/conf</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    * spark-env.sh.template</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    * mv spark-env.sh.template spark-env.sh</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">      * 配置java环境变量</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">        * export JAVA_HOME=/export/servers/jdk</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">      * 配置master的地址</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">        * export SPARK_MASTER_HOST=node1</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">      * 配置master的端口</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">        * export SPARK_MASTER_PORT=7077</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * mv slaves.template slaves</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    - 添加集群中的worker节点</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">      - node2</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">      - node3</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 7、修改环境变量</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * export SPARK_HOME=/export/servers/spark</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * export PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 8、分发spark安装目录到其他节点</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * scp -r spark root@node2:/export/service</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * scp -r spark root@node3:/export/service</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * scp /etc/profile root@node2:/etc</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * scp /etc/profile root@node3:/etc</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 9、刷新所有节点环境变量</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * 在所有节点执行</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">    * source /etc/profile</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 10、启动spark集群</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * $SPARK_HOME/sbin/start-all.sh</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 11、停止spark集群</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * $SPARK_HOME/sbin/stop-all.sh</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">* 12、spark web管理界面</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">  * http://master的地址:8080</div><p>四、基于zookeeper的Spark的高可用搭建</p><p>        * 1、搭建一个zookeeper集群<br>* 2、修改spark配置文件<br>  * vi spark-env.sh<br>    * 注释掉 export SPARK_MASTER_HOST=node1<br>    * 引入zk配置<br>    ```<br>    export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER  -Dspark.deploy.zookeeper.url=hdp-node-01:2181,node02:2181,node03:2181  -Dspark.deploy.zookeeper.dir=/spark"<br>    ```<br>* 3、启动spark HA<br>  * 1、启动zk<br>  * 2、可以在任意一台服务器上执行 start-all.sh<br>    * 会在当前机器上产生一个master进程<br>  * 3、在其他节点单独启动master进程(注意：需要实现当前节点到其他worker节点的ssh 免登陆)<br></p><p>    * start-master.sh</p><p>五、Spark角色介绍</p><p>        * 1、Driver<br>  * 就是运行客户端程序的main方法，会创建sparkContext<br>* 2、Application<br>  * 就是一个spark应用程序，包含driver以及运行任务所需要的所有资源<br>* 3、master<br>  * 在spark standalone模式下，用于对资源进行分配<br>* 4、Cluster manager<br>  * 为当前程序运行提供外部的资源<br>    * standalone<br>      * 由master提供资源分配<br>    * yarn<br>      * 由resourcemanager提供资源分配<br>    * mesos<br>      * 也是一个资源调度框架<br>* 5、worker<br>  * 就是运行任务的节点<br>* 6、executor<br>  * 是运行在worker节点上的进程，负责任务的计算<br>* 7、task<br></p><p>  * spark程序最终是以task线程的方式运行在executor进程中</p><p><br></p><p>六、检测是够安装成功</p><p>计算圆周率</p><p>* 普通模式提交<br>  ```<br>  bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master spark://node01:7077 \<br>  --executor-memory 1G \<br>  --total-executor-cores 2 \<br>  examples/jars/spark-examples_2.11-2.0.2.jar \<br>  10<br>  ```<br>* 高可用模式提交<br>```<br>bin/spark-submit \<br>--class org.apache.spark.examples.SparkPi \<br>--master spark://node01:7077,node02:7077,node03:7077 \<br>--executor-memory 1G \<br>--total-executor-cores 2 \<br>examples/jars/spark-examples_2.11-2.0.2.jar \<br>10<br>```<br></p><p>七、spark-shell</p><p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序<br><br><br>* 1、通过spark-shell  --master local[2] 读取本地数据文件做一个wordcount<br>  * --master local[2]<br>    * 表示本地运行程序，跟集群没有任何关系<br>    * local[N]这里的N，它是正整数，表示本地采用N个线程去运行<br>  * --master local[*]<br>    * local[*]: *表示使用当前机器上所有可用的资源去运行程序<br>* 2、通过spark-shell --master local[2]读取HDFS上数据文件做一个wordcount<br>  * 需要指定文件的路径为HDFS上文件路径<br>  * sc.textFile("hdfs://node1:9000/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(\_+\_).collect<br>  * spark整合HDFS<br>    * vi spark-env.sh<br>      * 添加<br>        * export HADOOP_CONF_DIR=/export/servers/hadoop/etc/hadoop<br>* 3、通过spark-shell --master spark://node1:7077 读取HDFS上数据文件做一个wordcount<br>  * spark-shell --master spark://node1:7077<br></p><p>  * sc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect</p><p>八使用IDEA编写WordCount案例</p><p>* 需要导入依赖<br>       &lt;dependency&gt;<br>              &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br>              &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br>              &lt;version&gt;2.0.2&lt;/version&gt;<br>      &lt;/dependency&gt;<br>      &lt;dependency&gt;<br>          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;<br>          &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;<br>          &lt;version&gt;2.7.4&lt;/version&gt;<br>      &lt;/dependency&gt;<br>  * 利用scala语言编写spark的wordcount程序(本地运行)<br>    package cn.itcast.wordcount<br>    import org.apache.spark.{SparkConf, SparkContext}<br>    import org.apache.spark.rdd.RDD<br>    //todo:需求：利用scala语言编写spark的wordcount程序<br>    object WordCount {<br>      def main(args: Array[String]): Unit = {<br>         //1、创建sparkConf对象 设置appName和master地址   local[2] 表示本地采用2个线程去运行<br>          val sparkConf: SparkConf = new SparkConf().setAppName("WordCount").setMaster("local[2]")<br>         //2、创建sparkContext对象<br>          val sc = new SparkContext(sparkConf)<br>          //设置日志输出级别<br>          sc.setLogLevel("WARN")<br>        //3、通过sparkContext对象加载数据文件<br>          val data: RDD[String] = sc.textFile("D:\\words.txt")<br>        //4、切分每一行<br>          val words: RDD[String] = data.flatMap(_.split(" "))<br>        //5、每个单词计为1<br>          val wordAndOne: RDD[(String, Int)] = words.map((_,1))<br>        //6、相同单词出现的次数累加<br>          val result: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)<br>        //7、收集打印<br>          println(result.sortBy(_._2,false).collect().toBuffer)<br>        //保存结果数据<br>          result.saveAsTextFile("d:\\out123")<br>        //8、关闭sc<br>        sc.stop()<br>      }<br>    }<br>  * ​    利用scala语言编写spark的wordcount程序(集群运行)<br>    package cn.itcast.wordcount<br>    import org.apache.spark.{SparkConf, SparkContext}<br>    import org.apache.spark.rdd.RDD<br>    //todo:利用scala编写spark wordcount程序打成jar提交到集群中运行<br>    object WordCount_Online {<br>      def main(args: Array[String]): Unit = {<br>        //1、创建sparkConf对象 设置appName<br>        val sparkConf: SparkConf = new SparkConf().setAppName("WordCount")<br>        //2、创建sparkContext对象<br>        val sc = new SparkContext(sparkConf)<br>        //设置日志输出级别<br>        sc.setLogLevel("WARN")<br>        //3、通过sparkContext对象加载数据文件<br>        val data: RDD[String] = sc.textFile(args(0))<br>        //4、切分每一行<br>        val words: RDD[String] = data.flatMap(_.split(" "))<br>        //5、每个单词计为1<br>        val wordAndOne: RDD[(String, Int)] = words.map((_,1))<br>        //6、相同单词出现的次数累加<br>        val result: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)<br>        //7、保存结果数据到HDFS上<br>        result.saveAsTextFile(args(1))<br>        //8、关闭sc<br>        sc.stop()<br>      }<br>    }<br>    提交任务脚本：<br>    spark-submit --master spark://node1:7077  --class cn.itcast.wordcount.WordCount_Online --executor-memory 1g --total-executor-cores 2 original-spark-class03-2.0.jar /words.txt /out2018<br>  *  利用java语言编写spark的wordcount程序(本地运行)<br>    package cn.itcast.wordcount;<br>    import org.apache.spark.SparkConf;<br>    import org.apache.spark.api.java.JavaPairRDD;<br>    import org.apache.spark.api.java.JavaRDD;<br>    import org.apache.spark.api.java.JavaSparkContext;<br>    import org.apache.spark.api.java.function.FlatMapFunction;<br>    import org.apache.spark.api.java.function.Function2;<br>    import org.apache.spark.api.java.function.PairFunction;<br>    import scala.Tuple2;<br>    import java.util.Arrays;<br>    import java.util.Iterator;<br>    import java.util.List;<br>    //todo:利用java语言实现spark wordcount程序<br>    public class WordCount_Java {<br>        public static void main(String[] args) {<br>             //1、创建sparkconf,设置appName和master地址<br>            SparkConf sparkConf = new SparkConf().setAppName("WordCount_Java").setMaster("local[2]");<br>            //2、构建javaSparkContext对象<br>            JavaSparkContext jsc = new JavaSparkContext(sparkConf);<br>            //3、读取数据文件<br>            JavaRDD&lt;String&gt; dataJavaRDD = jsc.textFile("d:\\words.txt");<br>            //4、切分每一行<br>            JavaRDD&lt;String&gt; wordsJavaRDD = dataJavaRDD.flatMap(new FlatMapFunction&lt;String, String&gt;() {<br>                public Iterator&lt;String&gt; call(String line) throws Exception {<br>                    String[] words = line.split(" ");<br>                    return Arrays.asList(words).iterator();<br>                }<br>            });<br>            //5、每个单词记为1<br>            JavaPairRDD&lt;String, Integer&gt; wordAndOneJavaPairRDD = wordsJavaRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {<br>                public Tuple2&lt;String, Integer&gt; call(String word) throws Exception {<br>                    return new Tuple2&lt;String, Integer&gt;(word, 1);<br>                }<br>            });<br>            //6、相同单词出现的次数累加<br>            JavaPairRDD&lt;String, Integer&gt; resultJavaPairRDD = wordAndOneJavaPairRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {<br>                public Integer call(Integer v1, Integer v2) throws Exception {<br>                    return v1 + v2;<br>                }<br>            });<br>            //按照单词出现的次数降序排列 resultJavaPairRDD 中(单词，次数) 位置颠倒（次数，单词）然后在使用sortByKey按照次数降序排列，最后位置复原<br>            JavaPairRDD&lt;String, Integer&gt; sortJavaPairRDD = resultJavaPairRDD.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() {<br>                public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; tuple) throws Exception {<br>                    return new Tuple2&lt;Integer, String&gt;(tuple._2, tuple._1);<br>                }<br>            }).sortByKey(false).mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() {<br>                public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; tuple) throws Exception {<br>                    return new Tuple2&lt;String, Integer&gt;(tuple._2, tuple._1);<br>                }<br>            });<br>            //7、打印输出<br>            List&lt;Tuple2&lt;String, Integer&gt;&gt; finalResult = sortJavaPairRDD.collect();<br>            for (Tuple2&lt;String, Integer&gt; tuple:finalResult){<br>                System.out.println("单词："+tuple._1+" 次数："+tuple._2);<br>            }<br>            //8、关闭jsc<br>            jsc.stop();<br>        }<br>    }<br><br></p>            </div>
                </div>