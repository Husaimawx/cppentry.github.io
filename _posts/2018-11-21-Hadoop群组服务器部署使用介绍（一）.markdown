---
layout:     post
title:      Hadoop群组服务器部署使用介绍（一）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/qq_33282992/article/details/78977873				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<span></span>Hadoop群组服务器部署使用介绍（一）<br>
<span> </span><br>
一、批量添加用户hadoop和设置密码，并为hadoop用户分配sudoer权限<br><span></span>[root@yumserver ~]# useradd hadoop<br><span></span>[root@yumserver .ssh]# echo 'hadoop' | passwd --stdin hadoop<br><span></span>[root@yumserver .ssh]# vim /etc/sudoers<br><br><br>
二、同步服务器时间。或可在内网架设时间服务器，具体另述。<br><span></span>1、安装ntp<br><span></span>[root@yumserver ~]# yum install -y ntp<br><span></span><br><span></span>2、同步时间<br><span></span>[root@yumserver ~]# ntpdate asia.pool.ntp.org<span>
</span><br><span></span>网上推荐的时间服务器如下。<br><span></span>time.nist.gov<br><span></span>time.nuri.net<br><span></span>asia.pool.ntp.org<br><span></span>asia.pool.ntp.org<br><span></span>asia.pool.ntp.org<br><span></span>asia.pool.ntp.org<br><span></span><br><span></span>3、设置定时同步计划。<br><span></span>[root@yumserver ~]# vim /etc/crontab<br><span></span>*/10 * * * * root ntpdate asia.pool.ntp.org<span>
</span><br><span></span>[root@yumserver ~]# /etc/init.d/crontab restart<span>
</span><br><br><br>
三、配置各主机名、内网主机名映射<br><span></span><br><span></span>[root@master ~]# vi /etc/sysconfig/network<br><span></span>NETWORKING=yes<br><span></span>HOSTNAME=master<br><span></span><br><span></span>批量修改/etc/hosts文件，配置内网主机名。hosts文件的作用相当如DNS，提供IP地址到hostname的对应。<br><br><br><span></span>cat &gt;&gt; /etc/hosts &lt;&lt; EOF<br><span></span>192.168.91.10 yumserver<br><span></span>192.168.91.11 master<br><span></span>192.168.91.12 master<br><span></span>192.168.91.21 slave1<br><span></span>192.168.91.22 slave2<br><span></span>192.168.91.23 slave3<br><span></span>EOF<span> </span><br><span></span><br>
四、关闭防火墙和SELinux<br><span></span>关闭防火墙<br><span></span>[root@master .ssh]# chkconfig iptables off<br><span></span>[root@master .ssh]# chkconfig iptables  --list<br><span></span><br><span></span>关闭SELinux<br><span></span># vi /etc/selinux/config<br><span></span>将SELINUX=enforcing改为SELINUX=disabled<br><span></span>[root@master .ssh]# setenforce 0<br><br><br><span></span><br>
五、配置hadoop主机ssh免密登陆其它<span> </span><br><span></span>切换到hadoop用户<br><span></span>[root@master ~]# su - hadoop<br><span></span>[hadoop@master ~]$ ssh-keygen<br><span></span>将公钥拷贝到其它机器上<br><span></span>编写脚本文件<br><span></span>#!/bin/bash<br><br><br><span></span>SERVERS="yumserver master master2 slave1 slave2 slave3"<br><span></span>PASSWORD=hadoop<br><span></span>auto_ssh_copy_id() {<br><span></span>expect -c "set timeout -1;<br><span></span>spawn ssh-copy-id $1;<br><span></span>expect {<br><span></span>*(yes/no)* {send -- yes\r;exp_continue;}<br><span></span>*assword:* {send -- $2\r;exp_continue;}<br><span></span>eof        {exit 0;}<br><span></span>}";<br><span></span>}<br><span></span>ssh_copy_id_to_all() {<br><span></span>for SERVER in $SERVERS<br><span></span>do<br><span></span>auto_ssh_copy_id $SERVER $PASSWORD<br><span></span>done<br><span></span>}<br><br><br><span></span>ssh_copy_id_to_all<br><span></span><br><span></span>检查文件<br><span></span>[hadoop@master sh]$ cat /home/hadoop/.ssh/authorized_keys<br><span></span><br><br><br>
六、安装JDK<br><span></span>见 http://blog.csdn.net/qq_33282992/article/details/78909046<br><br><br>
七、Hadoopp安装部署<br><br><br><span></span>1、上传hadoop安装包到服务，并解压到自定义目录apps下<span>
</span><br><span></span>tar -zxvf hadoop-2.7.5.tar.gz -C ~/apps/<span>
</span><br><span></span><br><span></span>2、编辑系统配置文件<br><span></span>[hadoop@master hadoop-2.7.5]$ vi /etc/profile<br><span></span>export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5<br><span></span>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br><span></span><br><span></span>3、进入目录/home/hadoop/apps/hadoop-2.7.5/etc/hadoop，编辑相关配置文件<br><span></span>3.1 vi  hadoop-env.sh<span>
</span><br><span></span>确保配置文件中JAVA_HOME为jdk安装目录。<br><span></span>export JAVA_HOME=/usr/local/jdk1.7.0_45<br><span></span>3.2 vi core-site.xml 配置默认文件系统和进程临时数据或工作数据<br><span></span>&lt;configuration&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;fs.defaultFS&lt;/name&gt;<br><span></span>&lt;value&gt;hdfs://master:9000&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br><span></span>&lt;value&gt;/home/hadoop/hdpdata&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;/configuration&gt;<br><span></span>3.3 vi hdfs-site.xml 配置数据副本数量<br><span></span>&lt;configuration&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;dfs.replication&lt;/name&gt;<br><span></span>&lt;value&gt;3&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;/configuration&gt;<br><span></span><br><span></span>3.4 vi  mapred-site.xml<br><span></span>&lt;configuration&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br><span></span>&lt;value&gt;yarn&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;/configuration&gt;<br><span></span>3.5 vi yarn-site.xml<br><span></span>&lt;configuration&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br><span></span>&lt;value&gt;master&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br><span></span>&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;/configuration&gt;<br><span></span>3.6 vi  slaves<br><span></span>slave1<br><span></span>slave2<br><span></span>slave3<br><span></span>4、将配置好的hadoop软件拷贝到其它服务器<br><span></span>[hadoop@master ~]$ scp -r apps  slave1:/home/hadoop/<br><span></span>[hadoop@master ~]$ scp -r apps  slave2:/home/hadoop/<br><span></span>[hadoop@master ~]$ scp -r apps  slave3:/home/hadoop/<br><br><br>
八、格式化文件系统并启动服务<br><span></span>1、格式化namenode<br><span></span>[hadoop@master ~]$ hdfs namenode -format<br><span></span>......<br><span></span>18/01/03 17:29:35 INFO common.Storage: Storage directory /home/hadoop/hdpdata/dfs/name has been successfully formatted.<br><span></span>18/01/03 17:29:35 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hdpdata/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression<br><span></span>......<br><span></span>表示成功<br><span></span><br><span></span>2、启动hadoop<br><span></span>先启动HDFS<br><span></span>[hadoop@master sbin]$ ./start-dfs.sh<br><span></span><br><span></span>再启动YARN<br><span></span>[hadoop@master sbin]$ ./start-yarn.sh<br><span></span><br><span></span>3 、验证是否启动成功<br><span></span>在主机namenode上使用jps命令验证<br><span></span>[hadoop@master .ssh]$ jps<br><span></span>5766 NameNode<br><span></span>12816 ResourceManager<br><span></span>12892 Jps<br><span></span>5957 SecondaryNameNode<br><br><br><span></span><br><span></span>在从机datanode上使用jps命令验证<br><span></span>[hadoop@slave1 ~]$ jps<br><span></span>9972 Jps<br><span></span>9845 NodeManager<br><span></span>4664 DataNode<br><span></span><br><span></span>或打开<br><span></span>http://master:50070 （HDFS管理界面）<br><span></span>http://master:8088 （MR管理界面）<br><br><br><span></span>一切顺利,其它再述!<br><span></span><br><br><br><br><br><br>            </div>
                </div>