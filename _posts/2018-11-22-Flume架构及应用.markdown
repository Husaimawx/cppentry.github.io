---
layout:     post
title:      Flume架构及应用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/bocai8058/article/details/81814523				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-variable">@Author</span>  <span class="hljs-symbol">:</span> <span class="hljs-constant">Spinach</span> | <span class="hljs-constant">GHB</span>
<span class="hljs-variable">@Link</span>    <span class="hljs-symbol">:</span> <span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/blog.csdn.net/bocai</span>8058</code></pre>

<hr>

<p></p><div class="toc"><div class="toc">
<ul>
<li><ul>
<li><ul>
<li><a href="#flume%E7%AE%80%E4%BB%8B" rel="nofollow">Flume简介</a></li>
<li><a href="#flume%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5" rel="nofollow">Flume基本概念</a></li>
<li><a href="#flume%E6%9E%B6%E6%9E%84" rel="nofollow">Flume架构</a></li>
<li><a href="#flume%E5%B9%BF%E4%B9%89%E7%94%A8%E6%B3%95" rel="nofollow">Flume广义用法</a></li>
<li><a href="#flume%E5%BA%94%E7%94%A8%E9%85%8D%E7%BD%AE" rel="nofollow">Flume应用配置</a><ul>
<li><ul>
<li><a href="#an-agent-flow%E9%85%8D%E7%BD%AE" rel="nofollow">an agent flow配置</a></li>
<li><a href="#multi-agent-flow%E9%85%8D%E7%BD%AE" rel="nofollow">multi-agent flow配置</a></li>
<li><a href="#fan-out-flow%E9%85%8D%E7%BD%AE" rel="nofollow">fan out flow配置</a></li>
<li><a href="#%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE" rel="nofollow">通用配置</a></li>
<li><a href="#%E8%BF%90%E8%A1%8C%E5%91%BD%E4%BB%A4flume-ng" rel="nofollow">运行命令(flume-ng)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%85%B3%E4%BA%8Eflume%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E7%9A%84%E5%87%A0%E4%B8%AA%E9%97%AE%E9%A2%98" rel="nofollow">关于Flume核心组件的几个问题</a><ul>
<li><ul>
<li><a href="#source" rel="nofollow">Source</a></li>
<li><a href="#channel" rel="nofollow">Channel</a></li>
<li><a href="#sink" rel="nofollow">Sink</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#flume%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF" rel="nofollow">Flume适用场景</a></li>
<li><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>




<h3 id="flume简介">Flume简介</h3>

<blockquote>
  <p>Flume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。后于2009年被捐赠了apache软件基金会，为hadoop相关组件之一。尤其近几年随着flume的不断被完善以及升级版本的逐一推出，特别是Flume NG（next generation）；同时Flume内部的各种组件不断丰富，用户在开发的过程中使用的便利性得到很大的改善，现已成为apache top项目之一。</p>
</blockquote>

<p>apache Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力。</p>



<h3 id="flume基本概念">Flume基本概念</h3>

<ul>
<li>Flume：是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方。</li>
</ul>

<p></p><center> <br>
<img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/flume%E6%B5%81%E7%A4%BA%E6%84%8F%E5%9B%BE.png" width="60%"> <br>
</center><p></p>

<ul>
<li>Even：包括event headers、event body、event信息（即文本文件中的单行记录），其中event信息就是flume收集到的日记记录。 </li>
</ul>



<pre class="prettyprint"><code class=" hljs mel">在整个数据的传输的过程中，流动的是<span class="hljs-keyword">event</span>，即事务保证是在<span class="hljs-keyword">event</span>级别进行的。那么什么是<span class="hljs-keyword">event</span>呢？
<span class="hljs-number">1.</span> <span class="hljs-keyword">event</span>将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，<span class="hljs-keyword">event</span>也是事务的基本单位。
<span class="hljs-number">2.</span> <span class="hljs-keyword">event</span>从<span class="hljs-keyword">source</span>，流向channel，再到sink，本身为一个字节数组，并可携带headers(头信息)信息。
<span class="hljs-number">3.</span> <span class="hljs-keyword">event</span>代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 </code></pre>

<p></p><center> <br>
<img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/event%E6%B5%81%E5%8A%A8%E7%A4%BA%E6%84%8F%E5%9B%BE.png" width="60%"> <br>
</center><p></p>



<h3 id="flume架构">Flume架构</h3>

<p>Flume的核心是agent，把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume再删除自己缓存的数据。</p>

<blockquote>
  <p>Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是source、 channel、 sink。</p>
</blockquote>

<ul>
<li><strong>sources 数据源</strong>：数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入一个或多个Channel中。Flume提供了很多内置的Source，如下表。</li>
<li><strong>channels 数据通道</strong>：一种短暂的存储容器,负责数据的存储持久化，可以持久化到jdbc,file,memory。Flume提供了很多内置的Channels，如下表。 <br>
<ul><li>将从source处接收到的event格式的数据缓存起来,直到它们被sinks消费掉，</li>
<li>可以把channel看成是一个队列，队列的优点是先进先出，放好后尾部一个个event出来，Flume比较看重数据的传输，因此几乎没有数据的解析预处理。</li>
<li>仅仅是数据的产生，封装成event然后传输。</li>
<li>数据只有存储在下一个存储位置（可能是最终的存储位置，如HDFS；也可能是下一个Flume节点的channel），数据才会从当前的channel中删除。</li>
<li>这个过程是通过事务来控制的，这样就保证了数据的可靠性。</li></ul></li>
</ul>

<blockquote>
  <p>备注：不过flume的持久化也是有容量限制的，比如内存如果超过一定的量，不够分配，也一样会爆掉。</p>
</blockquote>

<ul>
<li><strong>sinks 数据目的槽</strong>：Sink负责数据的转发，将数据存储到集中存储器，比如Hbase和HDFS，它从channel消费数据(events)并将其传递给目标地，目的地也可以是其他agent的Source。Flume提供了很多内置的Sinks，如下表。</li>
</ul>

<table>
<thead>
<tr>
  <th>sources类型</th>
  <th>描述</th>
  <th>channels类型</th>
  <th>描述</th>
  <th>sinks类型</th>
  <th>描述</th>
</tr>
</thead>
<tbody><tr>
  <td>avro</td>
  <td>监听Avro端口并接收Avro Client的流数据</td>
  <td>memory</td>
  <td>Event数据存储在内存中</td>
  <td>avor</td>
  <td>数据被转换成Avro Event，然后发送到配置的RPC端口上</td>
</tr>
<tr>
  <td>thrift</td>
  <td>监听Thrift端口并接收Thrift Client的流数据</td>
  <td>jdbc</td>
  <td>Event数据存储在持久化存储中，当前Flume Channel内置支持Derby</td>
  <td>thrift</td>
  <td>数据被转换成Thrift Event，然后发送到配置的RPC端口上</td>
</tr>
<tr>
  <td>exec</td>
  <td>基于Unix的command在标准输出上生产数据</td>
  <td>file</td>
  <td>Event数据存储在磁盘文件中</td>
  <td>hive</td>
  <td>数据导入到HIVE中</td>
</tr>
<tr>
  <td>jms</td>
  <td>从JMS（Java消息服务）采集数据</td>
  <td>SPILLABLEMEMORY</td>
  <td>Event数据存储在内存中和磁盘上，当内存队列满了，会持久化到磁盘文件（当前试验性的，不建议生产环境使用）</td>
  <td>logger</td>
  <td>数据写入日志文件</td>
</tr>
<tr>
  <td>spooldir</td>
  <td>监听指定目录</td>
  <td>PseudoTxnMemoryChannel</td>
  <td>测试用途</td>
  <td>hdfs</td>
  <td>数据写入HDFS</td>
</tr>
<tr>
  <td>KafkaSource</td>
  <td>采集Kafka topic中的message</td>
  <td>KafkaChannel</td>
  <td>Event存储在kafka集群</td>
  <td>KafkaSink</td>
  <td>数据写到Kafka Topic</td>
</tr>
<tr>
  <td>netcat</td>
  <td>监听端口（要求所提供的数据是换行符分隔的文本）</td>
  <td>Custom</td>
  <td>自定义Channel实现</td>
  <td>hbase</td>
  <td>数据写入HBase数据库</td>
</tr>
<tr>
  <td>seq</td>
  <td>序列产生器，连续不断产生event，用于测试</td>
  <td></td>
  <td></td>
  <td>null</td>
  <td>丢弃到所有数据</td>
</tr>
<tr>
  <td>syslogtcp、syslogudp、multiport_syslogtcp</td>
  <td>采集syslog日志消息，支持单端口TCP、多端口TCP和UDP日志采集</td>
  <td></td>
  <td></td>
  <td>DatasetSink</td>
  <td>写数据到Kite Dataset，试验性质的</td>
</tr>
<tr>
  <td>http</td>
  <td>接收HTTP POST和GET数据</td>
  <td></td>
  <td></td>
  <td>Custom</td>
  <td>自定义Sink实现</td>
</tr>
<tr>
  <td>其他</td>
  <td></td>
  <td></td>
  <td></td>
  <td>其他</td>
  <td></td>
</tr>
</tbody></table>


<ul>
<li><strong>channel selectors 通道选择器</strong>：设置从source到多个channel的过滤数据，默认是replicating。</li>
</ul>

<table>
<thead>
<tr>
  <th>sources类型</th>
  <th>描述</th>
</tr>
</thead>
<tbody><tr>
  <td>replicating</td>
  <td>默认管道选择器: 每一个管道传递的都是相同的events</td>
</tr>
<tr>
  <td>multiplexing</td>
  <td>多路复用通道选择器: 依据每一个event的头部header的地址选择管道</td>
</tr>
<tr>
  <td>Custom</td>
  <td>自定义selector实现</td>
</tr>
</tbody></table>


<p></p><center> <br>
<img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/%E9%80%9A%E9%81%93%E9%80%89%E6%8B%A9%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" width="70%"> <br>
</center><p></p>



<h3 id="flume广义用法">Flume广义用法</h3>

<ul>
<li><strong>an agent flow</strong>：简单一个agent流程。</li>
<li><strong>multi-agent flow</strong>：多个agent顺序连接。</li>
<li><strong>Consolidation</strong>：多个agent的数据汇聚到同一个agent。</li>
<li><strong>Multiplexing flow</strong>：多级流。</li>
</ul>

<p><img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/an%20agent%20flow.png" width="40%"><img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/multi-agent%20flow.png" width="60%"> <br>
<img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/consolidation.png" width="50%"><img src="https://raw.githubusercontent.com/GSpinach/MarkdownPictures/master/Hadoop%E8%A1%A5%E5%85%85%E4%B8%8E%E6%80%BB%E7%BB%93/multipleing%20flow.jpg" width="50%"></p>



<h3 id="flume应用配置">Flume应用配置</h3>



<h5 id="an-agent-flow配置">an agent flow配置</h5>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># example.conf: A single-node Flume configuration</span>

<span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span> = k1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = spooldir
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.spoolDir</span> = /home/ghb/HadoopCluster/<span class="hljs-keyword">Call</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.fileHeader</span> = true

<span class="hljs-preprocessor">#a1.sources.r1.type = exec </span>
<span class="hljs-preprocessor">#a1.sources.r1.command = tail -F /home/ghb/HadoopCluster/Call/CallLog.log</span>
<span class="hljs-preprocessor">#a1.sources.r1.fileHeader = true</span>
<span class="hljs-preprocessor">#a1.sources.r1.deserializer.outputCharset=UTF-8</span>

<span class="hljs-preprocessor"># Use a channel which buffers events in memory</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = memory
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.capacity</span> = <span class="hljs-number">1000</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.transactionCapacity</span> = <span class="hljs-number">100</span>

<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.flume</span><span class="hljs-preprocessor">.sink</span><span class="hljs-preprocessor">.kafka</span><span class="hljs-preprocessor">.KafkaSink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.kafka</span><span class="hljs-preprocessor">.topic</span> = CallLog
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.kafka</span><span class="hljs-preprocessor">.bootstrap</span><span class="hljs-preprocessor">.servers</span> =<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">6667</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.kafka</span><span class="hljs-preprocessor">.flumeBatchSize</span> = <span class="hljs-number">20</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.kafka</span><span class="hljs-preprocessor">.producer</span><span class="hljs-preprocessor">.acks</span>=<span class="hljs-number">1</span>

<span class="hljs-preprocessor">#a1.sinks.k1.type = hdfs</span>
<span class="hljs-preprocessor">#a1.sinks.k1.hdfs.path = hdfs://hadoop0:9000/CallLog</span>
<span class="hljs-preprocessor">#a1.sinks.k1.hdfs.filePrefix = log_%Y%m%d_%H</span>
<span class="hljs-preprocessor">#a1.sinks.k1.hdfs.writeFormat= Text</span>
<span class="hljs-preprocessor">#a1.sinks.k1.hdfs.fileType = CompressedStream</span>

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre>



<h5 id="multi-agent-flow配置">multi-agent flow配置</h5>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># Weblog.conf: A agent configuration</span>

<span class="hljs-preprocessor"># list sources, sinks and channels in the agent</span>
agent_foo<span class="hljs-preprocessor">.sources</span> = avro-AppSrv-source
agent_foo<span class="hljs-preprocessor">.sinks</span> = avro-forward-sink
agent_foo<span class="hljs-preprocessor">.channels</span> = file-channel

<span class="hljs-preprocessor"># define the flow</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source<span class="hljs-preprocessor">.channels</span> = file-channel
agent_foo<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.avro</span>-forward-sink<span class="hljs-preprocessor">.channel</span> = file-channel

<span class="hljs-preprocessor"># avro sink properties</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-forward-sink<span class="hljs-preprocessor">.type</span> = avro
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-forward-sink<span class="hljs-preprocessor">.hostname</span> = <span class="hljs-number">10.1</span><span class="hljs-number">.1</span><span class="hljs-number">.100</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-forward-sink<span class="hljs-preprocessor">.port</span> = <span class="hljs-number">10000</span>

<span class="hljs-preprocessor"># configure other pieces</span>
<span class="hljs-preprocessor">#...</span></code></pre>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># HDFS.conf: other agent configuration</span>

<span class="hljs-preprocessor"># list sources, sinks and channels in the agent</span>
agent_foo<span class="hljs-preprocessor">.sources</span> = avro-collection-source
agent_foo<span class="hljs-preprocessor">.sinks</span> = hdfs-sink
agent_foo<span class="hljs-preprocessor">.channels</span> = mem-channel

<span class="hljs-preprocessor"># define the flow</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-collection-source<span class="hljs-preprocessor">.channels</span> = mem-channel
agent_foo<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.hdfs</span>-sink<span class="hljs-preprocessor">.channel</span> = mem-channel

<span class="hljs-preprocessor"># avro sink properties</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-collection-source<span class="hljs-preprocessor">.type</span> = avro
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-collection-source<span class="hljs-preprocessor">.bind</span> = <span class="hljs-number">10.1</span><span class="hljs-number">.1</span><span class="hljs-number">.100</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-collection-source<span class="hljs-preprocessor">.port</span> = <span class="hljs-number">10000</span>

<span class="hljs-preprocessor"># configure other pieces</span>
<span class="hljs-preprocessor">#...</span></code></pre>



<h5 id="fan-out-flow配置">fan out flow配置</h5>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># fan_out_flow.conf: an agent configuration</span>

<span class="hljs-preprocessor"># list the sources, sinks and channels in the agent</span>
agent_foo<span class="hljs-preprocessor">.sources</span> = avro-AppSrv-source1
agent_foo<span class="hljs-preprocessor">.sinks</span> = hdfs-Cluster1-sink1 avro-forward-sink2
agent_foo<span class="hljs-preprocessor">.channels</span> = mem-channel-<span class="hljs-number">1</span> file-channel-<span class="hljs-number">2</span>

<span class="hljs-preprocessor"># set channels for source</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.channels</span> = mem-channel-<span class="hljs-number">1</span> file-channel-<span class="hljs-number">2</span>

<span class="hljs-preprocessor"># set channel for sinks</span>
agent_foo<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.hdfs</span>-Cluster1-sink1<span class="hljs-preprocessor">.channel</span> = mem-channel-<span class="hljs-number">1</span>
agent_foo<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.avro</span>-forward-sink2<span class="hljs-preprocessor">.channel</span> = file-channel-<span class="hljs-number">2</span>

<span class="hljs-preprocessor"># channel selector configuration</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.selector</span><span class="hljs-preprocessor">.type</span> = multiplexing
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.selector</span><span class="hljs-preprocessor">.header</span> = State
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.selector</span><span class="hljs-preprocessor">.mapping</span><span class="hljs-preprocessor">.CA</span> = mem-channel-<span class="hljs-number">1</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.selector</span><span class="hljs-preprocessor">.mapping</span><span class="hljs-preprocessor">.AZ</span> = file-channel-<span class="hljs-number">2</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.selector</span><span class="hljs-preprocessor">.mapping</span><span class="hljs-preprocessor">.NY</span> = mem-channel-<span class="hljs-number">1</span> file-channel-<span class="hljs-number">2</span>
agent_foo<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.avro</span>-AppSrv-source1<span class="hljs-preprocessor">.selector</span><span class="hljs-preprocessor">.default</span> = mem-channel-<span class="hljs-number">1</span>

<span class="hljs-preprocessor"># configure other pieces</span>
<span class="hljs-preprocessor">#...</span></code></pre>



<h5 id="通用配置">通用配置</h5>



<pre class="prettyprint"><code class=" hljs xml"># example1.conf: This is a generic configuration for the configuration file

# list the sources, sinks and channels for the agent
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.sources = <span class="hljs-tag">&lt;<span class="hljs-title">Source</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.sinks = <span class="hljs-tag">&lt;<span class="hljs-title">Sink</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.channels = <span class="hljs-tag">&lt;<span class="hljs-title">Channel1</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-title">Channel2</span>&gt;</span>

# properties for sources
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.sources.<span class="hljs-tag">&lt;<span class="hljs-title">Source</span>&gt;</span>.<span class="hljs-tag">&lt;<span class="hljs-title">someProperty</span>&gt;</span> = <span class="hljs-tag">&lt;<span class="hljs-title">someValue</span>&gt;</span>

# properties for sinks
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.sources.<span class="hljs-tag">&lt;<span class="hljs-title">Sink</span>&gt;</span>.<span class="hljs-tag">&lt;<span class="hljs-title">someProperty</span>&gt;</span> = <span class="hljs-tag">&lt;<span class="hljs-title">someValue</span>&gt;</span>

# channel selector configuration
# ...

# Bind the source and sink to the channel
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.sources.<span class="hljs-tag">&lt;<span class="hljs-title">Source</span>&gt;</span>.channels = <span class="hljs-tag">&lt;<span class="hljs-title">Channel1</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-title">Channel2</span>&gt;</span> ...
<span class="hljs-tag">&lt;<span class="hljs-title">Agent</span>&gt;</span>.sinks.<span class="hljs-tag">&lt;<span class="hljs-title">Sink</span>&gt;</span>.channel = <span class="hljs-tag">&lt;<span class="hljs-title">Channel1</span>&gt;</span></code></pre>



<h5 id="运行命令flume-ng">运行命令(flume-ng)</h5>



<pre class="prettyprint"><code class=" hljs lasso">$ bin/flume<span class="hljs-attribute">-ng</span> agent <span class="hljs-attribute">-n</span> <span class="hljs-variable">$agent_name</span> <span class="hljs-attribute">-c</span> conf <span class="hljs-attribute">-f</span> conf/flume<span class="hljs-attribute">-conf</span><span class="hljs-built_in">.</span>conf <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>INFO,console</code></pre>



<pre class="prettyprint"><code class=" hljs haml"># $flume-ng --help
Usage: /home/ghb/HadoopCluster/flume-1.7.0/bin/flume-ng &lt;command&gt; [options]...

commands:
  help                      display this help text
  agent                     run a Flume agent
  avro-client               run an avro Flume client
  version                   show Flume version info

global options:
  -<span class="ruby">-conf,-c &lt;conf&gt;          use configs <span class="hljs-keyword">in</span> &lt;conf&gt; directory
</span>  -<span class="ruby">-classpath,-<span class="hljs-constant">C</span> &lt;cp&gt;       append to the classpath
</span>  -<span class="ruby">-dryrun,-d               <span class="hljs-keyword">do</span> <span class="hljs-keyword">not</span> actually start <span class="hljs-constant">Flume</span>, just print the command
</span>  -<span class="ruby">-plugins-path &lt;dirs&gt;     colon-separated list of plugins.d directories. <span class="hljs-constant">See</span> the
</span>                            plugins.d section in the user guide for more details.
                            Default: $FLUME_HOME/plugins.d
  -<span class="ruby"><span class="hljs-constant">Dproperty</span>=value          sets a <span class="hljs-constant">Java</span> system property value
</span>  -<span class="ruby"><span class="hljs-constant">Xproperty</span>=value          sets a <span class="hljs-constant">Java</span> -<span class="hljs-constant">X</span> option
</span>
agent options:
  -<span class="ruby">-name,-n &lt;name&gt;          the name of this agent (required)
</span>  -<span class="ruby">-conf-file,-f &lt;file&gt;     specify a config file (required <span class="hljs-keyword">if</span> -z missing)
</span>  -<span class="ruby">-zkConnString,-z &lt;str&gt;   specify the <span class="hljs-constant">ZooKeeper</span> connection to use (required <span class="hljs-keyword">if</span> -f missing)
</span>  -<span class="ruby">-zkBasePath,-p &lt;path&gt;    specify the base path <span class="hljs-keyword">in</span> <span class="hljs-constant">ZooKeeper</span> <span class="hljs-keyword">for</span> agent configs
</span>  -<span class="ruby">-no-reload-conf          <span class="hljs-keyword">do</span> <span class="hljs-keyword">not</span> reload config file <span class="hljs-keyword">if</span> changed
</span>  -<span class="ruby">-help,-h                 display help text
</span>
avro-client options:
  -<span class="ruby">-rpcProps,-<span class="hljs-constant">P</span> &lt;file&gt;   <span class="hljs-constant">RPC</span> client properties file with server connection params
</span>  -<span class="ruby">-host,-<span class="hljs-constant">H</span> &lt;host&gt;       hostname to which events will be sent
</span>  -<span class="ruby">-port,-p &lt;port&gt;       port of the avro source
</span>  -<span class="ruby">-dirname &lt;dir&gt;        directory to stream to avro source
</span>  -<span class="ruby">-filename,-<span class="hljs-constant">F</span> &lt;file&gt;   text file to stream to avro source (<span class="hljs-function">default: </span>std input)
</span>  -<span class="ruby">-headerFile,-<span class="hljs-constant">R</span> &lt;file&gt; <span class="hljs-constant">File</span> containing event headers as key/value pairs on each new line
</span>  -<span class="ruby">-help,-h              display help text
</span>
  Either --rpcProps or both --host and --port must be specified.

Note that if &lt;conf&gt; directory is specified, then it is always included first
in the classpath.</code></pre>



<h3 id="关于flume核心组件的几个问题">关于Flume核心组件的几个问题</h3>

<p>Flume主要由3个重要的组件构成： <br>
1. Source： 完成对日志数据的收集，分成transtion 和 event 打入到channel之中。 <br>
2. Channel： Flume Channel主要提供一个队列的功能，对source提供中的数据进行简单的缓存。 <br>
3. Sink： Flume Sink取出Channel中的数据，进行相应的存储文件系统，数据库，或者提交到远程服务器。</p>

<h5 id="source">Source</h5>

<ol>
<li>Spool Source 如何使用？ <br>
在实际使用的过程中，可以结合log4j使用，使用log4j的时候，将log4j的文件分割机制设为1分钟一次，将文件拷贝到spool的监控目录。</li>
</ol>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-number">1</span>) log4j有一个TimeRolling的插件，可以把log4j分割的文件到spool目录。
<span class="hljs-number">2</span>) 基本实现了实时的监控。
<span class="hljs-number">3</span>) Flume在传完文件之后，将会修改文件的后缀，变为<span class="hljs-preprocessor">.COMPLETED</span>（后缀也可以在配置文件中灵活指定）</code></pre>

<ol>
<li>Exec Source和Spool Source比较</li>
</ol>



<pre class="prettyprint"><code class=" hljs ">1) ExecSource可以实现对日志的实时收集，但是存在Flume不运行或者指令执行出错时，将无法收集到日志数据，无法何证日志数据的完整性。
2) SpoolSource虽然无法实现实时的收集数据，但是可以使用以分钟的方式分割文件，趋近于实时。
3) 总结：如果应用无法实现以分钟切割日志文件的话，可以两种收集方式结合使用。</code></pre>



<h5 id="channel">Channel</h5>

<ol>
<li>MemoryChannel可以实现高速的吞吐， 但是无法保证数据完整性</li>
<li>MemoryRecoverChannel在官方文档的建议上已经建义使用FileChannel来替换。FileChannel保证数据的完整性与一致性。在具体配置不现的FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。 </li>
</ol>



<h5 id="sink">Sink</h5>

<p>Flume Sink在设置存储数据时，可以向文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。 </p>



<h3 id="flume适用场景">Flume适用场景</h3>

<p>如果你想把可文本化的日志数据提取到HDFS，那么Flume是非常适合的。</p>

<p>对于其他场景，有些东西是需要考量的：Flume被设计用来传输、提取定期生成的数据的，这些数据是传输在相对稳定的、可能是复杂的拓扑结构上的。每个数据就是一个event。“event data”的概念是非常广泛的。对于Flume而言，一个event就是一个blob字节数据。这个event的大小是有限制的，例如，不能大于内存或硬盘或单机可以存储的大小。事实上，flume的event可以是任何东西，从日志文本到图片文件。Event的关键点是不断生成、流式的。如果你的数据不是的定期生成的（比如一次性的向Hadoop集群导入数据），Flume可以工作，但是有点杀鸡用牛刀了。Flume喜欢相对稳定的拓扑结构。你的拓扑结构不必是不可改变的，因为Flume可以在不丢失数据的前提下处理拓扑结构的改变，并且能容忍由于故障转移导致的周期性的重新配置。但如果你每天都要改变拓扑结构，那么Flume将不能很好的工作，因为重新配置会产生开销。</p>

<p>简而言之，有两点： <br>
1. 数据。数据是定期生成的。 <br>
2. 网络拓扑相对稳定。</p>



<h3 id="总结">总结</h3>

<p>Flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入（单个或多个）Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。</p>

<hr>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>