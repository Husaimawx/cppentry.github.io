---
layout:     post
title:      hive相关报错问题解决办法
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><span style="color:#f33b45;"><strong>1.Unable to determine Hadoop version information. 'hadoop version' returned:</strong></span></p>

<p><strong>解决办法：</strong></p>

<p>修改bin/hive，</p>

<p>#    HADOOP_VERSION=$($HADOOP version | awk '{if (NR == 1) {print $2;}}');</p>

<p>HADOOP_VERSION=$($HADOOP version | awk '{if($0 ~ /[[:alpha:]]+ ([[:digit:]]+)\.([[:digit:]]+)\.([[:digit:]]+)?.*$/){print  $2};}')</p>

<p> </p>

<p><span style="color:#f33b45;"><strong>2. 在Hive中使用show tables;等命令报错：com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes</strong></span></p>

<p><strong>解决办法：</strong> </p>

<p>到mysql中的hive数据库里执行 alter database hive character set latin1;</p>

<p>改变hive元数据库的字符集，问题就可以解决！ 但是这时你如果建一个表里有中文，就会报错了。</p>

<p> </p>

<p><span style="color:#f33b45;"><strong>3.hive show tables; 报错：</strong></span></p>

<p><span style="color:#f33b45;"><strong>FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient</strong></span></p>

<p><span style="color:#f33b45;"><strong>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask</strong></span></p>

<p><strong>解决办法：</strong></p>

<p>cd $HIVE_HOME/conf,确认hive-site.xml中的javax.jdo.option.ConnectionURL等配置是否正确</p>

<p> </p>

<p><strong>4.hive自定义函数扩展</strong></p>

<p>在hive安装目录下建auxlib文件夹,与lib目录同级，把自己开发的扩展jar放到该目录下，</p>

<p>在使用之前执行一下类似如下的语句：</p>

<p>CREATE TEMPORARY FUNCTION ailk_udf_row_number AS 'com.ailk.cp.cloud.hiveudf.udf.AilkUdfRowNumber'</p>

<p> </p>

<p><strong>5.外部表建分区时需要指定location时</strong></p>

<p><strong>应该写成这样：</strong></p>

<p>alter table tc_social_all add partition (yyyy=2016,mm=6,dd=16,hh=0) location '2016/06/16/00';</p>

<p>alter table tc_social_all add partition (yyyy=2016,mm=6,dd=16,hh=1) location '2016/06/16/01';</p>

<p><strong>不能写成这样：</strong></p>

<p>alter table tc_social_all add partition (yyyy=2016,mm=6,dd=16,hh=0) location '2016/06/16/00'</p>

<p>partition (yyyy=2016,mm=6,dd=16,hh=1) location '2016/06/16/01';</p>

<p>否则，所有的分区都指向'2016/06/16/00'。</p>

<p>删除外部表时，数据不会删除</p>

<p>删除普通表时，数据会删除</p>

<p> </p>

<p><strong>6.显示表分区</strong></p>

<p>show partitions tabName</p>

<p> </p>

<p><strong>7.hive查询写文件报错<span style="color:#f33b45;">Failed with exception Unable to move source hdfs:xx to destination xxx</span></strong></p>

<p>解决方法：$HADOOP_HOME/share/hadoop/tools/lib/hadoop-distcp-2.6.0.jar放入hive的lib目录</p>

<p>并重启hive shell</p>

<p> </p>

<p><strong>8.启动hive metastore</strong></p>

<p>nohup hive --service metastore &amp;</p>

<p> </p>

<p><strong>9.执行hive报错</strong></p>

<p><span style="color:#f33b45;"><strong>Diagnostic Messages for this Task:</strong></span></p>

<p><span style="color:#f33b45;"><strong>java.lang.RuntimeException: native-lzo library not available</strong></span></p>

<p><span style="color:#f33b45;"><strong>        at com.hadoop.compression.lzo.LzoCodec.getCompressorType(LzoCodec.java:155)</strong></span></p>

<p><strong>解决方法：</strong></p>

<p>cp /export/home/hadoop/hadoop-2.6.0-cdh5.4.1/lib/native/libgplcompression.so /usr/java/jdk1.7.0_55/jre/lib/amd64</p>

<p>另外确认下/usr/java/jdk1.7.0_55/jre/lib/amd64下的libgplcompression.so有没有读取和执行权限。</p>

<p> </p>

<p><span style="color:#f33b45;"><strong>10.Does not contain a valid host:port authority: logicaljt</strong></span></p>

<p>zookeeper中的MR配置没有更新，需要格式化一下，在MR主节点（jobtracker主节点）执行：./hadoop mrzkfc -formatZK</p>

<p> </p>

<p><strong>11.查看table在hdfs上的存储路径</strong></p>

<p>show create table 表名;</p>

<p> </p>

<p><span style="color:#f33b45;"> <strong>12、hive报错java.io.IOException: Could not find status of job:job_1470047186803_131111</strong></span></p>

<p>        jobname过长导致，jobname默认是从hql中的开头和结尾截取一部分，如果sql开头或结尾有中文注释的话，会被截取进来，并进行url编码。导致作业的信息名变的非常长，超过了namenode所允许的最大的文件命名长度。导致任务无法写入historyserver。hive在historyserver无法获得这个job的状态，报开头的错误。</p>

<p><strong>简单的解决办法：</strong></p>

<p><strong>设置jobname的最大长度</strong></p>

<p>set hive.jobname.length=10;</p>            </div>
                </div>