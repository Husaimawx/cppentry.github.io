---
layout:     post
title:      Hadoop关于处理大量小文件的问题和解决方法
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1 class="title" style="list-style:none;font-weight:normal;font-size:28px;line-height:36px;font-family:'Microsoft YaHei';color:rgb(51,51,51);">
Hadoop关于处理大量小文件的问题和解决方法</h1>
<div class="summary" style="list-style:none;color:rgb(51,51,51);font-family:Helvetica, Tahoma, Arial, sans-serif;font-size:14px;line-height:24px;background:rgb(247,247,247);">
<br></div>
<div class="summary" style="list-style:none;color:rgb(51,51,51);font-family:Helvetica, Tahoma, Arial, sans-serif;font-size:14px;line-height:24px;background:rgb(247,247,247);">
<iframe frameborder="0" scrolling="no" src="http://hits.sinajs.cn/A1/weiboshare.html?url=http%3A%2F%2Fwww.csdn.net%2Farticle%2F2010-11-22%2F282301&amp;type=3&amp;count=&amp;appkey=&amp;title=%E5%B0%8F%E6%96%87%E4%BB%B6%E6%8C%87%E7%9A%84%E6%98%AF%E9%82%A3%E4%BA%9Bsize%E6%AF%94HDFS%20%E7%9A%84block%20size(%E9%BB%98%E8%AE%A464M)%E5%B0%8F%E7%9A%84%E5%A4%9A%E7%9A%84%E6%96%87%E4%BB%B6%E3%80%82%E5%A6%82%E6%9E%9C%E5%9C%A8HDFS%E4%B8%AD%E5%AD%98%E5%82%A8%E5%B0%8F%E6%96%87%E4%BB%B6%EF%BC%8C%E9%82%A3%E4%B9%88%E5%9C%A8HDFS%E4%B8%AD%E8%82%AF%E5%AE%9A%E4%BC%9A%E5%90%AB%E6%9C%89%E8%AE%B8%E8%AE%B8%E5%A4%9A%E5%A4%9A%E8%BF%99%E6%A0%B7%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6(%E4%B8%8D%E7%84%B6%E5%B0%B1%E4%B8%8D%E4%BC%9A%E7%94%A8hadoop%E4%BA%86)%E3%80%82%20%E8%80%8CHDFS%E7%9A%84%E9%97%AE%E9%A2%98%E5%9C%A8%E4%BA%8E%E6%97%A0%E6%B3%95%E5%BE%88%E6%9C%89%E6%95%88%E7%9A%84%E5%A4%84&amp;pic=&amp;ralateUid=&amp;language=zh_cn&amp;rnd=1458109798988" width="22" height="16"></iframe><strong>摘要：</strong>小文件指的是那些size比HDFS 的block size(默认64M)小的多的文件。如果在HDFS中存储小文件，那么在HDFS中肯定会含有许许多多这样的小文件(不然就不会用hadoop了)。 而HDFS的问题在于无法很有效的处</div>
<div class="con news_content" style="list-style:none;color:rgb(51,51,51);font-family:Helvetica, Tahoma, Arial, sans-serif;font-size:14px;line-height:24px;">
<p style="list-style:none;">
小文件指的是那些size比HDFS的block size(默认64M)小的多的文件。如果在HDFS中存储小文件，那么在HDFS中肯定会含有许许多多这样的小文件(不然就不会用hadoop了)。而HDFS的问题在于无法很有效的处理大量小文件。</p>
<p style="list-style:none;">
任何一个文件，目录和block，在HDFS中都会被表示为一个object存储在namenode的内存中，没一个object占用150 bytes的内存空间。所以，如果有10million个文件，没一个文件对应一个block，那么就将要消耗namenode 3G的内存来保存这些block的信息。如果规模再大一些，那么将会超出现阶段计算机硬件所能满足的极限。</p>
<p style="list-style:none;">
不仅如此，HDFS并不是为了有效的处理大量小文件而存在的。它主要是为了流式的访问大文件而设计的。对小文件的读取通常会造成大量从datanode到datanode的seeks和hopping来retrieve文件，而这样是非常的低效的一种访问方式。</p>
<p style="list-style:none;">
<strong>大量小文件在mapreduce中的问题</strong></p>
<p style="list-style:none;">
Map tasks通常是每次处理一个block的input(默认使用FileInputFormat)。如果文件非常的小，并且拥有大量的这种小文件，那么每一个map task都仅仅处理了非常小的input数据，并且会产生大量的map tasks，每一个map task都会消耗一定量的bookkeeping的资源。比较一个1GB的文件，默认block size为64M，和1Gb的文件，没一个文件100KB，那么后者没一个小文件使用一个map task，那么job的时间将会十倍甚至百倍慢于前者。</p>
<p style="list-style:none;">
hadoop中有一些特性可以用来减轻这种问题：可以在一个JVM中允许task reuse，以支持在一个JVM中运行多个map task，以此来减少一些JVM的启动消耗(通过设置mapred.job.reuse.jvm.num.tasks属性，默认为1，－1为无限制)。另一种方法为使用MultiFileInputSplit，它可以使得一个map中能够处理多个split。</p>
<p style="list-style:none;">
</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
<strong> HDFS文件读写流程</strong></p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
在正式介绍HDFS小文件存储方案之前，我们先介绍一下当前HDFS上文件存取的基本流程。</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
<strong>(1)  读文件流程</strong></p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
1）client端发送读文件请求给namenode，如果文件不存在，返回错误信息，否则，将该文件对应的block及其所在datanode位置发送给client</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
2） client收到文件位置信息后，与不同namenode建立socket连接并行获取数据。</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
<strong>(2) 写文件流程</strong></p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
1） client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
2） client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
3）  namenode收到的client信息后，发送确信信息给datanode</p>
<p style="color:rgb(54,46,43);font-family:Arial;font-size:14px;line-height:26px;">
4）  datanode同时收到namenode和datanode的确认信息后，提交写操作。</p>
<br><p style="list-style:none;">
<strong>为什么会产生大量的小文件？</strong></p>
<p style="list-style:none;">
至少有两种情况下会产生大量的小文件</p>
<p style="list-style:none;">
1.这些小文件都是一个大的逻辑文件的pieces。由于HDFS仅仅在不久前才刚刚支持对文件的append，因此以前用来向unbounde files(例如log文件)添加内容的方式都是通过将这些数据用许多chunks的方式写入HDFS中。</p>
<p style="list-style:none;">
2.文件本身就是很小。例如许许多多的小图片文件。每一个图片都是一个独立的文件。并且没有一种很有效的方法来将这些文件合并为一个大的文件</p>
<p style="list-style:none;">
这两种情况需要有不同的解决方式。对于第一种情况，文件是由许许多多的records组成的，那么可以通过件邪行的调用HDFS的sync()方法(和append方法结合使用)来解决。或者，可以通过些一个程序来专门合并这些小文件(see Nathan Marz’s post about a tool called the Consolidator which does exactly this)。</p>
<p style="list-style:none;">
对于第二种情况，就需要某种形式的容器来通过某种方式来group这些file。hadoop提供了一些选择：</p>
<p style="list-style:none;">
<strong>HAR files</strong></p>
<p style="list-style:none;">
Hadoop Archives (HAR files)是在0.18.0版本中引入的，它的出现就是为了缓解大量小文件消耗namenode内存的问题。HAR文件是通过在HDFS上构建一个层次化的文件系统来工作。一个HAR文件是通过hadoop的archive命令来创建，而这个命令实 际上也是运行了一个MapReduce任务来将小文件打包成HAR。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 visible &amp;&amp; accessible（using har://URL）。但在HDFS端它内部的文件数减少了。</p>
<p style="list-style:none;text-align:center;">
<img border="0" alt="" width="407" height="255" src="http://cms.csdnimg.cn/articlev1/uploads/allimg/101122/79_101122112258_1.png" style="vertical-align:middle;border:none;"></p>
<p style="list-style:none;">
通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层index文件的读取和文件本身数据的读取(见上图)。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。可以考虑通过创建一种input format，利用HAR文件的优势来提高MapReduce的效率，但是目前还没有人作这种input format。需要注意的是：MultiFileInputSplit，即使在HADOOP-4565的改进(choose
 files in a split that are node local)，但始终还是需要seek per small file。</p>
<p style="list-style:none;">
<strong>Sequence Files</strong></p>
<p style="list-style:none;">
通常对于“the small files problem”的回应会是：使用SequenceFile。这种方法是说，使用filename作为key，并且file contents作为value。实践中这种方式非常管用。回到10000个100KB的文件，可以写一个程序来将这些小文件写入到一个单独的SequenceFile中去，然后就可以在一个streaming fashion(directly or using mapreduce)中来使用这个sequenceFile。不仅如此，SequenceFiles也是splittable的，所以mapreduce可以break
 them into chunks，并且分别的被独立的处理。和HAR不同的是，这种方式还支持压缩。block的压缩在许多情况下都是最好的选择，因为它将多个records压缩到一起，而不是一个record一个压缩。</p>
<p style="list-style:none;">
将已有的许多小文件转换成一个SequenceFiles可能会比较慢。但是，完全有可能通过并行的方式来创建一个一系列的SequenceFiles。(Stuart Sierra has written a very useful post about converting a tar file into a SequenceFile—tools like this are very useful)。更进一步，如果有可能最好设计自己的数据pipeline来将数据直接写入一个SequenceFile。</p>
<p style="list-style:none;text-align:center;">
<img border="0" alt="" width="542" height="319" src="http://cms.csdnimg.cn/articlev1/uploads/allimg/101122/79_101122112322_1.png" style="vertical-align:middle;border:none;"></p>
<p style="list-style:none;">
译文链接：<a href="http://nicoleamanda.blog.163.com/blog/static/749961072009111805538447/" rel="nofollow" style="color:rgb(0,102,204);text-decoration:none;">http://nicoleamanda.blog.163.com/blog/static/749961072009111805538447/</a></p>
<p style="list-style:none;">
原文链接：<a href="http://www.cloudera.com/blog/2009/02/the-small-files-problem/" rel="nofollow" style="color:rgb(0,102,204);text-decoration:none;">http://www.cloudera.com/blog/2009/02/the-small-files-problem/</a></p>
</div>
            </div>
                </div>