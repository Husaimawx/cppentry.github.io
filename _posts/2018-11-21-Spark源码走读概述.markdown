---
layout:     post
title:      Spark源码走读概述
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：所有文章均为作者的学习笔记以及技术点整理，如果对您有帮助，请点个赞:)					https://blog.csdn.net/superman_xxx/article/details/52893345				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>Spark代码量 <br>
——Spark：20000loc <br>
——Hadoop 1.0：90000loc <br>
——Hadoop 2.0：220000loc <br>
Spark生态系统代码量 <br>
<img src="https://img-blog.csdn.net/20161022160519469" alt="这里写图片描述" title=""></p>

<p><img src="https://img-blog.csdn.net/20161022160557563" alt="这里写图片描述" title=""></p>

<p>Spark生态系统 <br>
<img src="https://img-blog.csdn.net/20161022160633474" alt="这里写图片描述" title=""> <br>
概述 <br>
——构建Spark源代码阅读环境 <br>
——Spark源代码构成 <br>
——Spark源代码阅读方法 <br>
构建源码阅读环境</p>

<p>1.IDE <br>
——Eclipse / IDEA <br>
——可直接导入 <br>
2.编译源代码 <br>
——Spark使用maven作为源代码编译工具 <br>
——build/mvn -DskipTests clean package <br>
——build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package <br>
——<a href="https://github.com/apache/spark/blob/master/README.md" rel="nofollow">https://github.com/apache/spark/blob/master/README.md</a> <br>
——<a href="http://spark.apache.org/docs/latest/building-spark.html" rel="nofollow">http://spark.apache.org/docs/latest/building-spark.html</a></p>

<p>源码构造 <br>
Spark源码构成 <br>
——Spark Core <br>
——Spark SQL <br>
——Spark Streaming <br>
——Spark MLlib <br>
——Spark Graphx <br>
——解释器 <br>
——提交模式：mesos，yarn，standalone</p>

<p><img src="https://img-blog.csdn.net/20161022161724881" alt="这里写图片描述" title=""></p>

<p>Spark Core源码阅读 <br>
——会编写Spark应用程序 <br>
1.SaprkContext <br>
2.RDD <br>
3.Transformation/action <br>
——熟悉Spark基础架构 <br>
1.Driver/executor <br>
2.YARN/ApplicationMaster/Container <br>
——Spark运行模式 <br>
1.本地模式 <br>
2.YARN模式 <br>
3.Shell模式 <br>
——Spark On YARN模式：梳理代码脉络 <br>
<img src="https://img-blog.csdn.net/20161022162400916" alt="这里写图片描述" title=""></p>

<p>追踪一个应用程序运行过程</p>

<pre class="prettyprint"><code class=" hljs r">bin/spark-submit --master yarn-cluster --class <span class="hljs-keyword">...</span></code></pre>

<p><img src="https://img-blog.csdn.net/20161022162554948" alt="这里写图片描述" title=""></p>

<p>——Spark On YARN 模式：分析具体算子执行 <br>
1.RDD表示 <br>
2.sc.textFile(“/input/a.txt”).count <br>
3.sc.textFile(“/input/a.txt”).map(…).reduceByKey(…).saveAsTextFile(…)</p>

<p>参考资料： <br>
——<a href="https://spark-internals.books.yourtion.com/" rel="nofollow">https://spark-internals.books.yourtion.com/</a> <br>
——Spark Shuffle ：<a href="http://blog.csdn.net/johnny_lee/article/details/22619585" rel="nofollow">http://blog.csdn.net/johnny_lee/article/details/22619585</a></p>

<p>阅读方法： <br>
Step 1：熟练开发Spark应用程序 <br>
——reduceByKey/groupByKey/sort/join/map/filter/flatMap … <br>
Step 2：收集资料 <br>
——Spark源码分析文章，书籍 <br>
——站在别人肩上 <br>
Step 3：选择一个阅读模式 <br>
——比如Saprk的yarn-cluster模式 <br>
Step 4：理清模块脉络 <br>
——架构、流程、相关类 <br>
Step 5：深入细节 <br>
——类，变量，数据结构，算法等</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>