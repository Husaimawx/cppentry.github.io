---
layout:     post
title:      Hive_on_Spark安装配置详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p align="left"><br></p>
<h2 style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">简介</span></h2>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">本文主要记录如何安装配置</span><span style="color:#333333;background:#FFFFFF;">Hive on Spark</span><span style="color:#333333;background:#FFFFFF;">，在执行以下步骤之前，请先确保已经安装</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">集群，</span><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">MySQL</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">JDK</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">Scala</span><span style="color:#333333;background:#FFFFFF;">，具体安装步骤不再赘述。</span></p>
<h2 id="2" style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">背景</span></h2>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">默认使用</span><span style="color:#333333;background:#FFFFFF;">MapReduce</span><span style="color:#333333;background:#FFFFFF;">作为执行引擎，即</span><span style="color:#333333;background:#FFFFFF;">Hive
 on mr</span><span style="color:#333333;background:#FFFFFF;">。实际上，</span><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">还可以使用</span><span style="color:#333333;background:#FFFFFF;">Tez</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">作为其执行引擎，分别为</span><span style="color:#333333;background:#FFFFFF;">Hive
 on Tez</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">Hive on Spark</span><span style="color:#333333;background:#FFFFFF;">。由于</span><span style="color:#333333;background:#FFFFFF;">MapReduce</span><span style="color:#333333;background:#FFFFFF;">中间计算均需要写入磁盘，而</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">是放在内存中，所以总体来讲</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">比</span><span style="color:#333333;background:#FFFFFF;">MapReduce</span><span style="color:#333333;background:#FFFFFF;">快很多。因此，</span><span style="color:#333333;background:#FFFFFF;">Hive
 on Spark</span><span style="color:#333333;background:#FFFFFF;">也会比</span><span style="color:#333333;background:#FFFFFF;">Hive on mr</span><span style="color:#333333;background:#FFFFFF;">快。为了对比</span><span style="color:#333333;background:#FFFFFF;">Hive on Spark</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">Hive
 on mr</span><span style="color:#333333;background:#FFFFFF;">的速度，需要在已经安装了</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">集群的机器上安装</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群（</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群是建立在</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">集群之上的，也就是需要先装</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">集群，再装</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群，因为</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">用了</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">的</span><span style="color:#333333;background:#FFFFFF;">HDFS</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">YARN</span><span style="color:#333333;background:#FFFFFF;">等），然后把</span><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">的执行引擎设置为</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">。</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">运行模式分为三种</span><span style="color:#333333;background:#FFFFFF;">1</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">Spark
 on YARN 2</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">Standalone Mode 3</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">Spark on Mesos</span><span style="color:#333333;background:#FFFFFF;">。</span><span style="color:#333333;background:#FFFFFF;"><br>
Hive on Spark</span><span style="color:#333333;background:#FFFFFF;">默认支持</span><span style="color:#333333;background:#FFFFFF;">Spark on YARN</span><span style="color:#333333;background:#FFFFFF;">模式，因此我们选择</span><span style="color:#333333;background:#FFFFFF;">Spark
 on YARN</span><span style="color:#333333;background:#FFFFFF;">模式。</span><span style="color:#333333;background:#FFFFFF;">Spark on YARN</span><span style="color:#333333;background:#FFFFFF;">就是使用</span><span style="color:#333333;background:#FFFFFF;">YARN</span><span style="color:#333333;background:#FFFFFF;">作为</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">的资源管理器。分为</span><span style="color:#333333;background:#FFFFFF;">Cluster</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">Client</span><span style="color:#333333;background:#FFFFFF;">两种模式。</span></p>
<h2 id="3" style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">一、环境说明</span></h2>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">本教程</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">相关软件全部基于</span><span style="color:#333333;background:#FFFFFF;">CDH5.5.1</span><span style="color:#333333;background:#FFFFFF;">，用</span><span style="color:#333333;background:#FFFFFF;">yum</span><span style="color:#333333;background:#FFFFFF;">安装，系统环境如下：</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">操作系统：</span><span style="color:#333333;background:#FFFFFF;">CentOS7.2</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">Hadoop 2.6.0</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">Hive1.1.0</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">Spark1.5.0</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">MySQL 5.6</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">JDK 1.8</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">Maven 3.3.3</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">Scala 2.10</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">各节点规划如下：</span></p>
<pre style="background:#FFFFFF;"><span style="background:#FFFFFF;">192.168.117.51     </span>Goblin01           nn1  jn1  rm1  worker  master  hive  metastore  mysql</pre>
<pre style="background:#FFFFFF;"><span style="background:#FFFFFF;">192.168.117.52     </span>Goblin02    zk2    nn2  jn2  rm2  worker          hive</pre>
<pre style="background:#FFFFFF;"><span style="background:#FFFFFF;">192.168.117.53     </span>Goblin03    zk3    dn1  jn3       worker          hive</pre>
<pre style="background:#FFFFFF;"><span style="background:#FFFFFF;">192.168.117.54     </span>Goblin04    zk4    dn2            worker          hive</pre>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">说明：</span><span style="color:#333333;background:#FFFFFF;">Goblin01~04</span><span style="color:#333333;background:#FFFFFF;">是每台机器的</span><span style="color:#333333;background:#FFFFFF;">hostname</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">zk</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">zookeeper</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">nn</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">hadoop</span><span style="color:#333333;background:#FFFFFF;">的</span><span style="color:#333333;background:#FFFFFF;">namenode</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">dn</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">datanode</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">jn</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">journalnode</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">rm</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">resourcemanager</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">worker</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">的</span><span style="color:#333333;background:#FFFFFF;">slaves</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">master</span><span style="color:#333333;background:#FFFFFF;">代表</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">的</span><span style="color:#333333;background:#FFFFFF;">master</span></p>
<h2 id="4" style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">二、编译和安装</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">（</span><span style="color:#333333;background:#FFFFFF;">Spark
 on YARN</span><span style="color:#333333;background:#FFFFFF;">）</span></h2>
<h3 id="5" style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">2.1
</span><span style="color:#333333;background:#FFFFFF;">编译</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">源码</span></h3>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">要使用</span><span style="color:#333333;background:#FFFFFF;">Hive on Spark</span><span style="color:#333333;background:#FFFFFF;">，所用的</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">版本必须不包含</span><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">的相关</span><span style="color:#333333;background:#FFFFFF;">jar</span><span style="color:#333333;background:#FFFFFF;">包，</span><span style="color:#333333;background:#FFFFFF;">hive
 on spark </span><span style="color:#333333;background:#FFFFFF;">的官网上说</span><span style="color:#333333;background:#FFFFFF;">“Note that you must have a version of Sparkwhich does not include the Hive jars”</span><span style="color:#333333;background:#FFFFFF;">。在</span><span style="color:#333333;background:#FFFFFF;">spark</span><span style="color:#333333;background:#FFFFFF;">官网下载的编译的</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">都是有集成</span><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">的，因此需要自己下载源码来编译，并且编译的时候不指定</span><span style="color:#333333;background:#FFFFFF;">Hive</span><span style="color:#333333;background:#FFFFFF;">。</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">我们这里用的</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">源码是</span><span style="color:#333333;background:#FFFFFF;">spark-1.5.0-cdh5.5.1</span><span style="color:#333333;background:#FFFFFF;">版本</span><span style="color:#333333;background:#FFFFFF;">,</span><span style="color:#333333;background:#FFFFFF;">下载地址如下：</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"><a href="http://archive.cloudera.com/cdh5/cdh/5/spark-1.5.0-cdh5.5.1-src.tar.gz" rel="nofollow"><span style="color:#333333;">http://archive.cloudera.com/cdh5/cdh/5/spark-1.5.0-cdh5.5.1-src.tar.gz</span></a></span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">下载完后用</span><span style="color:#333333;background:#FFFFFF;"> tar xzvf
</span><span style="color:#333333;background:#FFFFFF;">命令解压，进入解压完的文件夹，准备编译。</span></p>
<p style="background:#FFFFFF;"><strong><span style="background:#FFFFFF;">注意：编译前请确保已经安装</span><span style="background:#FFFFFF;">JDK</span><span style="background:#FFFFFF;">、</span><span style="background:#FFFFFF;">Maven</span><span style="background:#FFFFFF;">和</span><span style="background:#FFFFFF;">Scala</span><span style="background:#FFFFFF;">，</span><span style="background:#FFFFFF;">maven</span><span style="background:#FFFFFF;">为</span><span style="background:#FFFFFF;">3.3.3</span><span style="background:#FFFFFF;">及以上版本，并在</span><span style="background:#FFFFFF;">/etc/profile</span><span style="background:#FFFFFF;">里配置环境变量。</span></strong></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">命令行进入在源码根目录下，执行</span></p>
<pre style="background:#FFFFFF;"><span style="background:#FFFFFF;">  </span>./make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.6,parquet-provided"</pre>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">若编译过程出现内存不足的情况，需要在运行编译命令之前先运行：</span></p>
<pre style="background:#FFFFFF;"><span style="background:#FFFFFF;">export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</span></pre>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">来设置</span><span style="color:#333333;background:#FFFFFF;">Maven</span><span style="color:#333333;background:#FFFFFF;">的内存。</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">编译过程由于要下载很多</span><span style="color:#333333;background:#FFFFFF;">Maven</span><span style="color:#333333;background:#FFFFFF;">依赖的</span><span style="color:#333333;background:#FFFFFF;">jar</span><span style="color:#333333;background:#FFFFFF;">包，需要时间较长（大概一两个小时），要保证网络状况良好，不然很容易编译失败。</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">编译成功后，会在源码根目录下多出一个文件</span><span style="color:#333333;background:#FFFFFF;">(</span><span style="color:#333333;background:#FFFFFF;">红色部分）：</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">spark-1.5.0-cdh5.5.1-bin-hadoop2-without-hive.tgz</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">2.2 </span>
<span style="color:#333333;background:#FFFFFF;">安装</span><span style="color:#333333;background:#FFFFFF;">Spark</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">将编译完生成的</span><span style="color:#333333;background:#FFFFFF;">spark-1.5.0-cdh5.5.1-bin-hadoop2-without-hive.tgz</span><span style="color:#333333;background:#FFFFFF;">拷贝到</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">的安装路径，并用</span><span style="color:#333333;background:#FFFFFF;">tar
 -xzvf </span><span style="color:#333333;background:#FFFFFF;">命令解压</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">配置环境变量</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">$vim /etc/profile</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">export SPARK_HOME=spark</span><span style="color:#333333;background:#FFFFFF;">安装路径</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">$source /etc/profile</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">2.3 </span>
<span style="color:#333333;background:#FFFFFF;">配置</span><span style="color:#333333;background:#FFFFFF;">Spark</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">配置</span><span style="color:#333333;background:#FFFFFF;">spark-env.sh</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">slaves</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">spark-defaults.conf</span><span style="color:#333333;background:#FFFFFF;">三个文件</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">spark-env.sh</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">主要配置</span><span style="color:#333333;background:#FFFFFF;">JAVA\_HOME</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">SCALA\_HOME</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">HADOOP\_HOME</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">HADOOP\_CONF\_DIR</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">SPARK\_MASTER\_IP</span><span style="color:#333333;background:#FFFFFF;">等</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export JAVA_HOME=/usr/lib/jvm/java</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SCALA_HOME=/root/scala</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export HADOOP_HOME=/usr/lib/hadoop</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_LAUNCH_WITH_SCALA=0</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_WORKER_MEMORY=1g</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_DRIVER_MEMORY=1g</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_MASTER_IP=192.168.117.51</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">exportSPARK_LIBRARY_PATH=/root/spark-without-hive/lib</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_MASTER_WEBUI_PORT=18080</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">exportSPARK_WORKER_DIR=/root/spark-without-hive/work</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_MASTER_PORT=7077</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">export SPARK_WORKER_PORT=7078</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">exportSPARK_LOG_DIR=/root/spark-without-hive/log</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">exportSPARK_PID_DIR='/root/spark-without-hive/run'</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">slaves</span><span style="color:#333333;background:#FFFFFF;">（将所有节点都加入，</span><span style="color:#333333;background:#FFFFFF;">master</span><span style="color:#333333;background:#FFFFFF;">节点同时也是</span><span style="color:#333333;background:#FFFFFF;">worker</span><span style="color:#333333;background:#FFFFFF;">节点）</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">Goblin01</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">Goblin02</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">Goblin03</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">Goblin04</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">spark-defaults.conf</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.master                     yarn-cluster</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.home                       /root/spark-without-hive</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.eventLog.enabled           true</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.eventLog.dir               hdfs://Goblin01:8020/spark-log</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.serializer                org.apache.spark.serializer.KryoSerializer</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.executor.memory            1g</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.driver.memory              1g</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value-Dnumbers="one two three"</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">spark.master</span><span style="color:#333333;background:#FFFFFF;">指定</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">运行模式，可以是</span><span style="color:#333333;background:#FFFFFF;">yarn-client</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">yarn-cluster...</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">spark.home</span><span style="color:#333333;background:#FFFFFF;">指定</span><span style="color:#333333;background:#FFFFFF;">SPARK_HOME</span><span style="color:#333333;background:#FFFFFF;">路径</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">spark.eventLog.enabled</span><span style="color:#333333;background:#FFFFFF;">需要设为</span><span style="color:#333333;background:#FFFFFF;">true</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">spark.eventLog.dir</span><span style="color:#333333;background:#FFFFFF;">指定路径，放在</span><span style="color:#333333;background:#FFFFFF;">master</span><span style="color:#333333;background:#FFFFFF;">节点的</span><span style="color:#333333;background:#FFFFFF;">hdfs</span><span style="color:#333333;background:#FFFFFF;">中，端口要跟</span><span style="color:#333333;background:#FFFFFF;">hdfs</span><span style="color:#333333;background:#FFFFFF;">设置的端口一致（默认为</span><span style="color:#333333;background:#FFFFFF;">8020</span><span style="color:#333333;background:#FFFFFF;">），否则会报错</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">spark.executor.memory</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">spark.driver.memory</span><span style="color:#333333;background:#FFFFFF;">指定</span><span style="color:#333333;background:#FFFFFF;">executor</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">dirver</span><span style="color:#333333;background:#FFFFFF;">的内存，</span><span style="color:#333333;background:#FFFFFF;">512m</span><span style="color:#333333;background:#FFFFFF;">或</span><span style="color:#333333;background:#FFFFFF;">1g</span><span style="color:#333333;background:#FFFFFF;">，既不能太大也不能太小，因为太小运行不了，太大又会影响其他服务</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">三、配置</span><span style="color:#333333;background:#FFFFFF;">YARN</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">配置</span><span style="color:#333333;background:#FFFFFF;">yarn-site.xml</span><span style="color:#333333;background:#FFFFFF;">，跟</span><span style="color:#333333;background:#FFFFFF;">hdfs-site.xml</span><span style="color:#333333;background:#FFFFFF;">在同一个路径下（</span><span style="color:#333333;background:#FFFFFF;">$HADOOP_HOME/etc/hadoop)</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span> &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">四、配置</span><span style="color:#333333;background:#FFFFFF;">Hive</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">添加</span><span style="color:#333333;background:#FFFFFF;">spark</span><span style="color:#333333;background:#FFFFFF;">依赖到</span><span style="color:#333333;background:#FFFFFF;">hive(</span><span style="color:#333333;background:#FFFFFF;">将</span><span style="color:#333333;background:#FFFFFF;">spark-assembly-1.5.0-cdh5.5.1-hadoop2.6.0.jar</span><span style="color:#333333;background:#FFFFFF;">拷贝到</span><span style="color:#333333;background:#FFFFFF;">$HIVE\_HOME/lib</span><span style="color:#333333;background:#FFFFFF;">目录下）</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">进入</span><span style="color:#333333;background:#FFFFFF;">SPARK\_HOME</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">cpspark-assembly-1.5.0-cdh5.5.1-hadoop2.6.0.jar /usr/lib/hive/lib</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">配置</span><span style="color:#333333;background:#FFFFFF;">hive-site.xml</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">配置的内容与</span><span style="color:#333333;background:#FFFFFF;">spark-defaults.conf</span><span style="color:#333333;background:#FFFFFF;">相同，只是形式不一样</span><span style="color:#333333;background:#FFFFFF;">,</span><span style="color:#333333;background:#FFFFFF;">以下内容是追加到</span><span style="color:#333333;background:#FFFFFF;">hive-site.xml</span><span style="color:#333333;background:#FFFFFF;">文件中的</span><span style="color:#333333;background:#FFFFFF;">,</span><span style="color:#333333;background:#FFFFFF;">并且注意前两个配置，如果不设置</span><span style="color:#333333;background:#FFFFFF;">hive</span><span style="color:#333333;background:#FFFFFF;">的</span><span style="color:#333333;background:#FFFFFF;">spark</span><span style="color:#333333;background:#FFFFFF;">引擎用不了，在后面会有详细的错误说明。</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;hive.execution.engine&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;spark&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;hive.enable.spark.execution.engine&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;true&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.home&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;/root/spark-without-hive&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.master&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;yarn-client&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.enentLog.enabled&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;true&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.enentLog.dir&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;hdfs://Goblin01:8020/spark-log&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.serializer&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;value&gt;org.apache.spark.serializer.KryoSerializer&lt;/value&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.executor.memeory&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;1g&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.driver.memeory&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;1g&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;name&gt;spark.executor.extraJavaOptions&lt;/name&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;"> </span>&lt;value&gt;-XX:+PrintGCDetails -Dkey=value -Dnumbers="one twothree"&lt;/value&gt;</p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">&lt;/property&gt;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">五、验证是否安装配置成功</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">1.</span><span style="color:#333333;background:#FFFFFF;">验证</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">注意：在启动</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群之前，要确保</span><span style="color:#333333;background:#FFFFFF;">Hadoop</span><span style="color:#333333;background:#FFFFFF;">集群和</span><span style="color:#333333;background:#FFFFFF;">YARN</span><span style="color:#333333;background:#FFFFFF;">均已启动</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">进入</span><span style="color:#333333;background:#FFFFFF;">$SPARK_HOME</span><span style="color:#333333;background:#FFFFFF;">目录，执行：</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">./sbin/start-all.sh</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">用</span><span style="color:#333333;background:#FFFFFF;">jps</span><span style="color:#333333;background:#FFFFFF;">命令查看</span><span style="color:#333333;background:#FFFFFF;">51</span><span style="color:#333333;background:#FFFFFF;">节点上的</span><span style="color:#333333;background:#FFFFFF;">master</span><span style="color:#333333;background:#FFFFFF;">和</span><span style="color:#333333;background:#FFFFFF;">worker</span><span style="color:#333333;background:#FFFFFF;">，</span><span style="color:#333333;background:#FFFFFF;">52</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">53</span><span style="color:#333333;background:#FFFFFF;">、</span><span style="color:#333333;background:#FFFFFF;">54</span><span style="color:#333333;background:#FFFFFF;">节点上的</span><span style="color:#333333;background:#FFFFFF;">worker</span><span style="color:#333333;background:#FFFFFF;">是否都启动了</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">同样在</span><span style="color:#333333;background:#FFFFFF;">$SPARK_HOME</span><span style="color:#333333;background:#FFFFFF;">目录下，提交计算</span><span style="color:#333333;background:#FFFFFF;">Pi</span><span style="color:#333333;background:#FFFFFF;">的任务，验证</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群是否能正常工作，运行如下命令</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">./bin/spark-submit --classorg.apache.spark.examples.SparkPi --master yarn --deploy-mode clientlib/spark-examples-1.5.0-cdh5.5.1-hadoop2.6.0.jar 10</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">若无报错，并且算出</span><span style="color:#333333;background:#FFFFFF;">Pi</span><span style="color:#333333;background:#FFFFFF;">的值，说明</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">集群能正常工作</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;background:#FFFFFF;">2.</span><span style="color:#333333;background:#FFFFFF;">验证</span><span style="color:#333333;background:#FFFFFF;">Hiveon Spark</span><span style="color:#333333;background:#FFFFFF;">是否可用</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">命令行输入</span><span style="color:#333333;background:#FFFFFF;">hive</span><span style="color:#333333;background:#FFFFFF;">，进入</span><span style="color:#333333;background:#FFFFFF;">hive
 CLI</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">set hive.execution.engine=spark; (</span><span style="color:#333333;background:#FFFFFF;">将执行引擎设为</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">，默认是</span><span style="color:#333333;background:#FFFFFF;">mr</span><span style="color:#333333;background:#FFFFFF;">，退出</span><span style="color:#333333;background:#FFFFFF;">hive
 CLI</span><span style="color:#333333;background:#FFFFFF;">后，回到默认设置。若想让引擎默认为</span><span style="color:#333333;background:#FFFFFF;">Spark</span><span style="color:#333333;background:#FFFFFF;">，需要在</span><span style="color:#333333;background:#FFFFFF;">hive-site.xml</span><span style="color:#333333;background:#FFFFFF;">里设置）</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">create table test(ts BIGINT,line STRING); (</span><span style="color:#333333;background:#FFFFFF;">创建表）</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">select count(*) from test;</span></p>
<p style="background:#FFFFFF;"><span style="color:#333333;">·        </span><span style="color:#333333;background:#FFFFFF;">若整个过程没有报错，并出现正确结果，则</span><span style="color:#333333;background:#FFFFFF;">Hive on Spark</span><span style="color:#333333;background:#FFFFFF;">配置成功。</span></p>
<p> </p>
<p>全文点击：<a href="http://click.aliyun.com/m/13443/" rel="nofollow">http://click.aliyun.com/m/13443/</a>  
</p>
            </div>
                </div>