---
layout:     post
title:      kafka的安装 （单机）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p><img src="http://p57sa588p.bkt.clouddn.com/fab368b657070f0b6974921bc10811e6.jpg" width="100%"></p>



<h4 id="一环境配置">一、环境配置</h4>

<ul>
<li>kafka 版本 2.11</li>
<li><a href="http://kafka.apache.org/downloads" rel="nofollow">Kafka官网下载地址</a></li>
<li>JDK版本：1.8</li>
<li>Zookeeper</li>
</ul>



<h4 id="二操作过程">二、操作过程</h4>

<p><strong>1、下载Kafka并解压</strong> <br>
下载:</p>



<pre class="prettyprint"><code class=" hljs avrasm">wget http://mirrors<span class="hljs-preprocessor">.hust</span><span class="hljs-preprocessor">.edu</span><span class="hljs-preprocessor">.cn</span>/apache/kafka/<span class="hljs-number">1.1</span><span class="hljs-number">.0</span>/kafka_2<span class="hljs-number">.11</span>-<span class="hljs-number">1.1</span><span class="hljs-number">.0</span><span class="hljs-preprocessor">.tgz</span> </code></pre>

<p>解压：</p>



<pre class="prettyprint"><code class=" hljs autohotkey">tar zxvf kafk<span class="hljs-built_in">a_2</span>.<span class="hljs-number">11</span>-<span class="hljs-number">1.1</span>.<span class="hljs-number">0</span>.tgz </code></pre>

<p><strong>2、Kafka目录介绍</strong> <br>
/bin 操作kafka的可执行脚本，还包含windows下脚本 <br>
/config 配置文件所在目录 <br>
/libs 依赖库目录 <br>
/logs 日志数据目录，目录kafka把server端日志分为5种类型，分为:server,request,state，log-cleaner，controller</p>

<p><strong>3、配置</strong> <br>
先启动本机的zookeeper，这里演示的是单机版的。安装步骤非常简单 <br>
请<a href="https://blog.csdn.net/q282176713/article/details/81113241" rel="nofollow">参考zookeeper安装部署</a></p>

<p>进入kafka安装工程根目录编辑config/server.properties</p>

<p>kafka最为重要三个配置依次为：broker.id、log.dir、zookeeper.connect，kafka server端config/server.properties</p>

<p>参数说明和解释如下:</p>

<p><strong>server.properties配置属性说明</strong></p>

<table cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p><span>参数</span></p>
</td>
<td valign="top">
<p><span>说明</span><span>(</span><span>解释</span><span>)</span></p>
</td>
</tr><tr><td valign="top">
<p><span>broker.id =</span><span>0</span></p>
</td>
<td valign="top">
<p><span>每一个</span><span>broker</span><span>在集群中的唯一表示，要求是正数。当该服务器的</span><span>IP</span><span>地址发生改变时，</span><span>broker.id</span><span>没有变化，则不会影响</span><span>consumers</span><span>的消息情况</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.dirs=/data/kafka-logs</span></p>
</td>
<td valign="top">
<p><span>kafka</span><span>数据的存放地址，多个地址的话用逗号分割,多个目录分布在不同磁盘上可以提高读写性能
  </span><span>/data/kafka-logs-</span><span>1</span><span>，</span><span>/data/kafka-logs-</span><span>2</span></p>
</td>
</tr><tr><td valign="top">
<p><span>port =</span><span>9092</span></p>
</td>
<td valign="top">
<p><span>broker server</span><span>服务端口</span></p>
</td>
</tr><tr><td valign="top">
<p><span>message.max.bytes =</span><span>6525000</span></p>
</td>
<td valign="top">
<p><span>表示消息体的最大大小，单位是字节</span></p>
</td>
</tr><tr><td valign="top">
<p><span>num.network.threads =</span><span>4</span></p>
</td>
<td valign="top">
<p><span>broker</span><span>处理消息的最大线程数，一般情况下数量为cpu核数</span></p>
</td>
</tr><tr><td valign="top">
<p><span>num.io.threads =</span><span>8</span></p>
</td>
<td valign="top">
<p><span>broker</span><span>处理磁盘</span><span>IO</span><span>的线程数</span><span></span><span>，数值为<span>cpu核数2倍</span></span></p>
</td>
</tr><tr><td valign="top">
<p><span>background.threads =</span><span>4</span></p>
</td>
<td valign="top">
<p><span>一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改</span></p>
</td>
</tr><tr><td valign="top">
<p><span>queued.max.requests =</span><span>500</span></p>
</td>
<td valign="top">
<p><span>等待</span><span>IO</span><span>线程处理的请求队列最大数，若是等待</span><span>IO</span><span>的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。</span></p>
</td>
</tr><tr><td valign="top">
<p><span>host.name</span></p>
</td>
<td valign="top">
<p><span>broker</span><span>的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到</span><span>ZK</span><span>，一般不设置</span></p>
</td>
</tr><tr><td valign="top">
<p><span>socket.send.buffer.bytes=</span><span>100</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>socket</span><span>的发送缓冲区，</span><span>socket</span><span>的调优参数</span><span>SO_SNDBUFF</span></p>
</td>
</tr><tr><td valign="top">
<p><span>socket.receive.buffer.bytes =</span><span>100</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>socket</span><span>的接受缓冲区，</span><span>socket</span><span>的调优参数</span><span>SO_RCVBUFF</span></p>
</td>
</tr><tr><td valign="top">
<p><span>socket.request.max.bytes =</span><span>100</span><span></span><span>*</span><span>1024</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>socket</span><span>请求的最大数值，防止</span><span>serverOOM</span><span>，</span><span>message.max.bytes</span><span>必然要小于</span><span>socket.request.max.bytes</span><span>，会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.segment.bytes =</span><span>1024</span><span></span><span>*</span><span>1024</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>topic</span><span>的分区是以一堆</span><span>segment</span><span>文件存储的，这个控制每个</span><span>segment</span><span>的大小，会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.roll.hours =</span><span>24</span><span>*</span><span>7</span></p>
</td>
<td valign="top">
<p><span>这个参数会在日志</span><span>segment</span><span>没有达到</span><span>log.segment.bytes</span><span>设置的大小，也会强制新建一个</span><span>segment</span><span>会被</span><span>
 topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleanup.policy = delete</span></p>
</td>
<td valign="top">
<p><span>日志清理策略</span><span></span><span>选择有：</span><span>delete</span><span>和</span><span>compact</span><span>主要针对过期数据的处理，或是日志文件达到限制的额度，会被</span><span>
 topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.retention.minutes=</span><span>300</span></p>
<p><span>或</span></p>
<p><span>log.retention.hours=24<br></span></p>
</td>
<td valign="top">
<p><span>数据文件保留多长时间， 存储的最大时间</span><span></span><span>超过这个时间</span><span></span><span>会根据</span><span>log.cleanup.policy</span><span>设置数据清除策略</span></p>
<p><span>log.retention.bytes</span><span>和</span><span>log.retention.minutes或<span>log.retention.hours</span></span><span>任意一个达到要求，都会执行删除</span></p>
<p><br></p>
<p>有2删除数据文件方式：</p>
<p>      按照文件大小删除：<span>log.retention.bytes</span></p>
<p><span>  按照2中不同时间粒度删除：分别为分钟，小时</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.retention.bytes=-</span><span>1</span></p>
</td>
<td valign="top">
<p><span>topic</span><span>每个分区的最大文件大小，一个</span><span>topic</span><span>的大小限制</span><span>
 = </span><span>分区数</span><span>*log.retention.bytes</span><span>。</span><span>-</span><span>1</span><span></span><span>没有大小限</span><span>log.retention.bytes</span><span>和</span><span>log.retention.minutes</span><span>任意一个达到要求，都会执行删除，会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.retention.check.interval.ms=</span><span>5</span><span></span><span>minutes</span></p>
</td>
<td valign="top">
<p><span>文件大小检查的周期时间，是否处罚</span><span>
 log.cleanup.policy</span><span>中设置的策略</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.enable=</span><span><strong>false</strong></span></p>
</td>
<td valign="top">
<p><span>是否开启日志<span>清理</span></span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.threads =</span><span>
 2</span></p>
</td>
<td valign="top">
<p><span>日志<span>清理</span>运行的线程数</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.io.max.bytes.per.second=None</span></p>
</td>
<td valign="top">
<p><span>日志<span>清理</span>时候处理的最大大小</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.dedupe.buffer.size=</span><span>500</span><span>*</span><span>1024</span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>日志<span>清理</span>去重时候的缓存空间</span><span></span><span>，在空间允许的情况下，越大越好</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.io.buffer.size=</span><span>512</span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>日志清理时候用到的</span><span>IO</span><span>块大小</span><span></span><span>一般不需要修改</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.io.buffer.load.factor =</span><span>0.9</span></p>
</td>
<td valign="top">
<p><span>日志清理中</span><span>hash</span><span>表的扩大因子</span><span></span><span>一般不需要修改</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.backoff.ms =</span><span>15000</span></p>
</td>
<td valign="top">
<p><span>检查是否处罚日志清理的间隔</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.min.cleanable.ratio=</span><span>0.5</span></p>
</td>
<td valign="top">
<p><span>日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.cleaner.delete.retention.ms =</span><span>1</span><span></span><span>day</span></p>
</td>
<td valign="top">
<p><span>对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同</span><span>log.retention.minutes</span><span>的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.index.size.max.bytes =</span><span>10</span><span></span><span>*</span><span>1024</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>对于</span><span>segment</span><span>日志的索引文件大小限制，会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.index.interval.bytes =</span><span>4096</span></p>
</td>
<td valign="top">
<p><span>当执行一个</span><span>fetch</span><span>操作后，需要一定的空间来扫描最近的</span><span>offset</span><span>大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.flush.interval.messages=None</span></p>
<p><span><span>例如<span>log.flush.interval.messages=1000</span></span></span></p>
<p><span><span><span>表示每当消息记录数达到1000时flush一次数据到磁盘</span></span></span></p>
</td>
<td valign="top">
<p><span>log</span><span>文件</span><span>”sync”</span><span>到磁盘之前累积的消息条数</span><span>,</span><span>因为磁盘</span><span>IO</span><span>操作是一个慢操作</span><span>,</span><span>但又是一个</span><span>”</span><span>数据可靠性</span><span>”</span><span>的必要手段</span><span>,</span><span>所以此参数的设置</span><span>,</span><span>需要在</span><span><strong>“</strong></span><span><strong>数据可靠性</strong></span><span><strong>“</strong></span><span>与</span><span>”</span><span>性能</span><span>”</span><span>之间做必要的权衡</span><span>.</span><span>如果此值过大</span><span>,</span><span>将会导致每次</span><span>“fsync”</span><span>的时间较长</span><span>(IO</span><span>阻塞</span><span>)</span><span>,</span><span>如果此值过小</span><span>,</span><span>将会导致</span><span><strong>“fsync”</strong></span><span>的次数较多</span><span>,</span><span>这也意味着整体的</span><span>client</span><span>请求有一定的延迟</span><span>.</span><span>物理</span><span>server</span><span>故障</span><span>,</span><span>将会导致没有</span><span>fsync</span><span>的消息丢失</span><span>.</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.flush.scheduler.interval.ms =</span><span>3000</span></p>
</td>
<td valign="top">
<p><span>检查是否需要固化到硬盘的时间间隔</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.flush.interval.ms = None</span></p>
<p><span><span>例如：<span>log.flush.interval.ms=1000</span></span></span></p>
<p><span><span><span>表示每间隔1000毫秒<span>flush一次数据到磁盘</span></span></span></span></p>
</td>
<td valign="top">
<p><span>仅仅通过</span><span>interval</span><span>来控制消息的磁盘写入时机</span><span>,</span><span>是不足的</span><span>.</span><span>此参数用于控制</span><span><strong>“fsync”</strong></span><span>的时间间隔</span><span>,</span><span>如果消息量始终没有达到阀值</span><span>,</span><span>但是离上一次磁盘同步的时间间隔达到阀值</span><span>,</span><span>也将触发</span><span>.</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.delete.delay.ms =</span><span>60000</span></p>
</td>
<td valign="top">
<p><span>文件在索引中清除后保留的时间</span><span></span><span>一般不需要去修改</span></p>
</td>
</tr><tr><td valign="top">
<p><span>log.flush.offset.checkpoint.interval.ms =</span><span>60000</span></p>
</td>
<td valign="top">
<p><span>控制上次固化硬盘的时间点，以便于数据恢复</span><span></span><span>一般不需要去修改</span></p>
</td>
</tr><tr><td valign="top">
<p><span>auto.create.topics.enable =</span><span><strong>true</strong></span></p>
</td>
<td valign="top">
<p><span>是否允许自动创建</span><span>topic</span><span>，若是</span><span><strong>false</strong></span><span>，就需要通过命令创建</span><span>topic</span></p>
</td>
</tr><tr><td valign="top">
<p><span><strong>default</strong></span><span>.replication.factor
 =</span><span>1</span></p>
</td>
<td valign="top">
<p><span>是否允许自动创建</span><span>topic</span><span>，若是</span><span><strong>false</strong></span><span>，就需要通过命令创建</span><span>topic</span></p>
</td>
</tr><tr><td valign="top">
<p><span>num.partitions =</span><span>1</span></p>
</td>
<td valign="top">
<p><span>每个</span><span>topic</span><span>的分区个数，若是在</span><span>topic</span><span>创建时候没有指定的话</span><span></span><span>会被</span><span>topic</span><span>创建时的指定参数覆盖</span></p>
</td>
</tr><tr><td valign="top">
<p>
<br></p>
</td>
<td valign="top">
<p>
<br></p>
</td>
</tr><tr><td valign="top">
<p><span>以下是</span><span>kafka</span><span>中</span><span>Leader,replicas</span><span>配置参数</span></p>
</td>
<td valign="top">
<p>
<br></p>
</td>
</tr><tr><td valign="top">
<p><span>controller.socket.timeout.ms =</span><span>30000</span></p>
</td>
<td valign="top">
<p><span>partition leader</span><span>与</span><span>replicas</span><span>之间通讯时</span><span>,socket</span><span>的超时时间</span></p>
</td>
</tr><tr><td valign="top">
<p><span>controller.message.queue.size=</span><span>10</span></p>
</td>
<td valign="top">
<p><span>partition leader</span><span>与</span><span>replicas</span><span>数据同步时</span><span>,</span><span>消息的队列尺寸</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.lag.time.max.ms =</span><span>10000</span></p>
</td>
<td valign="top">
<p><span>replicas</span><span>响应</span><span>partition leader</span><span>的最长等待时间，若是超过这个时间，就将</span><span>replicas</span><span>列入</span><span>ISR(in-sync
 replicas)</span><span>，并认为它是死的，不会再加入管理中</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.lag.max.messages =</span><span>4000</span></p>
</td>
<td valign="top">
<p><span>如果</span><span>follower</span><span>落后与</span><span>leader</span><span>太多</span><span>,</span><span>将会认为此</span><span>follower[</span><span>或者说</span><span>partition relicas]</span><span>已经失效</span></p>
<p><span>##</span><span>通常</span><span>,</span><span>在</span><span>follower</span><span>与</span><span>leader</span><span>通讯时</span><span>,</span><span>因为网络延迟或者链接断开</span><span>,</span><span>总会导致</span><span>replicas</span><span>中消息同步滞后</span></p>
<p><span>##</span><span>如果消息之后太多</span><span>,leader</span><span>将认为此</span><span>follower</span><span>网络延迟较大或者消息吞吐能力有限</span><span>,</span><span>将会把此</span><span>replicas</span><span>迁移</span></p>
<p><span>##</span><span>到其他</span><span>follower</span><span>中</span><span>.</span></p>
<p><span>##</span><span>在</span><span>broker</span><span>数量较少</span><span>,</span><span>或者网络不足的环境中</span><span>,</span><span>建议提高此值</span><span>.</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.socket.timeout.ms=</span><span>30</span><span></span><span>*</span><span>1000</span></p>
</td>
<td valign="top">
<p><span>follower</span><span>与</span><span>leader</span><span>之间的</span><span>socket</span><span>超时时间</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.socket.receive.buffer.bytes=</span><span>64</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>leader</span><span>复制时候的</span><span>socket</span><span>缓存大小</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.fetch.max.bytes =</span><span>1024</span><span></span><span>*</span><span>1024</span></p>
</td>
<td valign="top">
<p><span>replicas</span><span>每次获取数据的最大大小</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.fetch.wait.max.ms =</span><span>500</span></p>
</td>
<td valign="top">
<p><span>replicas</span><span>同</span><span>leader</span><span>之间通信的最大等待时间，失败了会重试</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.fetch.min.bytes =</span><span>1</span></p>
</td>
<td valign="top">
<p><span>fetch</span><span>的最小数据尺寸</span><span>,</span><span>如果</span><span>leader</span><span>中尚未同步的数据不足此值</span><span>,</span><span>将会阻塞</span><span>,</span><span>直到满足条件</span></p>
</td>
</tr><tr><td valign="top">
<p><span>num.replica.fetchers=</span><span>1</span></p>
</td>
<td valign="top">
<p><span>leader</span><span>进行复制的线程数，增大这个数值会增加</span><span>follower</span><span>的</span><span>IO</span></p>
</td>
</tr><tr><td valign="top">
<p><span>replica.high.watermark.checkpoint.interval.ms =</span><span>5000</span></p>
</td>
<td valign="top">
<p><span>每个</span><span>replica</span><span>检查是否将最高水位进行固化的频率</span></p>
</td>
</tr><tr><td valign="top">
<p><span>controlled.shutdown.enable =</span><span><strong>false</strong></span></p>
</td>
<td valign="top">
<p><span>是否允许控制器关闭</span><span>broker
 ,</span><span>若是设置为</span><span><strong>true</strong></span><span>,</span><span>会关闭所有在这个</span><span>broker</span><span>上的</span><span>leader</span><span>，并转移到其他</span><span>broker</span></p>
</td>
</tr><tr><td valign="top">
<p><span>controlled.shutdown.max.retries =</span><span>3</span></p>
</td>
<td valign="top">
<p><span>控制器关闭的尝试次数</span></p>
</td>
</tr><tr><td valign="top">
<p><span>controlled.shutdown.retry.backoff.ms =</span><span>5000</span></p>
</td>
<td valign="top">
<p><span>每次关闭尝试的时间间隔</span></p>
</td>
</tr><tr><td valign="top">
<p><span>leader.imbalance.per.broker.percentage =</span><span>10</span></p>
</td>
<td valign="top">
<p><span>leader</span><span>的不平衡比例，若是超过这个数值，会对分区进行重新的平衡</span></p>
</td>
</tr><tr><td valign="top">
<p><span>leader.imbalance.check.interval.seconds =</span><span>300</span></p>
</td>
<td valign="top">
<p><span>检查</span><span>leader</span><span>是否不平衡的时间间隔</span></p>
</td>
</tr><tr><td valign="top">
<p><span>offset.metadata.max.bytes</span></p>
</td>
<td valign="top">
<p><span>客户端保留</span><span>offset</span><span>信息的最大空间大小</span></p>
</td>
</tr><tr><td valign="top">
<p><span>kafka</span><span>中</span><span>zookeeper</span><span>参数配置</span></p>
</td>
<td valign="top">
<p>
<br></p>
</td>
</tr><tr><td valign="top">
<p><span>zookeeper.connect = localhost:</span><span>2181</span></p>
</td>
<td valign="top">
<p><span>zookeeper</span><span>集群的地址，可以是多个，多个之间用逗号分割</span><span>
 hostname1:port1,hostname2:port2,hostname3:port3</span></p>
</td>
</tr><tr><td valign="top">
<p><span>zookeeper.session.timeout.ms=</span><span>6000</span></p>
</td>
<td valign="top">
<p><span>ZooKeeper</span><span>的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大</span></p>
</td>
</tr><tr><td valign="top">
<p><span>zookeeper.connection.timeout.ms =</span><span>6000</span></p>
</td>
<td valign="top">
<p><span>ZooKeeper</span><span>的连接超时时间</span></p>
</td>
</tr><tr><td valign="top">
<p><span>zookeeper.sync.time.ms =</span><span>2000</span></p>
</td>
<td valign="top">
<p><span>ZooKeeper</span><span>集群中</span><span>leader</span><span>和</span><span>follower</span><span>之间的同步实际那</span></p>
</td>
</tr></tbody></table>

<p><strong>4、启动Kafka</strong></p>

<p>启动</p>

<p>进入kafka目录，输入命令 </p>



<pre class="prettyprint"><code class=" hljs axapta">bin/kafka-<span class="hljs-keyword">server</span>-start.sh config/<span class="hljs-keyword">server</span>.properties &amp;</code></pre>

<p>检测2181与9092端口</p>



<pre class="prettyprint"><code class=" hljs 1c">netstat -tunlp<span class="hljs-string">|egrep "</span>(<span class="hljs-number">2181</span><span class="hljs-string">|9092)"</span></code></pre>



<pre class="prettyprint"><code class=" hljs ruby">tcp        <span class="hljs-number">0</span>      <span class="hljs-number">0</span> <span class="hljs-symbol">:</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:</span><span class="hljs-number">2181</span>                     <span class="hljs-symbol">:</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:*</span>                        <span class="hljs-constant">LISTEN</span>      <span class="hljs-number">19787</span>/java          
tcp        <span class="hljs-number">0</span>      <span class="hljs-number">0</span> <span class="hljs-symbol">:</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:</span><span class="hljs-number">9092</span>                     <span class="hljs-symbol">:</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:*</span>                        <span class="hljs-constant">LISTEN</span>      <span class="hljs-number">28094</span>/java </code></pre>

<p>Kafka的进程ID为28094，占用端口为9092</p>

<p>QuorumPeerMain为对应的zookeeper实例，进程ID为19787，在2181端口监听</p>

<p><strong>5、单机连通性测试</strong> <br>
启动2个XSHELL客户端，一个用于生产者发送消息，一个用于消费者接受消息。</p>

<p>运行producer，随机敲入几个字符，相当于把这个敲入的字符消息发送给队列。</p>



<pre class="prettyprint"><code class=" hljs lasso">bin/kafka<span class="hljs-attribute">-console</span><span class="hljs-attribute">-producer</span><span class="hljs-built_in">.</span>sh <span class="hljs-subst">--</span>broker<span class="hljs-attribute">-list</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">9092</span> <span class="hljs-subst">--</span>topic test</code></pre>

<p>说明：早版本的Kafka，–broker-list 127.0.0.1:9092需改为–zookeeper 127.0.0.1.181:2181</p>

<p>运行consumer，可以看到刚才发送的消息列表。</p>

<pre class="prettyprint"><code class=" hljs brainfuck"><span class="hljs-comment">bin/kafka</span><span class="hljs-literal">-</span><span class="hljs-comment">console</span><span class="hljs-literal">-</span><span class="hljs-comment">consumer</span><span class="hljs-string">.</span><span class="hljs-comment">sh</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">zookeeper</span> <span class="hljs-comment">127</span><span class="hljs-string">.</span><span class="hljs-comment">0</span><span class="hljs-string">.</span><span class="hljs-comment">0</span><span class="hljs-string">.</span><span class="hljs-comment">1:2181</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">topic</span> <span class="hljs-comment">test</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">from</span><span class="hljs-literal">-</span><span class="hljs-comment">beginning</span> </code></pre>

<p>演示结果 <br>
发送: <br>
<img src="http://p57sa588p.bkt.clouddn.com/15319725788188.jpg" alt="" title=""> <br>
接受: <br>
<img src="http://p57sa588p.bkt.clouddn.com/15319726091244.jpg" alt="" title=""></p>

<p><strong>注意</strong>：</p>

<p>producer，指定的Socket(127.0.0.1+9092),说明生产者的消息要发往kafka，也即是broker</p>

<p>consumer, 指定的Socket(127.0.0.1+2181),说明消费者的消息来自zookeeper（协调转发）</p>

<p>上面的只是一个单个的broker，下面我们来实验一个多broker的集群。</p>

<p>参考：<a href="https://www.cnblogs.com/justuntil/p/8033792.html" rel="nofollow">https://www.cnblogs.com/justuntil/p/8033792.html</a></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>