---
layout:     post
title:      Hive与ETL基础—学习笔记[3]
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/zwl_123/article/details/52239461				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="hive与etl基础"><strong>Hive与ETL基础</strong></h1>

<p><strong>1、日志收集</strong> <br>
<strong>2、Hive数据类型</strong> <br>
<strong>3、Hive表与分区</strong> <br>
<strong>4、Hive基本操作</strong> <br>
<strong>5、Hive常用函数</strong> <br>
<strong>6、HDFS文件格式</strong> <br>
<strong>7、Hive表压缩存储</strong> <br>
<strong>8、ORCFile</strong> <br>
<strong>9、Hive SerDe</strong></p>

<hr>

<p><font color="red" size="5">1、Flume：日志收集</font> <br>
<img src="https://img-blog.csdn.net/20160818145419932" alt="这里写图片描述" title=""> <br>
<font size="3"><strong>常见的收集日志的工具有两种：（1）最早收集日志的是Facebook开发的 Scribe。开源。（2）Flume更加完善如上图所示。HDFS批处理。Kafka是实时性处理，消息队列。Flume_bypass主要是帮我们做测试，查看日志是否打上。</strong></font></p>

<hr>

<p><font color="red" size="5">2、Hive数据类型</font> <br>
<font size="3">（1）当日志收集到HDFS上，实际上是使用Hive来做相关分析。对海量原始数据的操作的是ETL。那么Hive可用于ETL。 <br>
（2）Hive本质上是基于SQL的查询引擎。 <br>
（3）对于列的数据有原始数据类型：Tinyint、smallint、bigint、boolean、float、double、string、timestamp、、、 <br>
（4）数据库和数据仓库查询中数据类型：Struct/map/array <br>
（5）分隔符：\n，行分隔符；\A，列分隔符；\B,array、Struct分隔符；\C,Map的key和value分隔符。</font></p>

<hr>

<p><font color="red" size="5">3、Hive-table、Partition、Buckets和External table</font> <br>
<font size="3">（1）Hive中的table与数据库中table一样，每一个table在Hive中都对应一个目录来存储数据。</font></p>

<p><font size="3">（2）Hive中的partition与Partition列的密集索引。表中的一个Partition对应于表下的一个目录。</font></p>

<p><font size="3">（3）Hive中的Buckets与对指定列计算hash，根据hash值切分数据，目的是为了并行，每一个Bucket对一个文件。</font></p>

<p><font size="3">（4）External table指想已存在HDFS中存在的数据，创建Partition。</font></p>

<p><font size="3">（5）External table权限不够使用数据。</font></p>

<hr>

<p><font color="red" size="5">4、Hive-table 创建</font> <br>
与传统的sql创建表格式差不多，但要指定格式，比如行、列分隔符等。</p>

<hr>

<p><font color="red" size="5">5、Hive record的存储（JSON）</font> <br>
与传统的sql创建表格式差不多，但要指定格式，比如行、列分隔符等。</p>

<hr>

<p><font color="red" size="5">6、Hive 外部表</font> <br>
只删除元数据。</p>

<hr>

<p><font color="red" size="5">7、分区、导入数据</font> <br>
Partition。</p>

<hr>

<p><font color="red" size="5">8、Hive DML（数据操纵语言）</font> <br>
（1）Select  … from <br>
（2）where <br>
（3）group by <br>
（4）join <br>
（5）order by &amp; sort by <br>
（6）distribute by with sort by <br>
（7）cluster by <br>
（8）cast <br>
（9）union all <br>
（10）sample data：采样，对也很了解。</p>

<hr>

<p><font color="red" size="5">9、HDFS文件格式</font> <br>
（1）TextFile（XML/JSON）：解析开销大，不具备类型和模式； <br>
（2）SequenceFile：</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>