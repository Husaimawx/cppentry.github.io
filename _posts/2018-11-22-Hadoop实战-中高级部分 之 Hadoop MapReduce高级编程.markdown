---
layout:     post
title:      Hadoop实战-中高级部分 之 Hadoop MapReduce高级编程
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p align="center" style="background:#FAFAFA;"><strong>Hadoop实战-中高级部分 之 Hadoop MapReduce高级编程</strong></p>
<p align="left"><strong> </strong></p>
<p align="left"><strong>Hadoop RestFul</strong></p>
<p align="left"><strong>Hadoop HDFS原理1</strong></p>
<p align="left"><strong>Hadoop HDFS原理2</strong></p>
<p align="left"><strong>Hadoop作业调优参数调整及原理</strong></p>
<p align="left"><strong>Hadoop HA</strong></p>
<p align="left"><strong>Hadoop MapReduce高级编程</strong></p>
<p align="left"><strong>Hadoop IO</strong></p>
<p align="left"><strong>Hadoop MapReduce工作原理</strong></p>
<p align="left"><strong>Hadoop 管理</strong></p>
<p align="left"><strong>Hadoop 集群安装</strong></p>
<p align="left"><strong>Hadoop RPC</strong></p>
<p align="left"><strong> </strong></p>
<p align="left"><strong>第一部分：重要的组件</strong></p>
<p align="left"><strong>Combiner</strong></p>
<p align="left">•什么是Combiner</p>
<p align="left">•combine函数把一个map函数产生的&lt;key,value&gt;对（多个key, value）合并成一个新的&lt;key2,value2&gt;. 将新的&lt;key2,value2&gt;作为输入到reduce函数中，其格式与reduce函数相同。</p>
<p align="left">•这样可以有效的较少中间结果，减少网络传输负荷。</p>
<p align="left"> </p>
<p align="left">•什么情况下可以使用Combiner</p>
<p align="left">•可以对记录进行汇总统计的场景，如求和。</p>
<p align="left">•求平均数的场景就不可以使用了</p>
<p align="left"><strong>Combiner执行时机</strong></p>
<p align="left">•运行combiner函数的时机有可能会是merge完成之前，或者之后，这个时机可以由一个参数控制，即 <strong>min.num.spill.for.combine</strong>（default 3）</p>
<p align="left">•当job中设定了combiner，并且spill数最少有3个的时候，那么combiner函数就会在merge产生结果文件之前运行</p>
<p align="left">•通过这样的方式，就可以在spill非常多需要merge，并且很多数据需要做conbine的时候，减少写入到磁盘文件的数据数量，同样是为了减少对磁盘的读写频率，有可能达到优化作业的目的。</p>
<p align="left">•Combiner也有可能不执行， Combiner会考虑当时集群的负载情况。</p>
<p align="left"><strong>Combiner如何使用</strong></p>
<p align="left">•代码示例</p>
<p align="left">•继承Reducer类</p>
<p align="left">public static class Combiner extendsMapReduceBase implements</p>
<p align="left">          Reducer&lt;Text, Text, Text, Text&gt; {</p>
<p align="left">      public void reduce(Text key, Iterator&lt;Text&gt; values,</p>
<p align="left">              OutputCollector&lt;Text, Text&gt; output, Reporter reporter)</p>
<p align="left">              throws IOException {</p>
<p align="left">                }</p>
<p align="left">    }</p>
<p align="left"> </p>
<p align="left">•配置作业时加入conf.setCombinerClass(Combiner.class)</p>
<p align="left"> </p>
<p align="left"><strong>Partitioner</strong></p>
<p align="left">•什么是Partitioner</p>
<p align="left">•Mapreduce 通过Partitioner 对Key 进行分区，进而把数据按我们自己的需求来分发。</p>
<p align="left">•什么情况下使用Partitioner</p>
<p align="left">•如果你需要key按照自己意愿分发，那么你需要这样的组件。</p>
<p align="left">•例如：数据文件内包含省份，而输出要求每个省份输出一个文件。</p>
<p align="left">•框架默认的HashPartitioner</p>
<p align="left">•public class HashPartitioner&lt;K, V&gt;extends Partitioner&lt;K, V&gt; {  <br><br>
  /** Use {@link Object#hashCode()} to partition. */  <br>
  public int getPartition(K key, V value,  <br>
                         int numReduceTasks) {  <br>
    return (key.hashCode() &amp; Integer.MAX_VALUE) %numReduceTasks;  <br>
  } <br>
} </p>
<p align="left"><strong>Partitioner如何使用</strong></p>
<p align="left">•实现Partitioner接口覆盖getPartition()方法</p>
<p align="left">•配置作业时加入conf.setPartitionerClass(MyPartitioner.class);</p>
<p align="left">•Partitioner示例</p>
<p align="left">       public static class MyPartitioner implements Partitioner&lt;Text, Text&gt; {</p>
<p align="left">           </p>
<p align="left">        @Override <br>
            public intgetPartition(Text key, Text value, int numPartitions) {</p>
<p align="left">            }</p>
<p align="left"> </p>
<p align="left">}</p>
<p align="left"><strong>Partitioner需求示例</strong></p>
<p align="left">•需求描述</p>
<p align="left">•数据文件中含有省份</p>
<p align="left">•需要相同的省份送到相同的Reduce里</p>
<p align="left">•从而产生不同的文件</p>
<p align="left">•数据样例</p>
<p align="left">•1 liaoning</p>
<p align="left">•1 代表该省份有多少个直辖市</p>
<p align="left">•步骤</p>
<p align="left">•实现Partitioner，覆盖getPartition</p>
<p align="left">•根据省份字段进行切分</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"><strong>RecordReader</strong></p>
<p align="left">•什么是RecordReader</p>
<p align="left">•用于在分块中读取&lt;Key,Value&gt;对，也就是说每一次我们读取一条记录都会调用该类。</p>
<p align="left">•主要是处理经过InputFormat分片完的数据 </p>
<p align="left">•什么时候使用RecordReader</p>
<p align="left">•需要对输入的数据按自己的需求处理</p>
<p align="left">•如：要求输入的key不是文件的偏移量而是文件的路径或者名字</p>
<p align="left">•系统默认为LineRecordReader</p>
<p align="left">•按照每行的偏移量做为map输出时的key值，每行的内容作为map的value值，默认的分隔符是回车和换行。</p>
<p align="left"> </p>
<p align="left"><strong>RecordReader需求示例</strong></p>
<p align="left">•需求</p>
<p align="left">•更改map对应的输入的&lt;key,value&gt;值，key对应的文件的路径（或者是文件名），value对应的是文件的内容（content）。</p>
<p align="left">•步骤</p>
<p align="left">•重写InputFormat不对文件切分</p>
<p align="left">•重写RecordReader</p>
<p align="left">•在配置作业时使用自定义的组件进行数据处理</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"><strong>第二部分：Join</strong></p>
<p align="left"><strong>案例分析</strong></p>
<p align="left">•输入为2个文件，文件一内容如下</p>
<p align="left">•空格分割：用户名 手机号 年龄</p>
<p align="left">•内容样例</p>
<p align="left">•Tom 1314567890 14</p>
<p align="left">•文件二内容</p>
<p align="left">•空格分割：手机号 地市</p>
<p align="left">•内容样例</p>
<p align="left">•13124567890 hubei</p>
<p align="left">•需要统计出的汇总信息为 用户名 手机号 年龄 地市</p>
<p align="left"><strong>Map端Join</strong></p>
<p align="left">•设计思路</p>
<p align="left">•使用DistributedCache.addCacheFile()将地市的文件加入到所有Map的缓存里</p>
<p align="left">•在Map函数里读取该文件，进行Join</p>
<p align="left">•  将结果输出到reduce</p>
<p align="left">•需要注意的是</p>
<p align="left">•DistributedCache需要在生成Job作业前使用</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"><strong>Reduce端Join</strong></p>
<p align="left">•设计思路</p>
<p align="left">•Map端读取所有文件，并在输出的内容里加上标识代表数据时从哪个文件里来的</p>
<p align="left">•在reduce对按照标识对数据进行保存</p>
<p align="left">•然后根据Key的Join来求出结果直接输出</p>
<p align="left"> </p>
<p align="left"><strong>第三部分：排序</strong></p>
<p align="left"> </p>
<p align="left"><strong>普通排序</strong></p>
<p align="left">•Mapreduce本身自带排序功能</p>
<p align="left">•Text对象是不适合排序的，如果内容为整型不会安照编码顺序去排序</p>
<p align="left">•一般情况下我们可以考虑以IntWritable做为Key,同时将Reduce设置成0 ,进行排序</p>
<p align="left"> </p>
<p align="left"><strong>部分排序</strong></p>
<p align="left">•即输出的每个文件都是排过序的</p>
<p align="left">•如果我们不需要全局排序，那么这是个不错的选择。</p>
<p align="left"> </p>
<p align="left"><strong>全局排序</strong></p>
<p align="left">•产生背景</p>
<p align="left">•Hadoop平台没有提供全局数据排序，而在大规模数据处理中进行数据的全局排序是非常普遍的需求。</p>
<p align="left">•使用hadoop进行大量的数据排序排序最直观的方法是把文件所有内容给map之后，map不做任何处理，直接输出给一个reduce，利用hadoop的自己的shuffle机制，对所有数据进行排序，而后由reduce直接输出。</p>
<p align="left">•快速排序基本步骤就是需要现在所有数据中选取一个作为支点。然后将大于这个支点的放在一边，小于这个支点的放在另一边。</p>
<p align="left"> </p>
<p align="left">设想如果我们有 N 个支点（这里可以称为标尺），就可以把所有的数据分成 N+1 个 part ，将这 N+1 个 part 丢给 reduce，由 hadoop 自动排序，最后输出 N+1 个内部有序的文件，再把这 N+1 个文件首尾相连合并成一个文件，收工 。</p>
<p align="left">由此我们可以归纳出这样一个用 hadoop 对大量数据排序的步骤：</p>
<p align="left">1 ）   对待排序数据进行抽样；</p>
<p align="left">2 ）   对抽样数据进行排序，产生标尺；</p>
<p align="left">3 ）   Map 对输入的每条数据计算其处于哪两个标尺之间；将数据发给对应区间 ID 的 reduce</p>
<p align="left">4 ）   Reduce 将获得数据直接输出。</p>
<p align="left">•Hadoop 提供了Sampler接口可以返回一组样本，该接口为Hadoop的采样器。</p>
<p align="left">          public interface Sampler&lt;K, V&gt; {</p>
<p align="left">                       K[] getSample(InputFormat&lt;K, V&gt; inf, Job job)</p>
<p align="left">                        throws IOException, InterruptedException;</p>
<p align="left">           }</p>
<p align="left">•Hadoop提供了一个TotalOrderPartitioner，可以使我们来实现全局排序。</p>
<p align="left"><strong>二次排序</strong></p>
<p align="left">•产生背景</p>
<p align="left">•MapReduce默认会对key进行排序</p>
<p align="left">•将输出到Reduce的values也进行预先的排序</p>
<p align="left">•实现方式</p>
<p align="left">•重写Partitioner，完成key分区，进行第一次排序</p>
<p align="left">•实现WritableComparator，完成自己的排序逻辑，完成key的第2次排序</p>
<p align="left">•原理</p>
<p align="left">•Map之前的数据</p>
<p align="left">        key1  1</p>
<p align="left">        key2  2</p>
<p align="left">        key2  3</p>
<p align="left">        key3  4</p>
<p align="left">        key1  2</p>
<p align="left">•Mapduce只能排序key,所以为了二次排序我们要重新定义自己的key 简单说来就是&lt;key value&gt;value ,组合完后</p>
<p align="left">        &lt;key1  1 &gt;    1</p>
<p align="left">        &lt;key2  2 &gt;    2</p>
<p align="left">        &lt;key2  3 &gt;    3</p>
<p align="left">        &lt;key3  4&gt;     4</p>
<p align="left">        &lt;key1  2 &gt;    2</p>
<p align="left"> </p>
<p align="left">•原理</p>
<p align="left">•接下来实现自定义的排序类，分组类，数据变成</p>
<p align="left">        &lt;key1  1 &gt;    1</p>
<p align="left">        &lt;key1  2 &gt;    2</p>
<p align="left">        &lt;key2  2 &gt;    2</p>
<p align="left">        &lt;key2  3 &gt;    3</p>
<p align="left">        &lt;key3  4&gt;     4</p>
<p align="left">•最后 reduce处理后输出结果</p>
<p align="left">          key1  1</p>
<p align="left">          key1  2</p>
<p align="left">          key2  2</p>
<p align="left">          key2  3</p>
<p align="left">          key3  4</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"><strong>第四部分：计数器</strong></p>
<p align="left">•什么是计数器</p>
<p align="left">           计数器主要用来收集系统信息和作业运行信息，用于知道作业成功、失败等情况，比日志更便利进行分析。</p>
<p align="left">•内置计数器</p>
<p align="left">•Hadoop内置的计数器，记录作业执行情况和记录情况。包括MapReduce框架、文件系统、作业计数三大类。</p>
<p align="left">•计数器由关联任务维护，定期传递给tasktracker，再由tasktracker传给jobtracker。</p>
<p align="left">•计数器可以被全局聚集。内置的作业计数器实际上由jobtracker维护，不必在整个网络中传递。</p>
<p align="left">•当一个作业执行成功后，计数器的值才是完整可靠的。</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"><strong>用户自定义Java计数器</strong></p>
<p align="left">•MapReduce框架允许用户自定义计数器</p>
<p align="left">•计数器是全局使用的</p>
<p align="left">•计数器有组的概念，可以由一个Java枚举类型来定义</p>
<p align="left">•如何配置</p>
<p align="left">•0.20.2以下的版本使用Reporter,</p>
<p align="left">•0.20.2以上的版本使用context.getCounter(groupName,counterName) 来获取计数器配置并设置。</p>
<p align="left">•动态计数器</p>
<p align="left">•所谓动态计数器即不采用Java枚举的方式来定义</p>
<p align="left"> </p>
<p align="left">•Reporter中的获取动态计数器的方法</p>
<p align="left">•public void incrCounter(Stringgroup,String counter,long amount)</p>
<p align="left">           组名称，计数器名称，计数值</p>
<p align="left"> </p>
<p align="left">•一些原则</p>
<p align="left">•创建计数器时，尽量让名称易读</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left">•获取计数器</p>
<p align="left">•Web UI</p>
<p align="left">•命令行 hadoop job-counter</p>
<p align="left">•Java API</p>
<p align="left">•Java API</p>
<p align="left">•在作业运行完成后，计数器稳定后获取。 使用job.getCounters()得到Counters</p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"> </p>
<p align="left"><strong>第五部分：合并小文件示例</strong></p>
<p align="left">•产生背景</p>
<p align="left">•Hadoop不适合处理小文件</p>
<p align="left">•会占用大量的内存空间</p>
<p align="left">•解决方案</p>
<p align="left">•文件内容读取到SequenceFile内</p>
<p align="left"> </p>
<p> </p>
            </div>
                </div>