---
layout:     post
title:      Spark伪分布式搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>Spark分布式搭建</p>
<p>利用spark（2.1.1）和hadoop（2.8.0）最新版</p>
<p>一、下载spark</p>
<p>Spark地址：版本</p>
<p><a href="https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz" rel="nofollow">https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz</a></p>
<p>二、下载scala</p>
<p>此处scala版本要注意scala版本，spark有版本限制</p>
<p><img src="https://img-blog.csdn.net/20170705213644305?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY3lzc3h0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p>这边可以看到用了scala2.11的大版本，这边小版本随意，就下载2.11.11版本的scala</p>
<p>下载地址：<a href="https://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz" rel="nofollow">https://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz</a></p>
<p>三、下载hadoop（因为大数据生态圈离不开hdfs）</p>
<p>这边我们下载的spark依赖hadoop 2.7+</p>
<p>下载地址：<a href="http://apache.fayea.com/hadoop/common/hadoop-2.8.0/hadoop-2.8.0.tar.gz" rel="nofollow">http://apache.fayea.com/hadoop/common/hadoop-2.8.0/hadoop-2.8.0.tar.gz</a></p>
<p>四、下载jdk</p>
<p>Spark2.11.11以来jdk+</p>
<p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" rel="nofollow">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>
<p>五、解压各个项目，配置环境变量</p>
<p>vim /etc/profile</p>
<p>SPARK_HOME=/home/XXXX/software/spark-2.1.1-bin-hadoop2.7</p>
<p>SCALA_HOME=/usr/scala/scala-2.11.11</p>
<p>JAVA_HOME=/usr/java/jdk1.8.0_131</p>
<p>PATH=$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:$JAVA_HOME/bin:$SCALA_HOME/bin:$HADOOP_HOME/bin:$PARK_HOME/bin:$SPARK/sbin</p>
<p>在文件开头输入</p>
<p>由于此配置文件重启生效，为了不重启，我们可以输入如下命令：</p>
<p>source /etc/profile</p>
<p>六、配置spark</p>
<p>进入spark路径</p>
<p>cd  /home/XXXX /software/spark-2.1.1-bin-hadoop2.7/conf</p>
<p>cp spark-env.sh.template spark-env.sh</p>
<p>vim spark-env.sh</p>
<p>增加如下配置</p>
<p>·</p>
<p>export SPARK_MASTER_IP=192.168.126.128</p>
<p>cp spark-defaults.conf.template spark-defaults.conf</p>
<p>增加如下配置：</p>
<p>spark.master.ip                  192.168.126.128#本机ip</p>
<p>spark.master                     spark://192.168.126.128:7077</p>
<p>spark.driver.bindAddress         192.168.126.128</p>
<p>spark.driver.host                192.168.126.128</p>
<p>cp slaves.template slaves</p>
<p>vim slaves</p>
<p>增加如下配置</p>
<p>192.168.126.128 #设置本地ip，即为伪分布式</p>
<p>五、配置ssh免登录</p>
<p>ssh-keygen -t rsa</p>
<p>回车</p>
<p> cat id_rsa.pub &gt;&gt; .ssh/authorized_keys</p>
<p> chmod 700 .ssh/</p>
<p> chmod 600 .ssh/authorized_keys</p>
<p>这样就能面密码登陆了</p>
<p>六、启动spark</p>
<p>cd  /home/XXXX /software/spark-2.1.1-bin-hadoop2.7/sbin</p>
<p>./start-all.sh#启动spark</p>
<p>七、访问<a href="http://192.168.126.128:8080/" rel="nofollow">http://192.168.126.128:8080/</a></p>
<p><img src="https://img-blog.csdn.net/20170705213721311?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY3lzc3h0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p></p>
            </div>
                </div>