---
layout:     post
title:      Spark学习笔记——1
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>一、Spark简介<br>
1、Spark：完全基于Hadoop的一种计算框架<br>
<img src="https://img-blog.csdnimg.cn/20181120114910881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0VmZml5YQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
2、计算框架——》适用场景：<br>
Spark RDD——》离线批处理<br>
Spark Core——》流式计算<br>
Spark SQL——》交互式查询（基于Hive）<br>
Spark Streaming——》实时流式计算<br>
Spark MLlib——》机器学习<br>
Spark GraphX——》用于图计算<br>
3、Spark用于计算，Hadoop生态系统用于存储（HBase，HDFS，Hive）、资源调度（Yarn）<br>
4、<mark>Spark基于内存来计算，是MapReduce的数倍，即使是从磁盘读取计算，Spark计算速度也优于后者</mark><br>
<img src="https://img-blog.csdnimg.cn/20181120132913847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0VmZml5YQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
对于数据量巨大的数据源，Spark处理会出现一些OOM内存泄漏等问题，但是MapReduce基于磁盘处理的就不会出现这种情况。<br>
5、Spark整体架构<br>
<img src="https://img-blog.csdnimg.cn/20181120133157869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0VmZml5YQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<mark>注：对弹性分布式数据集（resilient distributed dataset，简<br>
称 RDD），RDD 表示分布在多个计算节点上可以并行操作的元素集合，是<br>
Spark 主要的编程抽象</mark><br>
6、Spark SQL与Hive的区别<br>
<img src="https://img-blog.csdnimg.cn/20181120135227351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0VmZml5YQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
1）Spark SQL支持多种数据源，速度比Hive快（Hive将HQL翻译成MapReduce，还是基于MapReuce处理查询的）<br>
2）部分Hive支持的高级特性，Spark SQL不支持。<br>
<em><strong>spark sql不支持的hive功能（去官网看了下最新的2.3.2版本，不过2.4就快更新了，到时候再去看看）</strong></em></p>
<p>Below is a list of Hive features that we don’t support yet. Most of these features are rarely used in Hive deployments.</p>
<p>以下是我们尚不支持的Hive功能列表。 大多数这些功能很少用于Hive部署。</p>
<p><strong>Major Hive Features</strong> Hive的主要功能</p>
<p>Tables with buckets: bucket is the hash partitioning within a Hive table partition. Spark SQL doesn’t support buckets yet.<br>
带buckets（桶）的表：bucket是Hive表分区中的散列分区。 Spark SQL尚不支持存储buckets。</p>
<p><strong>Esoteric Hive Features</strong> 神秘的Hive功能</p>
<p>UNION type<br>
Unique join<br>
Column statistics collecting: Spark SQL does not piggyback scans to collect column statistics at the moment and only supports populating the sizeInBytes field of the hive metastore.<br>
UNION类型<br>
独特的加入<br>
列统计信息收集：Spark SQL目前不会通过捎带扫描来收集列统计信息，只支持填充hive Metastore的sizeInBytes字段。<br>
metastore</p>
<p><strong>Hive Input/Output Formats</strong> Hive输入输出格式</p>
<p>File format for CLI: For results showing back to the CLI, Spark SQL only supports TextOutputFormat.<br>
Hadoop archive<br>
CLI的文件格式：对于返回到CLI的结果，SCALL SQL只支持ToTutuPuttFrand。<br>
Hadoop存档</p>
<p><strong>Hive Optimizations</strong> Hive优化</p>
<p>A handful of Hive optimizations are not yet included in Spark. Some of these (such as indexes) are less important due to Spark SQL’s in-memory computational model. Others are slotted for future releases of Spark SQL.</p>
<p>Block level bitmap indexes and virtual columns (used to build indexes)<br>
Automatically determine the number of reducers for joins and groupbys: Currently in Spark SQL, you need to control the degree of parallelism post-shuffle using “SET spark.sql.shuffle.partitions=[num_tasks];”.<br>
Meta-data only query: For queries that can be answered by using only meta data, Spark SQL still launches tasks to compute the result.<br>
Skew data flag: Spark SQL does not follow the skew data flags in Hive.<br>
STREAMTABLE hint in join: Spark SQL does not follow the STREAMTABLE hint.<br>
Merge multiple small files for query results: if the result output contains multiple small files, Hive can optionally merge the small files into fewer large files to avoid overflowing the HDFS metadata. Spark SQL does not support that.<br>
Spark中还没有包含一些Hive优化。由于Spark SQL的内存计算模型，其中一些（如索引）不太重要。其他版本用于未来的Spark SQL版本。</p>
<p>块级位图索引和虚拟列（用于构建索引）<br>
自动确定连接和groupbys的reducer数量：目前在Spark SQL中，需要使用“SET spark.sql.shuffle.partitions = [num_tasks];”来控制后洗牌的并行度。<br>
仅元数据查询：对于只能使用元数据来回答的查询，Spark SQL仍会启动计算结果的任务。<br>
偏斜数据标志：Spark SQL不遵循Hive中的偏斜数据标志。<br>
连接中的STREAMTABLE提示：Spark SQL不遵循STREAMTABLE提示。<br>
合并多个小文件以查找结果：如果结果输出包含多个小文件，Hive可以选择将小文件合并为较少的大文件，以避免溢出HDFS元数据。 Spark SQL不支持这一点。<br>
7、Spark Streaming与Storm<br>
1）关于Storm<br>
<img src="https://img-blog.csdnimg.cn/20181120172851922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX0VmZml5YQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
2）Spark Streaming不支持Storm的一种特性：分布式流式计算程序（Topology）在运行过程中，可以动态地调整并行度，动态提高并行能力。而且Spark Streaming属于准实时处理，如果对实时性要求比较高还是要用到Storm（其实现在阿里再用Flink，它是基于流式处理的，双十一的大屏幕就是用它做的，每秒可以处理4亿条数据）<br>
3）Spark Streaming吞吐量比Storm高，因为是基于batch（批）的，而Storm对于每一条来的数据都要分配资源，导致Storm的吞吐量比较低。<br>
4）Spark Streaming可以与Spark的其他框架相结合，可以对流式处理完的中间数据继续交给Spark其他框架来继续处理。<br>
二、总结<br>
至于MLlib和GraphX就在之后的学习的时候去查一下它们大致的一种情况吧。至于Spark的版本，将要迎来2.4.0版本，也新增了些功能，优于我初学Spark，先学习了Spark的基础之后，再去跟进最新的东西吧。</p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>