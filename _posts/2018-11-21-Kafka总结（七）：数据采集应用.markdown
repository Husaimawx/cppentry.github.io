---
layout:     post
title:      Kafka总结（七）：数据采集应用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><a href="https://mp.csdn.net/postedit/81283142" rel="nofollow">Kafka总结（一）：Kafka概述</a></p>

<p><a href="https://mp.csdn.net/postedit/81283229" rel="nofollow">Kafka总结（二）：Kafka核心组件</a></p>

<p><a href="https://mp.csdn.net/postedit/81283286" rel="nofollow">Kafka总结（三）：Kafka核心流程分析</a></p>

<p><a href="https://mp.csdn.net/postedit/81283397" rel="nofollow">Kafka总结（四）：Kafka命令操作</a></p>

<p><a href="https://mp.csdn.net/postedit/81283460" rel="nofollow">Kafka总结（五）：API编程详解</a></p>

<p><a href="https://mp.csdn.net/postedit/81283491" rel="nofollow">Kafka总结（六）：Kafka Stream详解</a></p>

<p><a href="https://mp.csdn.net/postedit/81283546" rel="nofollow">Kafka总结（七）：数据采集应用</a></p>

<p><a href="https://mp.csdn.net/postedit/81283568" rel="nofollow">Kafka总结（八）：KafKa与ELK整合应用</a></p>

<p><a href="https://mp.csdn.net/postedit/81283606" rel="nofollow">Kafka总结（九）：KafKa 与Spark整合应用</a></p>

<h2 style="margin-left:0cm;">1.Log4j集成KafKa应用</h2>

<h3 style="margin-left:0cm;">1.应用描述</h3>

<p style="margin-left:0cm;">一般都是将日子保存在本地服务器中，如果将应用日志同步写在远程服务器，则会导致应用程序依赖于远程服务器，如果远程服务器宕机或者网络请求超时都会影响到应用程序，同时也会增加应用程序的网络开销。</p>

<p style="margin-left:0cm;">因此，如果要将日志写到远程服务器，一定需要结合应用场景综合的考虑；</p>

<h3 style="margin-left:0cm;">2.具体实现</h3>

<p style="margin-left:0cm;">应用程序可以通过Log4j将日志同步到Kafka步骤：</p>

<ol><li>依赖：kafka-log4j-appender</li>
	<li>编写log4j.properties文件，为了保证不同级别的日志分别输出到不同的文件，并且每个级别的日志不再包含比该级别高的日志，可以自定义一个Appender，继承DailyRollingFileAppender类，重写isAsServerAsThresold()方法，让每个级别类型的日志只提取与之优先级（Priority）相等的日志；</li>
	<li>在Kafka上创建log4j.properties中配置的相应主题；</li>
</ol><p style="margin-left:0cm;"> </p>

<h2 style="margin-left:0cm;">2. KafKa与Flume整合应用</h2>

<p style="margin-left:0cm;">kafka-log4j-appender将应用程序的日志直接写入到kafka中，这种方式虽然简单，但是由于应用程序依赖于Kafka运行环境，因此存在一定的不足；</p>

<p style="margin-left:0cm;">通常的做法是：应用程序将日志写入到本地，然后通过日志采集工具将本地日志同步到远程服务器，而Flume就是最常用的数据采集工具之一。现在引入了Flume，通过Flume将应用程序产生的日志同步到Kafka；</p>

<h3 style="margin-left:0cm;">1. Flume简介</h3>

<p style="margin-left:0cm;"> </p>

<p style="margin-left:0cm;"><img alt="" class="has" height="486" src="https://img-blog.csdn.net/2018073013325740?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2ODA3ODYy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<h3 style="margin-left:0cm;">2. Flume与KafKa比较</h3>

<p style="margin-left:0cm;"> </p>

<h3 style="margin-left:0cm;">3. Flume的安装配置</h3>

<p style="margin-left:0cm;"> </p>

<h3 style="margin-left:0cm;">4. Flume采集日志写入KafKa</h3>

<p style="margin-left:0cm;"> </p>

<p style="margin-left:0cm;"> </p>

<p style="margin-left:0cm;">步骤：</p>

<ol><li>需要在配置文件中指定源、接收器和通道的名称；</li>
	<li>配置源</li>
	<li>配置通道信息</li>
	<li>配置接收器，接收器从通道获取信息写入到Kafka</li>
</ol><h2 style="margin-left:0cm;">3.KafKa与Flume和HDFS整合应用</h2>

<p style="margin-left:0cm;">Flume是一个连接各种组件和系统的桥梁，下载安装的lib目录下有Flume与Hbase、HDFS等集成的jar文件，可以很方便的与HBase和HDFS连接。在实际业务中，我们一般通过Flume从应用程序采集实时数据写入到Kafka，而将历史数据写入到HBASE和HDFS。</p>

<p style="margin-left:0cm;">例如：我们可以通过Kafka Steams实时对用户行为分析，将分析结果再写入到Kafka，然后有Flume写入Hbase，供可视化应用系统展示。通过将这些系统整合在一起，可以很方便的实现一个<strong>扩展性良好、高吞吐量的数据采集分析系统；</strong></p>

<h3 style="margin-left:0cm;">1.Hadoop安装配置</h3>

<p style="margin-left:0cm;">步骤：</p>

<ol><li>解压Hadoop安装包</li>
	<li>环境配置：</li>
</ol><ol><li>指定JDK安装路径，在hadoop-env.sh文件中添加JDK安装路径</li>
	<li>修改core-site.xml文件，在该文件中配置通信地址以及存储路径；</li>
	<li>修改hdfs-site.xml，在该文件中配置Hadoop文件块的数据备份数，由于是伪分布式模式，因此可以将其值设置为1，同时配置NameNode和DataNode节点文件块数据存储路径；</li>
	<li>修改mapred-site.xml。将配置模板文件mapred-site.xml.template复制一份，命令为mapred-site.xml，在该文件中配置JobTracker的主机名或者IP和端口。对于伪分布式模式该节点既是JobTracker也是TaskTracker。</li>
</ol><ol><li>启动运行。在运行Hadoop之前先进性格式化Hadoop工作空间。</li>
</ol><p style="margin-left:0cm;"> </p>

<p style="margin-left:0cm;"> </p>

<p style="margin-left:0cm;"> </p>

<h3 style="margin-left:0cm;">2.Flume采集KafKa消息写入HDFS</h3>

<p style="margin-left:0cm;">整合步骤：</p>

<ol><li>KafkaSource配置；</li>
</ol><p style="margin-left:0cm;">Flume提供了从Kafka采集数据的flume-kafka-source-1.0.7.jar文件，需要为该jar提供的KafkaSource进行相关的配置；</p>

<ol><li>KafkaChannel配置；</li>
</ol><p style="margin-left:0cm;">Flume采集数据的时候都采用内存通道，现在我们使用KafkaChannel，在安装lib目录下，有一个flume-kafka-channel-1.7.0.jar，该文件定义了一个Kafka通道，将采集的数据写入到Kafka主题中；</p>

<ol><li>写HDFS接收器配置；</li>
</ol><p style="margin-left:0cm;">在安装lib目录下有一个flume-hdfs-sink-1.7.0.jar文件，定义了一个将数据写入到HDFS的接入器HDFSEventSink，可以通过该接收器提供的配置写入HDFS<strong>数据格式</strong>、<strong>文件切割方式</strong>、<strong>文件压缩方式</strong>等；<br>
 </p>            </div>
                </div>