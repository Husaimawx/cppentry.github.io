---
layout:     post
title:      hadoop配置文件详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><strong>转载地址：<a href="http://blog.csdn.net/lin_fs/article/details/7349497" rel="nofollow">http://blog.csdn.net/lin_fs/article/details/7349497</a></strong><span style="font-size:12px;"></span></p>
<p><br></p>
<p><span></span>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core、hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>
<p><strong>一、      Hadoop伪分布配置</strong></p>
<div id="content">
<p>           1. <strong>在conf/hadoop-env.sh文件中增加：</strong><span style="color:rgb(0,176,80);">export JAVA_HOME=/home/Java/jdk1.6</span></p>
<p><strong>           2.  在conf/core-site.xml文件中增加如下内容：</strong></p>
<p><span style="color:#FF0000;"> &lt;!-- </span><strong> fs.default.name</strong> <span style="color:#FF0000;">- </span><span style="color:#FF0000;">这是一个描述集群中NameNode结点的URI(包括协议、主机名称、端口号)，集群里面的每一台机器都需要知道NameNode的地址。DataNode结点会先在NameNode上注册，这样它们的数据才可以被使用。独立的客户端程序通过这个URI跟DataNode交互，以取得文件的块列表。</span><span style="color:#FF0000;">--&gt;</span></p>
<p><strong> </strong><span style="color:rgb(0,176,80);">&lt;property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">              &lt;name&gt;fs.default.name&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">              &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">        &lt;/property&gt;</span></p>
<p><span style="color:rgb(0,176,80);"><br></span></p>
<p><span style="color:#FF0000;">       &lt;!—hadoop.tmp.dir </span><span style="color:#FF0000;">是hadoop文件系统依赖的基础配置，很多路径都依赖它。</span><span style="color:rgb(255,0,0);">如果hdfs-site.xml中不配置namenode和datanode的存放位置，默认就放在这个路径中</span><span style="color:#FF0000;">--&gt;</span></p>
<p><span style="color:rgb(0,176,80);">  &lt;property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">       &lt;value&gt;/home/hdfs/tmp&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">   &lt;/property&gt;</span></p>
<p><strong> 3.  在conf/hdfs-site.xml中增加如下内容：</strong></p>
<p><span style="color:#FF0000;">       &lt;!--</span><span style="color:#FF0000;"> <strong>dfs.replication </strong>-</span><span style="color:#FF0000;">它决定着 系统里面的文件块的数据备份个数。对于一个实际的应用，它 应该被设为3（这个数字并没有上限，但更多的备份可能并没有作用，而且会占用更多的空间）。少于三个的备份，可能会影响到数据的可靠性(系统故障时，也许会造成数据丢失)</span><span style="color:#FF0000;">--&gt;</span></p>
<p>   <span style="color:rgb(0,176,80);">  &lt;property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">         &lt;name&gt;dfs.replication&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">         &lt;value&gt;1&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">     &lt;/property&gt;</span></p>
<p> </p>
<p>          <span style="color:#FF0000;">&lt;!-- </span><strong> dfs.data.dir</strong> <span style="color:#FF0000;">- </span><span style="color:#FF0000;">这是DataNode结点被指定要存储数据的本地文件系统路径。DataNode结点上的这个路径没有必要完全相同，因为每台机器的环境很可能是不一样的。但如果每台机器上的这个路径都是统一配置的话，会使工作变得简单一些。默认的情况下，它的值h<span style="color:rgb(0,176,80);">adoop.tmp.dir</span>, 这个路径只能用于测试的目的，因为，它很可能会丢失掉一些数据。所以，这个值最好还是被覆盖。 </span></p>
<p><strong>dfs.name.dir</strong> <span style="color:#FF0000;">- </span><span style="color:#FF0000;">这是NameNode结点存储hadoop文件系统信息的本地系统路径。这个值只对NameNode有效，DataNode并不需要使用到它。上面对于/temp类型的警告，同样也适用于这里。在实际应用中，它最好被覆盖掉。--&gt;</span></p>
<p>           <span style="color:rgb(0,176,80);">&lt;property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">             &lt;name&gt;dfs.name.dir&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">             &lt;value&gt;/home/hdfs/name&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">        &lt;/property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">       &lt;property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">        &lt;name&gt;dfs.data.dir&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">        &lt;value&gt;/home/hdfs/data&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">   &lt;/property&gt;</span></p>
<p> </p>
<p> </p>
<p>    <span style="color:#FF0000;">           &lt;!—</span><span style="color:#FF0000;">解决：</span><span style="color:#FF0000;">org.apache.hadoop.security.AccessControlException</span><span style="color:#FF0000;">:</span><span style="color:#FF0000;">Permission                        
                         denied:user=Administrator,access=WRITE,inode="tmp":root:supergroup:rwxr-xr-x </span><span style="color:#FF0000;">。</span></p>
<p><span style="color:#FF0000;">因为Eclipse使用hadoop插件提交作业时，会默认以 DrWho 身份去将作业写入hdfs文件系统中，对应的也就是 HDFS 上的/user/hadoop ,  由于 DrWho 用户对hadoop目录并没有写入权限，所以导致异常的发生。解决方法为：放开 hadoop 目录的权限， 命令如下 ：$ hadoop fs -chmod 777 /user/hadoop </span><span style="color:#FF0000;">--&gt;</span></p>
<p> <span style="color:rgb(0,176,80);">              &lt;property&gt; </span></p>
<p><span style="color:rgb(0,176,80);">                   &lt;name&gt;dfs.permissions&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">                   &lt;value&gt;false&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">&lt;description&gt;</span></p>
<p><span style="color:rgb(0,176,80);">                      If "true", enable permission checking in HDFS. If "false", permission checking is turned                     off,   but all other behavior is unchanged. Switching from one parameter value to        
                           the other does   not change the mode, owner or group of files or directories</span></p>
<p><span style="color:rgb(0,176,80);">              &lt;/description&gt;</span></p>
<p> </p>
<p><span style="color:rgb(0,176,80);">        &lt;/property&gt;</span></p>
<p> </p>
<p><strong>   4.  在conf/mapred-site.xml中增加如下内容：</strong></p>
<p><span style="color:#FF0000;">&lt;!--</span> <span style="color:#FF0000;">mapred.job.tracker</span><span style="color:rgb(255,0,0);"> -JobTracker的主机（或者IP）和端口。</span><span style="color:#FF0000;">--&gt;</span></p>
<p>      <span style="color:rgb(0,176,80);">&lt;property&gt;</span></p>
<p><span style="color:rgb(0,176,80);">       &lt;name&gt;mapred.job.tracker&lt;/name&gt;</span></p>
<p><span style="color:rgb(0,176,80);">      &lt;value&gt;localhost:9001&lt;/value&gt;</span></p>
<p><span style="color:rgb(0,176,80);">&lt;/property&gt;</span></p>
<p><strong>二、操作命令</strong></p>
<p><strong>           1.  格式化工作空间</strong></p>
<p>进入bin目录，运行 ./hadoop namenode –format</p>
<p><strong>           2.  启动hdfs</strong></p>
<p>进入hadoop目录,在bin/下面有很多启动脚本，可以根据自己的需要来启动。</p>
<p>                    * start-all.sh 启动所有的Hadoop守护。包括namenode, datanode, jobtracker, tasktrack</p>
<p>* stop-all.sh 停止所有的Hadoop</p>
<p>* start-mapred.sh 启动Map/Reduce守护。包括Jobtracker和Tasktrack</p>
<p>* stop-mapred.sh 停止Map/Reduce守护</p>
<p>* start-dfs.sh 启动Hadoop DFS守护Namenode和Datanode</p>
<p>* stop-dfs.sh 停止DFS守护  </p>
<p><strong>三、Hadoop hdfs 整合</strong></p>
<p><strong>    </strong> 可按如下步骤删除和更改hdfs不需要的文件：</p>
<p>         1.将hadoop-core-1.0.0.jar 移动到lib目录下。</p>
<p>         2. 将ibexec目录下的文件移动到bin目录下。</p>
<p>         3. 删除除bin、lib、conf、logs之外的所有目录和文件。</p>
<p>         4. 如果需要修改日志存储路径，则需要在conf/hadoop-env.sh文件中增加：</p>
<p>            export    HADOOP_LOG_DIR=/home/xxxx/xxxx即可。</p>
<p><strong>四、HDFS文件操作</strong></p>
<p>Hadoop使用的是HDFS，能够实现的功能和我们使用的磁盘系统类似。并且支持通配符，如*。</p>
<p><strong>       1.  查看文件列表</strong></p>
<p>查看hdfs中/user/admin/hdfs目录下的文件。</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs -ls /user/admin/hdfs</p>
<p>查看hdfs中/user/admin/hdfs目录下的所有文件（包括子目录下的文件）。</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs -lsr /user/admin/hdfs</p>
<p><strong>       2.  创建文件目录</strong></p>
<p>查看hdfs中/user/admin/hdfs目录下再新建一个叫做newDir的新目录。</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs -mkdir /user/admin/hdfs/newDir</p>
<p><strong>       3.  删除文件</strong></p>
<p>删除hdfs中/user/admin/hdfs目录下一个名叫needDelete的文件</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs -rm /user/admin/hdfs/needDelete</p>
<p>      删除hdfs中/user/admin/hdfs目录以及该目录下的所有文件</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs -rmr /user/admin/hdfs</p>
<p><strong>        4. 上传文件</strong></p>
<p>上传一个本机/home/admin/newFile的文件到hdfs中/user/admin/hdfs目录下</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs –put /home/admin/newFile /user/admin/hdfs/</p>
<p>        <strong>5. 下载文件</strong></p>
<p>下载hdfs中/user/admin/hdfs目录下的newFile文件到本机/home/admin/newFile中</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>             b. 执行sh bin/hadoop fs –get /user/admin/hdfs/newFile /home/admin/newFile</p>
<p>        <strong>6. 查看文件内容</strong></p>
<p>查看hdfs中/user/admin/hdfs目录下的newFile文件</p>
<p>a. 进入HADOOP_HOME目录。</p>
<p>b. 执行sh bin/hadoop fs –cat /home/admin/newFile</p>
</div>
<p><span style="font-size:12px;"></span></p>
            </div>
                </div>