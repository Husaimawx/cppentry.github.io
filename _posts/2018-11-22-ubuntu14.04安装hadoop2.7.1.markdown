---
layout:     post
title:      ubuntu14.04安装hadoop2.7.1
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>配置完成jdk，并记录Jdk路径</p>

<p>1.<a href="http://www.cnblogs.com/kinglau/p/3794433.html" rel="nofollow">http://www.cnblogs.com/kinglau/p/3794433.html</a> <br>
一、在Ubuntu下创建hadoop组和hadoop用户 <br>
    增加hadoop用户组，同时在该组里增加hadoop用户，后续在涉及到hadoop操作时，我们使用该用户。</p>

<p>在终端输入命令 <br>
1、创建hadoop用户组</p>

<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">sudo</span> addgroup hadoop</code></pre>

<p>2、创建hadoop用户</p>



<pre class="prettyprint"><code class=" hljs bash">      <span class="hljs-built_in">sudo</span> adduser -ingroup hadoop hadoop</code></pre>

<pre><code>回车后会提示输入新的UNIX密码，这是新建用户hadoop的密码，输入回车即可。

如果不输入密码，回车后会重新提示输入密码，即密码不能为空。
最后确认信息是否正确，如果没问题，输入 Y，回车即可。
</code></pre>

<p>3、为hadoop用户添加权限</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">sudo</span> gedit /etc/sudoers
</code></pre>

<p>回车，打开sudoers文件 <br>
    给hadoop用户赋予和root用户同样的权限 <br>
   root ALL=(ALL:ALL)ALL <br>
   hadoop   ALL=(ALL:ALL)ALL</p>

<p>二、用新增加的hadoop用户登录Ubuntu系统</p>

<p>三、安装ssh <br>
1.</p>



<pre class="prettyprint"><code class=" hljs vbscript">sudo apt-<span class="hljs-keyword">get</span> install openssh-<span class="hljs-built_in">server</span></code></pre>

<p>2.</p>



<pre class="prettyprint"><code class=" hljs bash">安装完成后，启动服务

<span class="hljs-built_in">sudo</span> /etc/init.d/ssh start
</code></pre>



<pre class="prettyprint"><code class=" hljs bash">查看服务是否正确启动：
ps <span class="hljs-operator">-e</span> | grep ssh</code></pre>

<p><img src="https://img-blog.csdn.net/20170301093445510?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd20zMjY3MDcwMDA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
3.`</p>



<pre class="prettyprint"><code class=" hljs lasso">设置免密码登录，生成私钥和公钥

ssh<span class="hljs-attribute">-keygen</span> <span class="hljs-attribute">-t</span> rsa <span class="hljs-attribute">-P</span> <span class="hljs-string">""</span></code></pre>

<p>回车</p>



<pre class="prettyprint"><code class=" hljs avrasm">此时会在／home／hadoop/<span class="hljs-preprocessor">.ssh</span>下生成两个文件：id_rsa和id_rsa<span class="hljs-preprocessor">.pub</span>，前者为私钥，后者为公钥。

下面我们将公钥追加到authorized_keys中，它用户保存所有允许以当前用户身份登录到ssh客户端用户的公钥内容。

cat ~/<span class="hljs-preprocessor">.ssh</span>/id_rsa<span class="hljs-preprocessor">.pub</span> &gt;&gt; ~/<span class="hljs-preprocessor">.ssh</span>/authorized_keys</code></pre>

<p>4.登陆ssh</p>



<pre class="prettyprint"><code class=" hljs ">登录ssh

ssh localhost</code></pre>

<p><img src="https://img-blog.csdn.net/20170301093607934?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd20zMjY3MDcwMDA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>5.退出 <br>
退出 <br>
exit</p>

<p><a href="http://blog.csdn.net/ycisacat/article/details/53314144" rel="nofollow">http://blog.csdn.net/ycisacat/article/details/53314144</a> <br>
1.下载hadoop，将hadoop解压，并改名为hadoop2.7 <br>
放到home文件夹下 <br>
路径 /home/hadoop(用户名)/hadoop2.7(文件夹名) <br>
2.配置环境变量 </p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-comment">#修改bashrc文件</span>
<span class="hljs-variable">$sudo</span> gedit ~/.bashrc</code></pre>

<p>在文件最后添加</p>



<pre class="prettyprint"><code class=" hljs bash">在bashrc文件最后添加：

<span class="hljs-comment">#set java environment</span>
<span class="hljs-keyword">export</span> JAVA_HOME=/usr/local/javajdk1.<span class="hljs-number">8</span>     //jdk路径
<span class="hljs-keyword">export</span> JRE_HOME=<span class="hljs-variable">${JAVA_HOME}</span>/jre
<span class="hljs-keyword">export</span> CLASSPATH=.:<span class="hljs-variable">${JAVA_HOME}</span>/lib:<span class="hljs-variable">${JRE_HOME}</span>/lib
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">${JAVA_HOME}</span>/bin:<span class="hljs-variable">$PATH</span>

<span class="hljs-comment">#set hadoop environment</span>
<span class="hljs-keyword">export</span> HADOOP_INSTALL=/home/hadoop/hadoop2.<span class="hljs-number">7</span>   //hadoop路径
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HADOOP_INSTALL</span>/bin
<span class="hljs-keyword">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HADOOP_INSTALL</span>/sbin
<span class="hljs-keyword">export</span> HADOOP_MAPRED_HOME=<span class="hljs-variable">$HADOOP_INSTALL</span>
<span class="hljs-keyword">export</span> HADOOP_COMMON_HOME=<span class="hljs-variable">$HADOOP_INSTALL</span>
<span class="hljs-keyword">export</span> HADOOP_HDFS_HOME=<span class="hljs-variable">$HADOOP_INSTALL</span></code></pre>

<p>保存后关闭文件</p>



<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc</code></pre>

<p>3.修改hadoop 文件</p>



<pre class="prettyprint"><code class=" hljs lasso">修改hadoop2<span class="hljs-number">.7</span> 文件权限
<span class="hljs-variable">$sudo</span> chown <span class="hljs-attribute">-R</span> hadoop:hadoop /home/hadoop/hadoop2<span class="hljs-number">.7</span>
sudo gedit /etc/hadoop/hadoop<span class="hljs-attribute">-env</span><span class="hljs-built_in">.</span>sh
</code></pre>

<p>把jdk路径改了 <br>
<img src="https://img-blog.csdn.net/20170301095704056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd20zMjY3MDcwMDA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>4.hadoop单机运行例子验证 </p>

<p>以上如果配置无误的话，hadoop已经可以单机运行了。可以用自带的例子检验。  <br>
hadoop的例子在hadoop/share/hadoop/mapreduce/下，名为hadoop-mapreduce-examples-版本号.jar</p>



<pre class="prettyprint"><code class=" hljs ruby">cd /home/hadoop/hadoop2.<span class="hljs-number">7</span>
<span class="hljs-comment">#创建input目录，复制运行/home/hadoop/hadoop2.7/etc/hadoop/下所有xml文件到该目录下</span>
/home/hadoop/hadoop2.<span class="hljs-number">7</span><span class="hljs-variable">$ </span>sudo mkdir input
/home/hadoop/hadoop2.<span class="hljs-number">7</span><span class="hljs-variable">$ </span>sudo cp etc/hadoop/*.xml input 
<span class="hljs-comment">#运行示例，检测input中符合' '中正则匹配规则的单词出现的次数（这里为dfs开头的单词）</span>
/home/hadoop/hadoop2.<span class="hljs-number">7</span><span class="hljs-variable">$ </span>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>.jar grep input output <span class="hljs-string">'dfs[a-z.]+'</span>
<span class="hljs-comment">#查看结果</span>
/home/hadoop/hadoop2.<span class="hljs-number">7</span><span class="hljs-variable">$ </span>cat output/*</code></pre>

<p>如果正常运行，看到success即成功。hadoop下会自动生成一个output文件夹来存放结果，但是下次运行时不会自动覆盖，再次运行示例时会报错。要先把上次的结果删掉。 <br>
<code>sudo rm -R output</code></p>

<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">sudo</span> rm -R output</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>