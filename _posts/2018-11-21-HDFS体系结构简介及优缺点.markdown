---
layout:     post
title:      HDFS体系结构简介及优缺点
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><span style="font-size:18px;color:#ff0000;"><strong>1 HDFS体系结构简介及优缺点</strong></span></p>
<p><strong><span style="font-size:14px;">1.1 体系结构简介</span></strong></p>
<p>       HDFS是一个主/从(Master/slave)体系结构，从最终用户的角度来看，它就像传统的文件系统一样，可以通过目录路径对文件系统执行CRUD(Create、Read、Update、Delete)操作。但由于分布式存储的性质，HDFS集群拥有一个NameNode和一些DataNode。NameNode管理文件系统的元数据，DataNode存储实际的数据。客户端通过同NameNode和DataNode的交互访问文件系统。客户端联系NameNode以获取文件的元数据，而真正的文件I/O操作是直接和DataNode进行交互的。下面是HDFS总体结构示意图：</p>
<p><img src="https://img-blog.csdn.net/20140825135304794" alt=""><br></p>
<p><strong><span style="font-size:18px;color:#ff0000;">相关概念：</span></strong></p>
<p><span style="font-size:14px;"><strong>1.1.1 NameNode</strong></span></p>
<p>       NameNode可以看作是分布式文件系统中的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等，NameNode会将文件系统的Meta-data存储到内存中，这些信息主要包括了文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode的信息等。</p>
<p><span style="font-size:14px;"><strong>1.1.2 Secondary NameNode</strong></span></p>
<p>     首先要明确一个问题，Secondary Node不是NameNode的热备，Secondary NameNode主要是辅助NameNode，分担其工作量；定期合并fsimage和fsedits，推送给NameNode；在紧急情况下，可以恢复NameNode。</p>
<p><span style="font-size:14px;"><strong>1.1.3 DataNode</strong></span></p>
<p>      DataNode是文件存储的基本单元，他将Block存储在本地文件系统中，保存了Block的meta-data，同时周期性的将所有存在的Block信息发送给NameNode。slave存储实际的数据块，执行数据块的读写。</p>
<p><span style="font-size:14px;"><strong>1.1.4 Client</strong></span></p>
<p> <span> </span>文件切分与NameNode的交互，获取文件位置信息；与DataNode交互，读取或者写入数据；管理HDFS；访问HDFS。</p>
<p><span style="font-size:14px;"><strong>1.1.5 文件写入</strong></span></p>
<p><span></span>1):Client向NameNode发起文件读取的请求。</p>
<p><span></span>2):NameNode返回文件存储的DataNode的信息。</p>
<p><span></span>3):Client读取文件信息。</p>
<p>HDFS典型的部署时在一个专门的机器上运行NameNode，集群中的其他机器各运行一个DataNode；也可以在运行NameNode的机器上同时运行DataNode，或者一台机器上运行多个DataNode。一个集群只有一个NameNode的设计大大简化了系统架构。</p>
<p><br></p>
<p><span style="font-size:18px;color:#ff0000;"><strong>1.2 优点</strong></span></p>
<p><span style="font-size:14px;"><strong>1.2.1 处理超大文件</strong></span></p>
<p><span></span>这里的超大文件通常是指MB、设置数百TB大小的文件。目前在实际应用，HDFS已经能用来存储管理PB级的数据了。</p>
<p><span style="font-size:14px;"><strong>1.2.2流式的访问数据</strong></span></p>
<p><span></span>HDFS的设计建立在更多的响应“一次写入、多次读写”任务的基础上。这意味着一个数据集一旦由数据源生成，就会被复制分发到不同的存储节点中，然后响应各种各样的数据分析任务请求。在多数情况下，分析任务都会涉及数据集中的大部分数据，也就是说，对HDFS来说，请求读取整个数据集要比读取一条记录更加高效。</p>
<p><span style="font-size:14px;"><strong>1.2.3 运行在廉价的商用机器集群上</strong></span></p>
<p><span></span>Hadoop涉及对硬件的要求比较低，只需运行在低廉的商用硬件集群上，而无需昂贵的高可用性机器上。廉价的商用机也就意味着大型急群众出现节点故障情况的概率非常高。这就要求设计HDFS时要充分考虑数据的可靠性，安全性及高可用性。</p>
<p><span style="font-size:18px;color:#ff0000;"><strong>1.3 缺点</strong></span></p>
<p><span style="font-size:14px;"><strong>1.3.1 不适合低延迟数据访问</strong></span></p>
<p><span></span>如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务的，主要是达到高的数据吞吐量而设计的，这就可能要求以高延迟作为代价。</p>
<p><span style="font-size:14px;"><strong>改进策略：</strong></span></p>
<p><span></span>对于那些有低延迟要求的引用程序，HBase是一个更好的选择。通过上层数据管理项目来尽可能的弥补这个不足。在性能上有了很大的提升，它的括号就是goes real time。使用缓存或多master设计可以降低Client的数据请求压力，以减少延时。还有就是对HDFS系统内部的修改，这就得权衡大吞吐量与低延时了，HDFS不是万能的银弹。</p>
<p><span style="font-size:14px;"><strong>1.3.2 无法高效存储小文件</strong></span></p>
<p><span></span>因为nameNode把文件系统的元数据放置在内存中，所以文件系统所能容纳的文件数目是由NameNode的内存大小来决定的，一般来说，每一个文件、文件夹和Block需要占据150字节左右的空间，所以，如果你有100万个文件，每一个占据一个Block，你就至少需要300MB的内存，当前来说，数百万的文件还是可行的，当扩展到数十亿时，对于当前的硬件水平来说就无法实现了。还有一个问题就是，因为Map task的数量是由splits来决定的，所以用MR处理大量小文件时，就会产生过多的Maptask，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M，那么就会有10000个maptasks，会有很大的线程开销；若每个split为100M，则100个Maptasks，每个maptask将会有更多的事情做，而线程的管理开销也将会减小很多。</p>
<p><span style="font-size:14px;"><strong>改进策略：</strong></span></p>
<p><span></span>要想让HDFS能处理好小文件，有不少方法。利用SequenceFile、MapFile、Har等方式归档小文件，这个方法的原理就是把小文件归档起来管理，HBase就是基于此的。对于这种方法，如果想找回原来的小文件内容，那就必须得知道与归档文件的映射关系。横向扩展，一个Hadoop集群能管理的小文件有限，那就把几个Hadoop集群拖在一个虚拟服务器后面，形成一个大的Hadoop集群。google也是这么干过的。多Master设计，这个作用显而易见了。正在研发中的GFS
 II也要改为分布式多Master设计，还支持Master的Failover，而且Block大小改为1M，有意要调优处理小文件啊。附带个Alibaba DFS的设计，也是多Master设计，它把Metadata的映射存储和管理分开了，由多个Metadata存储节点和一个查询Master节点组成。<br></p>
<p><span style="font-size:14px;"><strong>1.3.3不支持多用户写入及任意修改文件</strong></span></p>
<p><span></span>在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前HDFS还不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。</p>
            </div>
                </div>