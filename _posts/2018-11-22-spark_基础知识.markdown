---
layout:     post
title:      spark_基础知识
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h3><a id="spark_day01_0"></a>spark_day01学习笔记</h3>
<h4><a id="1_2"></a>1、课程目标</h4>
<ul>
<li>1、掌握spark相关概念</li>
<li>2、掌握搭建一个spark集群</li>
<li>3、掌握编写简单的spark应用程序</li>
</ul>
<h4><a id="2spark_10"></a>2、spark概述</h4>
<h5><a id="21_spark_12"></a>2.1 spark是什么</h5>
<ul>
<li>
<p><strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing.</p>
</li>
<li>
<p>spark是一个针对于大规模数据处理的统一分析引擎。</p>
<pre><code>spark是一个基于内存计算的大数据处理框架，处理非常快。但是这里仅仅只涉及到数据的计算，并没有涉及到数据的存储，后期我们想要通过spark来处理数据，就需要跟外部的数据源进行对接。比如要处理hdfs上的数据。
</code></pre>
</li>
</ul>
<h5><a id="22_spark_22"></a>2.2 为什么要学习spark</h5>
<ul>
<li>spark计算速度比mapreduce快很多。</li>
</ul>
<h5><a id="23_spark_26"></a>2.3 spark的四大特性</h5>
<ul>
<li>
<p>1、速度快</p>
<ul>
<li>
<p>spark比mapreduce在内存中快100倍，比mapreduce在磁盘中快10倍</p>
</li>
<li>
<p>快的主要2个原因</p>
<pre><code>(1) spark在处理任务的时候，job的输出结果可以保存在内存中，而mapreduce它在处理任务的时候，job的输出结果是只能够保存在磁盘里面。后续又有其他的job需要依赖于前面job的输出结果，spark中可以直接从内存中获取得到，而mapreduce必须要进行大量的磁盘io操作才能够获取得到，这样一来，spark中的job就可以大大减少磁盘的io操作，最后提升性能。

select name age from (select * from user where age &gt;30 and age &lt;40)


(2) mapreduce任务它是以进程的方式运行在yarn集群中，比如一个mapreduce任务有100个MapTask,这个时候要处理这100个MapTask就需要启动100个进程。spark任务它是以线程的方式运行在进程中。比如一个spark的任务有100个MapTask，这个时候要处理这100个MapTask可以只启动一个进程，在这一个进程中运行100个线程就可以了。这里启动一个进程跟启动一个线程它的代价肯定是不一样。启动一个进程比启动一个线程需要的时间和资源都是大大增加。spark任务需要的时间和资源是减少了，任务的性能得到了提升。
</code></pre>
</li>
</ul>
</li>
<li>
<p>2、易用性</p>
<ul>
<li>可以快速写一个spark应用程序通过java、scala、python、R、sql语言来进行代码开发</li>
</ul>
</li>
<li>
<p>3、通用性</p>
<ul>
<li>spark框架就是一个生态系统，可以通过它的一些子项目sparksql、sparkStreaming、Mlib、Graphx来使用到不同的应用场景。</li>
</ul>
</li>
<li>
<p>4、兼容性</p>
<ul>
<li>spark程序就是一个计算任务的程序，哪里可以给当前这个任务提供计算的资源，我们就可以把spark程序提交到哪里去运行。
<ul>
<li>standalone
<ul>
<li>它是spark集群自带的集群模式，整个任务的资源分配由Master负责。</li>
</ul>
</li>
<li>yarn
<ul>
<li>可以把spark程序提交到yarn中去运行，整个任务的资源分配由ResourceManager负责</li>
</ul>
</li>
<li>mesos
<ul>
<li>它是一个apache类似于yarn的资源调度框架</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a id="3spark_63"></a>3、spark集群安装部署</h4>
<ul>
<li>
<p>1、下载对应spark版本安装包</p>
<ul>
<li><a href="http://mirror.bit.edu.cn/apache/spark/spark-2.1.3/spark-2.1.3-bin-hadoop2.7.tgz" rel="nofollow">http://mirror.bit.edu.cn/apache/spark/spark-2.1.3/spark-2.1.3-bin-hadoop2.7.tgz</a></li>
<li>spark-2.1.3-bin-hadoop2.7.tgz</li>
</ul>
</li>
<li>
<p>2、规划安装目录</p>
<ul>
<li>/export/servers</li>
</ul>
</li>
<li>
<p>3、上传安装包到服务器中</p>
</li>
<li>
<p>4、解压安装包到指定的规划目录</p>
<ul>
<li>tar -zxvf spark-2.1.3-bin-hadoop2.7.tgz -C /export/servers</li>
</ul>
</li>
<li>
<p>5、重命名解压目录</p>
<ul>
<li>mv spark-2.1.3-bin-hadoop2.7 spark</li>
</ul>
</li>
<li>
<p>6、修改配置文件</p>
<ul>
<li>
<p>进入到spark/conf目录下</p>
<ul>
<li>
<p>vim <a href="http://spark-env.sh" rel="nofollow">spark-env.sh</a> (mv spark-env.sh.template <a href="http://spark-env.sh" rel="nofollow">spark-env.sh</a>)</p>
<pre><code>#添加java环境变量
export JAVA_HOME=/export/servers/jdk
#配置master的IP地址
export SPARK_MASTER_HOST=node1
#配置master的端口
export SPARK_MASTER_PORT=7077
</code></pre>
</li>
<li>
<p>vim slaves (mv slaves.template slaves)</p>
<pre><code>#指定哪些节点是worker(把默认的值localhost干掉)
node2
node3
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>7、配置spark的环境变量</p>
<ul>
<li>
<p>vim /etc/profile</p>
<pre><code>export SPARK_HOME=/export/servers/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
</code></pre>
</li>
</ul>
</li>
<li>
<p>8、分发spark目录和spark环境变量到其他节点</p>
<pre><code>scp -r spark node2:/export/servers
scp -r spark node3:/export/servers

scp /etc/profile node2:/etc
scp /etc/profile node3:/etc
</code></pre>
</li>
<li>
<p>9、让所有spark节点的环境变量生效</p>
<ul>
<li>在所有spark节点上执行
<ul>
<li>source /etc/profile</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a id="4spark_133"></a>4、spark集群启动和停止</h4>
<ul>
<li>
<p>1、启动spark集群</p>
<ul>
<li>可以在主节点上进入到spark/sbin 执行
<ul>
<li>./start-all.sh</li>
</ul>
</li>
</ul>
</li>
<li>
<p>2、停止spark集群</p>
<ul>
<li>
<p>可以在主节点上进入到spark/sbin 执行</p>
<ul>
<li>./stop-all.sh</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4><a id="5sparkweb_147"></a>5、spark集群web管理界面</h4>
<ul>
<li>
<p>1、先启动spark集群</p>
</li>
<li>
<p>2、访问地址</p>
<ul>
<li>master主机名:8080</li>
<li>进去之后可以看到整个spark集群所有信息，这些信息主要包括：整个spark集群所有的资源信息，已经使用的spark集群资源信息、还剩的spark集群资源信息、还有worker相关的信息、还有正在运行的任务信息、还有已经完成的任务信息。</li>
</ul>
</li>
</ul>
<h4><a id="6zookeeperspark_157"></a>6、基于zookeeper构建spark高可用集群</h4>
<ul>
<li>
<p>1、需要安装zk集群</p>
</li>
<li>
<p>2、修改配置文件</p>
<ul>
<li>
<p>vim <a href="http://spark-env.sh" rel="nofollow">spark-env.sh</a></p>
<pre><code>#需要注释掉手动指定的masterip地址配置
#这个时候整个集群就有很多个master，活着的master不在由于我们自己手动指定，而是通过zk选举产生
#export SPARK_MASTER_HOST=node1

#引入zk 构建spark高可用集群
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER  -Dspark.deploy.zookeeper.url=node1:2181,n
ode2:2181,node3:2181  -Dspark.deploy.zookeeper.dir=/spark"
</code></pre>
</li>
</ul>
</li>
<li>
<p>3、<a href="http://xn--spark-env-4m4oz1n.sh" rel="nofollow">分发spark-env.sh</a> 到其他节点</p>
</li>
<li>
<p>4、先启动zk集群</p>
</li>
<li>
<p>5、启动spark集群</p>
<ul>
<li>1、可以在任意一台机器启动（前提条件：就是需要实现所有机器两两之间它的ssh免密登陆）
<ul>
<li><a href="http://start-all.sh" rel="nofollow">start-all.sh</a></li>
<li>在哪里执行上面这个脚本，它就会在当前机器启动一个Master进程</li>
<li>整个spark集群来说worker进程由slaves文件决定</li>
</ul>
</li>
</ul>
</li>
<li>
<p>6、可以在其他机器单独启动master</p>
<ul>
<li><a href="http://start-master.sh" rel="nofollow">start-master.sh</a></li>
</ul>
</li>
<li>
<p>7、高可用说明</p>
<pre><code>引入zk之后，整个spark集群就有很多个master，其中有一个master被zk选举成活着的master,其他多个master都处于standBy状态，启动完成之后这个时候它会把整个spark集群的元数据信息通过zk中的节点进行保存， 活着的master它会提供服务，处于standBy的master不会提供服务。

当前活着的master挂掉了，首先zk会感知到，接下来会在所有处于standBy中的master进行选举，然后再来产生一个活着的新的Master，它会读取zk中保存集群的元数据信息，最后恢复到上一次master挂掉的状态。整个恢复的过程需要一定的时间，一般就是1-2分钟。


当前整个spark集群中的master挂掉了，挂掉了之后到恢复结束整个过程对任务有什么影响？
（1）对于当前正在运行的任务，由于这个任务既然可以运行，说明它已经获取得到了任务计算需要的资源，对于它来有资源就可以继续运行，它是不受任何影响。

（2）对于后面要提交的任务，由于没有这样一个活着的master这个角色，整个任务就获取不到资源，它就无法运行。


</code></pre>
</li>
</ul>
<h4><a id="7spark_206"></a>7、spark角色介绍</h4>
<p><img src="https://img-blog.csdnimg.cn/20181106144338489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3OTk4NDUw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="_210"></a></h4>
<ul>
<li>
<p>1、Driver</p>
<ul>
<li>它会运行客户端写好的main方法，并且会创建SparkContext对象，该对象是所有spark程序的执行入口。</li>
</ul>
</li>
<li>
<p>2、Application</p>
<ul>
<li>它就是一个计算任务的应用程序，它是包括了Driver端的代码以及整个任务在运行的时候需要的资源信息。</li>
</ul>
</li>
<li>
<p>3、Cluster Manager</p>
<ul>
<li>它就是一个可以获取到外部资源的服务
<ul>
<li>standAlone
<ul>
<li>它是spark自带的集群模式，整个任务的资源分配由Master负责</li>
</ul>
</li>
<li>yarn
<ul>
<li>spark程序可以提交到yarn去运行，整个任务的资源分配由ResourceManager负责</li>
</ul>
</li>
<li>mesos
<ul>
<li>它是一个apache开源类似于yarn的资源调度平台</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>4、Master</p>
<ul>
<li>它就是整个spark集群的老大，它负责任务资源的分配</li>
</ul>
</li>
<li>
<p>5、Worker</p>
<ul>
<li>它就是整个spark集群的小弟，它会负责任务的真正计算。</li>
</ul>
</li>
<li>
<p>6、Executor</p>
<ul>
<li>它就是一个进程，它会在worker节点来启动进程去运行任务</li>
</ul>
</li>
<li>
<p>7、Task</p>
<ul>
<li>它就是一个线程，它会以线程的方式运行在worker节点的executor进程中</li>
</ul>
</li>
</ul>
<h4><a id="8spark_247"></a>8、初识spark程序</h4>
<ul>
<li>
<p>1、以普通模式提交任务到spark集群中运行（就是我们已经知道了整个spark集群活着的master地址）</p>
<pre><code>bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
examples/jars/spark-examples_2.11-2.1.3.jar \
10
</code></pre>
</li>
<li>
<p>2、以高可用模式提交任务到spark集群中运行( 就是我们并不知道哪一个master是活着的master)</p>
</li>
</ul>
<pre><code>bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1:7077,node2:7077,node3:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
examples/jars/spark-examples_2.11-2.1.3.jar \
10

在高可用模式下提交任务，需要把所有的master都罗列出来 spark://node1:7077,node2:7077,node3:7077
整个任务会轮训master列表，最后找到活着的master,然后向这个活着的master申请资源。
</code></pre>
<h4><a id="9sparkshell_278"></a>9、spark-shell使用</h4>
<h5><a id="91_sparkshell_master_localN_280"></a>9.1 通过spark-shell --master local[N]读取本地数据文件实现单词统计</h5>
<ul>
<li>
<p>–master local[N]</p>
</li>
<li>
<p>local 表示本地运行，跟spark集群没有任何关系，N表示一个正整数，在这里表示本地采用N个线程去跑任务</p>
</li>
<li>
<p>提交脚本</p>
<ul>
<li>
<p>spark-shell --master local[2]</p>
<pre><code class="prism language-scala">sc.textFile("file:///root/words.txt").flatMap(x=&gt;x.split(" ")).map(x=&gt;(x,1)).reduceByKey((x,y)=&gt;x+y).collect

sc.textFile("file:///root/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
</code></pre>
</li>
</ul>
</li>
</ul>
<h5><a id="92_sparkshell_master_localNHDFS_296"></a>9.2 通过spark-shell --master local[N]读取HDFS上数据文件实现单词统计</h5>
<ul>
<li>
<p>提交脚本</p>
<ul>
<li>
<p>spark-shell --master local[2]</p>
</li>
<li>
<p>spark整合HDFS</p>
<ul>
<li>
<p>vim <a href="http://spark-env.sh" rel="nofollow">spark-env.sh</a></p>
<pre><code>#spark整合hdfs
export HADOOP_CONF_DIR=/export/servers/hadoop/etc/hadoop
</code></pre>
</li>
</ul>
</li>
</ul>
<pre><code class="prism language-scala">sc.textFile("hdfs://node1:9000/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect

sc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
</code></pre>
</li>
</ul>
<h5><a id="93__sparkshell_sparkmaster_317"></a>9.3 通过 spark-shell 指定spark集群中活着的master</h5>
<ul>
<li>
<p>提交脚本</p>
<ul>
<li>
<p>spark-shell --master spark://node1:7077 --executor-memory 1g --total-executor-cores 2</p>
<pre><code class="prism language-scala">sc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("/out")
</code></pre>
</li>
</ul>
</li>
</ul>
<h4><a id="10IDEAspark_329"></a>10、基于IDEA开发spark的程序</h4>
<ul>
<li>
<p>1、引入pom依赖</p>
<pre><code class="prism language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>spark-core_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.11.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
</code></pre>
</li>
</ul>
<h5><a id="101_scalasparkwordcount_342"></a>10.1 利用scala语言实现spark的wordcount程序（本地运行）</h5>
<ul>
<li>
<p>1、代码开发</p>
<pre><code>package cn.itcast.spark

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

//todo:利用scala语言去开发spark的wordcount程序
object WordCount {
  def main(args: Array[String]): Unit = {
    //1、创建SparkConf对象 设置applicationName和master地址  local[2]表示本地采用2个线程运行任务
      val sparkConf: SparkConf = new SparkConf().setAppName("WordCount").setMaster("local[2]")

    //2、创建SparkContext对象 它是所有spark程序的入口，它内部会创建DAGScheduler和TaskScheduler
      val sc = new SparkContext(sparkConf)

    //3、读取数据文件
      val data: RDD[String] = sc.textFile("E:\\words.txt")

    //4、切分每一行，获取所有的单词
     val words: RDD[String] = data.flatMap(_.split(" "))

    //5、每个单词计为1
      val wordAndOne: RDD[(String, Int)] = words.map((_,1))

    //6、相同单词出现的1累加
      val result: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)

        //按照单词出现的次数降序排列  sortBy默认是升序，给false就是降序
         val sortRDD: RDD[(String, Int)] = result.sortBy(x=&gt;x._2,false)

    //7、收集打印
      val finalResult: Array[(String, Int)] = sortRDD.collect()
      finalResult.foreach(println)

    //8、关闭sparkcontext
      sc.stop()
  }
}

</code></pre>
</li>
</ul>
<h5><a id="102_scalasparkwordcount_387"></a>10.2 利用scala语言实现spark的wordcount程序（集群运行）</h5>
<ul>
<li>
<p>1、代码开发</p>
<pre><code class="prism language-scala">package cn.spark

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD

//todo:利用scala语言开发spark的wordcount（集群运行）
object WordCount_Online {
  def main(args: Array[String]): Unit = {
    //1、创建SparkConf对象 设置applicationName
    val sparkConf: SparkConf = new SparkConf().setAppName("WordCount_Online")

    //2、创建SparkContext对象 它是所有spark程序的入口，它内部会创建DAGScheduler和TaskScheduler
    val sc = new SparkContext(sparkConf)

    //3、读取数据文件
    val data: RDD[String] = sc.textFile(args(0))

    //4、切分每一行，获取所有的单词
    val words: RDD[String] = data.flatMap(_.split(" "))

    //5、每个单词计为1
    val wordAndOne: RDD[(String, Int)] = words.map((_,1))

    //6、相同单词出现的1累加
    val result: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)

    //7、把结果数据保存到hdfs上
    result.saveAsTextFile(args(1))

    //8、关闭sparkcontext
    sc.stop()
  }
}

</code></pre>
<ul>
<li>
<p>2、打成jar包提交到集群运行</p>
<pre><code>spark-submit --master spark://node1:7077 --class cn.itcast.spark.WordCount_Online --executor-memory 1g --total-executor-cores 4 original-spark_class12-1.0-SNAPSHOT.jar /words.txt  /out_spark
</code></pre>
</li>
</ul>
</li>
</ul>
<h5><a id="103_java_sparkwordcount_434"></a>10.3 利用java 语言开发spark的wordcount程序（本地运行）</h5>
<ul>
<li>
<p>1、代码开发</p>
<pre><code class="prism language-java"><span class="token keyword">package</span> cn<span class="token punctuation">.</span>spark<span class="token punctuation">;</span>

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>SparkConf<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>JavaPairRDD<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>JavaRDD<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>JavaSparkContext<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>function<span class="token punctuation">.</span>FlatMapFunction<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>function<span class="token punctuation">.</span>Function2<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>function<span class="token punctuation">.</span>PairFunction<span class="token punctuation">;</span>
<span class="token keyword">import</span> scala<span class="token punctuation">.</span>Tuple2<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Arrays<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Iterator<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>

<span class="token comment">//todo:利用java实现spark的wordcount程序</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCount_Java</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token comment">//1、创建SparkConf对象</span>
        SparkConf sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setAppName</span><span class="token punctuation">(</span><span class="token string">"WordCount_Java"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setMaster</span><span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//2、创建JavaSparkContext对象</span>
        JavaSparkContext jsc <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">JavaSparkContext</span><span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//3、读取数据文件</span>
        JavaRDD<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">&gt;</span></span> data <span class="token operator">=</span> jsc<span class="token punctuation">.</span><span class="token function">textFile</span><span class="token punctuation">(</span><span class="token string">"E:\\words.txt"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//4、切分每一行，获取所有的单词</span>
        JavaRDD<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">&gt;</span></span> words <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FlatMapFunction</span><span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> String<span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
            <span class="token keyword">public</span> Iterator<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">&gt;</span></span> <span class="token function">call</span><span class="token punctuation">(</span>String line<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
                String<span class="token punctuation">[</span><span class="token punctuation">]</span> words <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token keyword">return</span> Arrays<span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">iterator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//5、每个单词计为1</span>
        JavaPairRDD<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> wordAndOne <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token function">mapToPair</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PairFunction</span><span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
            <span class="token keyword">public</span> Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> <span class="token function">call</span><span class="token punctuation">(</span>String word<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
                <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//6、相同单词出现的1累加</span>
        JavaPairRDD<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> result <span class="token operator">=</span> wordAndOne<span class="token punctuation">.</span><span class="token function">reduceByKey</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Function2</span><span class="token generics function"><span class="token punctuation">&lt;</span>Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
            <span class="token keyword">public</span> Integer <span class="token function">call</span><span class="token punctuation">(</span>Integer v1<span class="token punctuation">,</span> Integer v2<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
                <span class="token keyword">return</span> v1 <span class="token operator">+</span> v2<span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//按照单词出现的次数降序 (单词，次数)-------&gt;(次数，单词).sortByKey-----&gt;(单词，次数)</span>

        JavaPairRDD<span class="token generics function"><span class="token punctuation">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token punctuation">&gt;</span></span> reverseJavaPairRDD <span class="token operator">=</span> result<span class="token punctuation">.</span><span class="token function">mapToPair</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PairFunction</span><span class="token operator">&lt;</span>Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span><span class="token punctuation">,</span> Integer<span class="token punctuation">,</span> String<span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
            <span class="token keyword">public</span> Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token punctuation">&gt;</span></span> <span class="token function">call</span><span class="token punctuation">(</span>Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> t<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
                <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token generics function"><span class="token punctuation">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> t<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        JavaPairRDD<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> sortedJavaPairRDD <span class="token operator">=</span> reverseJavaPairRDD<span class="token punctuation">.</span><span class="token function">sortByKey</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">mapToPair</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PairFunction</span><span class="token operator">&lt;</span>Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token punctuation">&gt;</span></span><span class="token punctuation">,</span> String<span class="token punctuation">,</span> Integer<span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
            <span class="token keyword">public</span> Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> <span class="token function">call</span><span class="token punctuation">(</span>Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>Integer<span class="token punctuation">,</span> String<span class="token punctuation">&gt;</span></span> t<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
                <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> t<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//7、收集打印</span>
        List<span class="token operator">&lt;</span>Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span><span class="token operator">&gt;</span> finalResult <span class="token operator">=</span> sortedJavaPairRDD<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Tuple2<span class="token generics function"><span class="token punctuation">&lt;</span>String<span class="token punctuation">,</span> Integer<span class="token punctuation">&gt;</span></span> tuple <span class="token operator">:</span> finalResult<span class="token punctuation">)</span> <span class="token punctuation">{</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"单词："</span><span class="token operator">+</span> tuple<span class="token punctuation">.</span>_1<span class="token operator">+</span><span class="token string">" 次数："</span><span class="token operator">+</span>tuple<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token comment">//8、关闭jsc</span>
            jsc<span class="token punctuation">.</span><span class="token function">stop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre>
</li>
</ul>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>