---
layout:     post
title:      号外：Spark 1.3.0发布了，快来一起飞！
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="spark-130-release-note">Spark 1.3.0 Release Note</h1>

<p>Spark 1.3.0在上周五正式发布，真是千呼万唤始出来。本次发布最大的惊喜就是DataFrame。另外一个值得关注的是Spark SQL从Alpha版毕业，我们终于可以欢快地使用Spark SQL了。本次发布还对Spark核心引擎改的可用性进行了改进，并扩展了Spark MLlib及Spark Streaming，详情见下。最后不得不提下，这次发布共接纳了1000多个patch，真是太火爆了<img src="http://img.t.sinajs.cn/t35/style/images/common/face/ext/normal/70/vw_thumb.gif" alt="" title="">。</p>

<p>Spark 1.3.0的下载位置在<a href="http://spark.apache.org/downloads.html" rel="nofollow">这里</a>。</p>



<h2 id="spark-core">Spark Core</h2>

<p>Spark 1.3.0在核心引擎中引入了很多可用性改进。现在Core API支持<a href="https://issues.apache.org/jira/browse/SPARK-5430" rel="nofollow">多层聚合</a>，有助于加速那些费时的reduce操作。对一些特定操作<a href="https://issues.apache.org/jira/browse/SPARK-5063" rel="nofollow">优化了错误信息</a>。Spark的<a href="https://issues.apache.org/jira/browse/SPARK-3996" rel="nofollow">jetty依赖已经被shade</a>，以避免和用户代码产生冲突。此外，Spark已经支持对Akka和HttpServer的连接进行<a href="https://issues.apache.org/jira/browse/SPARK-3883" rel="nofollow">SSL加密</a>。最后，实时的<a href="https://issues.apache.org/jira/browse/SPARK-3428" rel="nofollow">GC统计信息</a>和<a href="https://issues.apache.org/jira/browse/SPARK-4874" rel="nofollow">记录计数器</a>将会显示在Spark UI中。</p>



<h2 id="dataframe-api">DataFrame API</h2>

<p>Spark 1.3.0添加了一个新的API，<a href="http://spark.apache.org/docs/1.3.0/sql-programming-guide.html#dataframes" rel="nofollow">DataFrame</a>，提供了更加强有力和便捷的方式来操作结构化数据。DataFrame发展自基础的RDD API，包含了命名域和schema信息。我们能够很容易地从Hive表，JSON数据，JDBC数据库或者任意其他数据源中创建一个DataFrame。DataFrame将成为Spark各个模块之间以及Spark和其他系统交换数据的通用接口。Data frames支持Python，Scala和Java语言。</p>



<h2 id="spark-sql">Spark SQL</h2>

<p>在Spark 1.3.0中，Spark SQL正式<a href="https://issues.apache.org/jira/browse/SPARK-5166" rel="nofollow">从Alpha版毕业</a>，向后兼容HiveQL原语并提供稳定的编程接口。Spark SQL支持<a href="https://issues.apache.org/jira/browse/SPARK-5658" rel="nofollow">在数据源API中写表操作</a>。1.3.0版本还提供了从<a href="https://issues.apache.org/jira/browse/SPARK-5472" rel="nofollow">JDBC读写数据表的能力</a>，原生地支持同MySQL，Postgres及其他关系型数据库的交互能力，对HiveQL也做了大量的改进。最后，Spark SQL支持以<a href="https://issues.apache.org/jira/browse/SPARK-3851" rel="nofollow">兼容方式从Parquet文件中加载数据</a>。</p>



<h2 id="spark-mlmllib">Spark ML/MLlib</h2>

<p>Spark 1.3.0引入了一些新的算法：支持<a href="https://issues.apache.org/jira/browse/SPARK-1405" rel="nofollow">主题模型</a>的LDA算法，支持多分类的<a href="https://issues.apache.org/jira/browse/SPARK-2309" rel="nofollow">多元逻辑回归</a>，支持<a href="https://issues.apache.org/jira/browse/SPARK-5012" rel="nofollow">混合高斯模型</a>和<a href="https://issues.apache.org/jira/browse/SPARK-4259" rel="nofollow">PIC聚类</a>，支持频繁集挖掘的<a href="https://issues.apache.org/jira/browse/SPARK-4001" rel="nofollow">FP-growth</a>，此外还有为了支持分布式线性代数的<a href="https://issues.apache.org/jira/browse/SPARK-4409" rel="nofollow">块矩阵抽象</a>。初步支持模型输入输出的<a href="https://issues.apache.org/jira/browse/SPARK-4587" rel="nofollow">交换格式</a>，并将在今后的版本中支持更多的格式。K-means算法和ALS算法有了显著的<a href="https://issues.apache.org/jira/browse/SPARK-3424,%20https://issues.apache.org/jira/browse/SPARK-3541" rel="nofollow">性能提升</a>。PySpark现在也支持<a href="https://issues.apache.org/jira/browse/SPARK-4586" rel="nofollow">ML pipeline API</a>，<a href="https://issues.apache.org/jira/browse/SPARK-5094" rel="nofollow">Gradient Boosted Trees</a>以及混合高斯模型。最后，ML Pipeline API也支持新的DataFrame抽象。</p>



<h2 id="spark-streaming">Spark Streaming</h2>

<p>Spark 1.3.0引入了一个<a href="https://issues.apache.org/jira/browse/SPARK-4964" rel="nofollow">direct Kafka API</a>(<a href="http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html" rel="nofollow">docs</a>)，不需要配置WAL就可保证数据的可靠交付，同时实现了Exactly-Once原语保证强一致性。另外<a href="https://issues.apache.org/jira/browse/SPARK-5047" rel="nofollow">Python版的Kafka API</a>也被加了进来。支持<a href="https://issues.apache.org/jira/browse/SPARK-4979" rel="nofollow">online的逻辑回归算法</a>，支持<a href="https://issues.apache.org/jira/browse/SPARK-4969" rel="nofollow">二进制数据</a>的读取。对于那些有状态操作，增加了<a href="https://issues.apache.org/jira/browse/SPARK-3660" rel="nofollow">初始状态RDD</a>的支持。最后Spark Streaming的指导文档已经包含了SQL，DataFrame和容错等相关内容。</p>



<h2 id="graphx">GraphX</h2>

<p>Spark GraphX增加了很多实用的接口，包括如何将图转换成一个<a href="https://issues.apache.org/jira/browse/SPARK-4917" rel="nofollow">边规范化图</a></p>



<h2 id="升级到spark-13">升级到Spark 1.3</h2>

<p>Spark 1.3兼容1.X的版本，所以不需要修改任何代码。当然不包括那些被隐含标识为不稳定的API。</p>

<p>作为稳定版Spark SQL API的一部分，SchemaRDD已经被重命名为DataFrame。Spark SQL指引文档已经详细说明了如何去修改你的代码。</p>



<h2 id="待解决issue">待解决Issue</h2>

<p>下面这些issue将在Spark 1.3.1中修复</p>

<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-6194" rel="nofollow">SPARK-6194</a>: 解决PySpark collect()接口中的内存泄漏问题。</li>
<li><a href="https://issues.apache.org/jira/browse/SPARK-6222" rel="nofollow">SPARK-6222</a>: 修复Spark Streaming中一个失败恢复问题。</li>
<li><a href="https://issues.apache.org/jira/browse/SPARK-6315" rel="nofollow">SPARK-6315</a>: 解决Spark SQL无法读取Spark 1.1产生的parquet数据问题。</li>
<li><a href="https://issues.apache.org/jira/browse/SPARK-6247" rel="nofollow">SPARK-6247</a>: 解决Spark SQL中分析特定Join类型出错的问题。</li>
</ul>



<h4 id="附官方正版">附<a href="http://spark.apache.org/releases/spark-release-1-3-0.html" rel="nofollow">官方正版</a></h4>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>