---
layout:     post
title:      Hadoop详解(一)——大数据和Hadoop的简介、Hadoop伪分布式的安装步骤
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/u013087513/article/details/77720126				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1><span style="font-size:14px;">大数据简介</span></h1><h2><span style="font-size:14px;">大数据体现</span></h2><p><img src="https://img-blog.csdn.net/20170830200549064?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>以上几个例子体现了大数据无处不在。</p><p>目前很多网络巨头都在使用大数据</p><p><img src="https://img-blog.csdn.net/20170830201815030?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>分析趋势，随着互联网对网民的理解，网民对网络的反作用，互联网将变得越来越智能，它在满足用户需求的同时，也在创造新的需求。</p><p>谷歌的盈利在于所有的软件应用都是在线的，用户在免费使用这些软件产品的同时，把个人的行为、喜好等信息也免费的送给了谷歌。因此，谷歌的产品越丰富，它对用户的理解就越深入，它的广告就越精准，广告的价值就越高。</p><p>这是正向的循环，谷歌利用好用的、免费的软件产品，换取对用户的理解，通过精准的广告，找到生财之路，颠覆了微软卖软件拷贝赚钱的模式，成为互联网的巨头。</p><h2><span style="font-size:14px;">什么是大数据？</span></h2><p>大数据并不仅仅指海量数据，而是指数型增长的海量数据。</p><p><img src="https://img-blog.csdn.net/20170830205959544?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>注意：大数据并不是云计算，云计算指的是数据处理技术。大数据是以数据融合、综合处理为方向。</p><p><img src="https://img-blog.csdn.net/20170830210451241?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><h2><span style="font-size:14px;">大数据的4V特性</span></h2><p>① <span style="color:#3333ff;">体量Volume</span>。<span style="font-size:12px;"><span style="color:#ff0000;">非结构化数据</span>的超大规模和增长，占据总数据量的80%~90%，比结构化数据增长快到10倍到50倍，是传统数据的10倍到50倍。</span></p><p><span style="font-size:12px;">② <span style="color:#3333ff;">多样性Variety</span>。大数据的异构和多样性，存在很多不同的形式(文本、图像视频、机器数据)。无模式或者模式不明显，语法或语义不连贯。</span></p><p><span style="font-size:12px;">③<span style="color:#3333ff;"> 价值密度Value</span>。存在很多大量不相关的信息，用来对未来趋势与模式的可预测分析，深度复杂分析(机器学习、人工智能VS传统商务智能(咨询、报告等))</span></p><p><span style="font-size:12px;">④ <span style="color:#3333ff;">速度Velocity</span>。<span style="color:#ff0000;">实时分析</span>而非批量式分析，数据输入、处理与丢弃，立竿见影而非事后生效。</span></p><p>特性的具体体现如下：</p><p>-Value 价值：</p><p>挖掘大数据的价值类似于沙里淘金，从海量数据中挖掘出稀疏但珍贵的信息。<span style="color:#ff0000;">价值密度低</span>是大数据的一个典型特征。</p><p><img src="https://img-blog.csdn.net/20170830213319580?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>-Variety 多样性</p><p>企业内部的经营交易信息；物联网世界中商品,物流信息；互联网世界中人与人交互信息,位置信息是大数据的主要来源。能够在不同的数据模型中，进行交叉分析的技术，是大数据的核心技术之一，包括语义分析技术，图文转换技术，模式识别技术，地理信息技术等，都会在大数据分析时获得应用。</p><p><img src="https://img-blog.csdn.net/20170830215453552?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>-Velocity 速度</p><p>1s 是临界点</p><p>对于大数据应用而言，必须在1秒内形成答案，否则处理结果就是过时和无效的。</p><p>实时处理的要求，是区别大数据引用和传统数据仓库技术,BI技术的关键差别之一。</p><p>-Volume  数据量</p><p>PB是大数据的临界点 KB-&gt;MB-&gt;GB-&gt;TB-&gt;<span style="color:#ff0000;">PB</span>-&gt;EB-&gt;ZB-&gt;YB-&gt;NB-&gt;DB</p><p><img src="https://img-blog.csdn.net/20170830220518018?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><h2>大数据的组成和展现方式</h2><p>大数据的组成</p><p><img src="https://img-blog.csdn.net/20170830221546315?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>大数据的展现方式：大型控制中心、移动终端</p><p><img src="https://img-blog.csdn.net/20170830221914298?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><span style="color:#3333ff;">在多样性、体量、速度三大主要特征的指引下，大数据将有新型的展现方式：大型控制中心和移动终端，实现数据的实时处理和快速决策</span>。</p><h1><span style="font-size:14px;">哪些公司使用Hadoop</span></h1><p>雅虎北京全球软件研发中心、中国移动研究院，英特尔研究院，金山软件，百度，腾讯，新浪，搜狐，淘宝，IBM，FaceBook，Amazon，Yahoo</p><h1><span style="font-size:14px;">Hadoop简介</span></h1><h2><span style="font-size:14px;">Hadoop是什么？</span></h2><p>官方解释是：</p><p>What Is Apache Hadoop?<br>The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing.<br></p><p>Hadoop是一个开源，可靠，可扩展的分布式计算软件。</p><p>Hadoop用于解决以下问题：</p><p></p><ul><li><span style="color:#3333ff;">海量数据的存储(HDFS)</span></li><li><span style="color:#3333ff;">海量数据的分析(MapReduce)</span></li><li><span style="color:#3333ff;">资源管理调度(YARN)</span></li></ul><p></p><p>作者：Doug Cutting<br>受Google三篇论文的启发(GFS、MapReduce、BigTable)<br></p><p>始于apache项目Nutch<br>2003年Google发表了关于GFS的论文<br>2004年Nutch的开发者开发了NDFS<br>2004年Google发表了关于MapReduce的论文<br>2005年MapReduce被引入了NDFS<br>2006年改名为Hadoop，NDFS的创始人加入Yahoo，Yahoo成立了一个专门的小组发展Hadoop<br><br>Hadoop大事记<br>2004年 -- 最初的版本(现在称为HDFS和MapReduce)由Doug Cutting和Mike Cafarella开始实施。<br>2005年12月 -- Nutch移植到新的框架，Hadoop在20个节点上稳定运行。<br>2006年01月 -- Doug Cutting加入雅虎。<br>2006年02月 -- Apache Hadoop项目正式启动以支持MapReduce和HDFS的独立发展。<br>2006年02月 -- 雅虎的网格计算团队采用Hadoop。<br>2006年04月 -- 标准排序(10 GB每个节点)在188个节点上运行47.9个小时。<br>2006年05月 -- 雅虎建立了一个300个节点的Hadoop研究集群。<br>2006年05月 -- 标准排序在500个节点上运行42个小时(硬件配置比4月的更好)。<br>2006年11月 -- 研究集群增加到600个节点。<br>2006年12月 -- 标准排序在20个节点上运行1.8个小时，100个节点3.3小时，500个节点5.2小时，900个节点7.8个小时。<br>2007年01月 -- 研究集群到达900个节点。<br>2007年04月 -- 研究集群达到两个1000个节点的集群。<br>2008年04月 -- 赢得世界最快1TB数据排序在900个节点上用时209秒。<br>2008年10月 -- 研究集群每天装载10 TB的数据。<br>2009年03月 -- 17个集群总共24 000台机器。<br>2009年04月 -- 赢得每分钟排序，59秒内排序500 GB(在1400个节点上)和173分钟内排序100 TB数据(在3400个节点上)。<br></p><h2>hadoop具体能做什么工作？</h2><p><span style="color:#3333ff;">hadoop擅长日志分析</span>，facebook就用Hive来进行日志分析，2009年时facebook就有非编程人员的30%的人使用HiveQL进行数据分析；淘宝搜索中的自定义筛选也使用的Hive；利用Pig还可以做高级的数据处理，包括Twitter、LinkedIn 上用于发现您可能认识的人，可以实现类似Amazon.com的协同过滤的推荐效果。淘宝的商品推荐也是！在Yahoo！的40%的Hadoop作业是用pig运行的，包括垃圾邮件的识别和过滤，还有用户特征建模。（2012年8月25新更新，天猫的推荐系统是hive，少量尝试mahout！）<br></p><h2><span style="font-size:14px;">Hadoop整体流程</span></h2><p><img src="https://img-blog.csdn.net/20170830232154145?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>(1) 这一切起源于web上庞大的数据。</p><p>(2) 使用Nutch抓取WEB数据。</p><p>(3) 要保存web上庞大的数据—HDFS应运而生。</p><p>(4) 如何使用这些庞大的数据？</p><p>(5) 采用Java或任何的流/管道语言构建MapReduce框架用于编码并进行分析。</p><p>(6) 如何获取web日志，点击流，Apache日志，服务器日志等非结构化数据—fuse，webdav，chukwa，flume，scribe</p><p>(7) Hiho和sqoop将数据加载到<span style="font-size:12px;">HDFS中，关系型数据库也能加入到Hadoop队伍中。</span></p><p><span style="font-size:12px;">(8) MapReduce编程需要的高级接口—Pig，Hive，Jaql。</span></p><p><span style="font-size:12px;">(9) 具有先进的UI报表工具的BI工具—Intellicus。</span></p><p>(10) Map-Reduce处理过程使用的工作流工具及高级语言。</p><p>(11) 监控、管理Hadoop，运行jobs/hive，查看HDFS的高级视图—Hue，karmasphere,eclipse plugin，cacti。</p><p>(12) 支持框架—Avro (进行序列化)，ZooKeeper(用于协同)。</p><p>(13) 更多高级接口—Mahout，Elastic map Reduce。</p><p>(14) 同样可以进行OLTP—Hbase.</p><p>Hadoop版本</p><p>Apache 官方版本(2.x.x)<br>Cloudera：使用下载最多的版本，稳定，有商业支持，在Apache的基础上打上了一些patch。推荐使用。<br>HDP(Hortonworks Data Platform) Hortonworks公司发行版本。<br></p><p>hadoop的核心：</p><p>HDFS: Hadoop Distributed File System 分布式文件系统 <br>YARN: Yet Another Resource Negotiator 资源管理调度系统<br></p><p>如何解决海量数据的存储和海量数据的运算？</p><p><img src="https://img-blog.csdn.net/20170831081734237?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><h2>HDFS的架构</h2><p>主从结构：主节点，只有一个活跃的NameNode；从节点，有很多个DataNode。</p><p>NameNode负责：</p><p></p><ul><li><span style="color:#3333ff;">接收用户操作请求；</span></li><li><span style="color:#3333ff;">维护文件系统的目录结构；</span></li><li><span style="color:#3333ff;">管理文件与Block之间的关系，Block与DataNode之间的关系；</span></li></ul><p></p><p>DataNode负责：</p><p></p><ul><li><span style="color:#3333ff;">存储文件；</span></li><li><span style="color:#3333ff;">文件被分成Block存储在磁盘上；</span></li><li><span style="color:#3333ff;">为保证数据安全，文件会有多个副本</span>；</li></ul><p></p><p>如下图所示：</p><p><img src="https://img-blog.csdn.net/20170831082013846?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><h2><span style="font-size:14px;">Hadoop的特点</span></h2><p>扩容能力(Scalable)：能可靠的(reliably)存储和处理千兆(PB)字节。</p><p>成本低(Economical)：可以通过普通机器组成的服务器群来分发以及处理数据。这些服务器群总计可达上千个节点。</p><p>高效率(Efficient)：通过分发数据，Hadoop可以在数据所在的节点上并行地(parallel)处理它们，这使得处理非常快速。</p><p>可靠性(Reliable)：Hadoop能自动维护数据的多份副本，并且在任务失败后能自动的重新部署(reploy)计算任务。</p><h2>Hadoop1.0和Hadoop2.0对比</h2><p><img src="https://img-blog.csdn.net/20170831102013401?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzA4NzUxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p>Hadoop1.0没有yarn，计算框架非常单一只有MapReduce。2.0之后所有的计算框架都要基于YARN平台。</p><p>Hadoop部署方式</p><p></p><p class="p1">本地模式：在使用开发工具进行开发调试的时候使用的<span class="s1">  </span>只能启动一个<span class="s1">map</span>和一个<span class="s1">reduce</span>，适用于开发环境。</p><p class="p1">伪分布式：通过一台机器模拟分布式环境，在开发和学习时使用。适用于测试环境</p><p class="p1">集群模式：真实的开发环境。</p><h1><span style="font-size:14px;">Hadoop伪分布式安装</span></h1><div><span style="font-size:14px;">安装前戏：关闭防火墙，修改IP，修改hostname，设置ssh自动登录，安装jdk</span></div><div><span style="font-size:14px;">安装完成后最后安装Hadoop。</span></div><div><span style="font-size:14px;">首先下</span><span style="font-size:14px;">载Linux版的版的JDK安装包：</span></div><div><span style="font-size:14px;">再下载Hadoop安装包：<a href="http://archive.apache.org/dist/" rel="nofollow">http://archive.apache.org/dist/</a></span></div><div><span style="font-size:14px;">正式开始伪分布式的搭建.....</span></div><h2><span style="font-size:14px;">一.准备Linux环境</span></h2><div><span style="font-size:14px;color:#ff0000;"><strong>1.0 改用桥接联网</strong></span></div><div><span style="font-size:14px;color:#ff0000;"><strong>1.1 修改主机名</strong></span></div><div><span style="font-size:14px;">vim /etc/sysconfig/network<br></span></div><div><span style="font-size:14px;"><br></span></div><div><span style="font-size:14px;">NETWORKING=yes<br>HOSTNAME=hadoop0<br></span></div><div><span style="font-size:14px;color:#ff0000;"><strong>1.2修改IP</strong></span></div><div><span style="font-size:14px;">第一种：通过Linux图形界面进行修改(强力推荐)</span></div><div><span style="font-size:14px;">进入Linux图形界面 -&gt; 右键点击右上方的两个小电脑 -&gt; 点击Edit connections -&gt; 选中当前网络System eth0 -&gt; 点击edit按钮 -&gt; 选择IPv4 -&gt; method选择为manual -&gt; 点击add按钮 -&gt; 添加IP：192.168.0.3子网掩码：255.255.255.0 网关：192.168.0.1 -&gt; apply<br></span></div><div><span style="font-size:14px;">第二种：修改配置文件的方式</span></div><div><span style="font-size:14px;"> vim /etc/sysconfig/network-scripts/ifcfg-eth0<br></span></div><div><span style="font-size:14px;">配置内容如下：</span></div><div><span style="font-size:14px;">DEVICE=eth0<br>TYPE=Ethernet<br>UUID=1c433f4c-52e9-4be9-b48c-72889419c875<br>ONBOOT=yes<br>NM_CONTROLLED=yes<br><span style="color:#3333ff;">BOOTPROTO=static ####<br>IPADDR=192.168.0.3 ####</span><br>PREFIX=24<br><span style="color:#3333ff;">GATEWAY=192.168.0.1 ####<br>DNS1=8.8.8.8  ####<br>NETMASK=255.255.255.0 ####</span><br>DEFROUTE=yes<br>IPV4_FAILURE_FATAL=yes<br>IPV6INIT=no<br>NAME="System eth0"<br>HWADDR=08:00:27:47:CD:00<br>LAST_CONNECT=1504084493<br></span></div><div><span style="font-size:14px;color:#ff0000;"><strong>1.3修改主机名和IP的映射关系</strong></span></div><div><span style="font-size:14px;">vim /etc/hosts<br></span></div><div><span style="font-size:14px;">配置内容如下：</span></div><div><span style="font-size:14px;">192.168.0.3 hadoop0<br>127.0.0.1 localhost<br></span></div><div><span style="font-size:14px;">注意：localhost 不能去除，因为有很多配置引用到localhost</span></div><div><span style="font-size:14px;color:#ff0000;"><strong>1.4关闭防火墙</strong></span></div><div><span style="font-size:14px;">#查看防火墙状态</span></div><div><span style="font-size:14px;">service iptables status<br></span></div><div><span style="font-size:14px;">#关闭防火墙</span></div><div><span style="font-size:14px;">service iptables stop<br></span></div><div><span style="font-size:14px;">#查看防护墙开机启动状态</span></div><div><span style="font-size:14px;">chkconfig iptables --list<br></span></div><div><span style="font-size:14px;">#关闭防火墙开机启动</span></div><div><span style="font-size:14px;">chkconfig iptables off<br></span></div><div><span style="font-size:14px;color:#ff0000;"><strong>1.5重启Linux</strong></span></div><div><span style="font-size:14px;">reboot</span></div><h2><span style="font-size:14px;">二.安装JDK</span></h2><div><span style="font-size:14px;color:#ff0000;"><strong>2.0卸载Linux自带的JDK</strong></span></div><div><span style="font-size:14px;">rpm -qa|grep jdk #查询自带的JDK<br></span></div><div><span style="font-size:14px;">yum -y remove  &lt;老版JDK&gt;</span></div><div><span style="font-size:14px;color:#ff0000;"><strong>2.1解压JDK安装包</strong></span></div><div><span style="font-size:14px;">#创建文件夹</span></div><div><span style="font-size:14px;">mkdir /usr/java</span></div><div><span style="font-size:14px;">#解压</span></div><div><span style="font-size:14px;">tar -zxvf jdk-8u144-linux-x64.tar.gz -C /usr/java/</span></div><div><span style="font-size:14px;color:#ff0000;"><strong>2.2将Java添加至环境变量</strong></span></div><div><span style="font-size:14px;">vim /etc/profile<br></span></div><div><span style="font-size:14px;">#进入编辑界面按shift+g 在文件最后添加</span></div><div><span style="font-size:14px;">export JAVA_HOME=/usr/java/jdk1.8.0_144<br></span></div><div><span style="font-size:14px;">export PATH=$PATH:$JAVA_HOME/bin<br></span></div><div><span style="font-size:14px;"><br></span></div><div><span style="font-size:14px;">#刷新配置</span></div><div><span style="font-size:14px;">source /etc/profile</span></div><h2><span style="font-size:14px;">三.安装Hadoop2.7.4</span></h2><div><span style="font-size:14px;color:#ff0000;"><strong>3.0 上传Hadoop安装包</strong></span></div><div><span style="font-size:14px;color:#ff0000;"><strong>3.1 解压</strong></span></div><div><span style="font-size:14px;"># 在根目录下新建文件夹cloud用于安装与大数据相关的</span></div><div><span style="font-size:14px;">mkdir /cloud</span></div><div><span style="font-size:14px;">#解压</span></div><div><span style="font-size:14px;">tar -zxvf /root/Desktop/hadoop-2.7.4.tar.gz -C /cloud/</span></div><div><span style="font-size:14px;color:#ff0000;"><strong>3.2配置hadoop</strong></span></div><div><span style="font-size:14px;">注意：hadoop2.x的配置文件在 $HADOOP_HOME/etc/hadoop/目录下</span></div><div><span style="font-size:14px;">#进入配置目录</span></div><div><span style="font-size:14px;">cd /cloud/hadoop-2.7.4/etc/hadoop/<br></span></div><div><span style="font-size:14px;">伪分布式需要修改5个配置文件</span></div><div><span style="font-size:14px;"><span style="color:#3333ff;">第一个：hadoop-env.sh</span></span></div><div><span style="font-size:14px;">vim hadoop-env.sh <br></span></div><div><span style="font-size:14px;"># 第27行</span></div><div><span style="font-size:14px;"></span><pre><code class="language-plain">export JAVA_HOME=/usr/java/jdk1.8.0_144</code></pre><span style="color:#3333ff;"><span style="font-size:14px;">第二个：</span><span style="font-size:14px;">core-site.xml</span></span></div><div><span style="font-size:14px;">vim core-site.xml <br></span></div><div><span style="font-size:14px;"></span><pre><code class="language-html">&lt;configuration&gt;
        &lt;!-- 制定HDFS的老大（NameNode）的地址 --&gt;
        &lt;property&gt;
                &lt;name&gt;fs.defaultFS&lt;/name&gt;
                &lt;value&gt;hdfs://hadoop0:9000&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;
        &lt;property&gt;
                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
                &lt;value&gt;/cloud/hadoop-2.7.4/tmp&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;</code></pre><span style="color:#3333ff;">第三个：hdfs-site.xml </span></div><div><span style="font-size:14px;">vim hdfs-site.xml</span></div><div><span style="font-size:14px;"></span><pre><code class="language-html">&lt;configuration&gt;
        &lt;!-- 指定HDFS副本的数量 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.replication&lt;/name&gt;
                &lt;value&gt;1&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;</code></pre><span style="color:#3333ff;">第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml)<br></span>mv mapred-site.xml.template mapred-site.xml</div><div><span style="font-size:14px;">vim mapred-site.xml</span></div><div><span style="font-size:14px;"></span><pre><code class="language-html">&lt;configuration&gt;
        &lt;!-- 指定mr运行在yarn上 --&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
                &lt;value&gt;yarn&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;</code></pre><span style="color:#3333ff;">第五个：yarn-site.xml</span></div><div><span style="font-size:14px;">vim yarn-site.xml<br></span><pre><code class="language-html">&lt;configuration&gt;
        &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
                &lt;value&gt;hadoop0&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- reducer获取数据的方式 --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 解决NodeManager无法启动以及job卡住的问题 --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
                &lt;value&gt;2048&lt;/value&gt;
        &lt;/property&gt;
         &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
                &lt;value&gt;1&lt;/value&gt;
         &lt;/property&gt;
&lt;/configuration&gt;</code></pre><span style="color:#ff0000;">3.3将Hadoop添加至环境变量</span><br>vim /etc/profile</div><div><span style="font-size:14px;">修改配置内容如下：</span></div><div><span style="font-size:14px;">export JAVA_HOME=/usr/java/jdk1.8.0_144<br>export HADOOP_HOME=/cloud/hadoop-2.7.4<br>export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br><br>source /etc/profile</span></div><div><span style="font-size:14px;"><strong><span style="color:#ff0000;">3.4格式化NameNode(是对NameNode进行初始化)</span></strong><br>hdfs namenode -format (hadoop namenode -format)</span></div><div><span style="font-size:14px;color:#ff0000;"><strong>3.5启动hadoop</strong></span></div><div><span style="font-size:14px;">(start-all.ssh已经过时了，不建议使用)</span></div><div><span style="font-size:14px;">先启动HDFS</span></div><div><span style="font-size:14px;">sbin/start-dfs.sh</span></div><div><span style="font-size:14px;">再启动YARN</span></div><div><span style="font-size:14px;">sbin/start-yarn.sh</span></div><div><span style="font-size:14px;color:#ff0000;"><strong>3.6 验证是否启动成功</strong></span></div><div><span style="font-size:14px;">使用jps命令验证</span></div><div><span style="font-size:14px;">[root@hadoop0 hadoop]# jps<br>3827 ResourceManager<br>3673 SecondaryNameNode<br>3931 NodeManager<br>3372 NameNode<br>3502 DataNode<br>4142 Jps<br></span></div><div><span style="font-size:14px;background-color:rgb(255,255,255);">(注意：</span><p class="p1">yarn <span class="s1">的老大是</span>ResourceManager<span>    </span><span class="s1">负责资源的分配和调度</span></p><p class="p1">yarn<span class="s1">的小弟是</span>NodeManager<span>  </span>NodeManager<span class="s1">可以有一个或多个</span></p><p class="p2"><span class="s2">NameNode</span>是<span class="s2">HDFS</span>的老大<span class="s2"> </span>负责接收用户请求，接收一些元数据，维护目录树的映射关系<span class="s2"> </span>真正的集群有多个<span class="s2">NameNode </span>防止宕机<span class="s2"> </span>出现事故<span class="s2"> </span>分为主机和备机</p><p class="p1">DataNode<span class="s1">是</span>HDFS<span class="s1">的小弟</span> <span class="s1">负责存储部分数据，可以有多个</span>DataNode</p><p class="p1">SecondaryNameNode <span class="s1">并不是</span>NameNode<span class="s1">的热备，只是</span>NameNode<span class="s1">的一个助理，帮助</span>NameNode<span class="s1">完成一些事情(帮助主数据进行同步</span> <span class="s1">合并一些数据文件)</span></p>)</div><div><span style="font-size:14px;">浏览器查看</span></div><div><span style="font-size:14px;">http://192.168.0.3:50070 （HDFS管理界面）<br>http://192.168.0.3:8088 （MR管理界面）<br></span></div><h2><span style="font-size:14px;">四.配置ssh免登陆</span></h2><div><span style="font-size:14px;">#生成ssh免登陆密钥<br>#进入到我的home目录<br>cd ~/.ssh<br>ssh-keygen -t rsa （四个回车）<br>执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）<br>将公钥拷贝到要免登陆的机器上<br>ssh-copy-id localhost<br>比如A 要免登陆远程访问 B  需要将A的公钥发送给主机B<br></span></div><div><span style="font-size:14px;"><br></span></div><div><span style="font-size:14px;"><br></span></div><div><br></div>            </div>
                </div>