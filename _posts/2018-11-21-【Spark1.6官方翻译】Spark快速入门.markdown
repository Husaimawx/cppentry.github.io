---
layout:     post
title:      【Spark1.6官方翻译】Spark快速入门
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>英文标题：Quick Start <br>
英文原址：<a href="http://spark.apache.org/docs/latest/quick-start.html" rel="nofollow">http://spark.apache.org/docs/latest/quick-start.html</a> <br>
Spark Version:1.6.0</p>

<p></p><div class="toc">
<ul>
<li><ul>
<li><a href="#1-%E4%BD%BF%E7%94%A8spark-shell%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92%E5%88%86%E6%9E%90" rel="nofollow">使用Spark Shell进行交互分析</a><ul>
<li><a href="#1-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8" rel="nofollow">基础使用</a></li>
<li><a href="#2-%E5%A4%8D%E6%9D%82%E7%9A%84rdd%E6%93%8D%E4%BD%9C" rel="nofollow">复杂的RDD操作</a></li>
<li><a href="#3-%E7%BC%93%E5%AD%98caching" rel="nofollow">缓存Caching</a></li>
</ul>
</li>
<li><a href="#2-spark%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F" rel="nofollow">Spark应用程序</a></li>
<li><a href="#3-%E6%B7%B1%E5%85%A5spark" rel="nofollow">深入Spark</a></li>
</ul>
</li>
</ul>
</div>




<h2 id="1-使用spark-shell进行交互分析">1. 使用Spark Shell进行交互分析</h2>



<h3 id="1-基础使用">1. 基础使用</h3>

<p>Spark Shell提供了建议的学习API的方式，是一个进行交互式数据分析的强大工具，它提供了Scala和Python的交互环境。</p>

<ul>
<li>scala</li>
</ul>



<pre class="prettyprint"><code class=" hljs livecodeserver">./bin/spark-<span class="hljs-built_in">shell</span></code></pre>

<ul>
<li>python</li>
</ul>



<pre class="prettyprint"><code class=" hljs ">./bin/pyspark</code></pre>

<p>Spark的主要抽象是一个叫弹性分布式数据集（Resilient Distributed Dataset,RDD）的数据集合，RDD可以由Hadoop的输入格式（例如HDFS文件）创建或者由其他RDD转换而来。下例RDD由一个文件格式创建而来：</p>



<pre class="prettyprint"><code class=" hljs avrasm">scala&gt; val textFile = sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"/data/README.md"</span>)
<span class="hljs-label">textFile:</span> org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.rdd</span><span class="hljs-preprocessor">.RDD</span>[String] = MapPartitionsRDD[<span class="hljs-number">1</span>] at textFile at &lt;console&gt;:<span class="hljs-number">27</span></code></pre>

<p>RDD有动作-actions（返回值）和转换-transformations（返回指向新RDD的指针）两种操作：</p>

<ul>
<li>动作举例</li>
</ul>



<pre class="prettyprint"><code class=" hljs avrasm">scala&gt; val textFile<span class="hljs-preprocessor">.count</span>() //此RDD中项-items的数量
<span class="hljs-label">res0:</span> Long = <span class="hljs-number">95</span>
scala&gt; textFile<span class="hljs-preprocessor">.first</span>() //此RDD的第一个项
<span class="hljs-label">res1:</span> String = <span class="hljs-preprocessor"># Apache Spark</span></code></pre>

<ul>
<li>转换举例</li>
</ul>



<pre class="prettyprint"><code class=" hljs livecodeserver">scala&gt; val linesWithSpark = textFile.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">line</span> =&gt; <span class="hljs-built_in">line</span>.<span class="hljs-operator">contains</span>(<span class="hljs-string">"Spark"</span>))
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[<span class="hljs-number">2</span>] <span class="hljs-keyword">at</span> <span class="hljs-built_in">filter</span> <span class="hljs-keyword">at</span> &lt;console&gt;:<span class="hljs-number">29</span></code></pre>

<p>同时可以将动作和转换串起来执行：</p>



<pre class="prettyprint"><code class=" hljs livecodeserver">scala&gt; textFile.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">line</span> =&gt; <span class="hljs-built_in">line</span>.<span class="hljs-operator">contains</span>(<span class="hljs-string">"Spark"</span>)).count   <span class="hljs-comment"> //多少行包含单词‘Spark’</span>
res2: Long = <span class="hljs-number">17</span></code></pre>



<h3 id="2-复杂的rdd操作">2. 复杂的RDD操作</h3>

<p>RDD的动作和转换可以用来进行更复杂的计算。</p>

<ul>
<li>示例任务：找出README.md中包含单词数最多的行的单词数量</li>
</ul>



<pre class="prettyprint"><code class=" hljs coffeescript">scala&gt; textFile.map<span class="hljs-function"><span class="hljs-params">(line =&gt; line.split(<span class="hljs-string">" "</span>).size)</span>.<span class="hljs-title">reduce</span><span class="hljs-params">((a,b) =&gt; <span class="hljs-keyword">if</span>(a &gt; b) a <span class="hljs-keyword">else</span> b)</span>
<span class="hljs-title">res3</span>: <span class="hljs-title">Int</span> = 14</span></code></pre>

<p>首先将每一行映射（map）成一个整型值（创建了一个新的RDD），然后在此RDD上调用reduce找单词数量最大的。map和reduce的参数是Scala中函数，可以使用任意语言特征或Scala/Java库。例如，可以轻松的调用其他地方声明的函数，如下使用了Java中的Math.max()函数达到上述代码同样的效果：</p>



<pre class="prettyprint"><code class=" hljs avrasm">scala&gt; import java<span class="hljs-preprocessor">.lang</span><span class="hljs-preprocessor">.Math</span>
import java<span class="hljs-preprocessor">.lang</span><span class="hljs-preprocessor">.Math</span>

scala&gt; textFile<span class="hljs-preprocessor">.map</span>(line =&gt; line<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">" "</span>)<span class="hljs-preprocessor">.size</span>)<span class="hljs-preprocessor">.reduce</span>((a,b) =&gt; Math<span class="hljs-preprocessor">.max</span>(a,b))
<span class="hljs-label">res4:</span> Int = <span class="hljs-number">14</span></code></pre>

<pre><code>一个比较通用的数据计算模型是MapReduce，Spark可以轻松的执行MapReduce，如下：
</code></pre>



<pre class="prettyprint"><code class=" hljs coffeescript">scala&gt; val wordCounts = textFile.flatMap<span class="hljs-function"><span class="hljs-params">(line =&gt; line.split(<span class="hljs-string">" "</span>))</span>.<span class="hljs-title">map</span><span class="hljs-params">(word =&gt; (word,<span class="hljs-number">1</span>))</span>.<span class="hljs-title">reduceByKey</span><span class="hljs-params">((a,b) =&gt; a+b)</span>
<span class="hljs-title">wordCounts</span>: <span class="hljs-title">org</span>.<span class="hljs-title">apache</span>.<span class="hljs-title">spark</span>.<span class="hljs-title">rdd</span>.<span class="hljs-title">RDD</span>[<span class="hljs-params">(String, Int)</span>] = <span class="hljs-title">ShuffledRDD</span>[8] <span class="hljs-title">at</span> <span class="hljs-title">reduceByKey</span> <span class="hljs-title">at</span> &lt;<span class="hljs-title">console</span>&gt;:30</span></code></pre>

<p>此处结合了flatMap,map,reduceByKey几个不同的转换计算文件中每一个单词的数量，得到一个(String,Int)对的RDD。接下来使用collection动作收集和呈现单词技术的结果：</p>



<pre class="prettyprint"><code class=" hljs autohotkey">scala&gt; wordCounts.collect()
<span class="hljs-label">res5:</span> Array[(String, Int)] = Array((package,<span class="hljs-number">1</span>), (this,<span class="hljs-number">1</span>), (engine,<span class="hljs-number">1</span>), (version,<span class="hljs-number">1</span>), (file,<span class="hljs-number">1</span>), (documentation,,<span class="hljs-number">1</span>), (MASTER,<span class="hljs-number">1</span>), (example,<span class="hljs-number">3</span>), (are,<span class="hljs-number">1</span>), (systems.,<span class="hljs-number">1</span>), (params,<span class="hljs-number">1</span>), (scala&gt;,<span class="hljs-number">1</span>), (DataFrames,,<span class="hljs-number">1</span>), (provides,<span class="hljs-number">1</span>), (refer,<span class="hljs-number">2</span>)...</code></pre>



<h3 id="3-缓存caching">3. 缓存（Caching）</h3>

<p>Spark支持将数据集拉到集群的内存缓存中，当数据集被频繁的访问（如热点数据集或云心个交互式的算法例如PageRank）时，这个特性特别重要。请看示例：</p>



<pre class="prettyprint"><code class=" hljs avrasm">scala&gt; linesWithSpark<span class="hljs-preprocessor">.cache</span>()
<span class="hljs-label">res6:</span> linesWithSpark<span class="hljs-preprocessor">.type</span> = MapPartitionsRDD[<span class="hljs-number">2</span>] at filter at &lt;console&gt;:<span class="hljs-number">29</span>

scala&gt; linesWithSpark<span class="hljs-preprocessor">.count</span>()
<span class="hljs-label">res7:</span> Long = <span class="hljs-number">17</span>

scala&gt; linesWithSpark<span class="hljs-preprocessor">.count</span>()
<span class="hljs-label">res8:</span> Long = <span class="hljs-number">17</span></code></pre>

<p>示例中，将linesWithSpark这个RDD进行缓存（虽然看上去将一个100行左右的文件进行缓存很可笑，但是此操作同样可以应用于非常大的数据集之上，即使数据集跨上百个节点同样可以），然后对它进行了2次count操作。从<a href="http://localhost:4040/jobs/" rel="nofollow">http://localhost:4040/jobs/</a> 的作业完成来看，缓存之后第一次计算仍然需要0.1s，但是第二次计算由于已经缓存成功，所以所需时长为16ms。 <br>
<img src="https://img-blog.csdn.net/20160128171308145" alt="cache后的计算时长" title=""></p>



<h2 id="2-spark应用程序">2. Spark应用程序</h2>

<p>假设我们希望使用Spark API写一个自包含的应用程序，下面示例使用Scala（sbt编译）实现（Java（Maven编译）、Python请跳转至官方处查看）。</p>

<ul>
<li>scala</li>
</ul>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-comment">/* SimpleApp.scala */</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkContext</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkContext</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkConf</span>

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = <span class="hljs-string">"/data/README.md"</span> // Should be some file on your system
    val conf = new SparkConf()<span class="hljs-preprocessor">.setAppName</span>(<span class="hljs-string">"Simple Application"</span>)
    val sc = new SparkContext(conf)
    val logData = sc<span class="hljs-preprocessor">.textFile</span>(logFile, <span class="hljs-number">2</span>)<span class="hljs-preprocessor">.cache</span>()
    val numAs = logData<span class="hljs-preprocessor">.filter</span>(line =&gt; line<span class="hljs-preprocessor">.contains</span>(<span class="hljs-string">"a"</span>))<span class="hljs-preprocessor">.count</span>()
    val numBs = logData<span class="hljs-preprocessor">.filter</span>(line =&gt; line<span class="hljs-preprocessor">.contains</span>(<span class="hljs-string">"b"</span>))<span class="hljs-preprocessor">.count</span>()
    println(<span class="hljs-string">"Lines with a: %s, Lines with b: %s"</span><span class="hljs-preprocessor">.format</span>(numAs, numBs))
  }
}</code></pre>

<p>应用程序必须顶一个一个main方法，而不是直接继承scala.App，因为scala.App的子类可能会运行错误。示例程序只计算了在README.md中包含字母a和b的总行数，此外，与上述的Spark-Shell示例不同之处在于，在应用程序中需要自行初始化SparkContext，在Spark Shell中已经初始化为sc。 <br>
给SparkContext构造器传递一个SparkConf对象作为参数，这个SparkConf对象包含了应用程序的一些信息，此程序基于Spark API，程序的运行目录结构中将包含一个simple.sbt文件用户解释Spark的依赖：</p>



<pre class="prettyprint"><code class=" hljs ruby">name <span class="hljs-symbol">:</span>= <span class="hljs-string">"Simple Project"</span>

version <span class="hljs-symbol">:</span>= <span class="hljs-string">"1.0"</span>

scalaVersion <span class="hljs-symbol">:</span>= <span class="hljs-string">"2.10.5"</span>

libraryDependencies += <span class="hljs-string">"org.apache.spark"</span> <span class="hljs-string">%% "spark-core" %</span> <span class="hljs-string">"1.6.0"</span></code></pre>

<p>接下来，需要根据当前的目录结构部署SimpleApp.scala和simple.sbt，以使sbt正常运行，然后可以将此程序打包成JAR包，使用spark-submit脚本进行提交运行。</p>



<pre class="prettyprint"><code class=" hljs r"><span class="hljs-comment"># Your directory layout should look like this</span>
$ find .
.
./simple.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

<span class="hljs-comment"># Package a jar containing your application</span>
$ sbt package
<span class="hljs-keyword">...</span>
[info] Packaging {..}/{..}/target/scala-<span class="hljs-number">2.10</span>/simple-project_2.10-<span class="hljs-number">1.0</span>.jar

<span class="hljs-comment"># Use spark-submit to run your application</span>
$ YOUR_SPARK_HOME/bin/spark-submit \
  --class <span class="hljs-string">"SimpleApp"</span> \
  --master local[<span class="hljs-number">4</span>] \
  target/scala-<span class="hljs-number">2.10</span>/simple-project_2.10-<span class="hljs-number">1.0</span>.jar
<span class="hljs-keyword">...</span>
Lines with a: <span class="hljs-number">46</span>, Lines with b: <span class="hljs-number">23</span></code></pre>



<h2 id="3-深入spark">3. 深入Spark</h2>

<ul>
<li>API的深入学习，查看“Spark编程指南”</li>
<li>在集群上运行应用程序，查看“部署概述”</li>
<li>Spark示例（examples目录下，包括Scala、Java、Python、R），可运行如下：</li>
</ul>



<pre class="prettyprint"><code class=" hljs vhdl"># <span class="hljs-keyword">For</span> Scala <span class="hljs-keyword">and</span> Java, <span class="hljs-keyword">use</span> run-example:
./bin/run-example SparkPi

# <span class="hljs-keyword">For</span> Python examples, <span class="hljs-keyword">use</span> spark-submit directly:
./bin/spark-submit examples/src/main/python/pi.py

# <span class="hljs-keyword">For</span> R examples, <span class="hljs-keyword">use</span> spark-submit directly:
./bin/spark-submit examples/src/main/r/dataframe.R</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>