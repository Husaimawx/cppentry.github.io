---
layout:     post
title:      hive数据导入
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，转载请注明出处、链接，谢谢。					https://blog.csdn.net/jinliwei1990/article/details/56479460				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><div class="toc">
<ul>
<li><a href="#hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5" rel="nofollow">hive数据导入</a><ul>
<li><a href="#%E5%88%A0%E9%99%A4hive%E6%95%B0%E6%8D%AE%E5%BA%93" rel="nofollow">删除hive数据库</a></li>
<li><a href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93" rel="nofollow">创建数据库</a></li>
<li><a href="#sqoop%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE" rel="nofollow">sqoop导入数据</a><ul>
<li><a href="#%E5%88%97%E5%87%BAmysql%E4%B8%AD%E6%89%80%E6%9C%89%E7%9A%84%E5%BA%93" rel="nofollow">列出mysql中所有的库</a></li>
<li><a href="#%E5%88%A9%E7%94%A8sqoop%E6%89%A7%E8%A1%8Csql%E5%91%BD%E4%BB%A4" rel="nofollow">利用sqoop执行SQL命令</a></li>
<li><a href="#%E7%94%9F%E6%88%90hive%E8%A1%A8" rel="nofollow">生成hive表</a></li>
<li><a href="#%E5%B0%86%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5hive%E6%95%B0%E6%8D%AE%E5%BA%93" rel="nofollow">将外部数据库导入hive数据库</a></li>
<li><a href="#%E8%87%AA%E5%8A%A8%E5%A2%9E%E9%87%8F%E5%AF%BC%E5%85%A5" rel="nofollow">自动增量导入</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0partition" rel="nofollow">实现partition</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>




<h1 id="hive数据导入">hive数据导入</h1>



<h2 id="删除hive数据库">删除hive数据库</h2>

<p>DROP DATABASE是删除所有的表并删除数据库的语句。它的语法如下：</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">DATABASE</span> StatementDROP (<span class="hljs-keyword">DATABASE</span>|<span class="hljs-keyword">SCHEMA</span>) [<span class="hljs-keyword">IF</span> <span class="hljs-keyword">EXISTS</span>] database_name 
[<span class="hljs-keyword">RESTRICT</span>|<span class="hljs-keyword">CASCADE</span>];</span></code></pre>

<p>下面的查询用于删除数据库。假设要删除的数据库名称为userdb。</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">DATABASE</span> <span class="hljs-keyword">IF</span> <span class="hljs-keyword">EXISTS</span> userdb;</span></code></pre>

<p>以下是使用CASCADE查询删除数据库。这意味着要全部删除相应的表在删除数据库之前。</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">DATABASE</span> <span class="hljs-keyword">IF</span> <span class="hljs-keyword">EXISTS</span> userdb <span class="hljs-keyword">CASCADE</span>;</span></code></pre>

<p>以下使用SCHEMA查询删除数据库。</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">SCHEMA</span> userdb;</span></code></pre>



<h2 id="创建数据库">创建数据库</h2>

<p>创建数据库是用来创建数据库在Hive中语句。在Hive数据库是一个命名空间或表的集合。此语法声明如下：</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">DATABASE</span>|<span class="hljs-keyword">SCHEMA</span> [<span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span>] &lt;<span class="hljs-keyword">database</span> name&gt;</span></code></pre>

<p>在这里，IF NOT EXISTS是一个可选子句，通知用户已经存在相同名称的数据库。可以使用SCHEMA 在DATABASE的这个命令。下面的查询执行创建一个名为userdb数据库：</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">DATABASE</span> [<span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span>] userdb;</span></code></pre>

<p>或</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">SCHEMA</span> userdb;</span></code></pre>

<p>下面的查询用于验证数据库列表：</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">SHOW</span> DATABASES;</span></code></pre>



<h2 id="sqoop导入数据">sqoop导入数据</h2>



<h3 id="列出mysql中所有的库">列出mysql中所有的库</h3>

<p>利用sqoop的list-databases命令连接mysql并打印出mysql中所有的库，通过这一操作可以测试sqoop对mysql的连接是否正确。</p>



<pre class="prettyprint"><code class=" hljs brainfuck"><span class="hljs-comment">sqoop</span> <span class="hljs-comment">list</span><span class="hljs-literal">-</span><span class="hljs-comment">databases</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">connect</span> <span class="hljs-comment">jdbc:mysql://localhost:3306</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">username</span> <span class="hljs-comment">Test</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">password</span> <span class="hljs-comment">dbPWD</span></code></pre>



<h3 id="利用sqoop执行sql命令">利用sqoop执行SQL命令</h3>

<p>可以通过sqoop执行SQL命令。</p>



<pre class="prettyprint"><code class=" hljs bash">sqoop <span class="hljs-built_in">eval</span>  --connect jdbc:mysql://localhost:<span class="hljs-number">3306</span>/databaseName --username Test --password dbPWD <span class="hljs-operator">-e</span> <span class="hljs-string">"select * from tableName_1_pool limit 1"</span></code></pre>



<h3 id="生成hive表">生成hive表</h3>



<pre class="prettyprint"><code class=" hljs haml">sqoop create-hive-table \
-<span class="ruby">-connect <span class="hljs-symbol">jdbc:</span><span class="hljs-symbol">mysql:</span>/<span class="hljs-regexp">/localhost:3306/database</span>Name \
</span>-<span class="ruby">-username <span class="hljs-constant">Test</span> --password dbPWD \
</span>-<span class="ruby">-table tableName_1_pool \
</span>-<span class="ruby">-hive-table databaseName.tableName_1_pool</span></code></pre>

<p>上述命令在hive的databaseName数据库中生成名为tableName_1_pool的表，该表的schema是sqoop从mysql原表中推测得到的。</p>



<h3 id="将外部数据库导入hive数据库">将外部数据库导入hive数据库</h3>



<pre class="prettyprint"><code class=" hljs haml">sqoop import \
-<span class="ruby">-hive-import \
</span>-<span class="ruby">-connect <span class="hljs-symbol">jdbc:</span><span class="hljs-symbol">mysql:</span>/<span class="hljs-regexp">/localhost:3306/database</span>Name \
</span>-<span class="ruby">-username <span class="hljs-constant">Test</span> --password dbPWD \
</span>-<span class="ruby">-table tableName_1_pool \
</span>-<span class="ruby">-hive-database databaseName \
</span>-<span class="ruby">-hive-table tableName_1_pool \
</span>-<span class="ruby">-split-by <span class="hljs-constant">Id</span> \
</span>-<span class="ruby">-as-parquetfile \
</span>-<span class="ruby">-hive-overwrite</span></code></pre>

<ul>
<li><code>--split-by Id</code>表示以Id列作为划分并行度的依据，实际导入的过程中，sqoop会按照Id列的值将数据库划分为若干个map任务，分别导入Hive中。</li>
<li><code>-m 10</code>增加mapper的数量以提高并行度 。</li>
<li><code>--as-parquetfile</code>以parquet格式导入。Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发。Parquet列式存储和传统的行式存储相比有如下优势：可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量；压缩编码可以降低磁盘存储空间，由于同一列的数据类型是一样的，可以使用更高效的压缩编码进一步节约存储空间；只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
<li><code>--hive-overwrite</code>覆盖已有数据。</li>
<li><code>--where "Id&gt;500000"</code>实现条件导入。</li>
<li><code>--check-column Id --incremental append --last-value 699999</code>增量导入Hive数据库</li>
<li><code>--map-column-java DateTime=java.sql.Timestamp --map-column-hive DateTime=Timestamp</code>修改列值的类型。从Mysql到Hive的导入操作中，Sqoop会根据输入数据库列的类型寻找Hive中相对应的类型进行转换。但有时候，这些转换可能不能满足实际的要求，这样就需要显式对列的类型进行制定。以tableName_1_pool中为例DateTime列的类型为：DATETIME，但在转换到Hive后，其属性变为了bigint，也就是说，其值转换为了从1970年1月1日至今的毫秒数。在某些情况下，这并不是我们想要的结果，为了改变转换后DateTime列的类型，可利用如下命令在导入时对其进行指定。</li>
</ul>



<h3 id="自动增量导入">自动增量导入</h3>

<ol>
<li>由于每次增量导入前都需要知道上次导入的Id的值，颇为不便，可以使用sqoop job功能实现自动的增量导入。为了自动增量导入，首先在mysql数据库中建立新表，该表为原表的一部分（Id&lt;500000），生成的新表名称为：tableName_3_pool。</li>
<li>创建sqoop job，该job将tableName_3_pool表同步到hive的tableName_3_pool_parquet新表中。注意在这个job中我们在一开始就设置了–check-column –incremental 和–last-value 三个参数。</li>
</ol>

<pre class="prettyprint"><code class=" hljs haml">sqoop job \
-<span class="ruby">-create sync-data-job \
</span>-<span class="ruby">- import \
</span>-<span class="ruby">-hive-import \
</span>-<span class="ruby">-connect <span class="hljs-symbol">jdbc:</span><span class="hljs-symbol">mysql:</span>/<span class="hljs-regexp">/localhost:3306/database</span>Name \
</span>-<span class="ruby">-username <span class="hljs-constant">Test</span> --password dbPWD \
</span>-<span class="ruby">-table tableName_3_pool \
</span>-<span class="ruby">-hive-database databaseName \
</span>-<span class="ruby">-hive-table tableName_3_pool_parquet \
</span>-<span class="ruby">-split-by <span class="hljs-constant">Id</span> \
</span>-<span class="ruby">-as-parquetfile \
</span>-<span class="ruby">-check-column <span class="hljs-constant">Id</span> \
</span>-<span class="ruby">-incremental append \
</span>-<span class="ruby">-last-value <span class="hljs-number">1</span></span></code></pre>

<blockquote>
  <p>该命令生成了一个sqoop job，可以用如下命令查看：<code>sqoop job --list</code> <br>
  可以用如下命令删除该job：<code>sqoop job --delete sync-data-job</code> <br>
  运行该job：<code>sqoop job --exec sync-data-job</code> <br>
  运行结束后，可用如下命令查看同步的情况：<code>sqoop job --show sync-data-job</code> <br>
  其中可以看到具体的同步点：<code>incremental.last.value = 499999</code></p>
</blockquote>

<p>在mysql中，将原表剩余的部分导入到tableName_3_pool表中。 <br>
此时再次启动 sqoop job，此次启动不需要制定从何处开始增量导入。<code>sqoop job --exec sync-data-job</code> <br>
此次运行结束后，再次命令查看同步的情况：<code>sqoop job --show sync-data-job</code> <br>
可以已经完成了所有增量的同步：<code>incremental.last.value = 771500</code> <br>
使用sqoop job的方法实现增量导入无需记录–last-value的值，每次想要同步，只要运行该job即可，job会自动从job保存的数据中获取到上次保存的–last-value的值。</p>



<h3 id="实现partition">实现partition</h3>

<p>Hive Select查询一般会扫描整个表内容，但在很多场景下，我们只需要关注表中的一部分数据，此时可以在建表时引入partition概念。Hive的一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。为了演示分区导入的操作，我们在mysql数据库中创建一个新表，这个表在tableName_1_pool表的基础上增加一列acqdate，这一列类型为Date。 <br>
<strong>mysql中运行如下命令</strong></p>

<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> tableName_1_pool_tag <span class="hljs-keyword">as</span> <span class="hljs-keyword">select</span> *,DATE_FORMAT(DateTime, <span class="hljs-string">'%Y-%m-%d'</span>) <span class="hljs-keyword">as</span> acqdate <span class="hljs-keyword">from</span> tableName_1_pool;</span> </code></pre>

<p>新表的名称为tableName_1_pool_tag，新增acqdate列，其值为DateTime列中仅包含日期的字符串。 <br>
在导入Hive时，我们将会以acqdate列的值作为partition的key，为了实现这一目标，我们首先将mysql数据库中tableName_1_pool_tag表的数据导入一个Hive临时表，接着建立结构与tableName_1_pool_tag表相同并且以acqdate日期为partition key的Hive表，最后，我们通过Hiveql语句将临时表导入到最终的分区表。具体步骤如下： <br>
1.将mysql数据库中tableName_1_pool_tag表的数据导入一个Hive临时表</p>



<pre class="prettyprint"><code class=" hljs haml">sqoop import \
-<span class="ruby">-hive-import \
</span>-<span class="ruby">-connect <span class="hljs-symbol">jdbc:</span><span class="hljs-symbol">mysql:</span>/<span class="hljs-regexp">/localhost:3306/database</span>Name \
</span>-<span class="ruby">-username <span class="hljs-constant">Test</span> --password dbPWD \
</span>-<span class="ruby">-table tableName_1_pool_tag \
</span>-<span class="ruby">-hive-database databaseName \
</span>-<span class="ruby">-hive-table tableName_1_pool_tag \
</span>-<span class="ruby">-as-parquetfile \
</span>-<span class="ruby">-split-by <span class="hljs-constant">Id</span> \
</span>-<span class="ruby">m <span class="hljs-number">10</span></span></code></pre>

<p>2.建立Hive表tableName_1_pool_partition，其partition的key为：acqdate</p>



<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-string">`tableName_1_pool_partition`</span>(
    <span class="hljs-string">`id`</span> bigint,
    <span class="hljs-string">`DateTime`</span> bigint
    )
       partitioned <span class="hljs-keyword">by</span> (<span class="hljs-string">`acqdate`</span> string)
  <span class="hljs-keyword">ROW</span> FORMAT SERDE
    <span class="hljs-string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span>
  LOCATION 
 <span class="hljs-string">'hdfs://nameservice1/user/hive/warehouse/databaseName.db/tableName_1_pool_partition'</span>;</span></code></pre>

<p>连接Hive，并运行命令：</p>

<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">SET</span> hive.<span class="hljs-keyword">exec</span>.dynamic.partition=<span class="hljs-keyword">true</span>;</span>
<span class="hljs-operator"><span class="hljs-keyword">SET</span> hive.<span class="hljs-keyword">exec</span>.dynamic.partition.mode=nonstrict;</span>
<span class="hljs-operator"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> tableName_1_pool_partition partition (acqdate) <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> tableName_1_pool_tag;</span></code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>