---
layout:     post
title:      Spark架构
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/qq_15821041/article/details/76552760				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>①Spark的架构</p>
<p>②Spark的工作机制</p>
<p>③Spark的调度</p>
<p><br></p>
<p><br></p>
<h4><span style="font-size:18px;">=&gt;Spark的架构</span></h4>
<p><strong>Spark架构组件简介</strong></p>
<p>①Spark集群中Master负责集群整体资源管理和调度，Worker负责单个节点的资源管理。Driver程序是应用逻辑执行的起点，而多个Executor用来对数据进行并行处理。</p>
<p>②Spark的构成：ClusterManager:在standalone模式中，即为Master:主节点，控制整个集群，监控Worker。在YARN模式中为资源管理器；Worker：从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制；Driver：运行Application的main()函数并且创建SparkContext，可以理解为一个应用的运行、调度，而在整个集群中，可能会同时运行多个Application,也就是会存在多个Driver；Executor：执行器，是为某Application运行在worker
 node上的一个进程，启动线程池运行任务上。每个Aapplication拥有独立的一组executors；SparkContext:整个应用的上下文，控制应用的生命周期，在它创建之后，内部会启动Task Scheduler等，这些组件可以负责运行的调度；RDD：Spark的基本计算单元，一组RDD形成执行的有向无环图RDD GRAPH；DAG Scheduler：根据JOB构建基于Stage的DAG，并提交Stage给TaskScheduler，也就是将RDD的Graph拆分成一个个的Stage，这样stage构成了有向无环图，节点是一个stage，之后提交stage给TaskScheduler进行下一步的拆分与调度；TaskScheduler：将Task提交给Executor执行；SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。</p>
<p>③流程：在Client端，用户提交应用，ClassManager接收到应用后会启动Driver,Driver中会创建SparkContext对象，将应用拆分成多个RDD的DAG，之后每一个DAG会提交给DAG Scheduler，DAG Scheduler会将它拆分成不同的Stage,最后将它提交给TasksScheduler，TaskScheduler会将这些人物不断的分发给Worker节点的Executor中进行执行，而Executor会启动多线程，将每一个任务分配给一个线程进行执行；而SparkEnv会启动一些控制组件，进行shuffle管理或者广播变量等的管理等</p>
<p>④集群的执行机制</p>
<p>Client提交应用，Master找到一个Worker启动Driver，Driver向Master或者资源管理器申请资源（CPU\内存等），之后将应用转化为RDD Graph,再由DAGScheduler将RDDGraph转化为Stage的有向无环图提交给TaskScheduler，由TaskScheduler提交任务给Executor进行执行。任务执行的过程中其他组件再协同工作确保整个应用的顺利执行。每一轮次执行完将数据shuffle到磁盘，再进行下一轮次的stage,经过多伦stage,最终将数据输出spark。</p>
<h4><span style="font-size:18px;">=&gt;Spark的工作机制</span></h4>
<p><strong>Spark作业基本概念</strong></p>
<p>①Application：用户自定义的Spark程序，用户提交后，Spark为App分配资源将程序转换并执行</p>
<p>②Driver Program：运行Application的main函数并创建SparkContext</p>
<p>③RDD DAG：当RDD遇到Action算子，将之前所有的算子形成一个有向无环图（DAG）。再在Spark中转化为Job，提交到集群执行。一个App中可以包含多个Job。</p>
<p>④JOB：一个RDD Graph触发的作业，往往由Spark Action算子触发，在SparkContext中通过run Job方法向Spark提交Job</p>
<p>⑤Stage：每个JOB会根据RDD的宽依赖关系被切分成许多Stage，每个Stage中包含一组相同的Task，这一组Task也叫TaskSet(stage可以理解为hadoop的mapreduce中多少个map job串行运行)</p>
<p>⑥Task:一个分区对应一个Task，Task执行RDD中对应Stage中所包含的算子。Task被封装好后放入Executor的线程池中执行。</p>
<p><img src="" alt=""><br></p>
<p><br></p>
<p><strong>Spark程序与作业概念映射</strong></p>
<p>示例：</p>
<p>①val rawFile = sc.textFile("afs://tianqi.afs.baidu.com:9902/user/ubs/data/yanmenghan/spark/README.md")</p>
<p>输入文本文件转换为RDD，</p>
<p><img src="" alt=""><img src="https://img-blog.csdn.net/20170802140416642?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTU4MjEwNDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p><img src="https://img-blog.csdn.net/20170802140437350?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTU4MjEwNDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p>②val words = rawFile.flatMap(line =&gt; line.split(""))</p>
<p>将文本进行分词，转换为单词的RDD，</p>
<p><img src="" alt=""><img src="https://img-blog.csdn.net/20170802140525888?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTU4MjEwNDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p>③val wordNumber = words.map(w =&gt; (w,1))</p>
<p>将每个单词映射为以单词为Key,value为1的KEY-VALUE对</p>
<p><img src="https://img-blog.csdn.net/20170802140642782?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTU4MjEwNDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p>④val wordsCounts = wordsNumber.reduceByKey(_+_)</p>
<p>通过reduceByKey操作将同一个单词的数据进行聚集，进而统计好每一个单词的个数。</p>
<p><img src="https://img-blog.csdn.net/20170802140642782?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTU4MjEwNDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p>⑤wordsCounts.foreach(printIn)</p>
<p>通过fireach方法输出每一个单词的计数</p>
<p>⑥wordsCounts.saveAsTextFile</p>
<p>将结果保存到磁盘</p>
<p>说明1：整六行代表了一个Application，也就是应用程序。这个应用程序中有两个Job,①-⑤是一个Job，①-④+⑥是一个Job。foreach和saveAsTextFile中，会分别调用run job方法，提交整个RDD的Graph。</p>
<p>说明2：对第一个JOB进行Stage对应关系的说明。①-③是一个stage，而④-⑤是一个stage,这是因为③中map操作和④中reduceByKey操作之间要进行shuffle；①-③的stage中，将会根据用户设置的并行数量产生相应数量的任务，即Task，例如用户可以设定1000，将会产生1000个Task，每一个Task针对唯一的数据块执行以上①-③的函数，串行执行。</p>
<p>说明3：整个作业的执行流程。</p>
<p><img src="https://img-blog.csdn.net/20170802142323629?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTU4MjEwNDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p>例如，取到两行，根据split将每一行的数据切分为单词，然后将单词映射为key-value对，通过reduceByKey，将相同单词的数据聚集到一起，最后将value相加。</p>
<p>说明4：Spark运行流程。</p>
<p>Spark程序转换：将应用程序提交到集群，集群将程序由一个Application转换为不同的任务集；</p>
<p>输入数据块：在集群中输入数据块</p>
<p>执行Tasks：集群会根据调度策略，将Stage中的Tasks分发到各个节点，在每一个数据块上进行执行。执行完成后，根据shuffle，将数据进行混洗，再将下一阶段的Stage进行执行</p>
<p>输出：输出结果返回</p>
<p><br></p>
<h4><span style="font-size:18px;">=&gt;Spark的调度</span></h4>
<p><span style="font-size:12px;">作业调度<br>
Application调度<br>
Job调度<br>
Tasks延时调度</span></p>
            </div>
                </div>