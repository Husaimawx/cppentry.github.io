---
layout:     post
title:      第3周 分布式文件系统HDFS原理与操作
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<div>
<h3><strong>Hello, World!</strong></h3>
</div>
<div>##对刚安装好的hadoop集群做个测试</div>
<div>[root@hadoop1 hadoop]# pwd</div>
<div>/nosql/hadoop</div>
<div>[root@hadoop1 hadoop]# mkdir input</div>
<div>[root@hadoop1 hadoop]# cd input/</div>
<div>[root@hadoop1 input]# echo "hello word" &gt; test1.txt</div>
<div>[root@hadoop1 input]# echo "hello hadoop" &gt; test2.txt</div>
<div>[root@hadoop1 input]# cat test1.txt</div>
<div>hello word</div>
<div>[root@hadoop1 input]# cat test2.txt</div>
<div>hello hadoop</div>
<div>[root@hadoop1 input]#</div>
<div>[root@hadoop1 input]#</div>
<div>[root@hadoop1 input]# cd /nosql/hadoop/hadoop-0.20.2/bin/</div>
<div>[root@hadoop1 bin]# ./hadoop dfs -put /nosql/hadoop/input/ in</div>
<div>[root@hadoop1 bin]# ./hadoop dfs -ls ./in/*</div>
<div>-rw-r--r-- 2 root supergroup 11 2014-01-09 05:55 /user/root/in/test1.txt</div>
<div>-rw-r--r-- 2 root supergroup 13 2014-01-09 05:55 /user/root/in/test2.txt</div>
<div>[root@hadoop1 bin]#</div>
<div>[root@hadoop1 bin]#</div>
<div>[root@hadoop1 bin]# ./hadoop jar ../hadoop-0.20.2-examples.jar wordcount in out</div>
<div>14/01/09 07:58:35 INFO input.FileInputFormat: Total input paths to process : 2</div>
<div>14/01/09 07:58:35 INFO mapred.JobClient: Running job: job_201401090755_0001</div>
<div>14/01/09 07:58:36 INFO mapred.JobClient: map 0% reduce 0%</div>
<div>14/01/09 07:58:45 INFO mapred.JobClient: map 50% reduce 0%</div>
<div>14/01/09 07:58:49 INFO mapred.JobClient: map 100% reduce 0%</div>
<div>14/01/09 07:58:58 INFO mapred.JobClient: map 100% reduce 100%</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Job complete: job_201401090755_0001</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Counters: 17</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Job Counters</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Launched reduce tasks=1</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Launched map tasks=2</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Data-local map tasks=2</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: FileSystemCounters</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: FILE_BYTES_READ=54</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: HDFS_BYTES_READ=24</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: FILE_BYTES_WRITTEN=178</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: HDFS_BYTES_WRITTEN=24</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Map-Reduce Framework</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Reduce input groups=3</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Combine output records=4</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Map input records=2</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Reduce shuffle bytes=60</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Reduce output records=3</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Spilled Records=8</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Map output bytes=40</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Combine input records=4</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Map output records=4</div>
<div>14/01/09 07:58:59 INFO mapred.JobClient: Reduce input records=4</div>
<div>[root@hadoop1 bin]# ./hadoop dfs -ls</div>
<div>Found 2 items</div>
<div>drwxr-xr-x - root supergroup 0 2014-01-09 07:57 /user/root/in</div>
<div>drwxr-xr-x - root supergroup 0 2014-01-09 07:58 /user/root/out</div>
<div>[root@hadoop1 bin]# ./hadoop dfs -ls ./out</div>
<div>Found 2 items</div>
<div>drwxr-xr-x - root supergroup 0 2014-01-09 07:58 /user/root/out/_logs</div>
<div>-rw-r--r-- 2 root supergroup 24 2014-01-09 07:58 /user/root/out/part-r-00000</div>
<div>[root@hadoop1 bin]# ./hadoop dfs -cat ./out/*</div>
<div>hadoop 1</div>
<div>hello 2</div>
<div>word 1</div>
<div>cat: Source must be a file.</div>
<div>[root@hadoop1 bin]# </div>
<div>在做实验的过程中遇到很多错误，参考：</div>
<div><a href="http://p-x1984.iteye.com/blog/912885" rel="nofollow">http://p-x1984.iteye.com/blog/912885</a></div>
<div><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" rel="nofollow">http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html</a></div>
<div><br></div>
<div>
<h3><strong>通过web了解Hadoop的活动</strong></h3>
</div>
<div>通过用浏览器和http访问jobtracker所在节点的50030端口监控jobtracker</div>
<div>通过用浏览器和http访问namenode所在节点的50070端口监控集群</div>
<div>http://192.168.136.128:50030/jobtracker.jsp</div>
<div>http://192.168.136.128:50070/dfshealth.jsp</div>
<div><br></div>
<div>
<h3><strong>HDFS分布式文件系统</strong></h3>
</div>
<div>HDFS设计基础与目标</div>
<div>硬件错误是常态。因此需要冗余</div>
<div>流式数据访问。即数据批量读取而非随机读写，Hadoop擅长做的是数据分析而不是事务处理</div>
<div>大规模数据集</div>
<div>简单一致性模型。为了降低系统复杂度，对文件采用一次性写多次读的逻辑设计，即是文件一经写入，关闭，就再也不能修改</div>
<div>程序采用“数据就近”原则分配节点执行</div>
<div><br></div>
<div>HDFS文件操作</div>
<div>命令行方式</div>
<div>API方式</div>
<div><br></div>
<div>##列出HDFS下的文件</div>
<div>##注意，hadoop没有当前目录的概念，也没有cd命令</div>
<div># ./hadoop dfs -ls</div>
<div># ./hadoop dfs -ls ./in</div>
<div># ./hadoop dfs -ls ./out</div>
<div><br></div>
<div>##上传文件到HDFS</div>
<div># ./hadoop dfs -put /nosql/hadoop/input/ in</div>
<div><br></div>
<div>##将HDFS的文件复制到本地</div>
<div># ./hadoop dfs -get in /tmp/abc</div>
<div><br></div>
<div>##删除HDFS下的文档</div>
<div># ./hadoop dfs -rmr in</div>
<div><br></div>
<div>##查看HDFS下某个文件的内容</div>
<div># ./hadoop dfs -cat ./out/part-r-00000</div>
<div><br></div>
<div>##查看HDFS基本统计信息</div>
<div>[root@hadoop1 bin]# ./hadoop dfsadmin -report</div>
<div>Configured Capacity: 58035453952 (54.05 GB)</div>
<div>Present Capacity: 26640138240 (24.81 GB)</div>
<div>DFS Remaining: 26639933440 (24.81 GB)</div>
<div>DFS Used: 204800 (200 KB)</div>
<div>DFS Used%: 0%</div>
<div>Under replicated blocks: 0</div>
<div>Blocks with corrupt replicas: 0</div>
<div>Missing blocks: 0</div>
<div>-------------------------------------------------</div>
<div>Datanodes available: 2 (2 total, 0 dead)</div>
<div>Name: 192.168.136.130:50010</div>
<div>Decommission Status : Normal</div>
<div>Configured Capacity: 29017726976 (27.02 GB)</div>
<div>DFS Used: 102400 (100 KB)</div>
<div>Non DFS Used: 15697620992 (14.62 GB)</div>
<div>DFS Remaining: 13320003584(12.41 GB)</div>
<div>DFS Used%: 0%</div>
<div>DFS Remaining%: 45.9%</div>
<div>Last contact: Thu Jan 09 11:10:20 EST 2014</div>
<div>Name: 192.168.136.129:50010</div>
<div>Decommission Status : Normal</div>
<div>Configured Capacity: 29017726976 (27.02 GB)</div>
<div>DFS Used: 102400 (100 KB)</div>
<div>Non DFS Used: 15697694720 (14.62 GB)</div>
<div>DFS Remaining: 13319929856(12.41 GB)</div>
<div>DFS Used%: 0%</div>
<div>DFS Remaining%: 45.9%</div>
<div>Last contact: Thu Jan 09 11:10:21 EST 2014</div>
<div><br></div>
<div>##进入和退出安全模式</div>
<div>[root@hadoop1 bin]# ./hadoop dfsadmin -safemode enter</div>
<div>Safe mode is ON</div>
<div>[root@hadoop1 bin]# ./hadoop dfsadmin -safemode leave</div>
<div>Safe mode is OFF</div>
<div><br></div>
<div>
<h3><strong>怎样添加节点？</strong></h3>
</div>
<div>在新节点安装好hadoop</div>
<div>把namenode的有关配置文件复制到该节点</div>
<div>修改masters和slaves文件，增加该节点</div>
<div>设置ssh免密码进出该节点</div>
<div>单独启动该节点上的datanode和tasktracker（hadoop-daemon.sh start datanode/tasktracker）</div>
<div>运行start-balancer.sh进行数据负载均衡</div>
<div><br></div>
<div>
<h3><strong>启动某些特定后台进程而非所有后台进程</strong></h3>
</div>
<div>##Start-all.sh的内容</div>
<div>[root@hadoop1 bin]# cat start-all.sh</div>
<div>#!/usr/bin/env bash</div>
<div># Start all hadoop daemons. Run this on master node. </div>
<div>bin=`dirname "$0"`</div>
<div>bin=`cd "$bin"; pwd`</div>
<div>. "$bin"/hadoop-config.sh</div>
<div># start dfs daemons</div>
<div>"$bin"/start-dfs.sh --config $HADOOP_CONF_DIR</div>
<div># start mapred daemons</div>
<div>"$bin"/start-mapred.sh --config $HADOOP_CONF_DIR</div>
<div><br></div>
<div>
<h3><strong>负载均衡</strong></h3>
</div>
<div>##作用：当节点出现故障，或新增加节点时，数据块分布可能不均匀，负载均衡可以重新平衡各个datanode上数据块的分布</div>
<div>[root@hadoop1 bin]# ./start-balancer.sh</div>
<div>starting balancer, logging to /nosql/hadoop/hadoop-0.20.2/bin/../logs/hadoop-root-balancer-hadoop1.out</div>
<br>            </div>
                </div>