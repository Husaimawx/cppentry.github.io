---
layout:     post
title:      Hadoop学习笔记(1)安装配置与运行
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/gyming/article/details/41454583				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h3 style="text-align:center;">Hadoop学习笔记(1)安装配置与运行</h3>
<br><br>
1.什么是Hadoop<br>
    Hadoop是适合对大量数据进行分布式存储与计算的平台。<br>
    http://hadoop.apache.org/<br><br><br>
2.Hadoop架构<br>
    Hadoop是项目的总称。<br>
    Hadoop核心项目:<br>
    ·HDFS:      Hadoop分布式文件系统(HDFS，Hadoop Distributed File System)。Google File System（GFS）的开源实现。<br>
    ·MapReduce: 并行计算框架(org.apache.hadoop.mapreduce), Google MapReduce的开源实现。<br><br><br>
    Hadoop子项目:<br>
    ·HBase:     类似Google BigTable的分布式NoSQL列数据库。（HBase和Avro已经于2010年5月成为顶级 Apache 项目）<br>
    ·Hive:      数据仓库工具，由Facebook贡献。<br>
    ·Zookeeper: 分布式锁设施，提供类似Google Chubby的功能，由Facebook贡献。<br>
    ·Avro:      新的数据序列化格式与传输工具，将逐步取代Hadoop原有的IPC机制。<br>
    ·Pig:       大数据分析平台，为用户提供多种接口。<br>
    ·Ambari:    Hadoop管理工具，可以快捷的监控、部署、管理集群。<br>
    ·Sqoop:     于在HADOOP与传统的数据库间进行数据的传递。<br><br><br>
3.Hadoop分布式文件系统HDFS<br>
    主从结构<br>
    ·主节点只有一个: namenode<br>
    ·从节点有很多个: datanodes<br>
    namenode负责:<br>
    ·接收用户操作请求<br>
    ·维护文件系统的目录结构<br>
    ·管理文件与block之间，block与datanode之间关系<br>
    datanode负责:<br>
    ·存储文件<br>
    ·文件被分成block存储在磁盘上<br>
    ·为保证数据安全，文件会有多个副本<br><br><br>
4.MapReduce<br>
    主从结构<br>
    ·主节点只有一个: JobTracker<br>
    ·从节点有很多表: TaskTracker<br>
    JobTracker负责:<br>
    ·接收客户提交的计算任务<br>
    ·把计算任务分给TaskTracker<br>
    ·监控TaskTracker的执行情况<br>
    TaskTracker负责:<br>
    ·执行JobTracker分配的计算任务<br><br><br>
5.Hadoop集群主机物理分布<br>
    机架Rack1<br>
    主机node1: JobTacker<br>
    主机node2: NameNode<br>
    主机node3: TaskTracker/DataNode<br>
    主机node4: TaskTracker/DataNode<br>
    主机node5: TaskTracker/DataNode<br>
    主机node6: TaskTracker/DataNode<br>
    交换机Switch: 各node通过Switch连接<br><br><br>
    机架Rack2<br>
    主机node1: TaskTracker/DataNode<br>
    主机node2: TaskTracker/DataNode<br>
    主机node3: TaskTracker/DataNode<br>
    主机node4: TaskTracker/DataNode<br>
    主机node5: TaskTracker/DataNode<br>
    主机node6: TaskTracker/DataNode<br>
    交换机Switch: 各node通过Switch连接<br><br><br>
    每个Rack的各node之间通过交换机通信，各Rack的交换机之间再通过交换机通信。<br><br><br>
6.Hadoop主机节点结构<br>
    Master node:<br>
    Namenode<br>
    Secondary namenode<br>
    JobTracker<br>
    Hadoop utility<br><br><br>
    Slave node:<br>
    TaskTracker<br>
    Datanode<br><br><br>
7.安装Hadoop<br>
  7.1.安装Hadoop准备工作<br>
    使用虚拟机来模拟Hadoop，需要安装VMware<br>
    使用两台虚拟机，安装操作系统Redhat Enterprise Linux.<br>
    虚拟机的主机名分别为hadoop01与hadoop02.<br>
    ip地址分别设置为192.168.1.77，192.168.1.88<br>
    软件准备:<br>
    VMware Workstation 10.0.1 Build 1379776<br>
    rhel-server-6.5-x86_64-dvd.iso<br>
    jdk-6u45-linux-x64-rpm.bin<br>
    hadoop-2.2.0.tar.gz<br><br><br>
  7.2.安装VMware和Redhat后，配置主机<br>
    设置主机静态ip地址后重启动网络生效.<br>
    # vi /etc/sysconfig/network-scripts/ifcfg-eth0    // 可通过netconfig命令更改.<br>
    # service network restart                         // 重启网卡生效<br>
    # ifconfig<br><br><br>
    修改主机名.<br>
    # hostname                     // 查看主机名<br>
    # hostname hadoop01            // 更改主机名,但重启主机会失效<br>
    # vi /etc/sysconfig/network    // 更改主机名配置文件<br>
    # vi /etc/hosts                // 更改主机名与ip地址绑定<br>
    # ping hadoop01<br><br><br>
    关闭防火墙.<br>
    # service iptables status<br>
    # service iptables stop              // 关闭防火墙，如果服务配置是自动启动，重启电脑会再启动<br>
    # service iptables status<br>
    # chkconfig --list | grep iptables   // 查看防火墙服务配置是否自动启动<br>
    # chkconfig iptables off             // 停止防火墙自启动服务<br>
    # chkconfig --list | grep iptables<br><br><br>
    配置SSH(Secure Shell)节点互信<br>
    &lt;&lt;1&gt;&gt;创建.ssh目录<br>
    以hadoop用户登录节点，检查/home/hadoop/下是否有.ssh目录，如果没有就创建一个.<br>
    $ mkdir ~/.ssh<br>
    $ chmod 700 ~/.ssh<br><br><br>
    &lt;&lt;2&gt;&gt;生成RSA key<br>
    在每个主机节点上执行以下命令创建RSA Key<br>
    $ cd ~/.ssh<br>
    $ /usr/bin/ssh-keygen -t rsa<br>
    全部回车接受默认项,将生成id_rsa.pub文件<br><br><br>
    &lt;&lt;3&gt;&gt;将生成的RSA Key添加到authorized_keys中<br>
    以hadoop用户登录节点hadoop01<br>
    $cat id_rsa.pub &gt;&gt; authorized_keys                  //生成authorized_keys文件<br>
    $scp authorized_keys hadoop02:/home/hadoop/.ssh/    //把节点hadoop01的文件authorized_keys传输到hadoop02<br>
    以hadoop用户登录节点hadoop02<br>
    $cat id_rsa.pub &gt;&gt; authorized_keys                  //添加RSA Key到authorized_keys文件<br>
    $scp authorized_keys hadoop01:/home/hadoop/.ssh/    //把节点hadoop02的文件authorized_keys传回到hadoop01<br><br><br>
    &lt;&lt;4&gt;&gt;启用ssh协议<br>
    在每个节点上执行以下通信命令，此时需要输入密码.<br>
    $ ssh localhost<br>
    $ ssh hadoop01 date<br>
    $ ssh hadoop02 date<br>
    在每个节点上启动ssh代理，并将ssh key装载到内存中<br>
    $ exec /usr/bin/ssh-agent $SHELL<br>
    $ /usr/bin/ssh-add<br>
    再执行上面的通信命令，不再需要密码.<br><br><br>
  7.3.安装JDK.<br>
    使用winscp工具将jdk文件上传到主机，复制到/usr/local/目录下<br>
    # cp /root/Downloads /usr/local<br>
    # cd /usr/local<br>
    # chmod u+x jdk-6u45-linux-x64.bin    // 授予执行权限<br>
    # ./jdk-6u45-linux-x64.bin            // 执行命令解压缩<br>
    # java -version                           // 可以看到默认安装的OpenJDK<br>
    # vi /etc/profile                         // 修改配置文件，添加JAVA_HOME环境变量.<br>
    export JAVA_HOME=/usr/local/jdk<br>
    export PATH=.:$JAVA_HOME/bin:$PATH<br>
    # source /etc/profile                     // 执行命令让配置生效<br>
    # java -version                           // 可以看到安装的JDK<br><br><br>
  7.4.安装Hadoop.<br>
    使用winscp工具将jdk文件上传到主机，复制到/usr/local/目录下，解压，修改配置文件<br>
    # tar -zxvf hadoop-2.2.0.tar.gz<br>
    # chown -R hadoop:hadoop hadoop-2.2.0<br>
    # vi /etc/profile                         // 修改配置文件，添加HADOOP_HOME环境变量.<br>
    export JAVA_HOME=/usr/local/jdk<br>
    export HADOOP_HOME=/usr/local/hadoop<br>
    export PATH=.:$HADOOP_HOME:$JAVA_HOME/bin:$PATH<br>
    # source /etc/profile<br><br><br>
  7.5.配置Hadoop.<br>
    修改Hadoop的配置文件hadoop-env.sh,core-site.xml,hdfs-site.xml,mapred-site.xml,masters,slaves,hadoop-metrics.properties,log4j.properties<br>
    文件名称                   格式           描述<br>
    hadoop-env.sh              Bash脚本       记录脚本要用的环境变量，以运行Hadoop<br>
    core-site.xml              Hadoop配置xml  Hadoop的核心配置项，如HDFS和MapReduce常用的I/O设置等<br>
    hdfs-site.xml              Hadoop配置xml  Hadoop守护进程的配置项，包括namenode、辅助namenode和datanode<br>
    mapred-site.xml            Hadoop配置xml  MapReduce守护进程的配置项，包括jobtracker和tasktracker<br>
    masters                    文本文件       运行辅助namenode的机器列表(每行一个)<br>
    slaves                     文本文件       运行datanode和tasktracker的机器列表(每行一个)<br>
    hadoop-metrics.properties  Java属性<br>
    log4j.properties           Java属性<br><br><br>
    # vi hadoop-env.sh                    // 修改hadoop-env.sh文件，修改JAVA_HOME<br>
    export JAVA_HOME=/usr/local/jdk<br><br><br>
    # vi core-site.xml                    // 修改core-site.xml文件，配置namenode主机<br>
    &lt;configuration&gt;<br>
      &lt;property&gt;<br>
        &lt;name&gt;fs.default.name&lt;/name&gt;<br>
        &lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt;<br>
        &lt;description&gt;change your own nodename hostname&lt;/description&gt;<br>
      &lt;/property&gt;<br>
      &lt;property&gt;<br>
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>
        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;<br>
      &lt;/property&gt;<br>
    &lt;/configuration&gt;<br><br><br>
    # vi hdfs-site.xml                    // 修改hdfs-site.xml，配置hdfs文件系统<br>
    &lt;configuration&gt;<br>
      &lt;property&gt;<br>
        &lt;name&gt;dfs.replication&lt;/name&gt;<br>
        &lt;value&gt;1&lt;/value&gt;<br>
      &lt;/property&gt;<br>
      &lt;property&gt;<br>
        &lt;name&gt;dfs.permissions&lt;/name&gt;<br>
        &lt;value&gt;false&lt;/value&gt;<br>
      &lt;/property&gt;<br>
    &lt;/configuration&gt;<br><br><br>
    # vi mapred-site.xml                  // 修改mapred-site.xml，配置JobTracker主机<br>
    &lt;configuration&gt;<br>
      &lt;property&gt;<br>
        &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br>
        &lt;value&gt;hdfs://hadoop01:9001&lt;/value&gt;<br>
        &lt;description&gt;change your own jobtracker hostname&lt;/description&gt;<br>
      &lt;/property&gt;<br>
    &lt;/configuration&gt;<br><br><br>
8.运行Hadoop<br>
  8.1.格式化文件系统<br>
    以hadoop用户登录.<br>
    $ hadoop namenode -format<br><br><br>
  8.2.启动与停止Hadoop.<br>
    $ start-all.sh<br>
    $ jps                      // 查看java进程，验证启动的Hadoop进程<br>
    5个进程:NameNode、DataNode、SecondaryNameNode、ResourceManager、NodeManager<br>
    Hadoop1.2版本的5个进程:NameNode、DataNode、SecondaryNameNode、JobTracker、TaskTracker<br>
    通过浏览器访问http://hadoop01:50070，查看Hadoop NameNode,DataNode运行情况<br>
    通过浏览器访问http://hadoop:50030，查看Hadoop JobTracker、TaskTracker运行情况<br>
    $ stop-all.sh<br><br><br>
  8.3常见启动问题<br>
    NameNode进程没有启动成功<br>
    (1)没有格式化<br>
    (2)配置文件没有修改正确<br>
    (3)hostname与ip没有绑定<br>
    (4)SSH的免密码登录没有配置成功<br>
    多次格式化问题，需删除/usr/local/hadoop/tmp下文件，再重新格式化.
            </div>
                </div>