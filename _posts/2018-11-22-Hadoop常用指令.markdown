---
layout:     post
title:      Hadoop常用指令
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1 class="postTitle" style="font-size:14.7px;margin-bottom:10px;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;">进入hadoop bin目录 ./hadoop</h1><strong>                                  ./hadoop fs</strong><p></p><h1 class="postTitle" style="font-size:14.7px;margin-bottom:10px;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;"><a class="postTitle2" href="http://www.cnblogs.com/feong/p/5148361.html" rel="nofollow" style="color:rgb(26,139,200);">hadoop常用命令实例</a></h1><div class="blogpost-body" style="margin-bottom:20px;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;"><p style="line-height:1.5;margin:10px auto;">1、查看指定目录下内容：hadoop fs –ls [文件目录]<br>[root@cdh01 tmp]# hadoop fs -ls -h /tmp<br>Found 2 items<br>drwxrwxrwx   - hdfs supergroup          0 2016-01-21 10:24 /tmp/.cloudera_health_monitoring_canary_files<br>drwx-wx-wx   - hive supergroup          0 2016-01-21 10:02 /tmp/hive<br>[root@cdh01 tmp]# hadoop fs -ls -h /<br>Found 2 items<br>drwxrwxrwx   - hdfs supergroup          0 2016-01-21 10:02 /tmp<br>drwxrwxr-x   - hdfs supergroup          0 2016-01-21 10:01 /user<br><br>2、将本地文件夹存储至hadoop上：hadoop fs –put [本地目录] [hadoop目录]<br>[root@cdh01 /]# mkdir test_put_dir #创建目录<br>[root@cdh01 /]# chown hdfs:hadoop test_put_dir #赋目录权限给hadoop用户<br>[root@cdh01 /]# su hdfs #切换到hadoop用户<br>[hdfs@cdh01 /]$ ls<br>bin  boot  dev  dfs  dfs_bak  etc  home  lib  lib64  lost+found  media  misc  mnt  net  opt  proc  root  sbin  selinux  srv  sys  test_put_dir  tmp  usr  var  wawa.txt  wbwb.txt  wyp.txt<br>[hdfs@cdh01 /]$ hadoop fs -put test_put_dir /<br>[hdfs@cdh01 /]$ hadoop fs -ls /<br>Found 4 items<br>drwxr-xr-x   - hdfs supergroup          0 2016-01-21 11:07 /hff<br>drwxr-xr-x   - hdfs supergroup          0 2016-01-21 15:25 /test_put_dir<br>drwxrwxrwt   - hdfs supergroup          0 2016-01-21 10:39 /tmp<br>drwxr-xr-x   - hdfs supergroup          0 2016-01-21 10:39 /user<br><br>3、在hadoop指定目录内创建新目录：hadoop fs –mkdir [目录地址]<br>[root@cdh01 /]# su hdfs<br>[hdfs@cdh01 /]$ hadoop fs -mkdir /hff<br><br>4、在hadoop指定目录下新建一个空文件，使用touchz命令：<br>[hdfs@cdh01 /]$ hadoop fs -touchz /test_put_dir/test_new_file.txt<br>[hdfs@cdh01 /]$ hadoop fs -ls /test_put_dir<br>Found 1 items<br>-rw-r--r--   3 hdfs supergroup          0 2016-01-21 15:29 /test_put_dir/test_new_file.txt<br><br>5、将本地文件存储至hadoop上：hadoop fs –put [本地地址] [hadoop目录]<br>[hdfs@cdh01 /]$ hadoop fs -put wyp.txt /hff #直接目录<br>[hdfs@cdh01 /]$ hadoop fs -put wyp.txt hdfs://cdh01.cap.com:8020/hff #服务器目录<br>注：文件wyp.txt放在/根目录下，结构如：<br>bin   dfs_bak  lib64       mnt   root     sys           var<br>boot  etc      lost+found  net   sbin     test_put_dir  wawa2.txt<br>dev   home     media       opt   selinux  tmp           wbwb.txt<br>dfs   lib      misc        proc  srv      usr           wyp.txt<br><br>6、打开某个已存在文件：hadoop fs –cat [file_path]<br>[hdfs@cdh01 /]$ hadoop fs -cat /hff/wawa.txt<br>1       张三    男      135<br>2       刘丽    女      235<br>3       王五    男      335<br><br>7、将hadoop上某个文件重命名hadoop fs –mv [旧文件名] [新文件名]<br>[hdfs@cdh01 /]$ hadoop fs -mv /tmp /tmp_bak #修改文件夹名<br><br>8、将hadoop上某个文件down至本地已有目录下：hadoop fs -get [文件目录] [本地目录]<br>[hdfs@cdh01 /]$ hadoop fs -get /hff/wawa.txt /test_put_dir<br>[hdfs@cdh01 /]$ ls -l /test_put_dir/<br>total 4<br>-rw-r--r-- 1 hdfs hdfs 42 Jan 21 15:39 wawa.txt<br><br>9、删除hadoop上指定文件：hadoop fs -rm [文件地址]<br>[hdfs@cdh01 /]$ hadoop fs -ls /test_put_dir/<br>Found 2 items<br>-rw-r--r--   3 hdfs supergroup          0 2016-01-21 15:41 /test_put_dir/new2.txt<br>-rw-r--r--   3 hdfs supergroup          0 2016-01-21 15:29 /test_put_dir/test_new_file.txt<br>[hdfs@cdh01 /]$ hadoop fs -rm /test_put_dir/new2.txt<br>16/01/21 15:42:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.<br>Moved: 'hdfs://cdh01.cap.com:8020/test_put_dir/new2.txt' to trash at: hdfs://cdh01.cap.com:8020/user/hdfs/.Trash/Current<br>[hdfs@cdh01 /]$ hadoop fs -ls /test_put_dir/<br>Found 1 items<br>-rw-r--r--   3 hdfs supergroup          0 2016-01-21 15:29 /test_put_dir/test_new_file.txt<br><br>10、删除hadoop上指定文件夹（包含子目录等）：hadoop fs –rm -r [目录地址]<br>[hdfs@cdh01 /]$ hadoop fs -rmr /test_put_dir<br>16/01/21 15:50:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.<br>Moved: 'hdfs://cdh01.cap.com:8020/test_put_dir' to trash at: hdfs://cdh01.cap.com:8020/user/hdfs/.Trash/Current<br>[hdfs@cdh01 /]$ hadoop fs -ls /<br>Found 3 items<br>drwxr-xr-x   - hdfs supergroup          0 2016-01-21 11:07 /hff<br>drwxrwxrwt   - hdfs supergroup          0 2016-01-21 10:39 /tmp<br>drwxr-xr-x   - hdfs supergroup          0 2016-01-21 15:42 /user<br><br>11、将hadoop指定目录下所有内容保存为一个文件，同时down至本地<br>hadoop dfs –getmerge /user /home/t<br><br>12、将正在运行的hadoop作业kill掉<br>hadoop job –kill  [job-id]</p><p style="line-height:1.5;margin:10px auto;"><br></p><p style="line-height:1.5;margin:10px auto;"><strong>From https://www.cnblogs.com/feong/p/5148361.html</strong></p></div>            </div>
                </div>