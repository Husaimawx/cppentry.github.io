---
layout:     post
title:      Spark数据分析概念入门
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>　　</p>

<p><img alt="" class="has" src="https://user-gold-cdn.xitu.io/2018/10/17/1668117d5cd3a575?w=533&amp;h=254&amp;f=png&amp;s=64576"></p>

<p> </p>

<p>　　在大数据的时代,数据的各种术语层出不穷,比如数据仓库、数据湖,还有比较热门的Hadoop、Spark,让人眼花缭乱。在这里,我们主要介绍的是Spark,从宏观的角度来介绍Spark到底是什么。</p>

<p>　　我们将解决如下几个简单的问题:</p>

<p>　　· Spark是什么</p>

<p>　　· Spark的组成</p>

<p>　　· Spark的用户和用途</p>

<p>　　下面我们分别进行叙述。</p>

<p>　　Spark是什么</p>

<p>　　首先,我们开始第1个简单的问题,Spark是什么?</p>

<p>　　Spark是什么,Spark是1个用来实现快速而通用的集群计算的平台。</p>

<p>　　在速度方面,Spark扩展了广泛使用的MapReduce计算模型,高效地支持更多计算模型,包括交互式查询和流处理,并能够在内存中进行计算。</p>

<p>　　总的来说,Spark适用于各种各样原先需要在多种不同的分布式平台下的场景,包括批处理、交互式查询、流处理。并通过1个统一的框架支持这些不同的计算,大大减轻了原先需要对各种平台分别管理的负担。</p>

<p>　　另外,Spark还提供了丰富的接口(支持Python、Java、Scala)和程序库外,还能与其他大数据工具密切配合使用,例如运行在Hadoop集群上。</p>

<p>　　Spark的组成</p>

<p>　　Spark项目包含多个紧密集成的组件,其核心是1个可以对很多计算任务、多个工作机器或计算集群上的应用进行调度、分发以及监控的计算引擎。</p>

<p>　　其各个组件主要包括:</p>

<p>　　· Spark Core,Spark的基本功能,包括任务调度、内存管理、错误恢复与存储系统交互等模块,另外还有RDD(对弹性分布式数据集,resilient distributed dataset)的API定义</p>

<p>　　· Spark SQL,Spark操作结构化的程序包,用于数据的查询</p>

<p>　　· Spark Streaming,提供对实时数据进行流式计算的组件</p>

<p>　　· MLib,提供常见机器学习功能的程序库</p>

<p>　　· GraphX,进行并行图计算的程序库</p>

<p>　　· 集群管理器,提供Hadoop YARN,Apache Mesos的支持</p>

<p>　　Spark的用户和用途</p>

<p>　　Spark主要面向两大目标人群:</p>

<p>　　· 数据科学家</p>

<p>　　· 工程师</p>

<p>　　可以用于以下两方面:</p>

<p>　　· 数据科学,更多的主要是数据分析领域,例如统计、机器学习建模、数据转换</p>

<p>　　· 数据处理,通过丰富的接口来快速实现常见的任务以及应用的监视、审查和性能调优</p>

<p>​</p>            </div>
                </div>