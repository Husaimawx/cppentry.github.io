---
layout:     post
title:      大数据离线-HDFS（上）入门操作
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>本次介绍HDFS,分为上，中，下，三篇</p>

<ul>
<li>上篇入HDFS门介绍，常用操作</li>
<li>中篇为HDFS的读写原理介绍</li>
<li>下篇为HDFS的测试Demo，常用API</li>
</ul>

<hr>



<h2 id="1-hdfs的基本概念">1. HDFS的基本概念</h2>

<ul>
<li><p><strong>HDFS的介绍</strong> <br>
HDFS 是 Hadoop Distribute File System 的简称， 意为： Hadoop 分布式文件系统。 是 Hadoop 核心组件之一，作为最底层的<strong>分布式存储服务而存在</strong>。 <br>
分布式文件系统解决的问题就是大数据存储。 它们是横跨在多台计算机上的存储系统。分布式文件系统在大数据时代有着广泛的应用前景，它们为存储和处理超大规模数据提供所需的扩展能力</p></li>
<li><p><strong>HDFS的设计目的</strong></p>

<ul><li>要里了解一个框架设计的目的，需要从框架出现之前面对的问题入手，这里我们进行绘图演示，说明文件的存储从简单到复杂的演化过程，最终形成了安全可靠快速的HDFS文件存储系统。</li>
<li>过程1 <br>
<img src="https://img-blog.csdn.net/2018091214173140?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIyOTA1Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></li>
<li>过程2 <br>
<img src="https://img-blog.csdn.net/20180912143943724?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIyOTA1Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></li></ul></li>
</ul>

<p>存储模式2就是简单的HDFS的存储模型，具体的存储过程，在下一篇的存储和读取中会进行详细的讲解。</p>

<hr>

<h2 id="2hdfs的重要特性">2.HDFS的重要特性</h2>

<ul>
<li><p><strong>master/slave 架构</strong> <br>
HDFS 采用 master/slave 架构。 一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。 Namenode 是 HDFS 集群主节点， Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。</p></li>
<li><p><strong>分块存储</strong> <br>
HDFS 中的文件在物理上是分块存储（block） 的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。</p></li>
<li><p><strong>命名空间NameSpace</strong> <br>
HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。 <br>
<strong>Namenode</strong> 负责维护文件系统的名字空间，任何对文件系统名字空间或属性 <br>
的修改都将被 Namenode 记录下来。 <br>
HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件， <br>
形如： hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。</p></li>
<li><p><strong>Namenode 元数据管理</strong> <br>
-我们把目录结构及文件分块位置信息叫做元数据。 Namenode 负责维护整个hdfs 文件系统的目录树结构，以及每一个文件所对应的 block 块信息（block 的 <br>
id，及所在的 datanode 服务器）。</p></li>
<li><p><strong>DataNode数据存储</strong> <br>
文件的各个 block 的具体存储管理由 datanode 节点承担。 每一个 block 都可以在多个 datanode 上。 Datanode 需要定时向 Namenode 汇报自己持有的 block信息。存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3）。</p></li>
<li><p><strong>副本机制</strong> <br>
为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。</p></li>
<li><p><strong>一次写入，多次读出</strong> <br>
HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。正因为如此， HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做.网盘等应用，因为，修改不方便，延迟大，网络开销大，成本太高。</p></li>
</ul>

<hr>



<h2 id="3-hdfs的基本操作">3. HDFS的基本操作</h2>

<ul>
<li><strong>Shell命令行客户端</strong> <br>
Hadoop 提供了文件系统的 shell 命令行客户端，使用方法如下：</li>
</ul>



<pre class="prettyprint"><code class=" hljs xml">hadoop fs <span class="hljs-tag">&lt;<span class="hljs-title">args</span>&gt;</span>  //<span class="hljs-tag">&lt;<span class="hljs-title">args表示参数</span>&gt;</span></code></pre>

<p>文件系统 shell 包括与 Hadoop 分布式文件系统（ HDFS）以及 Hadoop 支持的其他文件系统（ 如本地 FS， HFTP FS， S3 FS 等） 直接交互的各种类似 shell <br>
的命令。 所有 FS shell 命令都将路径 URI 作为参数。</p>



<pre class="prettyprint"><code class=" hljs cs">URI 格式为 scheme:<span class="hljs-comment">//authority/path。`</span></code></pre>

<p>对于 HDFS，该 scheme 是 hdfs，对于本地 FS，该 scheme 是 file。 scheme 和 authority 是可选的。 如果未指定，则使用配置中指定的默认方案。</p>

<p>对于 HDFS,命令示例如下：</p>



<pre class="prettyprint"><code class=" hljs mel">hadoop fs -<span class="hljs-keyword">ls</span> hdfs:<span class="hljs-comment">//namenode:host/parent/child</span>
<span class="hljs-comment">//hadoop fs -ls /parent/child fs.defaultFS 中有配置</span></code></pre>

<p>对于本地文件系统，命令示例如下：</p>



<pre class="prettyprint"><code class=" hljs mel">hadoop fs -<span class="hljs-keyword">ls</span> <span class="hljs-keyword">file</span>:<span class="hljs-comment">///root/</span></code></pre>

<p>如果使用的文件系统是 HDFS，则使用 hdfs dfs 也是可以的，hadoop1.x的使用方式。 <br>
命令如下 <br>
<img src="https://img-blog.csdn.net/20180912132328442?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIyOTA1Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""> <br>
<img src="https://img-blog.csdn.net/20180912132341951?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIyOTA1Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>

<p>具体演示如下</p>

<ul>
<li><p>-ls</p>

<p>功能： 显示文件、 目录信息。</p></li>
</ul>



<pre class="prettyprint"><code class=" hljs lasso">使用方法： hadoop fs <span class="hljs-attribute">-ls</span> <span class="hljs-preprocessor">[</span><span class="hljs-attribute">-h</span><span class="hljs-preprocessor">]</span><span class="hljs-markup"> </span><span class="hljs-preprocessor">[</span><span class="hljs-attribute">-R</span><span class="hljs-preprocessor">]</span><span class="hljs-markup"> &lt;args&gt;
实例：hadoop fs -ls /user/hadoop/file1</span></code></pre>

<ul>
<li><p>-mkdir</p>

<p>功能： 在 hdfs 上创建目录， -p 表示会创建路径中的各级父目录。</p></li>
</ul>



<pre class="prettyprint"><code class=" hljs perl">使用方法： hadoop fs -<span class="hljs-keyword">mkdir</span> [-p] &lt;paths&gt;
示例： hadoop fs -<span class="hljs-keyword">mkdir</span> – p /user/hadoop/dir1</code></pre>

<ul>
<li><p>-put</p>

<p>功能： 将单个 src 或多个 srcs 从本地文件系统复制到目标文件系统。</p></li>
</ul>



<pre class="prettyprint"><code class=" hljs haml">使用方法： hadoop fs -put [-f] [-p] [ -|&lt;localsrc1&gt; .. ]. &lt;dst&gt;
-<span class="ruby">p：保留访问和修改时间，所有权和权限。
</span>-<span class="ruby">f：覆盖目的地（如果已经存在）
</span>示例： hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir</code></pre>

<ul>
<li>-get <br>
功能：将文件下载到本地系统</li>
</ul>



<pre class="prettyprint"><code class=" hljs lasso">使用方法： hadoop fs <span class="hljs-attribute">-get</span> <span class="hljs-preprocessor">[</span><span class="hljs-attribute">-ignorecrc</span><span class="hljs-preprocessor">]</span><span class="hljs-markup"> </span><span class="hljs-preprocessor">[</span><span class="hljs-attribute">-crc</span><span class="hljs-preprocessor">]</span><span class="hljs-markup"> </span><span class="hljs-preprocessor">[</span><span class="hljs-attribute">-p</span><span class="hljs-preprocessor">]</span><span class="hljs-markup"> </span><span class="hljs-preprocessor">[</span><span class="hljs-attribute">-f</span><span class="hljs-preprocessor">]</span><span class="hljs-markup"> &lt;src&gt; &lt;localdst&gt;
-ignorecrc：跳过对下载文件的 CRC 检查。
-crc：为下载的文件写 CRC 校验和。
示例： hadoop fs -get hdfs://host:port/user/hadoop/file localfile</span></code></pre>

<ul>
<li>-appendToFile <br>
功能：追加一个文件到已经存在的文件末尾</li>
</ul>



<pre class="prettyprint"><code class=" hljs r">使用方法： hadoop fs -appendToFile &lt;localsrc&gt; <span class="hljs-keyword">...</span> &lt;dst&gt;
示例： hadoop fs -appendToFile localfile /hadoop/hadoopfile</code></pre>

<ul>
<li>-cat <br>
功能：显示文件内容到 stdout</li>
</ul>



<pre class="prettyprint"><code class=" hljs r">使用方法： hadoop fs -cat [-ignoreCrc] URI [URI <span class="hljs-keyword">...</span>]
示例： hadoop fs -cat /hadoop/hadoopfile</code></pre>

<ul>
<li>-tail <br>
功能： 将文件的最后一千字节内容显示到 stdout。</li>
</ul>



<pre class="prettyprint"><code class=" hljs bash">使用方法： hadoop fs -tail [<span class="hljs-operator">-f</span>] URI
<span class="hljs-operator">-f</span> 选项将在文件增长时输出附加数据。
示例： hadoop fs -tail /hadoop/hadoopfile</code></pre>

<ul>
<li>-chmod <br>
功能： 改变文件的权限。使用-R 将使改变在目录结构下递归进行。</li>
</ul>



<pre class="prettyprint"><code class=" hljs perl">示例： hadoop fs -<span class="hljs-keyword">chmod</span> <span class="hljs-number">666</span> /hadoop/hadoopfile</code></pre>

<ul>
<li>-cp <br>
功能：从 hdfs 的一个路径拷贝 hdfs 的另一个路径</li>
</ul>



<pre class="prettyprint"><code class=" hljs avrasm">示例： hadoop fs -<span class="hljs-keyword">cp</span> /aaa/jdk<span class="hljs-preprocessor">.tar</span><span class="hljs-preprocessor">.gz</span> /bbb/jdk<span class="hljs-preprocessor">.tar</span><span class="hljs-preprocessor">.gz</span><span class="hljs-number">.2</span> </code></pre>

<ul>
<li>-mv <br>
功能：在 hdfs 目录中移动文件</li>
</ul>



<pre class="prettyprint"><code class=" hljs avrasm">示例： hadoop fs -mv /aaa/jdk<span class="hljs-preprocessor">.tar</span><span class="hljs-preprocessor">.gz</span> /</code></pre>

<ul>
<li>-rm <br>
功能： 删除指定的文件。只删除非空目录和文件。 -r 递归删除。</li>
</ul>



<pre class="prettyprint"><code class=" hljs lasso">示例： hadoop fs <span class="hljs-attribute">-rm</span> <span class="hljs-attribute">-r</span> /aaa/bbb<span class="hljs-subst">/</span></code></pre>

<ul>
<li>-df <br>
功能：统计文件系统的可用空间信息</li>
</ul>



<pre class="prettyprint"><code class=" hljs lasso">示例： hadoop fs <span class="hljs-attribute">-df</span> <span class="hljs-attribute">-h</span> <span class="hljs-subst">/</span></code></pre>

<ul>
<li>-du <br>
功能： 显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。</li>
</ul>



<pre class="prettyprint"><code class=" hljs lasso">示例： hadoop fs <span class="hljs-attribute">-du</span> /user/hadoop/dir1</code></pre>

<ul>
<li>-setrep <br>
功能： 改变一个文件的副本系数。 -R 选项用于递归改变目录下所有文件的副本 <br>
系数。</li>
</ul>



<pre class="prettyprint"><code class=" hljs lasso">示例： hadoop fs <span class="hljs-attribute">-setrep</span> <span class="hljs-attribute">-w</span> <span class="hljs-number">3</span> <span class="hljs-attribute">-R</span> /user/hadoop/dir1</code></pre>

<hr>

<p>上篇的内容的到此结束</p>

<p>本博客主要对<strong>大数据</strong>的学习进行系统的讲解，感兴趣的同学可以关注订阅。</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>