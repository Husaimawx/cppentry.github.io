---
layout:     post
title:      SparkSQL-简介
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1>一 概览</h1>

<p>Spark SQL 是 Spark用来处理结构化数据的一个模块。与基础的 Spark RDD API 不同的是，Spark SQL 提供了查询结构化数据及计算结果等信息的接口。在内部, Spark SQL 使用这个额外的信息去执行额外的优化。有几种方式可以跟 Spark SQL 进行交互，比如 SQL 和 Dataset API。当使用相同执行引擎进行计算时, 无论使用哪种 API / 语言都可以快速的计算。这意味着开发人员能够在基于提供最自然的方式来表达一个给定的 transformation API 之间实现轻松的来回切换。</p>

<h1>二 SQL</h1>

<p>Spark SQL 的功能之一是执行 SQL 查询。Spark SQL 也能够被用于从已存在的 Hive 环境中读取数据。当以另外的编程语言运行SQL时，查询结果将以Dataset/DataFrame的形式返回。</p>

<h1>三 Datasets 和 DataFrames</h1>

<p>Dataset 是一个分布式的数据集合。Dataset 是在 Spark 1.6 中被添加的新接口，它拥有 RDD 的优点（比如：强类型化，能够使用强大的 lambda 函数）与Spark SQL执行引擎的优点。Dataset 可以通过JVM 对象进行构造并且使用转换功能（map, flatMap, filter, 等等）。</p>

<p>DataFrame 是<em>Dataset</em> 组成的指定列。它的概念与关系型数据库或者在 R/Python 中的表是一样的，但是有很多优化.。创建DataFrames的方式有很多种，比如：结构化的文本文件，Hive中的表，外部数据库，或者已经存在的 RDDs。在Scala API中, <code>DataFrame</code> 仅仅是一个 <code>Dataset[Row]</code>类型的别名。然而，在Java API中，用户需要使用 <code>Dataset&lt;Row&gt;</code> 去代表一个 <code>DataFrame</code>.</p>            </div>
                </div>