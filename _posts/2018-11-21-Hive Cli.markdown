---
layout:     post
title:      Hive Cli
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
Hive Cli <br><br><br>
= hive启动  = <br>
*$ hive –f script.q <br>
*$ hive -e 'SELECT * FROM dummy‘ <br>
*$ hive -S -e 'SELECT * FROM dummy‘ <br>
*$ hive -hiveconf hive.root.logger=DEBUG,console <br><br><br>
= set  = <br>
*hive&gt; SET hive.enforce.bucketing=true;  //设置值 <br>
*hive&gt; SET hive.enforce.bucketing;  // 显示值 <br>
*hive&gt; set -v;  //显示所有的值，包括hadoop的。 <br>
*hive&gt; set; //显示跟基本的hadoop不同的配置，原理就是比对当前的所有选项与基本的配置是否不同，如修改过，已经不同了就打印该值。 <br><br><br>
= dfs 命令 = <br>
dfs 命令可以执行 Shell 环境下的 hadoop fs 的所有命令 <br>
例如： <br>
列出 HDFS 上的文件： <br>
hive&gt; dfs -ls /user/hive; <br><br><br>
= add  = <br>
*ADD { FILE[S] | JAR[S] | ARCHIVE[S] } &lt;filepath1&gt; [&lt;filepath2&gt;]* <br>
*hive&gt; add jar /tmp/a.jar; <br>
*hive&gt; add jar /tmp/a.jar /tmp/b.jar; <br><br><br>
= delete  = <br>
*DELETE { FILE[S] | JAR[S] | ARCHIVE[S] } [&lt;filepath1&gt; &lt;filepath2&gt; ..] <br><br><br>
= list  = <br>
*LIST { FILE[S] | JAR[S] | ARCHIVE[S] } [&lt;filepath1&gt; &lt;filepath2&gt; ..] <br>
*hive&gt; list jar; <br>
*hive&gt; list jars; <br><br><br>
= show  = <br>
*hive&gt; show functions; <br>
*hive&gt; show tables; <br>
*hive&gt; show tables '*tianzhao*'; <br>
*hive&gt; show partition tablename; <br>
*hive&gt; show table extended like table_with_partitions partition(ds=101); <br><br><br>
= desc  = <br>
*hive&gt; desc/describe function length; <br>
*hive&gt; desc/describe tablename; <br>
*hive&gt; desc/describe extended tablename;  //显示表的信息 <br>
*hive&gt; desc/describe extended tablename partition(ds=1); //显示partition的信息 <br>
*hive&gt; desc/describe formatted tablename;  //显示表的信息,跟extended相比，显示更友好 <br><br><br>
= source  = <br>
*hive&gt; source /home/username/sql/test.sql; <br><br><br>
= !  = <br>
*hive&gt; !ls;  //ls当前目录 <br><br><br>
= quit  = <br>
hive&gt; quit; 或者  hive&gt; exit; <br><br><br>
= hiveserver  = <br>
* $hive --service hiveserver <br>
* $hive --service help <br>
Usage ./hive &lt;parameters&gt; --service serviceName &lt;service parameters&gt; <br>
Service List: cli help hiveserver hwi jar lineage metastore rcfilecat start-hive stop-hive <br>
Parameters parsed: <br>
  --auxpath : Auxillary jars <br>
  --config : Hive configuration directory <br>
  --service : Starts specific service/component. cli is default <br>
Parameters used: <br>
  HADOOP_HOME : Hadoop install directory <br>
  HIVE_OPT : Hive options <br>
For help on a particular service: <br>
  ./hive --service serviceName --help <br><br><br><br><br>
*$hive --service start-hive <br>
Starting Hive Thrift Server in Daemon Mode <br>
starting jar, logging to /home/tianzhao/hive/hadoop-0.19.1/bin/../logs/hadoop-tianzhao-jar-ubuntu.out <br>
*$hive --service stop-hive <br>
Stopping Hive Thrift Server in Daemon Mode <br>
stopping jar <br><br><br><br><br>
[http://wiki.apache.org/hadoop/Hive/LanguageManual/Cli 官方的配置Wiki] <br><br><br>
alter table s_spu set TBLPROPERTIES ('EXTERNAL'='TRUE');  //内部表转外部表 <br><br><br>
set hive.auto.convert.join=true; <br>
set hive.exec.mode.local.auto=true; <br>
set hive.mapred.local.mem = 200; <br>
set hive.groupby.skewindata=true; <br><br><br>
alter serde <br>
数据： <br>
a,b <br>
c,d <br>
e,f <br>
hive&gt; create table delim(key string, value string); <br>
hive&gt; load data local inpath '/home/tianzhao/Documents/delim' into table delim; <br>
hive&gt; select * from delim; <br>
a,b<span> </span>NULL <br>
c,d<span> </span>NULL <br>
e,f<span> </span>NULL <br>
hive&gt; ALTER TABLE delim SET SERDEPROPERTIES ('field.delim' = ','); <br>
a<span> </span>b <br>
c<span> </span>d <br>
e<span> </span>f <br><br><br>
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DropPartitions <br>
ALTER TABLE page_view DROP PARTITION (dt='2008-08-08', country='us'); <br><br><br>
ALTER TABLE web_log ADD IF NOT EXISTS PARTITION (pt='20120325') LOCATION 'hdfs://localhost:9000/group/log1/2012/20120325'; <br>
ALTER TABLE web_log PARTITION(pt='20120325') SET FILEFORMAT SequenceFile; <br>
ALTER TABLE r_table SET SERDEPROPERTIES ('serialization.null.format'=); <br><br><br>
日志在： /tmp/$USER/hive.log  中
            </div>
                </div>