---
layout:     post
title:      Ubuntu14.04下安装Hadoop2.4.0 （伪分布模式）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
一、配置core-site.xml<br>
    /usr/local/hadoop/etc/hadoop/core-site.xml 包含了hadoop启动时的配置信息。<br><br>
    编辑器中打开此文件<br><p>    sudo gedit /usr/local/hadoop/etc/hadoop/core-site.xml</p>
<p><br></p>
    在该文件的&lt;configuration&gt;&lt;/configuration&gt;之间增加如下内容：<br>
    &lt;property&gt;<br>
        &lt;name&gt;fs.default.name&lt;/name&gt;<br>
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br>
    &lt;/property&gt;<br><br>
    保存、关闭编辑窗口。<br><br>
二、配置yarn-site.xml<br>
    /usr/local/hadoop/etc/hadoop/yarn-site.xml包含了MapReduce启动时的配置信息。<br><br>
    编辑器中打开此文件<br><br><p>    sudo gedit yarn-site.xml</p>
<p><br></p>
<p>在该文件的&lt;configuration&gt;&lt;/configuration&gt;之间增加如下内容：</p>
<p><br></p>
<p>&lt;property&gt;</p>
<p><br></p>
<p>&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</p>
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br><p>&lt;/property&gt;</p>
<p><br></p>
<p>&lt;property&gt;</p>
<p>&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</p>
<p>&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<br>
    保存、关闭编辑窗口<br><br><br>
三、创建和配置mapred-site.xml<br>
 <br><br><br>
    默认情况下，/usr/local/hadoop/etc/hadoop/文件夹下有mapred.xml.template文件，我们要复制该文件，并命名为mapred.xml，该文件用于指定MapReduce使用的框架。<br>
    <br>
    复制并重命名<br><br><br>
    cp mapred-site.xml.template mapred-site.xml<br><br><br>
    编辑器打开此新建文件<br><br><br>
    sudo gedit mapred-site.xml <br><br><br>
在该文件的&lt;configuration&gt;&lt;/configuration&gt;之间增加如下内容：<br><br><br>
    &lt;property&gt;<br><br><br>
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br><br><br>
        &lt;value&gt;yarn&lt;/value&gt;<br><br><br>
    &lt;/property&gt;<br><br><br>
    保存、关闭编辑窗口    <br><br>
四、配置hdfs-site.xml<br>
    /usr/local/hadoop/etc/hadoop/hdfs-site.xml用来配置集群中每台主机都可用，指定主机上作为namenode和datanode的目录。<br><br>
 <span style="color:#ff0000;">   创建文件夹name和data，并授权sudo chmod -R a+w /usr/local/hadoop/hdfs</span><br><br>
你也可以在别的路径下创建上图的文件夹，名称也可以与上图不同，但是需要和hdfs-site.xml中的配置一致。<br><br><br>
    编辑器打开hdfs-site.xml<br><br><br>
    在该文件的&lt;configuration&gt;&lt;/configuration&gt;之间增加如下内容：   <br><br><br>
    &lt;property&gt;<br><br><br>
        &lt;name&gt;dfs.replication&lt;/name&gt;<br><br><br>
        &lt;value&gt;1&lt;/value&gt;<br><br><br>
    &lt;/property&gt;<br><br><br>
    &lt;property&gt;<br><br><br>
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br><br><br>
        &lt;value&gt;file:/usr/local/hadoop/hdfs/name&lt;/value&gt;<br><br><br>
    &lt;/property&gt;<br><br><br>
    &lt;property&gt;<br><br><br>
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br><br><br>
        &lt;value&gt;file:/usr/local/hadoop/hdfs/data&lt;/value&gt;<br><br><br>
    &lt;/property&gt;<br><br><br>
 <br><br><br><p>    保存、关闭编辑窗口</p>
<p><br></p>
<br>
五、格式化hdfs<br>
 <br><br><br>
    hdfs namenode -format    <br><br><br>
    只需要执行一次即可，如果在hadoop已经使用后再次执行，会清除掉hdfs上的所有数据。<br><br><br>
六、启动Hadoop<br>
 <br><br><br>
    经过上文所描述配置和操作后，下面就可以启动这个单节点的集群    <br><br><br>
    执行启动命令：<br><br><br>
    sbin/start-dfs.sh    <br><br><br>
    执行该命令时，如果有yes /no提示，输入yes，回车即可。    <br><br><br>
    接下来，执行：<br><br><br>
    sbin/start-yarn.sh    <br><br><br>
    执行完这两个命令后，Hadoop会启动并运行    <br><br><br>
    执行 jps命令，会看到Hadoop相关的进程，如下图：<br><br><br><img src="http://images.cnitblog.com/blog/12097/201406/191129114898168.png" alt=""><br><br>
浏览器打开 http://localhost:50070/，会看到hdfs管理页面<br><br><img src="http://images.cnitblog.com/blog/12097/201406/191129120361268.png" alt=""><br><br><br><br>
浏览器打开http://localhost:8088，会看到hadoop进程管理页面<br><br><br><img src="http://images.cnitblog.com/blog/12097/201406/191129127706126.png" alt=""><br><br><br>
七、WordCount验证<br>
    dfs上创建input目录<br><br><br>
    bin/hadoop fs -mkdir -p input<br><br><img src="http://images.cnitblog.com/blog/12097/201406/191129130674684.png" alt=""><br><br><br><br>
把hadoop目录下的README.txt拷贝到dfs新建的input里<br><br><br>
    hadoop fs -copyFromLocal README.txt input<br><br><img src="http://images.cnitblog.com/blog/12097/201406/191129132704213.png" alt=""><br><br><br>
    运行WordCount<br><br><br>
    hadoop jar share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.4.0-sources.jar org.apache.hadoop.examples.WordCount input output<br><img src="http://images.cnitblog.com/blog/12097/201406/191129134733742.png" alt=""><br><br>
    <br>
可以看到执行过程<br><br><img src="http://images.cnitblog.com/blog/12097/201406/191129142235371.png" alt=""><br><br><br>
运行完毕后，查看单词统计结果<br><br><br><p>    hadoop fs -cat output/*</p>
<p><img src="http://images.cnitblog.com/blog/12097/201406/191129155043329.png" alt=""><br></p>
<br>            </div>
                </div>