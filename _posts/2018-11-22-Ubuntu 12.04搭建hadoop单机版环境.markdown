---
layout:     post
title:      Ubuntu 12.04搭建hadoop单机版环境
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>一. 你要安装Ubuntu这一步省略；</p>
<p>二. 在Ubuntu下创建hadoop用户组和用户;</p>
<p>1. 创建hadoop用户组：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo addgroup hadoop</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745190.png" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745190.png" border="0" height="125" width="411"></a></p>
<p>2. 创建hadoop用户：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo adduser -ingroup hadoop hadoop</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745191.jpg" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745191.jpg" border="0" height="276" width="498"></a></p>
<p>3. 给hadoop用户添加权限，打开/etc/sudoers文件：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit /etc/sudoers</span></td>
</tr></tbody></table><p>按回车键后就会打开/etc/sudoers文件了，给hadoop用户赋予root用户同样的权限。</p>
<p>在root   ALL=(ALL:ALL)   ALL下添加hadoop   ALL=(ALL:ALL)  ALL，</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> hadoop  ALL=(ALL:ALL) ALL</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745192.png" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745192.png" border="0" height="163" width="347"></a></p>
<p>三. 在Ubuntu下安装JDK</p>
<p>使用如下命令执行即可：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo apt-get install openjdk-6-jre</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745193.jpg" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745193.jpg" border="0" height="272" width="498"></a></p>
<p>四. 修改机器名</p>
<p>每当ubuntu安装成功时，我们的机器名都默认为：ubuntu ，但为了以后集群中能够容易分辨各台服务器，需要给每台机器取个不同的名字。机器名由 /etc/hostname文件决定。</p>
<p>1. 打开/etc/hostname文件：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit /etc/hostname</span></td>
</tr></tbody></table><p>2. 将/etc/hostname文件中的ubuntu改为你想取的机器名。这里我取"dubin-ubuntu"。 重启系统后才会生效。</p>
<p>五. 安装ssh服务</p>
<p>这里的ssh和三大框架:spring,struts,hibernate没有什么关系，ssh可以实现远程登录和管理，具体可以参考其他相关资料。</p>
<p>安装openssh-server，</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo apt-get install ssh openssh-server</span></td>
</tr></tbody></table><p>有时候直接安装不了，可以把提示的错误直接粘帖到百度就可以解决。<br></p>
<p>这时假设您已经安装好了ssh，您就可以进行第六步了哦~</p>
<p>六、 建立ssh无密码登录本机</p>
<p>首先要转换成hadoop用户，执行以下命令：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> su - hadoop</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745194.png" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745194.png" border="0" height="90" width="438"></a></p>
<p>ssh生成密钥有rsa和dsa两种生成方式，默认情况下采用rsa方式。</p>
<p>1. 创建ssh-key，，这里我们采用rsa方式：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> ssh-keygen -t rsa -P ""</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745195.jpg" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745195.jpg" border="0" height="333" width="498"></a></p>
<p>（注：回车后会在~/.ssh/下生成两个文件：id_rsa和id_rsa.pub这两个文件是成对出现的）</p>
<p>2. 进入~/.ssh/目录下，将id_rsa.pub追加到authorized_keys授权文件中，开始是没有authorized_keys文件的：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> cd ~/.ssh<br>
cat id_rsa.pub &gt;&gt; authorized_keys</span></td>
</tr></tbody></table><p>如图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745196.jpg" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745196.jpg" border="0" height="43" width="498"></a></p>
<p>（完成后就可以无密码登录本机了。）</p>
<p>3. 登录localhost：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> ssh localhost</span></td>
</tr></tbody></table><p>如图：</p>
<p>( 注：当ssh远程登录到其它机器后，现在你控制的是远程的机器，需要执行退出命令才能重新控制本地主机。)</p>
<p>4. 执行退出命令：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> exit</span></td>
</tr></tbody></table><p>七. 安装hadoop</p>
<p>我们采用的hadoop版本是：hadoop-0.20.203（<a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/" rel="nofollow">http://www.apache.org/dyn/closer.cgi/hadoop/common/</a>  ），因为该版本比较稳定。</p>
<p>1. 假设hadoop-0.20.203.tar.gz在桌面，将它复制到安装目录 /usr/local/下：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo cp hadoop-0.20.203.0rc1.tar.gz /usr/local/</span></td>
</tr></tbody></table><p>2. 解压hadoop-0.20.203.tar.gz：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> cd /usr/local<br>
sudo tar -zxf hadoop-0.20.203.0rc1.tar.gz</span></td>
</tr></tbody></table><p>3. 将解压出的文件夹改名为hadoop：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo mv hadoop-0.20.203.0 hadoop</span></td>
</tr></tbody></table><p>4. 将该hadoop文件夹的属主用户设为hadoop：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo chown -R hadoop:hadoop hadoop</span></td>
</tr></tbody></table><p>5. 打开hadoop/conf/hadoop-env.sh文件：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit hadoop/conf/hadoop-env.sh</span></td>
</tr></tbody></table><p>6. 配置conf/hadoop-env.sh（找到#export JAVA_HOME=...,去掉#，然后加上本机jdk的路径）：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> export JAVA_HOME=/usr/lib/jvm/java-6-openjdk</span></td>
</tr></tbody></table><p>7. 打开conf/core-site.xml文件：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit hadoop/conf/core-site.xml</span></td>
</tr></tbody></table><p>编辑如下：</p>
<pre></pre><ol class="dp-xml"><li class="alt"><span><span class="tag"></span></span>&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/hadoopdata&lt;/value&gt;
  &lt;description&gt;A base for other temporary directories.&lt;/description&gt;
&lt;/property&gt;<p>&lt;property&gt;
  &lt;name&gt;fs.default.name&lt;/name&gt;
  &lt;value&gt;hdfs://m131:9000&lt;/value&gt;
  &lt;description&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;/description&gt;
&lt;/property&gt;</p>&lt;/configuration&gt;</li><li><span></span></li></ol>
<p>8. 打开conf/mapred-site.xml文件：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit hadoop/conf/mapred-site.xml</span></td>
</tr></tbody></table><p>编辑如下：</p>
<pre></pre><ol class="dp-xml"><li class="alt"><span><span class="tag"></span></span>&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;mapred.job.tracker&lt;/name&gt;
  &lt;value&gt;m131:9001&lt;/value&gt;
  &lt;description&gt;The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  &lt;/description&gt;
&lt;/property&gt;
&lt;/configuration&gt;</li><li class="alt"><span></span></li></ol>
<p>9. 打开conf/hdfs-site.xml文件：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit hadoop/conf/hdfs-site.xml</span></td>
</tr></tbody></table><p>编辑如下：</p>
<pre></pre><ol class="dp-xml"><li class="alt"><span><span class="tag">&lt;</span><span class="tag-name">configuration</span><span class="tag">&gt;</span><span> </span></span></li><li><span class="tag">&lt;</span><span class="tag-name">property</span><span class="tag">&gt;</span><span> </span></li><li class="alt"><span class="tag">&lt;</span><span class="tag-name">name</span><span class="tag">&gt;</span><span>dfs.name.dir</span><span class="tag">&lt;/</span><span class="tag-name">name</span><span class="tag">&gt;</span><span> </span></li><li><span class="tag">&lt;</span><span class="tag-name">value</span><span class="tag">&gt;</span><span>/usr/local/hadoop/datalog1,/usr/local/hadoop/datalog2</span><span class="tag">&lt;/</span><span class="tag-name">value</span><span class="tag">&gt;</span><span> </span></li><li class="alt"><span class="tag">&lt;/</span><span class="tag-name">property</span><span class="tag">&gt;</span><span> </span></li><li><span class="tag">&lt;</span><span class="tag-name">property</span><span class="tag">&gt;</span><span> </span></li><li class="alt"><span class="tag">&lt;</span><span class="tag-name">name</span><span class="tag">&gt;</span><span>dfs.data.dir</span><span class="tag">&lt;/</span><span class="tag-name">name</span><span class="tag">&gt;</span><span> </span></li><li><span class="tag">&lt;</span><span class="tag-name">value</span><span class="tag">&gt;</span><span>/usr/local/hadoop/data1,/usr/local/hadoop/data2</span><span class="tag">&lt;/</span><span class="tag-name">value</span><span class="tag">&gt;</span><span> </span></li><li class="alt"><span class="tag">&lt;/</span><span class="tag-name">property</span><span class="tag">&gt;</span><span> </span></li><li><span class="tag">&lt;</span><span class="tag-name">property</span><span class="tag">&gt;</span><span> </span></li><li class="alt"><span class="tag">&lt;</span><span class="tag-name">name</span><span class="tag">&gt;</span><span>dfs.replication</span><span class="tag">&lt;/</span><span class="tag-name">name</span><span class="tag">&gt;</span><span> </span></li><li><span class="tag">&lt;</span><span class="tag-name">value</span><span class="tag">&gt;</span><span>2</span><span class="tag">&lt;/</span><span class="tag-name">value</span><span class="tag">&gt;</span><span> </span></li><li class="alt"><span class="tag">&lt;/</span><span class="tag-name">property</span><span class="tag">&gt;</span><span> </span></li><li><span class="tag">&lt;/</span><span class="tag-name">configuration</span><span class="tag">&gt;</span><span> </span></li></ol>
<p>10. 打开conf/masters文件，添加作为secondarynamenode的主机名，作为单机版环境，这里只需填写 localhost 就Ok了。</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit hadoop/conf/masters</span></td>
</tr></tbody></table><p>11. 打开conf/slaves文件，添加作为slave的主机名，一行一个。作为单机版，这里也只需填写 localhost就Ok了。</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> sudo gedit hadoop/conf/slaves</span></td>
</tr></tbody></table><p>八. 在单机上运行hadoop</p>
<p>1. 进入hadoop目录下，格式化hdfs文件系统，初次运行hadoop时一定要有该操作，</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> cd /usr/local/hadoop/<br>
bin/hadoop namenode -format</span></td>
</tr></tbody></table><p>2. 当你看到下图时，就说明你的hdfs文件系统格式化成功了。</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745197.jpg" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745197.jpg" border="0" height="103" width="498"></a></p>
<p>3. 启动bin/start-all.sh：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> bin/start-all.sh</span></td>
</tr></tbody></table><p>4. 检测hadoop是否启动成功：</p>
<table align="center" border="0" cellpadding="6" cellspacing="0" width="95%" style="border-bottom:#cccccc 1px dotted;border-left:#cccccc 1px dotted;table-layout:fixed;border-top:#cccccc 1px dotted;border-right:#cccccc 1px dotted;"><tbody><tr><td bgcolor="#fdfddf"><span style="color:#ff0000;"> jps</span></td>
</tr></tbody></table><p>如果有Namenode，SecondaryNameNode，TaskTracker，DataNode，JobTracker五个进程，就说明你的hadoop单机版环境配置好了！</p>
<p>如下图：</p>
<p style="text-align:center;"><a href="http://images.51cto.com/files/uploadimg/20121107/1745198.png" rel="nofollow"><img class="fit-image" alt="" src="http://images.51cto.com/files/uploadimg/20121107/1745198.png" border="0" height="202" width="406"></a></p>
至此，一个hadoop的单机版环境就搭建好了
            </div>
                </div>