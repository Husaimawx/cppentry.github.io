---
layout:     post
title:      Spark学习之基础相关组件（1）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为王小雷原创文章，未经博主允许不得转载					https://blog.csdn.net/dream_an/article/details/50523675				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2 id="spark学习之基础相关组件1">Spark学习之基础相关组件（1）</h2>

<h3 id="1-spark是一个用来实现快速而通用的集群计算的平台">1. Spark是一个用来实现快速而通用的集群计算的平台。</h3>



<h3 id="2-spark的一个主要特点是能够在内存中进行计算因而更快">2. Spark的一个主要特点是能够在内存中进行计算，因而更快。</h3>



<h3 id="3-rddresilient-distributed-dataset弹性分布式数据集表示分布在多个计算节点上可以并行操作的元素的集合是spark的主要编程抽象">3. RDD（resilient distributed dataset弹性分布式数据集）表示分布在多个计算节点上可以并行操作的元素的集合，是Spark的主要编程抽象。</h3>



<h3 id="4-spark是一个大一统的软件栈">4. Spark是一个大一统的软件栈：</h3>

<pre><code>4.1 Spark core实现了Spark的基本功能，包括任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集的API定义。
4.2 Spark Streaming是Spark提供的对实时数据进行流式计算的组件。
4.3 Mlib机器学习（ML），提供了很多的机器学习算法，包括分类、回归、聚类、协同过滤等，还包括模型评估、数据导入等额外支持功能。
4.4 Graph是用来操作图的程序库，可以进行并行的图计算。
4.5 集群管理器（cluster manager），包括自带的独立调度器，还有支持Hadoop YARN、Apache Mesos。
</code></pre>



<h3 id="5-spark的存储层包括hdfs分布式文件系统本地文件亚马逊s3cassandrahivehbase等">5. Spark的存储层，包括HDFS(分布式文件系统)、本地文件、亚马逊S3、Cassandra、Hive、Hbase等。</h3>



<h3 id="6-spark是用scala写的运行在java虚拟机jvm上">6. spark是用Scala写的，运行在Java虚拟机）（JVM）上。</h3>



<h3 id="7-独立应用在独立应用程序中使用spark需要自行初始化sparkcontext">7. 独立应用，在独立应用程序中使用Spark需要自行初始化SparkContext。</h3>

<pre><code>7.1. 初始化SparkContext：
    完成与Spark的连接后，接下来需要导入Spark包并且创建SparkContext。可以通过先创建一个SparkConf对象来配置应用，然后基于这个SparkConf创建一个SparkContext对象。
    在Python中初始化Spark
</code></pre>



<pre class="prettyprint"><code class=" hljs python">        <span class="hljs-keyword">from</span> pyspark <span class="hljs-keyword">import</span> SparkConf,SparkContext

        conf = SparkConf().setMaster(<span class="hljs-string">"local"</span>).setAppname(<span class="hljs-string">"My App"</span>)
        sc = SparkContext(conf = conf)</code></pre>

<pre><code>7.2 使用方法（如使用文本文件）来创建RDD并操控它们。
7.3 最后关闭Spark调用SparkContext的stop()方法，或者直接退出应用（System.exit(0)或者sys.exit())。
</code></pre>



<h3 id="8-构建独立应用1javascalapython创建应用2stbmaven打包3stbmaven运行">8. 构建独立应用：1）java、Scala、Python创建应用2）stb、maven打包3)stb、maven运行</h3>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>