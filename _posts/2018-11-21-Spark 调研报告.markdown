---
layout:     post
title:      Spark 调研报告
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/a1317171753/article/details/77720448				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="1总结部分">1.总结部分</h1>

<hr>



<h2 id="什么是spark"><strong>什么是Spark？</strong></h2>

<p>Spark是个通用的集群计算框架</p>



<h2 id="spark用来做什么"><strong>Spark用来做什么？</strong></h2>

<p>分发数据，分发计算</p>



<h2 id="spark的主要应用领域"><strong>Spark的主要应用领域？</strong></h2>

<p>机器学习，最优化算法 </p>



<h2 id="为什么选择spark"><strong>为什么选择Spark？</strong></h2>

<ol>
<li>Spark对迭代应用的计算特别有效</li>
<li>可以类似Python REPL的命令行提示符交互式访问</li>
<li>快</li>
</ol>



<h2 id="spark提供何种api"><strong>Spark提供何种API？</strong></h2>

<p>Scala、Java和Python</p>



<h2 id="spark性能如何"><strong>Spark性能如何？</strong></h2>

<p><strong>Hadoop，Spark速度对比</strong> <br>
<img src="https://pic2.zhimg.com/029ab6e670fa8f5248d5120ab5fb3a59_b.jpg" alt="此处输入图片的描述" title=""></p>

<blockquote>
  <p>从表格中可以看出排序100TB的数据（1万亿条数据），<strong>Spark只用了Hadoop所用1/10的计算资源，耗时只有Hadoop的1/3</strong>。</p>
</blockquote>

<p><strong>Hadoop，Spark回归算法速度对比</strong> <br>
<img src="https://pic3.zhimg.com/d353f362c17b5504c23a25092b599f0a_b.jpg" alt="此处输入图片的描述" title=""></p>

<blockquote>
  <p>在Hadoop的世界里，做迭代计算是非常耗资源，它每次的IO序列代价很大，所以每次迭代需要差不多的等待。而Spark第一次启动需要载入到内存，之后迭代直接在内存利用中间结果做不落地的运算，所以后期的迭代速度快到可以忽略不计。</p>
</blockquote>



<h2 id="spark的缺点"><strong>Spark的缺点？</strong></h2>

<ul>
<li><p>由于代码质量问题，Spark长时间运行会<strong>经常出错</strong>，在架构方面，由于大量数据被缓存在RAM中，Java回收垃圾缓慢的情况严重，导致Spark性能不稳定，在复杂场景中SQL的性能甚至不如现有的Map/Reduce。</p></li>
<li><p>不能处理大数据，单独机器处理数据过大，或者由于数据出现问题导致中间结果超过RAM的大小时，常常出现RAM空间不足或无法得出结果。然而，Map/Reduce运算框架可以处理大数据，在这方面，Spark不如Map/Reduce运算框架有效。</p></li>
<li><p>内存断电后会丢失数据，Spark不能用于处理需要长期保存的数据。</p></li>
<li><p>不能支持复杂的SQL统计；目前Spark支持的SQL语法完整程度还不能应用在复杂数据分析中。在可管理性方面，SparkYARN的结合不完善，这就为使用过程中埋下隐忧，容易出现各种难题。</p></li>
</ul>



<h2 id="spark的学习成本"><strong>Spark的学习成本？</strong></h2>

<ul>
<li>官方目前侧重Scala要优于Java与Python，所以建议学习Scala</li>
</ul>

<blockquote>
  <p>学习spark, 推荐用scala. 首先了解rdd和dsstream, 用mllib从头到尾实现一下官网的决策树, 逻辑斯蒂回归, 线性回归, 找到成就感, 在这个过程中你也许会踩到不少坑, 也会学习到一些基础的也是核心的概念, 比如何时需要persist, 何时需要cache, 何时需要广播变量, 全局accumulator, rdd stage划分, 为什么需要避免在class内的函数的rdd里引用外部变量, driver和executor区别。</p>
</blockquote>



<h2 id="spark的官方建议的部署要求维护成本"><strong>Spark的官方建议的部署要求（维护成本）</strong></h2>

<blockquote>
  <p><strong>通常，在内存容量为8GB到数百GB的主机上，Spark都能很好地运行。</strong>在任何情况下，我们都推荐最多只把物理主机上75%的内存分配给Spark；剩下的留给操作系统和缓存。</p>
  
  <p><strong>你需要多少内存取决于你的应用程序</strong>。要确定你的应用程序在某个数据集上执行时需要多少内存，可以在Spark <br>
  RDD中加载一部分数据集，并借用Spark 监控UI（http://:4040）上的存储表格（Storage <br>
  tab）来查看其内存用量。要注意的是内存用量受存储级别和序列化格式的影响极大——解决方法详见Spark调优一文。</p>
  
  <p>请注意，<strong>配置了200GB以上内存的Java虚拟机并不能总是正常工作</strong>。如果你购买的主机配备的内存超过了这个数值，那么可以在每个主机上启动多个worker JVM来解决这个问题。在Spark Standalone集群上，你可以使用配置文件conf/spark-env.sh中的变量SPARK_WORKER_INSTANCES来设置每个节点上的worker数量，用变量SPARK_WORKER_CORES来设置每个worker上分配的CPU核数。</p>
  
  <p>我们的经验表明，当数据加载到内存时，大多数Spark应用程序都将受制于网络。最好的办法就是<strong>使用10Gbps或更高带宽的网络来加快应用的执行速度</strong>。这个办法对于那些分布式reduce应用程序（如group-bys，reduce-bys，及SQL joins）特别有用。在任何一个应用程序中，你都可以从其监控UI（http://:4040）上看到Spark通过网络传输了多少数据量。</p>
</blockquote>

<p><em>综上所述：相比Hadoop，Spark所需的硬件成本<strong>更高</strong>（内存比硬盘贵）</em></p>



<h1 id="2实践部分">2.实践部分</h1>

<p><strong>在Python中应用Spark</strong></p>

<p>环境：</p>

<ul>
<li>JAVA 6+</li>
<li>Python 2.6+</li>
</ul>

<p>1.配置环境变量</p>

<pre><code>SPARK_HOME=/srv/spark
PATH=%SPARK_HOME%/bin
</code></pre>

<p>2.建立一个Spark模板</p>

<pre><code>from pyspark import SparkConf, SparkContext

## Module Constants
APP_NAME = "My Spark Application"

## Closure Functions

## Main functionality

def main(sc):
    pass

if __name__ == "__main__":
    # Configure Spark
    conf = SparkConf().setAppName(APP_NAME)
    conf = conf.setMaster("local[*]")
    sc   = SparkContext(conf=conf)

    # Execute Main functionality
    main(sc)
</code></pre>

<hr>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>