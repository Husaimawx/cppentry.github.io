---
layout:     post
title:      sparkSql入门1
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/xiaoqiang17/article/details/77607180				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>1.  Spark SQL</h1>
<h2>1.1.  Spark SQL概述</h2>
<h3>1.1.1.   什么是Spark SQL  </h3>
<p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。</p>
<h3>1.1.2.   为什么要学习Spark SQL</h3>
<p>我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将SparkSQL转换成RDD，然后提交到集群执行，执行效率非常快！</p>
<p>1.易整合</p>
<p>2.统一的数据访问方式</p>
<p>3.兼容Hive</p>
<p>4.标准的数据连接</p>
<h2>1.2.  DataFrames</h2>
<h3>1.2.1.   什么是DataFrames（1.3）</h3>
<p><span style="color:rgb(51,51,51);">与</span><span style="color:rgb(51,51,51);">RDD</span><span style="color:rgb(51,51,51);">类似，</span><span style="color:rgb(51,51,51);">DataFrame</span><span style="color:rgb(51,51,51);">也是一个分布式数据容器。然而</span><span style="color:rgb(51,51,51);">DataFrame</span><span style="color:rgb(51,51,51);">更像传统数据库的二维表格，除了数据以外，还</span><span style="color:rgb(51,51,51);">记录</span><span style="color:rgb(51,51,51);">数据的结构信息，即</span><span style="color:rgb(51,51,51);">schema</span><span style="color:rgb(51,51,51);">。同时，与</span><span style="color:rgb(51,51,51);">Hive</span><span style="color:rgb(51,51,51);">类似，</span><span style="color:rgb(51,51,51);">DataFrame</span><span style="color:rgb(51,51,51);">也支持嵌套数据类型（</span><span style="color:rgb(51,51,51);">struct</span><span style="color:rgb(51,51,51);">、</span><span style="color:rgb(51,51,51);">array</span><span style="color:rgb(51,51,51);">和</span><span style="color:rgb(51,51,51);">map</span><span style="color:rgb(51,51,51);">）。从</span><span style="color:rgb(51,51,51);">API</span><span style="color:rgb(51,51,51);">易用性的角度上</span><span style="color:rgb(51,51,51);">看，</span><span style="color:rgb(51,51,51);">DataFrame
 API</span><span style="color:rgb(51,51,51);">提供的是一套高层的关系操作，比函数式的</span><span style="color:rgb(51,51,51);">RDD API</span><span style="color:rgb(51,51,51);">要更加友好，门槛更低。由于与</span><span style="color:rgb(51,51,51);">R</span><span style="color:rgb(51,51,51);">和</span><span style="color:rgb(51,51,51);">Pandas</span><span style="color:rgb(51,51,51);">的</span><span style="color:rgb(51,51,51);">DataFrame</span><span style="color:rgb(51,51,51);">类似，</span><span style="color:rgb(51,51,51);">SparkDataFrame</span><span style="color:rgb(51,51,51);">很好地继承了传统单机数据分析的开发体验。</span>
</p>
<h3>1.2.2.   创建DataFrames</h3>
<p>1.在本地创建一个文件，有三列，分别是id、name、age，用空格分隔，然后上传到hdfs上</p>
<p><span style="color:rgb(51,51,51);">hdfs dfs -put person.txt /</span></p>
<p><span style="color:rgb(51,51,51);"> </span></p>
<p>2.在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割</p>
<p><span style="color:rgb(51,51,51);">val lineRDD =sc.textFile("hdfs://node1.xiaoniu.com:9000/person.txt").map(_.split(""))</span></p>
<p><span style="color:rgb(51,51,51);"> </span></p>
<p>3.定义case class（相当于表的schema）</p>
<p><span style="color:rgb(51,51,51);">case class Person(id:Int, name:String, age:Int)</span></p>
<p><span style="color:rgb(51,51,51);"> </span></p>
<p>4.将RDD和case class关联</p>
<p><span style="color:rgb(51,51,51);">val personRDD = lineRDD.map(x =&gt;Person(x(0).toInt, x(1), x(2).toInt))</span></p>
<p><span style="color:rgb(51,51,51);"> </span></p>
<p>5.将RDD转换成DataFrame</p>
<p><span style="color:rgb(51,51,51);">val personDF = personRDD.toDF</span></p>
<p> </p>
<p>6.对DataFrame进行处理</p>
<p><span style="color:rgb(51,51,51);">personDF.show</span></p>
<p><span style="color:rgb(51,51,51);"> </span></p>
<h2>1.3.  DataFrame常用操作</h2>
<h3>1.3.1.   SQLDemo1</h3>
<p>在旧的版本中，使用情况</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object SQLDemo1 {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p> </p>
<p>    val conf = new SparkConf().setAppName("SQLDemo1").setMaster("local[*]")</p>
<p> </p>
<p>    val sc = new SparkContext(conf)</p>
<p> </p>
<p><span style="color:#FF0000;">val sqlContext = new SQLContext(sc)</span></p>
<p><span style="color:#FF0000;">//</span><span style="color:#FF0000;">必须导入隐式</span></p>
<p>    import sqlContext.implicits._</p>
<p>    //准备操作</p>
<p>    //以后从哪里加载数据（先创建RDD，然后管理schema，将RDD转换成DataFrame）</p>
<p>    val lines = sc.parallelize(List("laoduan 99 30", "laozhao 9999 28", "laoyang 99 28", "laoxue 98 26"))</p>
<p>    //RDD -》 DataFrame</p>
<p>    val boyRDD: RDD[Boy] = lines.map(line =&gt; {</p>
<p>      val fields = line.split(" ")</p>
<p>      val n = fields(0)</p>
<p>      val f = fields(1).toDouble</p>
<p>      val a = fields(2).toInt</p>
<p>      Boy(n, f, a)</p>
<p>    })</p>
<p>    val boyDF: DataFrame = boyRDD.toDF</p>
<p>    //将DataFrame注册成一张表，然后调用SQL的语法</p>
<p>    boyDF.registerTempTable("t_boy")</p>
<p>    //书写SQL</p>
<p>    val df1: DataFrame = sqlContext.sql("SELECT * FROM t_boy ORDER BY fv DESC, age ASC")</p>
<p>    //查看结果（触发Action）</p>
<p>    df1.show()</p>
<p>    sc.stop()</p>
<p>  }</p>
<p>}</p>
<p>//创建case class类并序列化</p>
<p>case class Boy(name: String, fv: Double, age: Int) extends Serializable</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.2.   SQLDemo2</h3>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object SQLDemo2 {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val conf = new SparkConf().setAppName("SQLDemo2").setMaster("local[*]")</p>
<p>    val sc = new SparkContext(conf)</p>
<p>//new一个SQLContext</p>
<p>    val sqlContext = new SQLContext(sc)</p>
<p> </p>
<p>    //准备操作</p>
<p>    //以后从哪里加载数据（先创建RDD，然后管理schema，将RDD转换成DataFrame）</p>
<p>    val lines = sc.parallelize(List("laoduan 99 30", "laozhao 9999 28", "laoyang 99 28", "laoxue 98 26"))</p>
<p>    //RDD -》 DataFrame</p>
<p>    val rowRdd: <span style="color:#FF0000;">RDD[Row]</span>= lines.map(line =&gt; {</p>
<p>      val fields = line.split(" ")</p>
<p>      val n = fields(0)</p>
<p>      val f = fields(1).toDouble</p>
<p>      val a = fields(2).toInt</p>
<p>      Row(n, f, a)</p>
<p>    })</p>
<p>    val schema = <span style="color:#FF0000;">StructType</span>(</p>
<p>      List(</p>
<p>        StructField("name", StringType),</p>
<p>        StructField("fv", DoubleType),</p>
<p>        StructField("age", IntegerType)</p>
<p>      )</p>
<p>    )</p>
<p>    //通过createDataFrame将RDD管理schema</p>
<p>    val df:DataFrame = sqlContext.createDataFrame(rowRdd, schema)</p>
<p>    //将DataFrame注册成一张表，然后调用SQL的语法</p>
<p>    df.registerTempTable("t_boy")</p>
<p>    //书写SQL</p>
<p>    val df1: DataFrame = sqlContext.sql("SELECT * FROM t_boy ORDER BY fv DESC, age ASC")</p>
<p>    //查看结果（触发Action）</p>
<p>    df1.show()</p>
<p>    sc.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.3.   SQLDemo3</h3>
<p>sparkSQL新特性，使用DSL的风格写SQL</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object SQLDemo3 {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val conf = new SparkConf().setAppName("SQLDemo3").setMaster("local[*]")</p>
<p>    val sc = new SparkContext(conf)</p>
<p>    val sqlContext = new SQLContext(sc)</p>
<p>    //准备操作</p>
<p>    //以后从哪里加载数据（先创建RDD，然后管理schema，将RDD转换成DataFrame）</p>
<p>    val lines = sc.parallelize(List("laoduan 99 30", "laozhao 9999 28", "laoyang 99 28", "laoxue 98 26"))</p>
<p>    //RDD -》 DataFrame</p>
<p>    val rowRdd: RDD[Row]= lines.map(line =&gt; {</p>
<p>      val fields = line.split(" ")</p>
<p>      val n = fields(0)</p>
<p>      val f = fields(1).toDouble</p>
<p>      val a = fields(2).toInt</p>
<p>      Row(n, f, a)</p>
<p>    })</p>
<p>    val schema = StructType(</p>
<p>      List(</p>
<p>        StructField("name", StringType),</p>
<p>        StructField("fv", DoubleType),</p>
<p>        StructField("age", IntegerType)</p>
<p>      )</p>
<p>    )</p>
<p>    //将RDD管理schema</p>
<p>    val df1:DataFrame = sqlContext.createDataFrame(rowRdd, schema)</p>
<p> </p>
<p><span style="color:#FF0000;">//</span><span style="color:#FF0000;">使用</span><span style="color:#FF0000;">DSL</span><span style="color:#FF0000;">风格的语法，直接使用</span><span style="color:#FF0000;">DataFrame</span><span style="color:#FF0000;">上的方法</span></p>
<p><span style="color:#FF0000;">    //val df2: DataFrame = df1.select("name", "fv")</span></p>
<p><span style="color:#FF0000;">    //val df3: DataFrame = df2.where("fv &gt; 98")</span></p>
<p><span style="color:#FF0000;">    import sqlContext.implicits._</span></p>
<p><span style="color:#FF0000;">    //val df4: DataFrame = df3.orderBy($"fv" desc)</span></p>
<p><span style="color:#FF0000;">    val r = df1.select("name", "fv").where("fv &gt; 98").orderBy($"fv" desc)</span></p>
<p>    r.show()</p>
<p>    sc.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.4.   DataSetWordCount</h3>
<p>sparkSQL使用DSL写Wordcount</p>
<p>在Spark2.0里，想要使用Dataset、DataFrame、SQL，程序执行的入口是SparkSession,,</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object DataSetWordCount {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    //如果在Spark2.0里，想要使用Dataset、DataFrame、SQL，程序执行的入口是SparkSession</p>
<p>    val session: SparkSession = SparkSession.builder()</p>
<p>      .appName("DataSetWordCount")</p>
<p>      .master("local[*]")</p>
<p>      .getOrCreate()</p>
<p>    //指定以后从哪里读取数据</p>
<p>    val lines: Dataset[String] = session.read.textFile(args(0))</p>
<p>    //导入隐式转换</p>
<p>    import session.implicits._</p>
<p>    val words: Dataset[String] = lines.flatMap(_.split(" "))</p>
<p>    import org.apache.spark.sql.functions._</p>
<p>    //val r: DataFrame = words.groupBy($"value" as "word").agg(count("*") as "counts").sort($"counts" desc)</p>
<p>    //第一个count是在组内进行Count， 第二个count是计算dataframe有多少行</p>
<p>    //val r = words.groupBy($"value" as "word").count().count()//5</p>
<p><span style="color:#FF0000;">    val r = words.groupBy($"value" as "word").count().withColumnRenamed("count", "counts").sort($"counts" desc)</span></p>
<p>    r.show()</p>
<p>    //println(r.toBuffer)</p>
<p>    session.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.5.   SQLWordCount</h3>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object SQLWordCount {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val session: SparkSession = SparkSession.builder()</p>
<p>      .appName("SQLWordCount")</p>
<p>      .master("local[*]")</p>
<p>      .getOrCreate()</p>
<p>    //指定以后从哪里读取数据</p>
<p>    val lines: Dataset[String] = session.read.textFile(args(0))</p>
<p>    //导入隐式转换</p>
<p>    import session.implicits._</p>
<p>    val words: Dataset[String] = lines.flatMap(_.split(" "))</p>
<p>    //第二中方式，用SQL的方式</p>
<p>    //创建一个临时视图</p>
<p>    words.createTempView("v_words")</p>
<p>    val r: DataFrame = session.sql("SELECT value word, COUNT(*) counts FROM v_words GROUP BY word ORDER BY counts DESC")</p>
<p>    r.show()</p>
<p>    //println(r.toBuffer)</p>
<p>    session.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.6.   DataSourceDemo1</h3>
<p>文件读取有多种格式，写文件也有多种格式</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object DataSourceDemo1 {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val spark = SparkSession.builder()</p>
<p>      .appName("DataSourceDemo1")</p>
<p>      .master("local[*]")</p>
<p>      .getOrCreate()</p>
<p>    import spark.implicits._</p>
<p>    //指定以后从哪里读取数据（并行化的方式）</p>
<p>    //val lines: DataFrame = spark.read.text(args(0))</p>
<p>    val lines: DataFrame = spark.read.csv(args(0))</p>
<p>    val df = lines.withColumnRenamed("_c0", "name").withColumnRenamed("_c1", "age")</p>
<p>    //val result: Dataset[Row] = lines.select("name", "age").where("age &gt;= 13")</p>
<p>//result.write.json(args(1))</p>
<p>//设置模式进行覆盖重写，也可以追加等多种模式</p>
<p>    //result.write.mode("overwrite").csv(args(1))</p>
<p>    df.write.parquet(args(1))</p>
<p>    //df.show()</p>
<p> </p>
<p>    spark.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.7.   ParquetSource</h3>
<p>将文件读取为Parquet时，运算效果是最好的</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object ParquetSource {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val spark = SparkSession.builder()</p>
<p>      .appName("DataSourceDemo1")</p>
<p>      .master("local[*]")</p>
<p>      .getOrCreate()</p>
<p>    val lines: DataFrame = <span style="color:#FF0000;">spark.read.parquet(args(0))</span></p>
<p>    lines.show()</p>
<p>    spark.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><h3>1.3.8.   JDBCSource</h3>
<p>实现从数据库中读取数据，并写入到数据库中</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object JDBCSource {</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val spark = SparkSession.builder()</p>
<p>      .appName("DataSourceDemo1")</p>
<p>      .master("local[*]")</p>
<p>      .getOrCreate()</p>
<p>//从数据库中读取，先链接数据库</p>
<p>    val logs: DataFrame =<span style="color:#FF0000;"> spark.read.format("jdbc").options(</span></p>
<p><span style="color:#FF0000;">      Map("url" -&gt; "jdbc:mysql://localhost:3306/bigdata",</span></p>
<p><span style="color:#FF0000;">        "driver" -&gt; "com.mysql.jdbc.Driver",</span></p>
<p><span style="color:#FF0000;">        "dbtable" -&gt; "logs",</span></p>
<p><span style="color:#FF0000;">        "user" -&gt; "root",</span></p>
<p><span style="color:#FF0000;">        "password" -&gt; "123568")</span></p>
<p><span style="color:#FF0000;">    ).load()</span></p>
<p> </p>
<p>    val filtered = logs.where("age &gt;= 13")</p>
<p> </p>
<p>    //创建Properties存储数据库相关属性</p>
<p>    val prop = new Properties()</p>
<p>    prop.put("user", "root")</p>
<p>    prop.put("password", "123568")</p>
<p>    filtered.write.mode("append").jdbc("jdbc:mysql://localhost:3306/bigdata", "bigdata.logs", prop)</p>
<p>    //logs.show()</p>
<p> </p>
<p>    spark.stop()</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
<h3>1.3.9.   IPLocation</h3>
<p>找出IP对应的省份，注意注册函数并指定名字，spark.udf.register("ip2Long", ip2Long)</p>
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top">
<p>object IPLocation_SQL {</p>
<p>  val ip2Long = (ip: String) =&gt; {</p>
<p>    val fragments = ip.split("[.]")</p>
<p>    var ipNum = 0L</p>
<p>    for (i &lt;- 0 until fragments.length){</p>
<p>      ipNum =  fragments(i).toLong | ipNum &lt;&lt; 8L</p>
<p>    }</p>
<p>    ipNum</p>
<p>  }</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>    val spark = SparkSession.builder()</p>
<p>      .appName("DataSourceDemo1")</p>
<p>      .master("local[*]")</p>
<p>      .getOrCreate()</p>
<p>//导入隐式转换</p>
<p>    import spark.implicits._</p>
<p>    //先读取IP规则数据</p>
<p>    val ipLines: Dataset[String] = spark.read.textFile(args(0))</p>
<p>    //整理ip规则数据</p>
<p>    val ruleDF: DataFrame = ipLines.map(line =&gt; {</p>
<p>      val fields = line.split("[|]")</p>
<p>      val startNum = fields(2).toLong</p>
<p>      val endNum = fields(3).toLong</p>
<p>      val province = fields(6)</p>
<p>      (startNum, endNum, province)</p>
<p>    }).toDF("start_num", "end_num", "province")</p>
<p> </p>
<p>    //读取并整理访问log数据</p>
<p>    val logLine: Dataset[String] = spark.read.textFile(args(1))</p>
<p>    val ipDF: DataFrame = logLine.map(line =&gt; {</p>
<p>      val fields = line.split("[|]")</p>
<p>      val ip = fields(1)</p>
<p>      ip</p>
<p>    }).toDF("ip")</p>
<p> </p>
<p>    ruleDF.createTempView("v_rules")</p>
<p>    ipDF.createTempView("v_logs")</p>
<p> </p>
<p><span style="color:#FF0000;">    //</span><span style="color:#FF0000;">注册函数并指定名字</span></p>
<p><span style="color:#FF0000;">    spark.udf.register("ip2Long", ip2Long)</span></p>
<p> </p>
<p>    val r = spark.sql("SELECT province FROM v_logs JOIN v_rules ON ip2Long(ip) &gt;= start_num AND ip2Long(ip) &lt;= end_num")</p>
<p> </p>
<p>    r.createTempView("v_temp")</p>
<p>    r.show(10)</p>
<p>  }</p>
<p>}</p>
</td>
</tr></tbody></table><p> </p>
            </div>
                </div>