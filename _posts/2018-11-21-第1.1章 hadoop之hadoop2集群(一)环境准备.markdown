---
layout:     post
title:      第1.1章 hadoop之hadoop2集群(一)环境准备
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/warrah/article/details/52248000				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>此篇文章主要面向hadoop的初学者，选取的hadoop版本是2.6.4，搭建是hadoop完全分布式集群（单节点或伪分布式没有实际价值，所以不考虑）。 <br>
写此文的原因是因为我初学hadoop时，加入多个hadoop的QQ群，每次问hadoop的问题，基本都没有人回复，故将hadoop2.6.4的完成集群配置写出来，供初学者参考。</p>

<p><strong>1.准备3台虚拟机</strong></p>

<table>
<thead>
<tr>
  <th>IP</th>
  <th align="center">hadoop节点</th>
  <th align="right">主机名</th>
</tr>
</thead>
<tbody><tr>
  <td>192.168.5.174</td>
  <td align="center">namenode</td>
  <td align="right">dashuju174</td>
</tr>
<tr>
  <td>192.168.5.172</td>
  <td align="center">datanode</td>
  <td align="right">dashuju172</td>
</tr>
<tr>
  <td>192.168.5.173</td>
  <td align="center">datanode</td>
  <td align="right">dashuju173</td>
</tr>
</tbody></table>


<p><strong>2.配置主机名</strong></p>

<pre><code>在172,、173、174三台主机的/etc/hosts中配置

192.168.5.174   dashuju174
192.168.5.172   dashuju172
192.168.5.173   dashuju173
</code></pre>

<p>检查172、173、174三台主机的/etc/sysconfig/network中HOSTNAME是否正确，如果不正确，修改后需要reboot重启机器</p>

<pre><code>174 的机器配置成HOSTNAME=dashuju174
173 的机器配置成HOSTNAME=dashuju173
172 的机器配置成HOSTNAME=dashuju172
</code></pre>

<p><strong>3.JDK1.7配置</strong></p>

<pre><code>在/etc/profile中配置JDK的环境变量，注意profile的修改需要source /etc/profile进行生效

JAVA_HOME=/usr/java/jdk1.7.0_79
export JAVA_HOME
JRE_HOME=/usr/java/jdk1.7.0_79/jre
export JRE_HOME
CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
export CLASSPATH
PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH:.
export PATH
</code></pre>

<p><strong>4.hadoop用户创建</strong></p>

<pre><code> groupadd hadoop // 创建hadoop用户组
 useradd -g hadoop hadoop// 添加用户，并将hadoop用户添加到hadoop用户组中
 passwd hadoop // 设置hadoop的密码，姑且默认为000000
</code></pre>

<p><strong>5.SSH免登陆</strong> <br>
su - hadoop，进入到hadoop用户</p>

<pre><code>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
</code></pre>

<p>进入 /home/hadoop/.ssh目录</p>

<p>vi authorized_keys,将172,173,174中id_rsa.pub的内容写入到authorized_keys中，例如</p>

<pre><code>ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwb5Tv4HDaBddIbsehSas+UqVHdMxPRs3DkY5cL2t29ohfgG9kqNjOi4mOglSu/gO1qejzPoq8ZTWWbOvRqdNio1vtUiLbxheFw9ip4SwoHfaSxvI9vgq5obk6Hl36pWtdgEJ4crBeYfeGS2tLmuOnGQmvMGYz/UmgiJBaGOYuPZP6AsNSy/OsfYoDJpBXPOt+Lod6ch4EnJBbNMQqZTVImwKo1d5Jw0Oeqnjqxg9ocv9KXoPMZX9yiOHSjldHpf3/sSDxalyJEj7Z3Jii/1iwQiFLBSuCvXzdEuZXMwSo6s3zaXuUu4y1yeL2gcL5CZhS3qay2nBbxKKnrL/NoBhzQ== hadoop@dashuju172
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA2wRC1AqG6mjiXLorZe9z39MUuo0vXwBdO7dHriIOpbU2rxHYMvo4NyYQYw0pItyfTjt0YYe8wDT49Cj/vDsf939hFbN0EfOc648DeC9+RUtGlby03Hii3WD/qKpCQlk6+sxUs99To1+Kw5YwH9uaVhbrR0lwa4URagUVtzHMlb2eh9zSMxWApsWKWfu59BCfYOvKLemkVgCc78OWU1nE6FRpV8+R1a2nKisWWecDgQ/jcLi9YFb2Kyga/0ukZv/iXLIVge+ENT8VfeXkW5AqCBuEgvLH5gVaE3cSqnCBiayrlgcmtEW9w5aD3S40VpypIW3KEzikGsLJ8JUfWZ1bPw== hadoop@dashuju174
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAvR5duVdaeysdVDafp3u41zjDuYoJBOsKGvIFIhfo4nYcqN1MXrwa8yVbP3+rmxy8ylZTtfmoqQF0egOvzZp6Ot/pVnl6LPsBdz/qPZnwzfaPvwfM9F1kLSQAWGJSAnoulr5tyC0OLCWKpXV/aMU1uC/zDSD6A+TXwoVk6Uh/z2SIZJolESj38moFw/jCm1uSC+N9A8S+ynLQqbgWoow67EvgXcR8WEyotnbtbun6oFICCXXaHm1/8I+7ZcbFBJVxlLp/+igAKLk+enajqVszytS/gJ1LYouMnfKiRmn5JP8EEdZ280zX0hgVPHrkxzVo5dS3FgF1Rj5RhbQyCxnVkw== hadoop@dashuju173
</code></pre>

<p>执行命令，将authorized_keys从173上复制到172和174上</p>

<pre><code>scp -P 2222 authorized_keys hadoop@dashuju172:/home/hadoop/.ssh
</code></pre>

<p>执行命令，即可免密码登录了，注意2222因为服务器端口更改了，如果是22端口，则不需要加</p>

<pre><code>ssh -p 2222 hadoop@dashuju172
</code></pre>

<p>注意id_rsa的权限问题</p>



<pre class="prettyprint"><code class=" hljs avrasm">chmod <span class="hljs-number">755</span> ~/<span class="hljs-preprocessor">.ssh</span>/    
chmod <span class="hljs-number">600</span> ~/<span class="hljs-preprocessor">.ssh</span>/id_rsa ~/<span class="hljs-preprocessor">.ssh</span>/id_rsa<span class="hljs-preprocessor">.pub</span>     
chmod <span class="hljs-number">644</span> ~/<span class="hljs-preprocessor">.ssh</span>/known_hosts    
chmod g-w authorized_keys</code></pre>

<p><strong>6.检查GLIBC版本</strong> <br>
    在root用户下执行以下命令，hadoop2.6.4版本依赖glibc2.14，如下图所示则需要对linux中glibc库进行升级</p>

<pre><code>strings /lib64/libc.so.6 |grep GLIBC
</code></pre>

<p><img src="https://img-blog.csdn.net/20160819104721318" alt="这里写图片描述" title=""> <br>
升级命令如下：</p>

<pre><code>wget http://mirror.bjtu.edu.cn/gnu/libc/glibc-2.14.tar.xz
tar xvf glibc-2.14.tar.gz
cd glibc-2.14
mkdir build
cd build
../configure --prefix=/usr/local/glibc-2.14   // 配置glibc并设置当前glibc-2.14安装目录
make -j4
make install
cp /usr/local/glibc-2.14/lib/libc-2.14.so /lib64/libc-2.14.so 
mv /lib64/libc.so.6 /lib64/libc.so.6.bak
LD_PRELOAD=/lib64/libc-2.14.so ln -s /lib64/libc-2.14.so /lib64/libc.so.6
</code></pre>

<p>如果最后一行命令执行出错，可通过 <br>
<code>LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6</code>再改回去 <br>
最后，执行strings /lib64/libc.so.6 |grep GLIBC，查看glibc是否更新 <br>
<img src="https://img-blog.csdn.net/20160819105123213" alt="这里写图片描述" title=""></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>