---
layout:     post
title:      Hadoop 2.7.2 集群搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/u010048823/article/details/51913608				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<blockquote>
  <p>版本信息：Centos7 + Hadoop 2.7.2</p>
</blockquote>

<p>Hadoop + Spark 集群搭建系列文章，建议按顺序参考：</p>

<blockquote>
  <p><a href="http://blog.csdn.net/u010048823/article/details/51931012" rel="nofollow">Hadoop &amp; Spark 集群搭建 理念思想</a></p>
  
  <p><a href="http://blog.csdn.net/u010048823/article/details/51908817" rel="nofollow">Hadoop 2.7.2 集群搭建－预备工作</a></p>
  
  <p><a href="#" rel="nofollow">Hadoop 2.7.2 集群搭建</a> (不用点了，就是本文)</p>
  
  <p><a href="http://blog.csdn.net/u010048823/article/details/51925977" rel="nofollow">Spark 1.6.2 + Hadoop 2.7.2 集群搭建</a></p>
</blockquote>

<hr>

<p>Hadoop 共有3种运行模式，不同模式需要不同的配置：</p>

<ul>
<li>独立（或本地）模式：无需任何守护进程，所有程序都在同一个 JVM 上执行，开发阶段测试和调试 MapReduce 程序很方便</li>
<li>伪公布模式：守护进程运行在本地机器上，模拟小规模的集群</li>
<li>全分布模式：守护进程运行在集群上</li>
</ul>

<blockquote>
  <p>注意：</p>
  
  <p>1.开展以下操作前，请先做好上一篇的<a href="http://blog.csdn.net/u010048823/article/details/51908817" rel="nofollow">预备工作</a></p>
  
  <p>2.开展以下操作时，尽量用集群里各节点相同的用户名 liuyao 来操作，除非需要 root 权限时使用 root</p>
</blockquote>

<hr>



<h1 id="独立模式">独立模式</h1>

<p>独立模式是默认模式，不需要配置，此时 core-site.xml、hdfs-site.xml、mapred-site.xml 和 yarn-site.xml 等配置文件为空。</p>

<hr>



<h1 id="伪分布模式">伪分布模式</h1>

<p>以下几乎参考<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" rel="nofollow">官网 documentation</a></p>



<h2 id="配置xml文件">配置xml文件</h2>



<pre class="prettyprint"><code class=" hljs xml"># etc/hadoop/core-site.xml
<span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>  #配置Hadoop的默认文件系统，默认为HDFS
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>hdfs://localhost:9000<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>  #配置Hadoop临时文件根目录，namenode和datanode分别存放在该目录下的/dfs/name和/dfs/data
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>/home/liuyao/00Hadoop/hadoop-2.7.2/tmp<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> 
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>



<pre class="prettyprint"><code class=" hljs xml"># etc/hadoop/hdfs-site.xml
<span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>  #配置文件系统块复本数量，默认为3，当本地或伪公布模式时，设为1
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>



<h2 id="执行-locally">执行 (Locally)</h2>

<p>1.Format the filesystem</p>



<pre class="prettyprint"><code class=" hljs markdown">[liuyao@master hadoop-2.7.2]$ bin/hdfs namenode -format
16/07/14 19:39:56 INFO namenode.NameNode: STARTUP_MSG: 
/<span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span><span class="hljs-strong">*****</span>
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = localhost/127.0.0.1
……</code></pre>

<blockquote>
  <p>注意：！！只需要对master上的namenode格式化（在{dfs.name.dir}/里生成相应的文件），且只需要一次！！除非把{dfs.name.dir}/里的内容删除了才需要重新格式化。</p>
</blockquote>

<p>2.Start NameNode daemon and DataNode daemon</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]<span class="hljs-preprocessor"># sbin/start-dfs.sh </span>
Starting namenodes on [localhost]
<span class="hljs-label">localhost:</span> starting namenode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-namenode-master<span class="hljs-preprocessor">.out</span>
<span class="hljs-label">localhost:</span> starting datanode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-datanode-master<span class="hljs-preprocessor">.out</span>
Starting secondary namenodes [<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>]
<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>: starting secondarynamenode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-secondarynamenode-master<span class="hljs-preprocessor">.out</span>

[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]<span class="hljs-preprocessor"># jps</span>
<span class="hljs-number">24785</span> DataNode
<span class="hljs-number">25986</span> Jps
<span class="hljs-number">24494</span> NameNode
<span class="hljs-number">25166</span> SecondaryNameNode</code></pre>

<blockquote>
  <p>可以看出，在 Localhost 上创建了 namenode 和 datanode，在0.0.0.0上创建了 secondary namenode。</p>
</blockquote>

<p>3.Browse the web interface for the NameNode</p>

<blockquote>
  <p>By default it is available at: NameNode - <a href="http://localhost:50070/" rel="nofollow">http://localhost:50070/</a></p>
</blockquote>

<p>4.Make the HDFS directories required to execute MapReduce jobs</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hdfs dfs -mkdir /user</span>
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hdfs dfs -mkdir /user/liuyao</span></code></pre>

<blockquote>
  <p>注意：此处是在 HDFS 分布式文件系统上创建文件夹<code>/user</code>和<code>/user/liuyao</code>，用于表示用户目录，注意不是在本地文件系统！</p>
</blockquote>

<p>5.Copy the input files into the distributed filesystem</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hdfs dfs -mkdir /user/liuyao/input</span>
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hdfs dfs -put etc/hadoop/*.xml /user/liuyao/input</span></code></pre>

<blockquote>
  <p>注意：官网上代码报错！需改成上面两行！（可能原因：分布式系统当前目录并没有在 /user/liuyao/ 下，理想情况下应该是在 /user/liuyao/ 下的）</p>
</blockquote>

<p>6.Run some of the examples provided</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment">#  bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'</span>
……</code></pre>

<p>7.Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hdfs dfs -get /user/root/output output</span>
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># cat output/output/*</span>
<span class="hljs-number">1</span>   dfsadmin
<span class="hljs-number">1</span>   dfs.replication</code></pre>

<p>Or view the output files on the distributed filesystem</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hdfs dfs -cat output/*</span>
<span class="hljs-number">1</span>   dfsadmin
<span class="hljs-number">1</span>   dfs.replication</code></pre>

<p>8.When you’re done, stop the daemons with</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]<span class="hljs-preprocessor"># sbin/stop-dfs.sh</span>
Stopping namenodes on [localhost]
<span class="hljs-label">localhost:</span> stopping namenode
<span class="hljs-label">localhost:</span> stopping datanode
Stopping secondary namenodes [<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>]
<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>: stopping secondarynamenode</code></pre>



<h2 id="执行-yarn">执行 (YARN)</h2>

<p>You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.</p>

<p>0.Execute  <strong>1. ~ 5. steps</strong> of the above instructions</p>

<blockquote>
  <p>注意：官网上只写1～4步，其实第5步（在分布式系统上创建 input 并把配置文件复制进去）也需要（可能会提示文件夹已经存在），否则报错！</p>
</blockquote>

<p>1.Configure parameters as follows</p>



<pre class="prettyprint"><code class=" hljs xml"># etc/hadoop/mapred-site.xml
<span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #配置mapreduce框架，默认为classic，这里设为yarn
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>



<pre class="prettyprint"><code class=" hljs xml"># etc/hadoop/yarn-site.xml
<span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>2.Start ResourceManager daemon and NodeManager daemon</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]<span class="hljs-preprocessor"># sbin/start-yarn.sh</span>
starting yarn daemons
starting resourcemanager, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/yarn-liuyao-resourcemanager-master<span class="hljs-preprocessor">.out</span>
<span class="hljs-label">localhost:</span> starting nodemanager, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/yarn-liuyao-nodemanager-master<span class="hljs-preprocessor">.out</span>

<span class="hljs-preprocessor"># 使用jps命令查看当前进程</span>
[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]<span class="hljs-preprocessor"># jps</span>
<span class="hljs-number">10017</span> SecondaryNameNode
<span class="hljs-number">10840</span> NodeManager
<span class="hljs-number">9641</span> DataNode
<span class="hljs-number">10506</span> ResourceManager
<span class="hljs-number">11852</span> Jps
<span class="hljs-number">9325</span> NameNode</code></pre>

<blockquote>
  <p>可以看到，先启动 yarn 守护进程，然后启动 resourcemanager，最后启动 nodemanager。加上 HDFS，一共有 NameNode、DataNode、SecondaryNameNode、ResourceManager 和 NodeManager 五个进程。</p>
</blockquote>

<p>3.Browse the web interface for the ResourceManager</p>

<blockquote>
  <p>By default it is available at: ResourceManager - <a href="http://localhost:8088/" rel="nofollow">http://localhost:8088/</a></p>
</blockquote>

<p>4.Run a MapReduce job</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'</span></code></pre>

<p>5.When you’re done, stop the daemons with</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># sbin/stop-dfs.sh</span>
……
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># sbin/stop-yarn.sh</span>
……</code></pre>

<p>或者使用<code>stop-all.sh</code>命令：</p>



<pre class="prettyprint"><code class=" hljs vbnet">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]<span class="hljs-preprocessor"># sbin/stop-all.sh</span>
This script <span class="hljs-keyword">is</span> Deprecated. Instead use <span class="hljs-keyword">stop</span>-dfs.sh <span class="hljs-keyword">and</span> <span class="hljs-keyword">stop</span>-yarn.sh
Stopping namenodes <span class="hljs-keyword">on</span> [localhost]
localhost: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>]
<span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>: stopping secondarynamenode
stopping yarn daemons
stopping resourcemanager
localhost: stopping nodemanager
localhost: nodemanager did <span class="hljs-keyword">not</span> <span class="hljs-keyword">stop</span> gracefully after <span class="hljs-number">5</span> seconds: killing <span class="hljs-keyword">with</span> kill -<span class="hljs-number">9</span>
no proxyserver <span class="hljs-keyword">to</span> <span class="hljs-keyword">stop</span></code></pre>

<blockquote>
  <p>依次停止namenode、datanode、secondarynamenode、resoucemanager和nodemanager。</p>
</blockquote>

<hr>



<h1 id="全分布模式">全分布模式</h1>

<blockquote>
  <p>基于YARN的全分布模式集群各节点守护进程如下表所示</p>
</blockquote>

<table>
<thead>
<tr>
  <th>节点</th>
  <th>HDFS</th>
  <th>YARN</th>
</tr>
</thead>
<tbody><tr>
  <td>master</td>
  <td>NameNode &amp; SecondaryNameNode</td>
  <td>ResourceManager</td>
</tr>
<tr>
  <td>slave1</td>
  <td>DataNode</td>
  <td>NodeManager</td>
</tr>
<tr>
  <td>slave2</td>
  <td>DataNode</td>
  <td>NodeManager</td>
</tr>
<tr>
  <td>……</td>
  <td>……</td>
  <td>……</td>
</tr>
</tbody></table>




<h2 id="配置相关文件">配置相关文件</h2>

<blockquote>
  <p>注意：以下配置先只在 master 上配置。</p>
</blockquote>

<p>很多配置同伪分布模式，比如：</p>

<ul>
<li>配置 hadoop-env.sh 里的 JAVA_HOME</li>
<li>配置 core-site.xml 里的 fs.defaultFS 和 hadoop.tmp.dir</li>
<li>配置 mapred-site.xml 里的 mapreduce.framework.name</li>
<li>配置 yarn-site.xml 里的 yarn.nodemanager.aux-services</li>
</ul>

<p>不过，在伪分布模式配置基础上，需要添加一些额外的配置，依次配置 yarn-env.sh、slaves、core-site.xml、hdfs-site.xml、mapred-site.xml 和 yarn-site.xml 共6个文件。</p>

<p>1.配置 yarn-env.sh 里的 JAVA_HOME，方法同 hadoop-env.sh</p>

<p>2.配置 slaves 文件，添加作为 slave 的节点IP，添加后的结果可以是以下2种，推荐第1种，因为第1种稳定保险，而第2种需要保证在 /etc/hosts 里成功配置了 slave1 和 slave2 的IP（详情见上一篇文章：<a href="http://blog.csdn.net/u010048823/article/details/51908817" rel="nofollow">预备工作</a>）</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master hadoop]$ cat slaves
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2</code></pre>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop]<span class="hljs-variable">$ </span>cat slaves
slave1
slave2</code></pre>

<p>3.配置 core-site.xml 文件，添加核心配置，配置后结果如下</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>hdfs://master:9000<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #此处不能是localhost！
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span> 
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> 
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>/home/liuyao/00Hadoop/hadoop-2.7.2/tmp<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> 
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>io.file.buffer.size<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>131072<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.proxyuser.spark.hosts<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>hadoop.proxyuser.spark.groups<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>4.配置 hdfs-site.xml 文件，配置后结果如下：</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:9001<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #此处不能是localhost！
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #此处因为只有2个从节点，所以是2?!
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>dfs.webhdfs.enabled<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>5.配置 mapred-site.xml 文件，配置后结果如下：</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:10020<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #此处不能是localhost！
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span> #有没有必要，有些教程里没有
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:19888<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #此处不能是localhost！
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>6.配置 yarn-site.xml 文件，配置后结果如下：</p>



<pre class="prettyprint"><code class=" hljs xml"><span class="hljs-tag">&lt;<span class="hljs-title">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8032<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span> #此处不能是localhost！下同！
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8030<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8035<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8033<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-title">property</span>&gt;</span>
       <span class="hljs-tag">&lt;<span class="hljs-title">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-title">name</span>&gt;</span>
       <span class="hljs-tag">&lt;<span class="hljs-title">value</span>&gt;</span>master:8088<span class="hljs-tag">&lt;/<span class="hljs-title">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-title">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-title">configuration</span>&gt;</span></code></pre>

<p>7.将以上配置好的文件发送到各从节点上，注意尽量让 slave 各节点与 master 节点的目录结构完全一致！</p>



<pre class="prettyprint"><code class=" hljs java">[liuyao<span class="hljs-annotation">@master</span> ~]$ scp -r <span class="hljs-number">00</span>Hadoop/ slave1:~/
……
[liuyao<span class="hljs-annotation">@master</span> ~]$ scp -r <span class="hljs-number">00</span>Hadoop/ slave2:~/
……</code></pre>

<blockquote>
  <p>注意：此时需要检查从节点的 JAVA_HOME 是否需要修改，若需要，则修改 hadoop-env.sh 和 yarn-env.sh 里相应内容。</p>
</blockquote>



<h2 id="启动集群">启动集群</h2>

<p>0.在启动集群前，需要关闭所有节点的防火墙</p>

<blockquote>
  <p>注意：Centos7 里用 firewalld 代替了传统的 iptables</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs perl">[root<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># firewall-cmd --state</span>
running
[root<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># systemctl disable firewalld</span>
Removed <span class="hljs-keyword">symlink</span> /etc/systemd/<span class="hljs-keyword">system</span>/dbus-org.fedoraproject.FirewallD1.service.
Removed <span class="hljs-keyword">symlink</span> /etc/systemd/<span class="hljs-keyword">system</span>/basic.target.wants/firewalld.service.
[root<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># systemctl stop firewalld</span>
[root<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-comment"># exit</span>
<span class="hljs-keyword">exit</span>
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]$ firewall-cmd --<span class="hljs-keyword">state</span>
<span class="hljs-keyword">not</span> running</code></pre>

<p>否则会警告：No Route to Host，若使用传统 iptables，则输入命令<code>service iptables stop</code>即可。</p>

<p>1.格式化 namenode，只需在 master 上操作（且只需格式化一次）</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>hdfs namenode -format</code></pre>

<p>2.启动 HDFS，只需在 master 上操作，各 slave 上相应进程会被激活</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ start-dfs<span class="hljs-preprocessor">.sh</span>
Starting namenodes on [localhost]
<span class="hljs-label">localhost:</span> starting namenode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-namenode-master<span class="hljs-preprocessor">.out</span>
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2: starting datanode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-datanode-slave2<span class="hljs-preprocessor">.out</span>
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1: starting datanode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-datanode-slave1<span class="hljs-preprocessor">.out</span>
Starting secondary namenodes [localhost]
<span class="hljs-label">localhost:</span> starting secondarynamenode, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/hadoop-liuyao-secondarynamenode-master<span class="hljs-preprocessor">.out</span></code></pre>

<blockquote>
  <p>在 master 里使用 jps，可以看到 master 里进程 NameNode 和 SecondaryNameNode 在运行（并没有伪分布模式中有的 DataNode ）</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">26386</span> <span class="hljs-constant">SecondaryNameNode</span>
<span class="hljs-number">25993</span> <span class="hljs-constant">NameNode</span>
<span class="hljs-number">26686</span> <span class="hljs-constant">Jps</span></code></pre>

<blockquote>
  <p>在 slave 里使用 jps，可以看到 slave 里进程 DataNode 被 master 激活</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@slave1</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">6260</span> <span class="hljs-constant">Jps</span>
<span class="hljs-number">5917</span> <span class="hljs-constant">DataNode</span></code></pre>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@slave2</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">5746</span> <span class="hljs-constant">Jps</span>
<span class="hljs-number">5365</span> <span class="hljs-constant">DataNode</span></code></pre>

<p>3.启动 YARN，只需在 master 上操作，各 slave 上相应进程会被激活</p>



<pre class="prettyprint"><code class=" hljs avrasm">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ start-yarn<span class="hljs-preprocessor">.sh</span>
starting yarn daemons
starting resourcemanager, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/yarn-liuyao-resourcemanager-master<span class="hljs-preprocessor">.out</span>
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>2: starting nodemanager, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/yarn-liuyao-nodemanager-slave2<span class="hljs-preprocessor">.out</span>
<span class="hljs-number">101.</span><span class="hljs-built_in">X</span><span class="hljs-preprocessor">.XX</span><span class="hljs-preprocessor">.XX</span>1: starting nodemanager, logging to /home/liuyao/<span class="hljs-number">00</span>Hadoop/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>/logs/yarn-liuyao-nodemanager-slave1<span class="hljs-preprocessor">.out</span></code></pre>

<blockquote>
  <p>在 master 里使用 jps，可以看到 master 里进程 NameNode、SecondaryNameNode 和 ResourceManager 在运行（并没有伪分布模式中有的 DataNode 和 NodeManager ）</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">26386</span> <span class="hljs-constant">SecondaryNameNode</span>
<span class="hljs-number">25993</span> <span class="hljs-constant">NameNode</span>
<span class="hljs-number">27658</span> <span class="hljs-constant">ResourceManager</span>
<span class="hljs-number">27931</span> <span class="hljs-constant">Jps</span></code></pre>

<blockquote>
  <p>在 slave 里使用 jps，可以看到 slave 里进程 DataNode 和 NodeManager 被 master 激活</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@slave1</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">7127</span> <span class="hljs-constant">NodeManager</span>
<span class="hljs-number">5917</span> <span class="hljs-constant">DataNode</span>
<span class="hljs-number">7438</span> <span class="hljs-constant">Jps</span></code></pre>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@slave1</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>jps
<span class="hljs-number">5365</span> <span class="hljs-constant">DataNode</span>
<span class="hljs-number">6616</span> <span class="hljs-constant">NodeManager</span>
<span class="hljs-number">6936</span> <span class="hljs-constant">Jps</span></code></pre>

<p>4.查看集群状态</p>



<pre class="prettyprint"><code class=" hljs vhdl">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ hdfs dfsadmin -<span class="hljs-keyword">report</span>
Configured Capacity: <span class="hljs-number">0</span> (<span class="hljs-number">0</span> B)
Present Capacity: <span class="hljs-number">0</span> (<span class="hljs-number">0</span> B)
DFS Remaining: <span class="hljs-number">0</span> (<span class="hljs-number">0</span> B)
DFS Used: <span class="hljs-number">0</span> (<span class="hljs-number">0</span> B)
DFS Used%: NaN%
Under replicated blocks: <span class="hljs-number">0</span>
Blocks <span class="hljs-keyword">with</span> corrupt replicas: <span class="hljs-number">0</span>
Missing blocks: <span class="hljs-number">0</span>
Missing blocks (<span class="hljs-keyword">with</span> replication factor <span class="hljs-number">1</span>): <span class="hljs-number">0</span>

<span class="hljs-comment">-------------------------------------------------</span></code></pre>

<blockquote>
  <p>查看HDFS： <a href="http://master:50070/" rel="nofollow">http://master:50070/</a> <br>
  查看RM： <a href="http://master:8088/" rel="nofollow">http://master:8088/</a></p>
</blockquote>



<h2 id="运行程序">运行程序</h2>

<blockquote>
  <p>以 Hadoop 里示例 wordcount 举例说明</p>
</blockquote>

<p>1.在本地创建 input 目录，新建2个文件并填写内容</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>mkdir input
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>vi input/file1
<span class="hljs-comment">#添加内容：Hello world bye liuyao</span>
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>vi input/file2
<span class="hljs-comment">#添加内容：Hello Hadoop bye Hadoop</span></code></pre>

<p>2.在 HDFS 上创建 input 目录，把以上2个文件复制进去</p>



<pre class="prettyprint"><code class=" hljs perl">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]$ hdfs dfs -<span class="hljs-keyword">mkdir</span> /user
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]$ hdfs dfs -<span class="hljs-keyword">mkdir</span> /user/liuyao
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]$ hdfs dfs -<span class="hljs-keyword">mkdir</span> /user/liuyao/input
[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]$ hdfs dfs -put input/ <span class="hljs-regexp">/user/liuyao</span><span class="hljs-regexp">/
[liuyao@master hadoop-2.7.2]$ hdfs dfs -ls /user</span><span class="hljs-regexp">/liuyao/input</span><span class="hljs-regexp">/
Found 2 items
-rw-r--r--   2 liuyao supergroup         23 2016-07-16 14:28 /user</span><span class="hljs-regexp">/liuyao/input</span><span class="hljs-regexp">/file1
-rw-r--r--   2 liuyao supergroup         24 2016-07-16 14:28 /user</span><span class="hljs-regexp">/liuyao/input</span><span class="hljs-regexp">/file2</span></code></pre>

<p>3.执行 wordcount 程序</p>



<pre class="prettyprint"><code class=" hljs applescript">[liuyao@master hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-<span class="hljs-number">2.7</span><span class="hljs-number">.2</span>.jar wordcount /user/liuyao/input/ output
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">14</span>:<span class="hljs-number">31</span>:<span class="hljs-number">24</span> INFO client.RMProxy: Connecting <span class="hljs-keyword">to</span> ResourceManager <span class="hljs-keyword">at</span> master/<span class="hljs-number">101.6</span><span class="hljs-number">.86</span><span class="hljs-number">.175</span>:<span class="hljs-number">8032</span>
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">14</span>:<span class="hljs-number">31</span>:<span class="hljs-number">25</span> INFO input.FileInputFormat: Total input paths <span class="hljs-keyword">to</span> process : <span class="hljs-number">2</span>
<span class="hljs-number">16</span>/<span class="hljs-number">07</span>/<span class="hljs-number">16</span> <span class="hljs-number">14</span>:<span class="hljs-number">31</span>:<span class="hljs-number">25</span> INFO mapreduce.JobSubmitter: <span class="hljs-type">number</span> <span class="hljs-keyword">of</span> splits:<span class="hljs-number">2</span></code></pre>

<p>4.查看程序结果</p>



<pre class="prettyprint"><code class=" hljs ruby">[liuyao<span class="hljs-variable">@master</span> hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>]<span class="hljs-variable">$ </span>hdfs dfs -cat /user/liuyao/output/*
<span class="hljs-constant">Hadoop</span>  <span class="hljs-number">2</span>
<span class="hljs-constant">Hello</span>   <span class="hljs-number">2</span>
bye <span class="hljs-number">2</span>
liuyao  <span class="hljs-number">1</span>
world   <span class="hljs-number">1</span></code></pre>

<hr>



<h1 id="常见问题">常见问题</h1>

<p>1.datanode没有启动</p>

<blockquote>
  <p>原因：多次格式化<code>hdfs namenode -format</code>后，master和slave里的clusterID或namespaceID不一致：master的{dfs.name.dir}下VERSION里的clusterID每次格式化都会改变，而slave的{dfs.data.dir}下VERSION里的clusterID保留的只是namenode第一次格式化的clusterID（准确来说，保留的是{dfs.data.dir}文件夹创建时master里的clusterID）。</p>
  
  <p>解决：方法1-停止 Hadoop，删除master里{dfs.name.dir}/（若没设置，则默认是 {dfs.tmp.dir}/dfs/name）文件夹及其内容，删除slave里{dfs.data.dir}/（若没设置，则默认是{dfs.tmp.dir}/dfs/data）文件夹及其内容，再次格式化 namenode，重启 Hadoop。方法2-复制master里的clusterID，slave里相应clusterID为该复制的clusterID。</p>
  
  <p>建议：除特殊需要之外，只需对master的namenode格式化，且只需格式化一次。</p>
</blockquote>

<p>2.namenode没有启动</p>

<blockquote>
  <p>原因：master的{dfs.name.dir}文件夹不存在</p>
  
  <p>解决：停止Hadoop，重新对master的namenode格式化，即可生成{dfs.name.dir}文件夹</p>
</blockquote>

<hr>

<h1 id="参考资料">参考资料</h1>

<blockquote>
  <p><a href="http://www.bkjia.com/Linux/941106.html#top" rel="nofollow">http://www.bkjia.com/Linux/941106.html#top</a></p>
  
  <p><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" rel="nofollow">http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html</a></p>
  
  <p><a href="http://blog.csdn.net/zolalad/article/details/11470449" rel="nofollow">http://blog.csdn.net/zolalad/article/details/11470449</a></p>
</blockquote>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>