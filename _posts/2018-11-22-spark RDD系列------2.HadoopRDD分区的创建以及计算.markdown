---
layout:     post
title:      spark RDD系列------2.HadoopRDD分区的创建以及计算
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>     Spark经常需要从hdfs读取文件生成RDD，然后进行计算分析。这种从hdfs读取文件生成的RDD就是HadoopRDD。那么HadoopRDD的分区是怎么计算出来的？如果从hdfs读取的文件非常大，如何高效的从hdfs加载文件生成HadoopRDD呢？本篇文章探讨这两个问题。</p>
<p>    SparkContext.objectFile方法经常用于从hdfs加载文件，从加载hdfs文件到生成HadoopRDD的函数调用过程为：</p>
<p>SparkContext.objectFile-&gt;<br></p>
<p><span>SparkContext.</span>sequenceFile-&gt;</p>
<p><span>SparkContext.</span>hadoopFile-&gt;</p>
<p><span></span>new HadoopRDD</p>
<p>SparkContext.objectFile方法定义如下：</p>
<p></p>
<pre><code class="language-java">def objectFile[T: ClassTag](
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[T] = withScope {
    assertNotStopped()
    sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions)
      .flatMap(x =&gt; Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader))
  }</code></pre><pre><code class="language-java">def defaultMinPartitions: Int = math.min(defaultParallelism, 2)</code></pre>从以上代码可知如果不设置加载的hdfs文件生成的HadoopRDD的分区个数，默认最小分区个数是2
<p></p>
<p>SparkContext.objectFile方法的minPartitions参数没有改变，一直传递到HadoopRDD的构造函数<br></p>
<p><br></p>
<p>HadoopRDD.getPartitions方法用于创建HadoopRDD的分区。</p>
<p>在本篇文章中，以如下语句加载hdfs文件创建HadoopRDD，创建的HadoopRDD分区个数最小是24个</p>
<p></p>
<pre><code class="language-java">val login1RDD = sc.objectFile[String]("/data/login/1448419800022", 24)</code></pre>
<p></p>
<p><br></p>
hdfs uri为:hdfs://ddos12/
<p>hdfs上/data/login/1448419800022/目录内的文件情况，如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20151126113101189?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p>目录下存在12个文件，12个文件总共10974245字节，如果按照24个分区计算，<strong><span style="color:#cc0000;">平均一个分区是457260字节</span></strong></p>
<p>分区的 创建在HadoopRDD.getPartitions方法：</p>
<p></p>
<pre><code class="language-java">override def getPartitions: Array[Partition] = {
    val jobConf = getJobConf()
    // add the credentials here as this can be called before SparkContext initialized
    SparkHadoopUtil.get.addCredentials(jobConf)
    val inputFormat = getInputFormat(jobConf)
    if (inputFormat.isInstanceOf[Configurable]) {
      inputFormat.asInstanceOf[Configurable].setConf(jobConf)
    }
    /*
    * 确定当前目录下有多少个分区，分区个数最小是minPartitions，以及每个分区多大，每个分区的数据所在的文件，分区数据在文件中的起始位置
    * */
    val inputSplits = inputFormat.getSplits(jobConf, minPartitions)
    val array = new Array[Partition](inputSplits.size)
    for (i &lt;- 0 until inputSplits.size) {
      /*创建分区*/
      array(i) = new HadoopPartition(id, i, inputSplits(i))
    }
    array
  }</code></pre>
<p></p>
<p><br></p>
根据上面的文件，创建的分区大概信息如下：
<p><img src="https://img-blog.csdn.net/20151126140212893?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br></p>
<p><span style="background-color:rgb(255,255,255);"><span style="color:#cc0000;">可见总共创建了25个分区，通过上图可得出结论在切分hdfs目录中的文件的时候，对每个文件按照分区平均长度457260进行切分，每个分区的长度不能大于457260.比如说part-00009文件，切分成了3个分区，最后一个分区的长度只有62107字节</span></span></p>
<p>下图表示了一个index为1分区的详细信息：</p>
<p><img src="https://img-blog.csdn.net/20151126141008216?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br><br><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p>可见里面包含了分区所在的hdfs的文件，以及分区数据在这个文件的起始位置，分区的大小</p>
<p>但是默认情况下，HadoopRDD一个分区最大是128M，假设一个文件大小是1G+10M，但是我设置4个分区，那么在计算分区个数的时候会计算为9个分区，最后一个分区大小为10M。</p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
            </div>
                </div>