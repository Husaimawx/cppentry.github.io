---
layout:     post
title:      Spark及其子项目
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/bingduanlbd/article/details/52144375				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>Apache Spark是一个通用的分布式计算框架，基于内存的特点使得其以高性能著称。Spark应用可以运行在本地模式或者集群模式，集群模式中通过ClusterManager来管理整个应用，目前Spark提供了3中Cluster Manager：Standalone，Mesos，YARN。</p>

<p>除了核心的计算引擎Spark Core以外，Spark还有一系列相关的项目：</p>

<p><img src="http://static.zybuluo.com/BrandonLin/t6r0z0jqg4t9yi3qzihthkmn/image_1apib0mkr1c0bc5m1ol2bgv3q13.png" alt="image_1apib0mkr1c0bc5m1ol2bgv3q13.png-35kB" title=""></p>

<h2 id="spark-core">Spark Core</h2>

<p>Spark Core是Spark的核心组件，其基于核心的RDD抽象，提供了分布式作业分发、调度及丰富的RDD操作，这些操作通过Java、Python、Scala、R语言接口暴露。RDD是分布于集群各节点上的、不可变的弹性数据集，容纳的是Java/Python/Scala/R的对象实例。Spark定义了RDD之上的两种操作：Transformation和Action，RDD上运行一个操作之后，生成另一个RDD或者执行某些动作。广播变量和累积器是Spark提供的另外两个特性。使用Spark来完成单词统计的代码片段如下：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> SparkConf().setAppName(<span class="hljs-string">"wiki_test"</span>)
<span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> SparkContext(conf)
<span class="hljs-keyword">val</span> data = sc.textFile(<span class="hljs-string">"hdfs://master:9000/path/to/data"</span>)
<span class="hljs-keyword">val</span> tokens = data.flatMap( _.spalit(<span class="hljs-string">" "</span>))
<span class="hljs-keyword">val</span> wordFreq = tokens.map(_,<span class="hljs-number">1</span>).reduceByKey(_+_)
wordFreq.sortBy(s=&gt;-s._2).map(x=&gt;(x._2,x._1)).top(<span class="hljs-number">10</span>)</code></pre>



<h2 id="spark-streaming">Spark Streaming</h2>

<p>Spark Streaming基于Spark的计算性能进行流式的分析，将流式数据根据时间切分为小批量的数据（RDD），并在RDD上执行Transformation、Action操作完成分析。这种方式使得现有的很多批处理代码可以直接工作在流式处理的模式下。但是这种模式以牺牲一定的延时为代价，相比于其他基于事件或者消息的流式处理框架（<a href="https://storm.apache.org" rel="nofollow">Storm</a>，<a href="https://samza.apache.org/" rel="nofollow">Samza</a>，<a href="https://flink.apache.org/" rel="nofollow">Flink</a>），延时比较大。Spark Streaming内置支持来自Kafka，Flume，ZeroMQ，Kinesis，Twitter，TCP/IP Socket的数据。</p>



<h2 id="spark-sql">Spark SQL</h2>

<p>Spark SQL引入了称为DataFrames的数据抽象，提供对结构化和半结构化数据操作的支持。Spark提供了一套DSL用于DataFrame的操作，DSL可以通过Scala，Java，Python来表示。同时Spark SQK提供了对标准SQL语言的支持，包括命令行接口和ODBC/JDBC支持（驱动）。下面是使用JDBC的一个代码片段：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-keyword">import</span> org.apache.spark.sql.SQLContext

<span class="hljs-keyword">val</span> url=<span class="hljs-string">"jdbc:mysql://ip:port/test?user=user:password=password"</span>
<span class="hljs-keyword">val</span> sqlContext = <span class="hljs-keyword">new</span> org.apache.spark.sql.SQLContext(sc)

<span class="hljs-keyword">val</span> df = sqlContext
     .read
     .format(<span class="hljs-string">"jdbc"</span>)
     .option(<span class="hljs-string">"url"</span>,url)
     .option(<span class="hljs-string">"dbtable"</span>,<span class="hljs-string">"people"</span>)
     .load()

df.printSchema()
<span class="hljs-keyword">val</span> countsByAge = df.groupBy(<span class="hljs-string">"age"</span>).count()</code></pre>

<p>结构化数据处理和查询</p>



<h2 id="mlib">MLib</h2>

<p>MLib是基于Spark的机器学习框架，由于Spark的分布式内存框架，其性能相比基于磁盘的Mahout有了大幅度的提升。MLib实现了很多常用的机器学习算法：</p>

<ul>
<li>概要统计、相关性、抽样、假设检验等</li>
<li>分类和回归：SVM，逻辑回归，线性回归，决策树，朴素贝叶斯等</li>
<li>协同过滤：包含ALS等实现</li>
<li>聚类分析：K-Means，LDA</li>
<li>降维：SVD奇异值分解，PCA主成分分析等</li>
<li>特征提取与转换函数</li>
<li>优化算法：SGD，L-BFGS等</li>
</ul>

<p>下面是使用MLib中的K-Means进行聚类的例子：</p>



<pre class="prettyprint"><code class=" hljs avrasm">// 导入mllib相关类
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.mllib</span><span class="hljs-preprocessor">.clustering</span>.{KMeans, KMeansModel}
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.mllib</span><span class="hljs-preprocessor">.linalg</span><span class="hljs-preprocessor">.Vectors</span>

// 数据准备
val data = sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"data/mllib/kmeans_data.txt"</span>)
val parsedData = data<span class="hljs-preprocessor">.map</span>(s =&gt; Vectors<span class="hljs-preprocessor">.dense</span>(s<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">' '</span>)<span class="hljs-preprocessor">.map</span>(_<span class="hljs-preprocessor">.toDouble</span>)))<span class="hljs-preprocessor">.cache</span>()

// 聚类
val numClusters = <span class="hljs-number">2</span>
val numIterations = <span class="hljs-number">20</span>
val clusters = KMeans<span class="hljs-preprocessor">.train</span>(parsedData, numClusters, numIterations)

// 评估聚类效果
val WSSSE = clusters<span class="hljs-preprocessor">.computeCost</span>(parsedData)
println(<span class="hljs-string">"Within Set Sum of Squared Errors = "</span> + WSSSE)

// 保存模型
clusters<span class="hljs-preprocessor">.save</span>(sc, <span class="hljs-string">"target/org/apache/spark/KMeansExample/KMeansModel"</span>)
//重新使用保存的模型
val sameModel = KMeansModel<span class="hljs-preprocessor">.load</span>(sc, <span class="hljs-string">"target/org/apache/spark/KMeansExample/KMeansModel"</span>)
</code></pre>



<h2 id="graphx">GraphX</h2>

<p>GraphX则是基于Spark的分布式图处理框架，对标到Hadoop体系下基于MapReduce（因此基于磁盘）的图处理框架<a href="http://giraph.apache.org/" rel="nofollow">Giraph</a>.由于RDD是不可变的，因此GraphX不适合需要更新操作的场景。GraphX提供了两套API，一套类似于Google Pregel提供的API，另一套则更像是MapReduce的风格。</p>

<p>GraphX与Pregel、Giraph都是基于BSP（Bulk Synchronous Parallel）模型，而GraphLab则基于MPI的异步模型。在分布式切割上，GraphX采用的是顶点分割而不是边分割，主要基于现实世界中的图边的数量往往大于定点的数量。顶点维护一个相关边的路由，记录哪些存储边的机器需要使用到该顶点，数据则通过广播分发到对应的边节点上。</p>

<p>GraphX支持基于Neo4J和Titan的图存储。GraphX将属性图模型抽象为一对RDD，封装顶点和边的属性：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Graph</span>[<span class="hljs-title">VD</span>,<span class="hljs-title">ED</span>] {</span>
  <span class="hljs-keyword">val</span> vertices: VertexRDD[VD]
  <span class="hljs-keyword">val</span> edges: EdgeRDD[ED]
}</code></pre>

<p>VertexRDD[VD]扩展自RDD[(VertexID,VD)]并进行了优化，EdgeRDD[ED]则扩展自RDD[Edge[ED]]。假设我们有下面这样的属性图：</p>

<p><img src="http://static.zybuluo.com/BrandonLin/9i2vusw2d3dlovdyeanb1pyy/image_1api6cv8u1psu89tdff13un19pn9.png" alt="image_1api6cv8u1psu89tdff13un19pn9.png-216.8kB" title=""></p>

<p>在GraphX中，声明这个图的方式如下：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-keyword">val</span> userGraph = Graph[(String,String),String]</code></pre>

<p>泛型参数分别代表顶点和边的类型。图的数据除了可以来自原始的文件，也可以来自RDD，GraphX统一通过Graph Builder来完成。下面的代码示例从RDD中构建一个图：</p>



<pre class="prettyprint"><code class=" hljs cpp">val sc: SparkContext

val users: RDD[VertexId,(String,String)] =
    sc.parallelize(Array((<span class="hljs-number">3L</span>,(<span class="hljs-string">"rxin"</span>,<span class="hljs-string">"student"</span>)),((<span class="hljs-number">7L</span>,(<span class="hljs-string">"jgonzal"</span>,<span class="hljs-string">"postdoc"</span>)),(<span class="hljs-number">5L</span>,(<span class="hljs-string">"franklin"</span>,<span class="hljs-string">"prof"</span>)),(<span class="hljs-number">2L</span>,(<span class="hljs-string">"istoica"</span>,<span class="hljs-string">"prof"</span>))))

val relationships : RDD[Edge[Stirng]] =
    sc.parallelize(Array(Edge(<span class="hljs-number">3L</span>,<span class="hljs-number">7L</span>,<span class="hljs-string">"collab"</span>),Edge(<span class="hljs-number">5L</span>,<span class="hljs-number">3L</span>,<span class="hljs-string">"advisor"</span>),Edge(<span class="hljs-number">2L</span>,<span class="hljs-number">5L</span>,<span class="hljs-string">"colleague"</span>),Edge(<span class="hljs-number">5L</span>,<span class="hljs-number">7L</span>,<span class="hljs-string">"pi"</span>)))

val defaultUser = (<span class="hljs-string">"John Doe"</span>,<span class="hljs-string">"Missing"</span>)

val graph = Graph(users,relationships,defaultUser)</code></pre>

<p>基于Graph，可以进行各种操作，例如属性操作，结构操作，关联操作等。可以如下过滤边：</p>



<pre class="prettyprint"><code class=" hljs avrasm">graph<span class="hljs-preprocessor">.edges</span><span class="hljs-preprocessor">.filter</span>( e =&gt; e<span class="hljs-preprocessor">.srcId</span> &gt; e<span class="hljs-preprocessor">.dstId</span>)<span class="hljs-preprocessor">.count</span></code></pre>

<p><img src="http://static.zybuluo.com/BrandonLin/anwxpk7vhkpjihhf8klr9vbc/image_1api86f4m185t1ruue0a4i99pmm.png" alt="image_1api86f4m185t1ruue0a4i99pmm.png-131.1kB" title=""></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>