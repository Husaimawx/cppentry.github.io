---
layout:     post
title:      Spark-02
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>Spark<br>
--------------------<br>
    通用性。</p>

<p><br>
Spark模块<br>
-------------<br>
    Spark Core            //核心库<br>
    Spark SQL            //SQL<br>
    Spark Streaming        //准实时计算。<br>
    Spark MLlib            //机器学习库<br>
    Spark graph            //图计算</p>

<p>Spark集群运行<br>
--------------------<br>
    1.local            //本地模式<br>
    2.standalone    //独立模式<br>
    3.yarn            //yarn模式<br>
    4.mesos            //mesos</p>

<p>start-all.sh<br>
------------------<br>
    start-master.sh    //RPC端口 7077<br>
    start-slave.sh    spark://s201:7077</p>

<p>webui<br>
------------------<br>
    http://s201:8080</p>

<p><br>
添加针对scala文件的编译插件<br>
------------------------------<br>
    &lt;?xml version="1.0" encoding="UTF-8"?&gt;<br>
    &lt;project xmlns="http://maven.apache.org/POM/4.0.0"<br>
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"<br>
             xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;<br>
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</p>

<p>        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;<br>
        &lt;artifactId&gt;SparkDemo1&lt;/artifactId&gt;<br>
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</p>

<p>        &lt;build&gt;<br>
            &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt;<br>
            &lt;plugins&gt;<br>
                &lt;plugin&gt;<br>
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;<br>
                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;<br>
                    &lt;configuration&gt;<br>
                        &lt;source&gt;1.8&lt;/source&gt;<br>
                        &lt;target&gt;1.8&lt;/target&gt;<br>
                    &lt;/configuration&gt;<br>
                &lt;/plugin&gt;<br>
                &lt;plugin&gt;<br>
                    &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;<br>
                    &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;<br>
                    &lt;version&gt;3.2.2&lt;/version&gt;<br>
                    &lt;configuration&gt;<br>
                        &lt;recompileMode&gt;incremental&lt;/recompileMode&gt;<br>
                    &lt;/configuration&gt;<br>
                    &lt;executions&gt;<br>
                        &lt;execution&gt;<br>
                            &lt;goals&gt;<br>
                                &lt;goal&gt;compile&lt;/goal&gt;<br>
                                &lt;goal&gt;testCompile&lt;/goal&gt;<br>
                            &lt;/goals&gt;<br>
                        &lt;/execution&gt;<br>
                    &lt;/executions&gt;<br>
                &lt;/plugin&gt;<br>
            &lt;/plugins&gt;<br>
        &lt;/build&gt;</p>

<p>        &lt;dependencies&gt;<br>
            &lt;dependency&gt;<br>
                &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br>
                &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br>
                &lt;version&gt;2.1.0&lt;/version&gt;<br>
            &lt;/dependency&gt;<br>
        &lt;/dependencies&gt;<br>
    &lt;/project&gt;</p>

<p><br>
C:\Users\Administrator\.m2\repository\net<br>
C:\Users\Administrator\.m2\repository\net\alchim31\maven\...</p>

<p>SparkContext:<br>
------------------<br>
    Spark集群的连接。主要入口点。<br>
    SparkConf = new ();<br>
    conf.setApp("")<br>
    conf.setMaster("local") ;<br>
    sc = new SparkContext(conf);<br>
    //RDD : Resilient distributed dataset,弹性分布式数据集。<br>
    val rdd1 = sc.textFile("d:/scala/test.txt");<br>
    val rdd2 = rdd1.flatMap(line=&gt;line.split(" "));<br>
    val rdd3 = rdd2.map(word=&gt;(word,1));<br>
    val rdd4 = rdd3.reduceByKey(_ + _) ;<br>
    val list = rdd4.collect()<br>
    list.foreach(e=&gt;println(e));</p>

<p><br>
    //<br>
    sc.textFile("d:/scala").flatMap(_.split(" ")).map((_1)).reduceByKey(_ + _).collect().foreach(println)</p>

<p><br>
spark<br>
-------------<br>
    基于hadoop的mr，扩展MR模型高效使用MR模型，内存型集群计算，提高app处理速度。</p>

<p>spark特点<br>
-------------<br>
    速度:在内存中存储中间结果。<br>
    支持多种语言.<br>
    内置了80+的算子.<br>
    高级分析:MR，SQL/ Streamming /mllib / graph</p>

<p>spark模块<br>
    core        //通用执行引擎，提供内存计算和对外部数据集的引用。<br>
    SQL            //构建在core之上，引入新的抽象SchemaRDD，提供了结构化和半结构化支持。</p>

<p>    Streaming    //小批量计算，RDD.</p>

<p>    MLlib        //机器学习库。core在。<br>
    Graphx        //图计算。</p>

<p><br>
RDD:<br>
----------------<br>
    是spark的基本数据结构，是不可变数据集。RDD中的数据集进行逻辑分区，每个分区可以单独在集群节点<br>
    进行计算。可以包含任何java,scala，python和自定义类型。</p>

<p>    RDD是只读的记录分区集合。RDD具有容错机制。</p>

<p>    创建RDD方式，一、并行化一个现有集合。</p>

<p>    hadoop 花费90%时间用户rw。、<br>
    <br>
    内存处理计算。在job间进行数据共享。内存的IO速率高于网络和disk的10 ~ 100之间。</p>

<p>    内部包含5个主要属性<br>
    -----------------------<br>
    1.分区列表<br>
    2.针对每个split的计算函数。<br>
    3.对其他rdd的依赖列表<br>
    4.可选，如果是KeyValueRDD的话，可以带分区类。<br>
    5.可选，首选块位置列表(hdfs block location);</p>

<p>//默认并发度<br>
local.backend.defaultParallelism() = scheduler.conf.getInt("spark.default.parallelism", totalCores)<br>
taskScheduler.defaultParallelism = backend.defaultParallelism()<br>
sc.defaultParallelism =...; taskScheduler.defaultParallelism<br>
defaultMinPartitions = math.min(defaultParallelism, 2)<br>
sc.textFile(path,defaultMinPartitions)            //1,2</p>

<p><br>
RDD变换<br>
------------------<br>
    返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。<br>
    <br>
    map()                                    //对每个元素进行变换，应用变换函数<br>
                                            //(T)=&gt;V</p>

<p><br>
    filter()                                //过滤器,(T)=&gt;Boolean<br>
    flatMap()                                //压扁,T =&gt; TraversableOnce[U]</p>

<p>    mapPartitions()                            //对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。<br>
                                            //Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</p>

<p>    mapPartitionsWithIndex(func)            //同上，(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</p>

<p>    sample(withReplacement, fraction, seed)    //采样返回采样的RDD子集。<br>
                                            //withReplacement 元素是否可以多次采样.<br>
                                            //fraction : 期望采样数量.[0,1]</p>

<p>    union()                                    //类似于mysql union操作。<br>
                                            //select * from persons where id &lt; 10 <br>
                                            //union select * from id persons where id &gt; 29 ;</p>

<p>    intersection                            //交集,提取两个rdd中都含有的元素。<br>
    distinct([numTasks]))                    //去重,去除重复的元素。</p>

<p>    groupByKey()                            //(K,V) =&gt; (K,Iterable&lt;V&gt;)</p>

<p>    reduceByKey(*)                            //按key聚合。 </p>

<p>    aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])<br>
                                            //按照key进行聚合<br>
    key:String U:Int = 0</p>

<p>    sortByKey                                //排序</p>

<p>    join(otherDataset, [numTasks])            //连接,(K,V).join(K,W) =&gt;(K,(V,W)) </p>

<p>    cogroup                                    //协分组<br>
                                            //(K,V).cogroup(K,W) =&gt;(K,(Iterable&lt;V&gt;,Iterable&lt;!-- &lt;W&gt; --&gt;)) <br>
    cartesian(otherDataset)                    //笛卡尔积,RR[T] RDD[U] =&gt; RDD[(T,U)]</p>

<p>    pipe                                    //将rdd的元素传递给脚本或者命令，执行结果返回形成新的RDD<br>
    coalesce(numPartitions)                    //减少分区<br>
    repartition                                //可增可减<br>
    repartitionAndSortWithinPartitions(partitioner)<br>
                                            //再分区并在分区内进行排序</p>

<p><br>
RDD Action<br>
------------------<br>
    collect()                                //收集rdd元素形成数组.<br>
    count()                                    //统计rdd元素的个数<br>
    reduce()                                //聚合,返回一个值。<br>
    first                                    //取出第一个元素take(1)<br>
    take                                    //<br>
    takeSample (withReplacement,num, [seed])<br>
    takeOrdered(n, [ordering])<br>
    <br>
    saveAsTextFile(path)                    //保存到文件<br>
    saveAsSequenceFile(path)                //保存成序列文件</p>

<p>    saveAsObjectFile(path) (Java and Scala)</p>

<p>    countByKey()                            //按照key,统计每个key下value的个数.<br>
    </p>

<p>spark集成hadoop ha<br>
-------------------------<br>
    1.复制core-site.xml + hdfs-site.xml到spark/conf目录下<br>
    2.分发文件到spark所有work节点<br>
    3.启动spark集群<br>
    4.启动spark-shell,连接spark集群上<br>
        $&gt;spark-shell --master spark://s201:7077<br>
        $scala&gt;sc.textFile("hdfs://mycluster/user/centos/test.txt").collect();    <br>
 </p>            </div>
                </div>