---
layout:     post
title:      Spark基础
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/suixinlun/article/details/78462229				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2 id="一spark是一个快速且通用的集群计算平台"><strong>一、Spark是一个快速且通用的集群计算平台。</strong></h2>

<p><strong>Spark是快速的：</strong></p>

<blockquote>
  <p>Spark扩充了流行的MapReduce计算模型；</p>
  
  <p>Spark是基于内存的计算。</p>
</blockquote>

<p><strong>Spark是通用的：</strong></p>

<blockquote>
  <p>Spark的设计容纳了其它分布式系统拥有的功能；</p>
  
  <p>批处理，迭代式计算，交互查询和流处理等。</p>
  
  <p>优点：降低了维护成本。</p>
</blockquote>

<p><strong>Spark是高度开放的：</strong></p>

<blockquote>
  <p>Spark提供了Python、Java、Scala、SQL的API和丰富的内置库。</p>
  
  <p>Spark和其它的大数据工具整合的很好，包括Hadoop，kafka等。</p>
</blockquote>



<h2 id="二spark历史"><strong>二、Spark历史。</strong></h2>

<blockquote>
  <p>诞生于2009年，加州大学伯克利分校RAD实验室的一个研究项目</p>
  
  <p>最初是基于Hadoop MapReduce的</p>
  
  <p>发现MapReduce在迭代式计算和交互式上低效，引入了内存存储</p>
  
  <p>2010年3月份Spark开源</p>
  
  <p>2011年AMP实验室在Spark上开发高级组件，像Spark Streaming</p>
  
  <p>2013年转移到了Apache下，不久便成为顶级项目</p>
</blockquote>



<h2 id="三spark包括多个紧密集成的组件"><strong>三、Spark包括多个紧密集成的组件。</strong></h2>

<blockquote>
  <p>Spark Core：</p>

<pre><code>  包含Spark的基本功能，包含任务调度，内存管理，容错机制等。
  内部定义了RDDs(弹性分布式数据集)。
  提供了很多APIs来创建和操作这些RDDs。
  应用场景，为其他组件提供底层的服务。
</code></pre>
  
  <p>Spark SQL：</p>

<pre><code>  是Spark处理结构化数据的库，就像Hive SQL，Mysql一样。
  应用场景，企业中用来做报表统计。
</code></pre>
  
  <p>Spark Streaming：</p>

<pre><code>  是实时数据流处理组件，类似Storm。
  Spark Streaming提供了API来操作实时流数据。
  应用场景，企业中用来从Kafka接收数据做实时统计。
</code></pre>
  
  <p>MLlib：</p>

<pre><code>  一个包含通用机器学习功能的包，Machine learning lib。
  包含分类、聚类、回归等，还包括模型评估，和数据导入。
  MLlib提供的上面这些方法，都支持集群上的横向扩展。
  应用场景，机器学习。
</code></pre>
  
  <p>Graphx：</p>

<pre><code>  是处理图的库(例如，社交网络图)，并进行图的并行计算。
  像Spark Streaming，Spark SQL一样，它也继承了RDD API。
  它提供了各种图的操作，和常用的图算法，例如PageRank算法。
  应用场景，图计算。
</code></pre>
  
  <p>Cluster Managers：</p>

<pre><code>  就是集群管理，Spark自带一个集群管理是单独调度器。
  常见集群管理包括Hadoop YARN，Apache Mesos
</code></pre>
</blockquote>

<p><strong>紧密集成的优点：</strong></p>

<blockquote>
  <p>Spark底层优化了，基于Spark底层的组件，也得到了相应的优化。</p>
  
  <p>紧密集成，节省了各个组件组合使用时的部署，测试等时间。</p>
  
  <p>向Spark增加新的组件时，其它组件，可立刻享用新组件的功能。</p>
</blockquote>



<h2 id="四spark与hadoop的比较"><strong>四、Spark与Hadoop的比较</strong></h2>

<p><strong>Hadoop应用场景：</strong></p>

<blockquote>
  <p>离线处理</p>
  
  <p>对时效性要求不高(中间数据在硬盘上)</p>
</blockquote>

<p><strong>Spark应用场景</strong></p>

<blockquote>
  <p>时效性要求高的场景(中间数据多在内存)</p>
  
  <p>机器学习等领域</p>
</blockquote>

<p><strong>Doug Cutting的观点：</strong></p>

<blockquote>
  <p>这是生态系统，每个组件都有其作用，各善其职即可</p>
  
  <p>Spark不具有HDFS的存储能力，要借助HDFS等持久化数据。</p>
  
  <p>大数据将会孕育出更多的新技术。</p>
</blockquote>



<h2 id="五spark基础介绍"><strong>五、Spark基础介绍</strong></h2>

<p><strong>Spark的Shell：</strong></p>

<blockquote>
  <p>Spark的shell使你能够处理分布在集群上的数据。</p>
  
  <p>Spark把数据加载到节点的内存中，因此分布式处理可在秒级完成。</p>
  
  <p>快速使迭代式计算，实时查询、分析一般能够在shells中完成。</p>
  
  <p>Spark提供了Python shells和Scala shells。</p>

<pre><code>  Python Shell：
  在bin/pyspark目录中，使用 exit(); 退出pyspark

  Scala Shell：
  在bin/spark-shell目录中
</code></pre>
  
  <p>【注】修改日志级别log4j.rootCategory=WARN,console</p>
</blockquote>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>