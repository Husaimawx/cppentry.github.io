---
layout:     post
title:      Hadoop cluster（第一篇）Hadoop cluster in Local（Standalone） Mode
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/HereIcome/article/details/79495405				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>    按照Hadoop官网start Hadoop cluster ，参考链接http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html</p><p><span style="color:#ff0000;">执行bin/hadoop命令后的显示：</span></p><p><img src="https://img-blog.csdn.net/2018030911454381" alt=""><br></p><p style="text-align:left;"><span style="font-family:SimSun;font-size:16px;color:#ff0000;">然后执行官网的操作验证：</span><img src="https://img-blog.csdn.net/20180309114823823" alt=""></p><p style="text-align:left;">执行如下命令后的回显：</p><pre class="source" style="border:1px solid rgb(153,153,153);font-size:13px;background-color:rgb(255,255,255);">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output 'dfs[a-z.]+'</pre><p></p><p>bin etc  include  input lib  libexec  LICENSE.txt NOTICE.txt  README.txt  sbin share</p><p>[root@localhost hadoop-2.6.5]# <span style="color:#FF0000;">bin/hadoop jarshare/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar grep input output'dfs[a-z]+'</span></p><p>18/03/09 10:16:11 INFOConfiguration.deprecation: session.id is deprecated. Instead, usedfs.metrics.session-id</p><p>18/03/09 10:16:11 INFO jvm.JvmMetrics:Initializing JVM Metrics with processName=JobTracker, sessionId=</p><p>18/03/09 10:16:13 WARN mapreduce.JobResourceUploader:No job jar file set.  User classes maynot be found. See Job or Job#setJar(String).</p><p>18/03/09 10:16:13 INFOinput.FileInputFormat: Total input paths to process : 8</p><p>18/03/09 10:16:13 INFOmapreduce.JobSubmitter: number of splits:8</p><p>18/03/09 10:16:14 INFOmapreduce.JobSubmitter: Submitting tokens for job: job_local1688762170_0001</p><p>18/03/09 10:16:15 INFO mapreduce.Job: Theurl to track the job: http://localhost:8080/</p><p>18/03/09 10:16:15 INFO mapreduce.Job:Running job: job_local1688762170_0001</p><p>18/03/09 10:16:15 INFOmapred.LocalJobRunner: OutputCommitter set in config null</p><p>18/03/09 10:16:15 INFOmapred.LocalJobRunner: OutputCommitter isorg.apache.hadoop.mapreduce.lib.output.FileOutputCommitter</p><p>18/03/09 10:16:15 INFOmapred.LocalJobRunner: Waiting for map tasks</p><p>18/03/09 10:16:15 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_m_000000_0</p><p>18/03/09 10:16:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:15 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/hadoop-policy.xml:0+9683</p><p>18/03/09 10:16:16 INFO mapreduce.Job: Jobjob_local1688762170_0001 running in uber mode : false</p><p>18/03/09 10:16:16 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:16 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:16 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:16 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:16 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:16 INFO mapreduce.Job:  map 0% reduce 0%</p><p>18/03/09 10:16:16 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:16 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:16 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:16 INFO mapred.MapTask:Spilling map output</p><p>18/03/09 10:16:16 INFO mapred.MapTask:bufstart = 0; bufend = 17; bufvoid = 104857600</p><p>18/03/09 10:16:16 INFO mapred.MapTask:kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600</p><p>18/03/09 10:16:16 INFO mapred.MapTask:Finished spill 0</p><p>18/03/09 10:16:16 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000000_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:16 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:16 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000000_0' done.</p><p>18/03/09 10:16:16 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_m_000000_0</p><p>18/03/09 10:16:16 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_m_000001_0</p><p>18/03/09 10:16:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:16 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/kms-site.xml:0+5511</p><p>18/03/09 10:16:17 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:17 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:17 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:17 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:17 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:17 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:17 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:17 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000001_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:17 INFO mapred.LocalJobRunner:map</p><p>18/03/09 10:16:17 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000001_0' done.</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_m_000001_0</p><p>18/03/09 10:16:17 INFO mapred.LocalJobRunner:Starting task: attempt_local1688762170_0001_m_000002_0</p><p>18/03/09 10:16:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/capacity-scheduler.xml:0+4436</p><p>18/03/09 10:16:17 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:17 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:17 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:17 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:17 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:17 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:17 INFO mapred.LocalJobRunner:</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:17 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000002_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:17 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000002_0' done.</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_m_000002_0</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_m_000003_0</p><p>18/03/09 10:16:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/kms-acls.xml:0+3523</p><p>18/03/09 10:16:17 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:17 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:17 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:17 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:17 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:17 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:17 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:17 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000003_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:17 INFO mapred.LocalJobRunner:map</p><p>18/03/09 10:16:17 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000003_0' done.</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_m_000003_0</p><p>18/03/09 10:16:17 INFO mapred.LocalJobRunner:Starting task: attempt_local1688762170_0001_m_000004_0</p><p>18/03/09 10:16:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/hdfs-site.xml:0+775</p><p>18/03/09 10:16:17 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:17 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:17 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:17 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:17 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:17 INFO mapreduce.Job:  map 100% reduce 0%</p><p>18/03/09 10:16:17 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:17 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:17 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000004_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:17 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000004_0' done.</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_m_000004_0</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_m_000005_0</p><p>18/03/09 10:16:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/core-site.xml:0+774</p><p>18/03/09 10:16:17 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:17 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:17 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:17 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:17 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:17 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:17 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:17 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000005_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:17 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000005_0' done.</p><p>18/03/09 10:16:17 INFO mapred.LocalJobRunner:Finishing task: attempt_local1688762170_0001_m_000005_0</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_m_000006_0</p><p>18/03/09 10:16:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Processing split: file:/home/hewy/hadoop/hadoop-2.6.5/input/yarn-site.xml:0+690</p><p>18/03/09 10:16:17 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:17 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:17 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:17 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:17 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:17 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:17 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:17 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000006_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:17 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000006_0' done.</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_m_000006_0</p><p>18/03/09 10:16:17 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_m_000007_0</p><p>18/03/09 10:16:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:17 INFO mapred.MapTask:Processing split:file:/home/hewy/hadoop/hadoop-2.6.5/input/httpfs-site.xml:0+620</p><p>18/03/09 10:16:18 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:18 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:18 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:18 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:18 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:18 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:18 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:18 INFO mapred.Task:Task:attempt_local1688762170_0001_m_000007_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:18 INFO mapred.Task: Task'attempt_local1688762170_0001_m_000007_0' done.</p><p>18/03/09 10:16:18 INFO mapred.LocalJobRunner:Finishing task: attempt_local1688762170_0001_m_000007_0</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: map task executor complete.</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: Waiting for reduce tasks</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: Starting task: attempt_local1688762170_0001_r_000000_0</p><p>18/03/09 10:16:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:18 INFO mapred.ReduceTask:Using ShuffleConsumerPlugin:org.apache.hadoop.mapreduce.task.reduce.Shuffle@573c189a</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: MergerManager: memoryLimit=363285696,maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10,memToMemMergeOutputsThreshold=10</p><p>18/03/09 10:16:18 INFO reduce.EventFetcher:attempt_local1688762170_0001_r_000000_0 Thread started: EventFetcher forfetching Map Completion Events</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of map attempt_local1688762170_0001_m_000003_0decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000003_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;2</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of mapattempt_local1688762170_0001_m_000000_0 decomp: 21 len: 25 to MEMORY</p><p>18/03/09 10:16:18 INFO reduce.InMemoryMapOutput:Read 21 bytes from map-output for attempt_local1688762170_0001_m_000000_0</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       atjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 21,inMemoryMapOutputs.size() -&gt; 2, commitMemory -&gt; 2, usedMemory -&gt;23</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of mapattempt_local1688762170_0001_m_000006_0 decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       atorg.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000006_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 3, commitMemory -&gt; 23, usedMemory -&gt;25</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of mapattempt_local1688762170_0001_m_000005_0 decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       atorg.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       atjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000005_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 4, commitMemory -&gt; 25, usedMemory -&gt;27</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of mapattempt_local1688762170_0001_m_000002_0 decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000002_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 5, commitMemory -&gt; 27, usedMemory -&gt;29</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of map attempt_local1688762170_0001_m_000001_0decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000001_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 6, commitMemory -&gt; 29, usedMemory -&gt;31</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       atjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       atjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of mapattempt_local1688762170_0001_m_000007_0 decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000007_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 7, commitMemory -&gt; 31, usedMemory -&gt;33</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       atorg.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       atjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>        atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       atorg.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 INFO reduce.LocalFetcher:localfetcher#1 about to shuffle output of mapattempt_local1688762170_0001_m_000004_0 decomp: 2 len: 6 to MEMORY</p><p>18/03/09 10:16:18 INFOreduce.InMemoryMapOutput: Read 2 bytes from map-output forattempt_local1688762170_0001_m_000004_0</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2,inMemoryMapOutputs.size() -&gt; 8, commitMemory -&gt; 33, usedMemory -&gt;35</p><p>18/03/09 10:16:18 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>        atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       atorg.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:18 INFO reduce.EventFetcher:EventFetcher is interrupted.. Returning</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: 8 / 8 copied.</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: finalMerge called with 8 in-memory map-outputs and 0on-disk map-outputs</p><p>18/03/09 10:16:18 INFO mapred.Merger:Merging 8 sorted segments</p><p>18/03/09 10:16:18 INFO mapred.Merger: Downto the last merge-pass, with 1 segments left of total size: 10 bytes</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: Merged 8 segments, 35 bytes to disk to satisfy reducememory limit</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: Merging 1 files, 25 bytes from disk</p><p>18/03/09 10:16:18 INFOreduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</p><p>18/03/09 10:16:18 INFO mapred.Merger:Merging 1 sorted segments</p><p>18/03/09 10:16:18 INFO mapred.Merger: Downto the last merge-pass, with 1 segments left of total size: 10 bytes</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: 8 / 8 copied.</p><p>18/03/09 10:16:18 INFOConfiguration.deprecation: mapred.skip.on is deprecated. Instead, usemapreduce.job.skiprecords</p><p>18/03/09 10:16:18 INFO mapred.Task:Task:attempt_local1688762170_0001_r_000000_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: 8 / 8 copied.</p><p>18/03/09 10:16:18 INFO mapred.Task: Taskattempt_local1688762170_0001_r_000000_0 is allowed to commit now</p><p>18/03/09 10:16:18 INFO output.FileOutputCommitter:Saved output of task 'attempt_local1688762170_0001_r_000000_0' tofile:/home/hewy/hadoop/hadoop-2.6.5/grep-temp-744353686/_temporary/0/task_local1688762170_0001_r_000000</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: reduce &gt; reduce</p><p>18/03/09 10:16:18 INFO mapred.Task: Task'attempt_local1688762170_0001_r_000000_0' done.</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: Finishing task: attempt_local1688762170_0001_r_000000_0</p><p>18/03/09 10:16:18 INFOmapred.LocalJobRunner: reduce task executor complete.</p><p>18/03/09 10:16:19 INFO mapreduce.Job:  map 100% reduce 100%</p><p>18/03/09 10:16:19 INFO mapreduce.Job: Jobjob_local1688762170_0001 completed successfully</p><p>18/03/09 10:16:19 INFO mapreduce.Job:Counters: 33</p><p>       File System Counters</p><p>                FILE: Number of bytesread=234656</p><p>                FILE: Number of byteswritten=2301095</p><p>                FILE: Number of readoperations=0</p><p>                FILE: Number of large readoperations=0</p><p>                FILE: Number of writeoperations=0</p><p>       Map-Reduce Framework</p><p>                Map input records=745</p><p>                Map output records=1</p><p>                Map output bytes=17</p><p>                Map output materializedbytes=67</p><p>                Input split bytes=973</p><p>                Combine input records=1</p><p>               Combine output records=1</p><p>                Reduce input groups=1</p><p>                Reduce shuffle bytes=67</p><p>                Reduce input records=1</p><p>                Reduce output records=1</p><p>                Spilled Records=2</p><p>                Shuffled Maps =8</p><p>                Failed Shuffles=0</p><p>                Merged Map outputs=8</p><p>                GC time elapsed (ms)=346</p><p>                CPU time spent (ms)=0</p><p>                Physical memory (bytes)snapshot=0</p><p>                Virtual memory (bytes)snapshot=0</p><p>                Total committed heap usage(bytes)=1417719808</p><p>       Shuffle Errors</p><p>                BAD_ID=0</p><p>                CONNECTION=0</p><p>                IO_ERROR=0</p><p>                WRONG_LENGTH=0</p><p>                WRONG_MAP=0</p><p>                WRONG_REDUCE=0</p><p>       File Input Format Counters </p><p>                Bytes Read=26012</p><p>       File Output Format Counters </p><p>                Bytes Written=123</p><p>18/03/09 10:16:19 INFO jvm.JvmMetrics:Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - alreadyinitialized</p><p>18/03/09 10:16:19 WARNmapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job orJob#setJar(String).</p><p>18/03/09 10:16:20 INFOinput.FileInputFormat: Total input paths to process : 1</p><p>18/03/09 10:16:20 INFO mapreduce.JobSubmitter:number of splits:1</p><p>18/03/09 10:16:20 INFOmapreduce.JobSubmitter: Submitting tokens for job: job_local65761480_0002</p><p>18/03/09 10:16:20 INFO mapreduce.Job: Theurl to track the job: http://localhost:8080/</p><p>18/03/09 10:16:20 INFO mapreduce.Job:Running job: job_local65761480_0002</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: OutputCommitter set in config null</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: OutputCommitter isorg.apache.hadoop.mapreduce.lib.output.FileOutputCommitter</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: Waiting for map tasks</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: Starting task: attempt_local65761480_0002_m_000000_0</p><p>18/03/09 10:16:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:20 INFO mapred.MapTask:Processing split:file:/home/hewy/hadoop/hadoop-2.6.5/grep-temp-744353686/part-r-00000:0+111</p><p>18/03/09 10:16:20 INFO mapred.MapTask:(EQUATOR) 0 kvi 26214396(104857584)</p><p>18/03/09 10:16:20 INFO mapred.MapTask:mapreduce.task.io.sort.mb: 100</p><p>18/03/09 10:16:20 INFO mapred.MapTask: softlimit at 83886080</p><p>18/03/09 10:16:20 INFO mapred.MapTask:bufstart = 0; bufvoid = 104857600</p><p>18/03/09 10:16:20 INFO mapred.MapTask:kvstart = 26214396; length = 6553600</p><p>18/03/09 10:16:20 INFO mapred.MapTask: Mapoutput collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: </p><p>18/03/09 10:16:20 INFO mapred.MapTask:Starting flush of map output</p><p>18/03/09 10:16:20 INFO mapred.MapTask:Spilling map output</p><p>18/03/09 10:16:20 INFO mapred.MapTask:bufstart = 0; bufend = 17; bufvoid = 104857600</p><p>18/03/09 10:16:20 INFO mapred.MapTask:kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600</p><p>18/03/09 10:16:20 INFO mapred.MapTask:Finished spill 0</p><p>18/03/09 10:16:20 INFO mapred.Task:Task:attempt_local65761480_0002_m_000000_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: map</p><p>18/03/09 10:16:20 INFO mapred.Task: Task 'attempt_local65761480_0002_m_000000_0'done.</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: Finishing task: attempt_local65761480_0002_m_000000_0</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: map task executor complete.</p><p>18/03/09 10:16:20 INFO mapred.LocalJobRunner:Waiting for reduce tasks</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: Starting task: attempt_local65761480_0002_r_000000_0</p><p>18/03/09 10:16:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</p><p>18/03/09 10:16:20 INFO mapred.ReduceTask:Using ShuffleConsumerPlugin:org.apache.hadoop.mapreduce.task.reduce.Shuffle@6750b41c</p><p>18/03/09 10:16:20 INFOreduce.MergeManagerImpl: MergerManager: memoryLimit=363285696,maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10,memToMemMergeOutputsThreshold=10</p><p>18/03/09 10:16:20 INFO reduce.EventFetcher:attempt_local65761480_0002_r_000000_0 Thread started: EventFetcher for fetchingMap Completion Events</p><p>18/03/09 10:16:20 INFO reduce.LocalFetcher:localfetcher#2 about to shuffle output of mapattempt_local65761480_0002_m_000000_0 decomp: 21 len: 25 to MEMORY</p><p>18/03/09 10:16:20 INFOreduce.InMemoryMapOutput: Read 21 bytes from map-output forattempt_local65761480_0002_m_000000_0</p><p>18/03/09 10:16:20 INFOreduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 21,inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;21</p><p>18/03/09 10:16:20 WARN io.ReadaheadPool:Failed readahead on ifile</p><p>EBADF: Bad file descriptor</p><p>       at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(NativeMethod)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</p><p>       atorg.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</p><p>       at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)</p><p>       atjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</p><p>       atjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</p><p>       at java.lang.Thread.run(Thread.java:748)</p><p>18/03/09 10:16:20 INFO reduce.EventFetcher:EventFetcher is interrupted.. Returning</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: 1 / 1 copied.</p><p>18/03/09 10:16:20 INFOreduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0on-disk map-outputs</p><p>18/03/09 10:16:20 INFO mapred.Merger:Merging 1 sorted segments</p><p>18/03/09 10:16:20 INFO mapred.Merger: Downto the last merge-pass, with 1 segments left of total size: 11 bytes</p><p>18/03/09 10:16:20 INFOreduce.MergeManagerImpl: Merged 1 segments, 21 bytes to disk to satisfy reducememory limit</p><p>18/03/09 10:16:20 INFOreduce.MergeManagerImpl: Merging 1 files, 25 bytes from disk</p><p>18/03/09 10:16:20 INFOreduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</p><p>18/03/09 10:16:20 INFO mapred.Merger:Merging 1 sorted segments</p><p>18/03/09 10:16:20 INFO mapred.Merger: Downto the last merge-pass, with 1 segments left of total size: 11 bytes</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: 1 / 1 copied.</p><p>18/03/09 10:16:20 INFO mapred.Task:Task:attempt_local65761480_0002_r_000000_0 is done. And is in the process ofcommitting</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: 1 / 1 copied.</p><p>18/03/09 10:16:20 INFO mapred.Task: Taskattempt_local65761480_0002_r_000000_0 is allowed to commit now</p><p>18/03/09 10:16:20 INFOoutput.FileOutputCommitter: Saved output of task'attempt_local65761480_0002_r_000000_0' tofile:/home/hewy/hadoop/hadoop-2.6.5/output/_temporary/0/task_local65761480_0002_r_000000</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: reduce &gt; reduce</p><p>18/03/09 10:16:20 INFO mapred.Task: Task'attempt_local65761480_0002_r_000000_0' done.</p><p>18/03/09 10:16:20 INFOmapred.LocalJobRunner: Finishing task: attempt_local65761480_0002_r_000000_0</p><p>18/03/09 10:16:20 INFO mapred.LocalJobRunner:reduce task executor complete.</p><p>18/03/09 10:16:21 INFO mapreduce.Job: Jobjob_local65761480_0002 running in uber mode : false</p><p>18/03/09 10:16:21 INFO mapreduce.Job:  map 100% reduce 100%</p><p>18/03/09 10:16:21 INFO mapreduce.Job: Jobjob_local65761480_0002 completed successfully</p><p>18/03/09 10:16:21 INFO mapreduce.Job:Counters: 33</p><p>       File System Counters</p><p>                FILE: Number of bytesread=66594</p><p>                FILE: Number of byteswritten=1013122</p><p>                FILE: Number of readoperations=0</p><p>                FILE: Number of large readoperations=0</p><p>                FILE: Number of writeoperations=0</p><p>       Map-Reduce Framework</p><p>                Map input records=1</p><p>                Map output records=1</p><p>                Map output bytes=17</p><p>                Map output materializedbytes=25</p><p>                Input split bytes=133</p><p>                Combine input records=0</p><p>                Combine output records=0</p><p>                Reduce input groups=1</p><p>               Reduce shuffle bytes=25</p><p>                Reduce input records=1</p><p>                Reduce output records=1</p><p>                Spilled Records=2</p><p>                Shuffled Maps =1</p><p>                Failed Shuffles=0</p><p>                Merged Map outputs=1</p><p>               GC time elapsed (ms)=33</p><p>                CPU time spent (ms)=0</p><p>                Physical memory (bytes)snapshot=0</p><p>                Virtual memory (bytes)snapshot=0</p><p>                Total committed heap usage(bytes)=270032896</p><p>       Shuffle Errors</p><p>                BAD_ID=0</p><p>                CONNECTION=0</p><p>                IO_ERROR=0</p><p>                WRONG_LENGTH=0</p><p>                WRONG_MAP=0</p><p>                WRONG_REDUCE=0</p><p>       File Input Format Counters </p><p>                Bytes Read=123</p><p>       File Output Format Counters </p><p>                Bytes Written=23</p><p>[root@localhost hadoop-2.6.5]#</p><p style="text-align:left;">然后执行cat output/*显示：</p><p style="text-align:left;"><img src="https://img-blog.csdn.net/20180309135421187" alt=""><br></p><p style="text-align:left;">只显示上图中圈住的第二行 1  dfsadmin,因为</p><pre class="source" style="background-color:rgb(255,255,255);font-size:13px;border:1px solid rgb(153,153,153);">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output 'dfs[a-z.]+'</pre>的意思是：把input中的所有文件作为输入，在这些文件中查找匹配正则‘dfs[a-z.]*’的词并统计个数。正则‘dfs[a-z.]*’的意思匹配是一dfs开头，后面有n个任意字母的词。而我第一遍测试的时候显示只有一个匹配的词dfsadmin，<span style="color:#ff0000;">删除output目录（最好删除了第一遍测试生成的output文件夹，因为我看其他大神的博客他们第一次测试失败的时候生成了这个文件夹，再次测试又失败，提示这个文件夹已经存在。原文链接http://www.xuebuyuan.com/2563083.html）</span>，讲input中任意文件中添加一个dfshewy(符合正则条件的词)，再次测试就显示上图中的内容：有2个词匹配正则条件。<p></p><p><br></p>            </div>
                </div>