---
layout:     post
title:      Hadoop 2.7.2    linux分布式环境配置（centos 6.8   32bit）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
 <br><br>
///////////////////////////////////////////////<br><br><br>
Hadoop 272 Linux分布式环境：<br><br><br><span></span>1. 创建用户hadoop，<br><span></span>sudo useradd   –d /home/hadoop –m hadoop  –G admin,root<br><span></span>sudo passwd hadoop<br><span></span><br><span></span>主机名设置，<br><span></span>master,slave1,slave2<br><span></span>网络设置<br><span></span>1，IP设置<br><span></span>2.配置 文件路径为;/etc/hosts<br><span></span>
<p></p>
<p class="p1"><span style="font-family:SimSun;font-size:14px;"><strong><span class="s1">10.0.0.110<span></span></span><span class="s2">master.localdomain</span></strong></span></p>
<p class="p2"><span class="s3"><span style="font-family:SimSun;font-size:14px;"><strong>10.0.0.111<span></span>slave1.localdomain</strong></span></span></p>
<p class="p2"><span class="s3"><span style="font-family:SimSun;font-size:14px;"><strong>10.0.0.112<span></span>slave2.localdomain</strong></span></span></p>
<p><span style="color:rgb(51,51,51);font-family:Consolas, 'Bitstream Vera Sans Mono', 'Courier New', Courier, monospace;font-size:13px;letter-spacing:.5px;"><br></span></p>
<p><span style="color:rgb(51,51,51);font-family:Consolas, 'Bitstream Vera Sans Mono', 'Courier New', Courier, monospace;font-size:13px;letter-spacing:.5px;">重启网络生效：service network restart</span></p>
<p><span style="color:rgb(51,51,51);font-family:Consolas, 'Bitstream Vera Sans Mono', 'Courier New', Courier, monospace;font-size:13px;letter-spacing:.5px;"><br></span></p>
<p><br></p>
<span></span>2. OpenSSH, 免密码登陆<br><span></span>(1)     每个结点分别产生公私密钥。同样的方法在剩下的两个结点中如法炮制即可。<br>
  <br><span></span><br><span></span>(2)     单机回环ssh免密码登录测试<br><span></span>(3)     让主结点(master)能通过SSH免密码登录两个子结点（slave）<br><span></span>为了实现这个功能，两个slave结点的公钥文件中必须要包含主结点的公钥信息<br><span></span>将master结点的公钥文件追加至authorized_keys文件中<br><span></span><br><span></span>scp hadoop@192.168.110.130:~/.ssh/id_rsa.pub ~/.ssh/master_rsa.pub<br><span></span>cat master_rsa.pub &gt;&gt; authroized_keys<br><span></span><br><span></span><br><span></span>3. JDK(tar.gz),环境变量<br><span></span>卸载：<br><span></span>rpm -qa|grep java查看<br><span></span>rpm -e --nodeps ？？？<br><span></span><br><span></span><br><span></span>1)./usr/local/java/<br><span></span>
<p>2)<span> </span></p>
<p><span>注意，如果编译hadoop2.6.5，需要jdk7</span></p>
<p><span><br></span></p>
<span></span>export JAVA_HOME=/usr/local/java/jdk1.8.0_102/<br><span></span>export JAVA_BIN=$JAVA_HOME/bin<br><span></span>export JRE_HOME=$JAVA_HOME/jre<br><span></span>export JAVA_LIB=$JAVA_HOME/lib<br><span></span>#export CLASSPATH=.:$JAVA_LIB/tools.jar:$JAVA_LIB/dt.jar<br><span></span>export CLASSPATH=.:$JAVA_LIB<br><span></span>export PATH=$JAVA_BIN:$PATH<br><span></span><br>
 <span> </span><br><span></span><br>
 <span> </span>4. Hadoop设置------本机伪分布式／集群<br>
 <span> </span><br>
 <span> </span>1）目录：<br>
 <span> </span><br>
 <span> </span>~/hadoop<br>
 <span> </span>---hadoop2.7.2<br>
 <span> </span> <br>
 <span> </span>---hadoop-dir<br>
 <span> </span>---hadoop7.7.2-src<br>
 <span> </span>---hadoop-plugin<br>
 <span> </span>#创建目录<br><span></span>cd ~/hadoop/hadoop-dir<br><span></span>mkdir tmp<br><span></span>mkdir -p hdfs/name<br><span></span>mkdir hdfs/data<br><span></span><br>
 <span> </span>2）配置文件：<br>
 <span> </span> <br>
 <span> </span>可能需要在hadoop-env.sh  yarn-env.sh 中设置JAVA_HOME<br>
 <br>
1&gt;  配置hadoop-env.sh ??<br><span></span>export JAVA_HOME=/usr/local/java/jdk1.7.0_80/<br><span></span><br>
2&gt; Core-site.xml配置<br><br><br>
&lt;configuration&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br><span></span>&lt;value&gt;/home/hadoop/hadoop/hadoop-dir/tmp&lt;/value&gt;<br><span></span>&lt;description&gt;A base for other temporary directories.&lt;/description&gt;<br><span></span>&lt;/property&gt;<br><span></span><br><span></span>&lt;property&gt;  <br><span></span>&lt;name&gt;fs.default.name&lt;/name&gt;  <br><span></span>&lt;value&gt;hdfs://master:9000&lt;/value&gt;  <br><span></span>&lt;/property&gt;<span> </span>
<br>
&lt;/configuration&gt;<br><br><br>
3&gt;<span> </span>Hdfs-site.xml配置<br><br><br>
&lt;configuration&gt;<br><span></span>&lt;property&gt;<br><span></span>    &lt;name&gt;dfs.replication&lt;/name&gt;<br><span></span>    &lt;value&gt;2&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span><br><span></span>&lt;property&gt;<br><span></span>    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br><span></span>    &lt;value&gt;/home/hadoop/hadoop/hadoop-dir/hdfs/name&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span><br><span></span>&lt;property&gt;<br><span></span>    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br><span></span>    &lt;value&gt;/home/hadoop/hadoop/hadoop-dir/hdfs/data&lt;/value&gt;<br><span></span>&lt;/property&gt;<br>
&lt;/configuration&gt;<br><br><br>
4&gt; mapred-site.xml文件<br><br><br>
&lt;configuration&gt;<br><span></span>  <br><span></span>&lt;property&gt;<br><span></span>       &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br><span></span>&lt;value&gt;yarn&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;property&gt;<br>
       &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br>
       &lt;value&gt;hdfs://master:9001&lt;/value&gt;<br>
    &lt;/property&gt;<br><span></span> <br>
&lt;/configuration&gt;<br><br><br>
5&gt;<span> </span>yarn-site.xml;<br><br><br>
&lt;configuration&gt;<br><br><br>
&lt;!-- Site specific YARN configuration properties --&gt;<br><br><br><span></span>&lt;property&gt;<span> </span>
<br><span></span>&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br><span></span>&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><br><br><span></span>&lt;property&gt;<br><span></span>        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br><span></span>        &lt;value&gt;master&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><br><br>
&lt;/configuration&gt;<br><br><br>
6&gt; <br>
hadoop--配置masters和slaves文件<br>
在masters文件中填入:<br><span></span>master<br>
在slaves文件中填入：<br><span></span>slave1<br><span></span>slave2<br><span></span><span></span><span></span><br>
 <span> </span>3）环境变量：<br>
 <span> </span>export HADOOP_HOME=/home/hadoop/hadoop/hadoop-2.7.2<br><span></span>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br>
 <span> </span><br>
 <span> </span><br>
 <span> </span><br>
 <span> </span><br>
 <span> </span><br>
 <span> </span>6.0 赋予用户对hadoop安装目录可写的权限 sudo chown -hR 用户 hadoop根目录<br>
 <span> </span>6.1 通过SSH 向各节点复制hadoop<br>
 <span> </span><br>
 <span> </span>7.格式化，---主结点master上进行操作：<br>
 <span> </span>hadoop namenode -format<br>
 <span> </span><br>
 <span> </span><br>
 <span> </span>8.启动，----也在主结点master上进行操作<br>
 <span> </span><br>
 <span> </span><br>
 <span> </span>9.用jps检验各后台进程是否成功启动<br>
 <span> </span><br><span></span>在主结点master上查看namenode,jobtracker,secondarynamenode进程是否启动。<br><span></span>在node1和node2结点了查看tasktracker和datanode进程是否启动。<br><span></span><br><span></span><br>
 <span> </span>10.监测， 运行<br>
 <span> </span><br>
 <span> </span>通过网站查看集群情况<br>
    在浏览器中输入：http://192.168.110.130:50030，网址为master结点所对应的IP：<br>
    在浏览器中输入：http://192.168.110.130:50070，网址为master结点所对应的I<br>
    <br>
    <br>
    <br>
    <br>
============================<br><span></span><br><span></span><br><span></span>1.hadoop编译<br><span></span>1)maven的安装<br><span></span>export MAVEN_HOME=/home/hadoop/hadoop/tools/apache-maven-3.3.9<br><span></span>export PATH=$PATH:$MAVEN_HOME/bin<br><span></span>2)ant安装 yum <br><span></span><br><span></span>3)findbugs<br><span></span>export FINDBUGS_HOME=/home/hadoop/hadoop/tools/findbugs-3.0.1<br><span></span>export PATH=$PATH:$FINDBUGS_HOME/bin<br><span></span><br><span></span>3）protobuf250<br><span></span>源码安装<br><span></span><br><span></span>3):others:<br><span></span><br><span></span>yum install openssl-devel<br><span></span>yum install snappy snappy-devel<br><span></span><br><span></span>4) 检查 cmake  版本<br><span></span><br><span></span>下面开始编译hadoop （-X）<br><span></span>进入到hadoop-2.7.1-src目录<br><span></span>使用命令：<br><span></span>mvn clean package -Pdist,native -DskipTests -Dtar<br><span></span>或者：<br><span></span>
<p>mvn package -Pdist,native -DskipTests -Dtar</p>
<p><br></p>
<p>注意，如果编译hadoop2.6.5,</p>
<p></p>
<p>mvn clean package -Pdist,native -DskipTests -Dtar  -Dmaven.javadoc.skip=true </p>
<p><br></p>
<span></span>进行编译。　　<br><span></span>务必保持网络畅通，<br><span></span><br><span></span><br><span></span>file hadoop/hadoop-2.7.2/lib/native/*<br><span></span>hadoop checknative -a<br><span></span><br><span></span><br><span></span><br><span></span><br><span></span>2.eclipse 插件编译<br><span></span>按照readme,  修改build.xml,  lib命名等<br><span></span><br><span></span><br><span></span><br><span></span><br><span></span>4.本地伪分布式<br><span></span><br><span></span>0)STOP ALL !!!!<br><span></span>check -version<br><span></span>1)bash_profile 修改<br><span></span>export HADOOP_HOME=/home/hadoop/hadoop/hadoop-2.7.2-pseudo<br><span></span><br><span></span>check -version<br><span></span><br><span></span>2）eclipse 修改<br><span></span>hadoop home:<br><span></span>dfs location:<br><span></span><br><span></span>3.插件验证<br><span></span>eclipse 版本很重要，  hadoop272在centos6.x 使用 Eclipse IDE for Java Developers ，版本号Mars <br><span></span>验证可用<br><span></span><br><span></span>5.开发<br><span></span>
            </div>
                </div>