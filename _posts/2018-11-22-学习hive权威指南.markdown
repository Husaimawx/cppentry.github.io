---
layout:     post
title:      学习hive权威指南
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>目录：</p>
<p><strong>ETL介绍</strong><br>
大数据平台架构概述<br>
系统数据流动<br>
hive概述<br>
hive在hadoop生态系统中<br>
hive体系结构<br>
hive安装及使用<br>
hive客户端的基本语句<br>
hive在HDFS文件系统中的结构<br>
修改hive元数据储存的数据库<br>
hive操作命令<br>
hive常用配置<br>
hive常用的Linux命令选项<br>
hive三种表的创建方式<br>
hive外部表<br>
hive临时表<br>
hive分区表<br>
hive桶表<br>
hive分析函数<br>
hive数据的导入和导出<br>
hive常见的hql语句<br>
hive和MapReduce的相关运行参数<br>
hive自定义UDF函数<br>
hiveserver2与jdbc客户端<br>
hive运行模式与虚拟列<br>
【案列一】日志数据文件分析<br>
【案列二】hive shell 自动化加载数据<br>
hive优化<br>
hadoop &amp; hive压缩<br>
hive存储格式<br>
hive总结&amp;应用场景</p>
<h2><a id="ETL_34"></a><strong>一、ETL介绍：</strong></h2>
<p><img src="https://img-blog.csdnimg.cn/20181115114500754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
数据抽取：把不同的数据源数据抓取过来，存到某个地方<br>
　　数据清洗：过滤那些不符合要求的数据或者修正数据之后再进行抽取<br>
　　　　不完整的数据：比如数据里一些应该有的信息缺失，需要补全后再写入数据仓库<br>
　　　　错误的数据：比如字符串数据后面有一个回车操作、日期格式不正确、日期越界等，需要修正之后再抽取<br>
　　　　重复的数据：重复数据记录的所有字段，需要去重<br>
　　数据转换：不一致的数据转换，比如同一个供应商在结算系统的编码是XX0001,而在CRM中编码是YY0001，统一编码 实现有多种方法：<br>
1、借助ETL工具(如Oracle的OWB、SQL Server的DTS、SQL Server的SSIS服务、Informatic等等)实现<br>
　　OWB:Oracle Warehouse Builder<br>
　　DTS:Data Transformation Service<br>
　　SSIS:SQL Server Integration Services<br>
2、SQL方式实现<br>
3、ETL工具和SQL相结合-----》间接引出hive<br>
　　借助工具可以快速的建立起ETL工程，屏蔽了复杂的编码任务，提高了速度，降低了难度，但是缺少灵活性。<br>
　　SQL的方法优点是灵活，提高ETL运行效率，但是编码复杂，对技术要求比较高。<br>
　　第三种是综合了前面二种的优点，会极大地提高ETL的开发速度和效率</p>
<p>**</p>
<h2><a id="_55"></a>二、大数据平台架构概述：</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115114535238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
数据抽取：Canal/Sqoop（主要解决数据库数据接入问题）、还有大量的数据采用Flume解决方案<br>
　　数据存储：HDFS（文件存储）、HBase（KV存储）、Kafka（消息缓存）<br>
　　调度：采用了Yarn的统一调度以及Kubernetes的基于容器的管理和调度的技术<br>
　　计算分析：MR、HIVE、Storm、Spark、Kylin以及深度学习平台比如Caffe、Tensorflow等等<br>
　　应用平台：交互分析sql，多维分析：时间、地域等等，<br>
　　可视化：数据分析tableau，阿里datav、hcharts、echarts<br>
　　数据应用就是指数据的业务</p>
<p>**</p>
<h2><a id="_69"></a>三、系统数据流动：</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115114601191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
四、hive概述：<br>
<img src="https://img-blog.csdnimg.cn/20181115114621144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
由Facebook开源用于解决海量结构化日志的数据统计，后称为Apache Hive为一个开源项目<br>
　　结构化数据：数据类型，字段，value—》hive<br>
　　非结构化数据：比如文本、图片、音频、视频—》会有非关系型数据库存储，或者转换为结构化<br>
　　结构化日志数据：服务器生成的日志数据,会以空格或者指表符分割的数据,比如：apache、nginx等等</p>
<p><img src="https://img-blog.csdnimg.cn/20181115114642585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
Hive 是一个基于 Hadoop 文件系统之上的数据仓库架构，存储用hdfs，计算用mapreduce<br>
Hive 可以理解为一个工具，不存在主从架构,不需要安装在每台服务器上，只需要安装几台就行了<br>
hive还支持类sql语言，它可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能<br>
hive有个默认数据库：derby，默认存储元数据—》后期转换成关系型数据库存储mysql<br>
　　hive的版本：apache-hive-1.2.1 、hive-0.13.1-cdh5.3.6<br>
　　<a href="https://github.com/apache/" rel="nofollow">https://github.com/apache/</a> 主要查看版本的依赖<br>
下载地址：<br>
　　apache的：<a href="http://archive.apache.org/dist/hive/" rel="nofollow">http://archive.apache.org/dist/hive/</a><br>
　　cdh的：<a href="http://archive.cloudera.com/cdh5/cdh/5/" rel="nofollow">http://archive.cloudera.com/cdh5/cdh/5/</a><br>
sql on hadoop的框架：<br>
hive–》披着sql外衣的map-reduce<br>
impala–》查询引擎，适用于交互式的实时处理场景<br>
presto–》分布式的sql查询引擎，适用于实时的数据分析<br>
spark sql<br>
等等。。。。<br>
详细了解sql on hadoop请访问博主的博客：</p>
<p><a href="https://blog.csdn.net/qq_35036995/article/details/80297129" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80297129</a></p>
<h2><a id="HiveHadoop_100"></a><strong>五、Hive在Hadoop生态体系中</strong></h2>
<p><img src="https://img-blog.csdnimg.cn/20181115114731651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
六、hive体系结构：<br>
<img src="https://img-blog.csdnimg.cn/20181115114748416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115114758634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
client：<br>
命令行   -常用<br>
JDBC<br>
metastore元数据：存储在数据库<br>
默认的数据库derby 后期开发改成mysql<br>
元数据：表名，表的所属的数据库，表的拥有者，表的分区信息，表的类型，表数据的存储的位置<br>
cli-》metastore</p>
<pre><code>    TBLS-》DBS-》hdfs的路径
</code></pre>
<p>Hadoop：</p>
<pre><code>使用mapreduce的计算模型
使用hdfs进行存储hive表数据
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20181115115008635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/2018111511501985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
Driver：<br>
解析器：解析的HQL语句<br>
编译器：把HQL翻译成mapreduce代码<br>
优化器：优化</p>
<pre><code>执行器：把代码提交给yarn
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20181115115050752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115115100211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
hive通过client提交的job，Driver经过解析、编译、优化、执行-》最后提交给hadoop的MapReduce处理</p>
<p>**</p>
<h2><a id="Hive_136"></a>七、Hive安装及使用</h2>
<p>**<br>
请访问:<a href="https://blog.csdn.net/qq_35036995/article/details/80249944" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80249944</a></p>
<p>**</p>
<h2><a id="Hive_143"></a>八、Hive客户端的基础语句：</h2>
<p>**<br>
　　1、进入到hive的客户端：bin/hive;<br>
　　2、查看数据库：show databases;<br>
　　3、创建数据库：create database test;<br>
　　4、进入到数据库：use test;<br>
　　5、查看表：show tables;<br>
　　6、数据类型：<br>
　　　　tinyint、smallint、int、bigint -》int<br>
　　　　float、double、date<br>
　　　　string、vachar、char -》string<br>
　　7、create table hive_table(<br>
　　　　　　id int,<br>
　　　　　　name string<br>
　　　　);<br>
　　8、加载数据：<br>
　　　　　　load data local inpath ‘/opt/datas/hive_test.txt’ into table hive_table;<br>
　　　　　　local：指定本地的数据文件存放路径<br>
　　　　　　不加local：指定数据在hdfs的路径<br>
　　9、查询语句：<br>
　　　　select * from hive_table;<br>
　　10、hive的默认数据分隔符是\001,也就是^A ,分割符 " “, “,” ,”\t"等等<br>
　　　　如果说数据的分隔符与表的数据分隔符不一致的话，读取数据为null<br>
　　　　　　按下crtl+v然后再按下crtl+a就会出来^A(\001)<br>
　　　　create table row_table(<br>
　　　　　　id int,<br>
　　　　　　name string<br>
　　　　　　)ROW FORMAT DELIMITED FIELDS TERMINATED BY " ";</p>
<p>load data local inpath ‘/opt/datas/hive_test.txt’ into table row_table;<br>
**</p>
<h2><a id="hivehdfs_176"></a>九、hive在hdfs上的文件结构</h2>
<p>**<br>
　　    数据仓库的位置                数据库目录           表目录          表的数据文件<br>
　　/user/hive/warehouse             /test.db             /row_table       /hive_test.txt	<br>
　　default默认的数据库：指的就是这个/user/hive/warehouse路径<br>
**</p>
<h2><a id="_184"></a>十、修改元数据存储的数据库：</h2>
<p>**<br>
　　1、用bin/hive同时打开多个客户端会报错<br>
　　　　java.sql.SQLException: Another instance of Derby may have already booted the database /opt/modules/apache/hive-1.2.1/metastore_db.<br>
<img src="https://img-blog.csdnimg.cn/20181115124725588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
derby数据库默认只能开启一个客户端，这是一个缺陷，换个数据库存储元数据<br>
　　　　数据库可选有这几种：derby mssql mysql oracle postgres<br>
　　　　一般选择mysql元数据存储<br>
MySQL安装请访问：<a href="https://blog.csdn.net/qq_35036995/article/details/80297000" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80297000</a></p>
<p>hive与mysql集成请访问：<a href="https://blog.csdn.net/qq_35036995/article/details/80297070" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80297070</a></p>
<p>**</p>
<h2><a id="Hive_199"></a>十一、Hive操作命令</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115124748415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115124805466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115124817396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115124827985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
　1、描述表信息<br>
　　　　desc tablename;<br>
　　　　desc extended 表名;<br>
　　　　desc formatted 表名;<br>
　　2、修改表名<br>
　　　　alter table table_oldname rename to new_table_name;<br>
　　3、给表增加一个列<br>
　　　　alter table new_table add columns(age int);<br>
　　　　alter table new_table add columns(sex string comment ‘sex’);添加注释<br>
　　4、修改列的名字以及类型<br>
　　　　create table test_change(a int,b int,c int);<br>
　　　　修改列名 a -&gt; a1<br>
　　　　alter table test_change change a a1 int;<br>
　　　　a1改a2，数据类型改成String，并且放在b的后面；<br>
　　　　alter table test_change change a1 a2 string after b int;<br>
　　　　将c改成c1，并放在第一列<br>
　　　　alter table test_change change c c1 int first;<br>
　　5、替换列（不能删除列，但是可以修改和替换，）是全表替换<br>
　　　　ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …)<br>
　　　　alter table test_change replace columns(foo int , too string);<br>
<img src="https://img-blog.csdnimg.cn/20181115124849871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
6、清除表数据truncate<br>
　　　　只清除表数据，元数据信息还是存在的，表的结构已经表还是在的</p>
<p>truncate table row_table;<br>
　　　　<img src="https://img-blog.csdnimg.cn/2018111512490631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
　　　　7、删除表数据drop<br>
　　　　drop table row_table;<br>
　　　　清除数据，表以及表的结构清除，元数据也清除<br>
　　8、删除数据库<br>
　　　　drop database test_db CASCADE;<br>
　　　删除数据库的信息，如果数据库不为空的话，则要加CASCADE字段<br>
　　9、查看hive自带的函数： show functions;<br>
　　　　desc function when;<br>
**</p>
<h2><a id="hive_242"></a>十二、hive的常用配置</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115124932319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
1、hive的日志文件log4j：默认是在<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>j</mi><mi>a</mi><mi>v</mi><mi>a</mi><mi mathvariant="normal">.</mi><mi>i</mi><mi>o</mi><mi mathvariant="normal">.</mi><mi>t</mi><mi>m</mi><mi>p</mi><mi>d</mi><mi>i</mi><mi>r</mi></mrow><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">{java.io.tmpdir}/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.05724em;">j</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right: 0.03588em;">v</span><span class="mord mathit">a</span><span class="mord">.</span><span class="mord mathit">i</span><span class="mord mathit">o</span><span class="mord">.</span><span class="mord mathit">t</span><span class="mord mathit">m</span><span class="mord mathit">p</span><span class="mord mathit">d</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span></span><span class="mord">/</span></span></span></span></span>{<a href="http://user.name" rel="nofollow">user.name</a>}也就是/tmp/hadoop/hive.log<br>
　　　　修改 hive-log4j.properties.template 修改为hive-log4j.properties<br>
　　　　修改 hive.log.dir=/opt/modules/apache/hive-1.2.1/logs<br>
　　2、显示数据库和列名，添加配置信息到hive-site.xml<br>
　　　　<br>
　　　　　　hive.cli.print.header<br>
　　　　　　true<br>
　　　　<br>
　　　　<br>
　　　　　　hive.cli.print.current.db<br>
　　　　　　true<br>
　　　　</p>
<p>如下图：<br>
<img src="https://img-blog.csdnimg.cn/20181115124952912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
**</p>
<h2><a id="hivelinux_263"></a>十三、hive常用的linux命令选项</h2>
<p>**<br>
　　查看帮助信息 bin/hive -help<br>
　　　　1、–database指定登陆到哪个database 下面去<br>
　　　　　　bin/hive --database mydb;<br>
　　　　2、指定一条sql语句，必须用引号包裹<br>
　　　　　　bin/hive -e ‘show databses’<br>
　　　　　　bin/hive -e ‘select * from mydb.new_table’<br>
　　　　3、指定写sql语句的文件，执行sql<br>
　　　　　　bin/hive -f hivesql<br>
　　　　　　指定一些较为的sql语句，周期性的执行<br>
　　　　4、查看sql语句文件<br>
　　　　　　bin/hive -i hivesql<br>
　　　　　　执行文件的sql语句并进入到hive的客户端<br>
　　　　　　用来初始化一些操作<br>
　　　　5、bin/hive -S hivesql<br>
　　　　　　静默模式<br>
　　　　6、在当前回话窗口修改参数的属性，临时生效<br>
　　　　　　bin/hive --hiveconf hive.cli.print.header=false;<br>
　　　　7、在hive的客户端中使用set修改参数属性(临时生效),以及查看参数的属性<br>
　　　　　　set hive.cli.print.header -》查看参数的属性<br>
　　　　　　set hive.cli.print.header=true; -》修改参数属性<br>
　　　　8、常用的shell : ! 和 dfs<br>
　　　　　　-》！ 表示访问的linux本地的文件系统 -&gt;! ls /opt/modules/apache/<br>
　　　　　　-》dfs表示访问的是hdfs的文件系统 -&gt; dfs -ls /;<br>
　　　　9、CREATE database_name[LOCATION hdfs_path]<br>
　　　　　　create database hive_test LOCATION “/location”;<br>
　　　　　　自定义数据库在hdfs上的路径，把指定/location当成默认的数据库，<br>
　　　　　　所以这边数据库的名字不显示</p>
<p>**</p>
<h2><a id="hive_296"></a>十四、hive三种表的创建方式</h2>
<p>**<br>
1、【普通的创建】<br>
create table stu_info(<br>
num int,<br>
name string<br>
)<br>
row format delimited fields terminated by " ";<br>
加载数据到本地：将本地的数据复制到表对应的位置</p>
<p>load data local inpath ‘/opt/datas/test.txt’ into table stu_info; 注意:’/opt/datas/test.txt’ 本地数据目录位置</p>
<p>加载hdfs数据：将hdfs上的数据移动到表对应的位置<br>
load data inpath ‘/table_stu.txt’ into table stu_info; 注意:’/table_stu.txt’  本地数据目录位置<br>
<img src="https://img-blog.csdnimg.cn/2018111512501394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
2、【子查询方式： as select】<br>
[AS select_statement];<br>
create table stu_as as select name from stu_info;<br>
将查询的数据和表的结构赋予一张新的表<br>
类似于保存一个中间结果集<br>
<img src="https://img-blog.csdnimg.cn/20181115125038448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
3、【like方式】<br>
LIKE existing_table_or_view_name<br>
create table stu_like  like  stu_info;<br>
复制表的结构赋予一张新的表</p>
<p>接下来创建两张表（为学习其他表建立基本表）：<br>
<img src="https://img-blog.csdnimg.cn/20181115125053642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
【databases】</p>
<p>create database db_emp;<br>
【员工表】<br>
create table emp(<br>
empno int comment ‘员工编号’,<br>
ename string comment ‘员工姓名’,<br>
job string comment ‘员工职位’,<br>
mgr int comment ‘领导编号’,<br>
hiredate string comment ‘入职时间’,<br>
sal double comment ‘薪资’,<br>
comm double comment ‘奖金’,<br>
deptno int comment ‘部门编号’<br>
)<br>
row format delimited fields terminated by ‘\t’;<br>
load data local inpath ‘/opt/datas/emp.txt’ into table emp;注意: '/opt/datas/emp.txt’本地数据目录位置<br>
【部门表】<br>
create table dept(<br>
deptno int comment ‘部门编号’,<br>
dname string comment ‘部门名称’,<br>
loc string comment ‘地址’<br>
)<br>
row format delimited fields terminated by ‘\t’;<br>
load data local inpath ‘/opt/datas/dept.txt’ into table dept;<br>
【覆盖表的数据overwrite】内部机制有删除的操作，删除原来的数据加载表中</p>
<p>load data local inpath ‘/opt/datas/dept.txt’ overwrite  into table dept;注意: ‘/opt/datas/dept.txt’ 本地数据目录位置</p>
<p>**</p>
<h2><a id="hive_355"></a>十五、hive外部表</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125112747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125128695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
关键词 [EXTERNAL]<br>
[LOCATION hdfs_path]共享数据：去加载hdfs上所属路径下的数据<br>
create table emp_part(<br>
empno int comment ‘员工编号’,<br>
ename string comment ‘员工姓名’,<br>
job string comment ‘员工职位’,<br>
mgr int comment ‘领导编号’,<br>
hiredate string comment ‘入职时间’,<br>
sal double comment ‘薪资’,<br>
comm double comment ‘奖金’,<br>
deptno int comment ‘部门编号’<br>
)<br>
row format delimited fields terminated by ‘\t’</p>
<p>LOCATION ‘/user/hive/warehouse/emp_db.db/emp’;</p>
<p>如果你改变emp这张表的数据，那么emp1也会发生改变<br>
如果你改变emp1这张表的数据，那么emp也会发生改变</p>
<p>删除表<br>
drop table emp1;<br>
-》数据共用一份数据，结果就把共享的数据删除了<br>
-》删除表的时候会删除表对应的元数据信息（emp）<br>
-》清除表对应的hdfs上的文件夹</p>
<p>创建外部表<br>
create EXTERNAL table dept_ext(<br>
deptno int comment ‘部门编号’,<br>
dname string comment ‘部门名称’,<br>
loc string comment ‘地址’<br>
)<br>
row format delimited fields terminated by ‘\t’</p>
<p>LOCATION ‘/user/hive/warehouse/db_emp.db/dept’;</p>
<p>表的类型Table Type: EXTERNAL_TABLE</p>
<p>删除表：drop table dept_ext;<br>
-》外部表只是删除元数据（dept_ext）<br>
-》不会删除对应的文件夹</p>
<p>-》一般先创建内部表，然后根据需求创建多张外部表<br>
-》外部表主要是数据安全性的作用</p>
<p>hive内部表和外部表的区别：<br>
1）创建表的时候：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何的改变</p>
<p>2）删除表时：在删除表的时候，内部表的元数据和数据一起删除，而外部表只删除元数据，不删除数据。这样外部表相对于内部表来说更加安全一些，数据组织也更加灵活，方便数据共享</p>
<p>**</p>
<h2><a id="hive_411"></a>十六、hive临时表</h2>
<p>**<br>
create  table dept_tmp(<br>
deptno int,<br>
dname string,<br>
loc string<br>
)<br>
row format delimited fields terminated by ‘\t’;<br>
load data local inpath ‘/opt/datas/dept.txt’ into table dept_tmp;<br>
数据存放路径location：<br>
Location:hdfs://hadoop01.com:8020/tmp/hive/hadoop/23a93177-f22f-4035-a2e3-c51cf315625f/_tmp_space.db/962463c2-6563-47a8-9e62-2a1e8eb6ed19<br>
关闭hive cli：<br>
自动删除临时表<br>
也可以手动删除drop<br>
CREATE TEMPORARY  TABLE IF NOT EXISTS dept_tmp(<br>
deptno int ,<br>
dname string ,<br>
loc string<br>
)row format delimited fields terminated by ‘\t’;<br>
LOCATION ‘/user/hive/warehouse/db_emp.db/dept’;<br>
关闭hive cli：<br>
自动删除临时表的数据</p>
<p>也可以手动删除drop，删除临时表的数据及数据文件</p>
<p>**</p>
<h2><a id="hive_439"></a>十七、hive分区表</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125152145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125202399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125213605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125225150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
企业中如何使用分区表<br>
<img src="https://img-blog.csdnimg.cn/20181115125307147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
[PARTITIONED BY (col_name data_type [COMMENT col_comment], …)]<br>
普通表：select * from logs where <code>date</code>=‘2018120’<br>
执行流程：对全表的数据进行查询，然后才会进行过滤<br>
分区表：select * from logs where <code>date</code>=‘2018120’ and hour=‘00’<br>
执行流程：直接加载对应文件夹路径下的数据<br>
分区表的字段是逻辑性的，体现在hdfs上形成一个文件夹存在，并不在数据中，</p>
<p>必须不能是数据中包含的字段</p>
<p>【一级分区】<br>
create table emp_part(<br>
empno int ,<br>
ename string ,<br>
job string ,<br>
mgr int ,<br>
hiredate string,<br>
sal double ,<br>
comm double ,<br>
deptno int<br>
)PARTITIONED BY(<code>date</code> string)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>load data local inpath ‘/opt/datas/emp.txt’ into table emp_part partition(<code>date</code>=‘2018120’);<br>
load data local inpath ‘/opt/datas/emp.txt’ into table emp_part partition(<code>date</code>=‘2018121’);<br>
load data local inpath ‘/opt/datas/emp.txt’ into table emp_part partition(<code>date</code>=‘2018122’);</p>
<p>select * from emp_part where <code>date</code>=‘2018120’;</p>
<p>【二级分区】<br>
create table emp_part2(<br>
empno int ,<br>
ename string ,<br>
job string ,<br>
mgr int ,<br>
hiredate string,<br>
sal double ,<br>
comm double ,<br>
deptno int<br>
)<br>
PARTITIONED BY(<code>date</code> string,hour string)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>load data local inpath ‘/opt/datas/emp.txt’ into table emp_part2 partition(<code>date</code>=‘2018120’,hour=‘01’);<br>
load data local inpath ‘/opt/datas/emp.txt’ into table emp_part2 partition(<code>date</code>=‘2018120’,hour=‘02’);</p>
<p>select * from emp_part2 where <code>date</code>=‘2018120’;<br>
select * from emp_part2 where <code>date</code>=‘2018120’ and hour=‘01’;</p>
<p>-》分区表的作用主要是提高了查询的效率</p>
<p>**</p>
<h2><a id="hive_501"></a>十八、hive桶表</h2>
<p>**<br>
桶表：获取更高的处理效率、join、抽样数据</p>
<p>[CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS]</p>
<p>create table emp_bu(<br>
empno int ,<br>
ename string ,<br>
job string ,<br>
mgr int ,<br>
hiredate string,<br>
sal double ,<br>
comm double ,<br>
deptno int<br>
)CLUSTERED BY (empno) INTO 4 BUCKETS<br>
row format delimited fields terminated by ‘\t’;<br>
先建表，然后设置<br>
set hive.enforce.bucketing = true;<br>
insert overwrite table emp_bu select * from emp;</p>
<p>**</p>
<h2><a id="hive_525"></a>十九、hive分析函数</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125333109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125342816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" rel="nofollow">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a><br>
分析函数over：分析函数用于计算基于组的某种聚合值，它和聚合函数不同之处是对于每个组返回多行，而聚合函数对于每个组返回一行数据。<br>
主要作用：对于分组后的数据进行处理，然后输出处理后的结果</p>
<p>需求1: 查询部门编号为10的所有员工，按照薪资进行降序排序desc（默认升序）<br>
select * from emp where deptno=10 order by sal desc;<br>
结果:<br>
7839    KING    PRESIDENT       NULL    1981-11-17      5000.0  NULL    10<br>
7782    CLARK   MANAGER 7839    1981-6-9        2450.0  NULL    10<br>
7934    MILLER  CLERK   7782    1982-1-23       1300.0  NULL    10</p>
<p>需求2：将每个部门薪资最高的那个人的薪资显示在最后一列<br>
select empno,ename, deptno,sal, max(sal) over(partition by deptno order by sal desc ) as sal_111 from emp;<br>
结果：<br>
7839    KING    10      5000.0  5000.0 1<br>
7782    CLARK   10      2450.0  5000.0 2<br>
7934    MILLER  10      1300.0  5000.0 3</p>
<p>7788    SCOTT   20      3000.0  3000.0<br>
7902    FORD    20      3000.0  3000.0<br>
7566    JONES   20      2975.0  3000.0<br>
7876    ADAMS   20      1100.0  3000.0<br>
7369    SMITH   20      800.0   3000.0</p>
<p>7698    BLAKE   30      2850.0  2850.0<br>
7499    ALLEN   30      1600.0  2850.0<br>
7844    TURNER  30      1500.0  2850.0<br>
7654    MARTIN  30      1250.0  2850.0<br>
7521    WARD    30      1250.0  2850.0</p>
<p>7900    JAMES   30      950.0   2850.0</p>
<p>需求3：将每个部门薪资最高的那个人的薪资显示在最后一列并且显示唯一的编号</p>
<p>select empno,ename, deptno,sal, row_number() over(partition by deptno order by sal desc ) as sal_111 from emp  ;<br>
结果：<br>
7839    KING    10      5000.0  1<br>
7782    CLARK   10      2450.0  2<br>
7934    MILLER  10      1300.0  3</p>
<p>7788    SCOTT   20      3000.0  1<br>
7902    FORD    20      3000.0  2<br>
7566    JONES   20      2975.0  3<br>
7876    ADAMS   20      1100.0  4<br>
7369    SMITH   20      800.0   5</p>
<p>7698    BLAKE   30      2850.0  1<br>
7499    ALLEN   30      1600.0  2<br>
7844    TURNER  30      1500.0  3<br>
7654    MARTIN  30      1250.0  4<br>
7521    WARD    30      1250.0  5<br>
7900    JAMES   30      950.0   6</p>
<p>需求4：获取每个部门薪资最高的前两位（嵌套子查询的方式）<br>
select empno,ename, deptno,sal from(select empno,ename, deptno,sal,row_number() over (partition by deptno  order by sal desc) as rn from  emp) as tmp where rn &lt; 3;<br>
结果：<br>
7839    KING    10      5000.0<br>
7782    CLARK   10      2450.0<br>
7788    SCOTT   20      3000.0<br>
7902    FORD    20      3000.0<br>
7698    BLAKE   30      2850.0<br>
7499    ALLEN   30      1600.0</p>
<p>**</p>
<h2><a id="hive_595"></a>二十、hive数据导入和导出</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125410310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>注意：这种方式适合数据量非常小的情况下去使用，如果说数据量大，避免这种操作</p>
<p>show tables出现values__tmp__table__1这个临时表，关闭会话就消失</p>
<p>5、location方式</p>
<p><img src="https://img-blog.csdnimg.cn/20181115125439110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>【导出】<br>
1、insert<br>
insert overwrite [local] directory ‘path’ select sql;</p>
<p>-》本地<br>
insert overwrite local directory ‘/opt/datas/emp_in01’ select * from emp;</p>
<p>insert overwrite local directory ‘/opt/datas/emp_in01’ row format delimited fields terminated by “\t” select * from emp;</p>
<p>-》HDFS<br>
insert overwrite  directory ‘/emp_insert’ select * from emp;<br>
insert overwrite directory ‘/emp_insert’ row format delimited fields terminated by “\t” select * from emp;</p>
<p>2、bin/hdfs dfs -get xxx 下载数据文件<br>
hive&gt; dfs -get  xxx   （hive的客户端）</p>
<p>3、bin/hive  -e 或者  -f    +   &gt;&gt; 或者 &gt;    (追加和覆盖符号)</p>
<p>4、sqoop 方式：import导入和export 导出</p>
<p>**</p>
<h2><a id="hivehql_630"></a>二十一、hive常见的hql语句</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125452170.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125459854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125509928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
【过滤】<br>
where 、limit、distinct、between and 、null、 is not null</p>
<p>select * from emp  where sal &gt;3000;<br>
select * from emp limit 5;<br>
select distinct deptno  from emp;<br>
select * from emp where sal  between 1000 and 3000;<br>
select empno,ename from emp where comm is null;<br>
select empno,ename from emp where comm is not null;</p>
<p>【聚合函数】<br>
count、sum、max、min、avg、group by 、having</p>
<p>select avg(sal) avg_sal from  emp;<br>
按部门分组求出每个部门的平均工资<br>
select 中出现的字段，需要用聚合函数包裹或者放入group by 中<br>
select deptno,avg(sal) from emp group by deptno;<br>
10      2916.6666666666665<br>
20      2175.0<br>
30      1566.6666666666667<br>
select deptno,max(job),avg(sal) from emp group by deptno;<br>
10      PRESIDENT       2916.6666666666665<br>
20      MANAGER 		2175.0<br>
30      SALESMAN        1566.6666666666667<br>
select deptno,avg(sal) from emp group by deptno,job;</p>
<p>having和where 是差不多的用法，都是筛选语句<br>
可以一起使用，先执行where后执行having<br>
select deptno,avg(sal)  avg_sal from emp group by deptno  having avg_sal &gt;2000 ;</p>
<p>【join】<br>
left join, right join,inner join（等值）,全join<br>
创建两张表A,B<br>
A表：					 B表:<br>
ID	Name     	ID	phone<br>
1	张三 		 1	111<br>
2	李四 		 2	112<br>
3	王五 		 3	113<br>
5	赵六 		 4	114</p>
<p>drop table A;<br>
drop table B;<br>
create table A(<br>
id int,<br>
name string<br>
)row format delimited fields terminated by “\t”;</p>
<p>create table B(<br>
id int,<br>
name int<br>
)row format delimited fields terminated by “\t”;</p>
<p>load data local inpath ‘/opt/datas/A’ into table A;<br>
load data local inpath ‘/opt/datas/B’ into table B;</p>
<p>等值join ：id值都会出现<br>
select <a href="http://aaa.ID" rel="nofollow">aaa.ID</a>,<a href="http://aaa.Name" rel="nofollow">aaa.Name</a> ,<a href="http://bbb.ID" rel="nofollow">bbb.ID</a> ,bbb.phone  from A aaa  join B bbb on <a href="http://aaa.ID=bbb.ID" rel="nofollow">aaa.ID=bbb.ID</a>;</p>
<p>左join：以左表为基准，没有匹配到的字段就是null<br>
select <a href="http://aaa.ID" rel="nofollow">aaa.ID</a>,<a href="http://aaa.Name" rel="nofollow">aaa.Name</a> ,<a href="http://bbb.ID" rel="nofollow">bbb.ID</a> ,bbb.phone  from A aaa left join B bbb on <a href="http://aaa.ID=bbb.ID" rel="nofollow">aaa.ID=bbb.ID</a>;</p>
<p>右join： 以右表为基准，没有匹配到的字段就是null<br>
select <a href="http://aaa.ID" rel="nofollow">aaa.ID</a>,<a href="http://aaa.Name" rel="nofollow">aaa.Name</a> ,<a href="http://bbb.ID" rel="nofollow">bbb.ID</a> ,bbb.phone  from A aaa right join B bbb on <a href="http://aaa.ID=bbb.ID" rel="nofollow">aaa.ID=bbb.ID</a>;</p>
<p>全join：所有的字段都会出现，匹配或者匹配不上都会出现，没有匹配上的字段就是null</p>
<p>select <a href="http://aaa.ID" rel="nofollow">aaa.ID</a>,<a href="http://aaa.Name" rel="nofollow">aaa.Name</a> ,<a href="http://bbb.ID" rel="nofollow">bbb.ID</a> ,bbb.phone  from A aaa full join B bbb on <a href="http://aaa.ID=bbb.ID" rel="nofollow">aaa.ID=bbb.ID</a>;</p>
<p>【排序】</p>
<p>1、order by ：全局排序，设置多个reduce没有太大的作用<br>
select * from emp order by sal;</p>
<p>2、sort by ：局部排序，对于每个reduce的结果进行排序，设置reduce=4,导出到本地</p>
<p>insert overwrite local directory ‘/opt/datas/emp_sort’ row format delimited fields terminated by “\t” select * from emp sort by sal;</p>
<p>3、distribute by（底层是mr分区）<br>
-》可以按照指定的值进行分区<br>
-》先分区后排序，一般和sort by连用<br>
insert overwrite local directory ‘/opt/datas/emp_dist’ row format delimited fields terminated by “\t” select * from emp distribute by deptno sort by sal;</p>
<p>4、cluster by ：相当于 distribute by +sort by 组合使用<br>
-》排序只能倒序排序，不能指定排序的规则为desc或者asc;<br>
-》相同的工资放在一起，因为数据的原因，相同工资有点少;<br>
insert overwrite local directory ‘/opt/datas/emp_cluster’ row format delimited fields terminated by “\t” select * from emp cluster by sal;</p>
<p>**</p>
<h2><a id="hivemapreduce_725"></a>二十二、hive和mapreduce的运行参数</h2>
<p>**<br>
设置每个reduce处理的数据量：（单位是字节）<br>
set hive.exec.reducers.bytes.per.reducer=<br>
<br>
hive.exec.reducers.bytes.per.reducer<br>
256000000<br>
size per reducer.The default is 256Mb, i.e if the input size is 1G, it will use 4 reducers.<br>
<br>
</p>
<p>设置最大运行的reduce的个数：默认1009<br>
set hive.exec.reducers.max=<br>
<br>
hive.exec.reducers.max<br>
1009<br>
<br>
max number of reducers will be used. If the one specified in the configuration parameter mapred.reduce.tasks is<br>
negative, Hive will use this one as the max number of reducers when automatically determine number of reducers.<br>
<br>
</p>
<p>设置实际运行的reduce的个数，默认是1个<br>
set mapreduce.job.reduces=</p>
<p>**</p>
<h2><a id="hiveUDF_753"></a>二十三、hive自定义UDF函数</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125543586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125552294.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
UDF ：一进一出</p>
<p>转换大小写</p>
<p>用IDEA开发工具写一个简单的UDF函数</p>
<p>hive的maven依赖：</p>
<p><img src="https://img-blog.csdnimg.cn/20181115125605692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<br>
org.apache.hadoop<br>
hadoop-client<br>
2.7.3<br>
<br>
<br>
org.apache.hive<br>
hive-exec<br>
1.2.1<br>
<br>
<br>
org.apache.hive<br>
hive-jdbc<br>
1.2.1<br>
</p>
<p>阿里的镜像资源下载：<br>
<br>
<br>
nexus-aliyun<br>
Nexus aliyun<br>
<a href="http://maven.aliyun.com/nexus/content/groups/public" rel="nofollow">http://maven.aliyun.com/nexus/content/groups/public</a><br>
<br>
<br>
<img src="https://img-blog.csdnimg.cn/20181115125626652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
继承UDF类,实现evaluate(Text str)<br>
public class Lower extends UDF {<br>
public Text evaluate(Text str){<br>
if (str == null) {<br>
return null;<br>
}<br>
if (StringUtils.isBlank(str.toString())) {<br>
return null;<br>
}<br>
return new Text(str.toString().toLowerCase());<br>
}<br>
//测试<br>
public static void main(String[] args) {<br>
// TODO Auto-generated method stub<br>
System.out.println(new Lower().evaluate(new Text(“TAYLOR swift”)));<br>
}<br>
}<br>
包名.类名<br>
com.xningge.Lower</p>
<p>将写好的代码打成jar包，上传linux<br>
将jar包添加到hive里面去<br>
add jar /opt/datas/lower_hive.jar;<br>
创建一个函数<br>
create temporary function LowerUdf as ‘com.xningge.Lower’;<br>
查看函数：show functions;<br>
-》使用函数<br>
select empno,ename, LowerUdf(ename) lower_name from emp;</p>
<p>**</p>
<h2><a id="hiveserver2jdbc_823"></a>二十四、hiveserver2与jdbc客户端</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/2018111512564875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125656800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20181115125706783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
将hive编程一个服务对外开放，通过客户端去连接<br>
启动服务端：bin/hiveserver2<br>
后台启动：bin/hiveserver2 &amp;</p>
<p>开启客户端：bin/beeline<br>
查看客户端帮助信息： bin/beeline --help<br>
beeline&gt; !help （不加分号）<br>
官网：beeline -u jdbc:hive2://hadoop01.xningge.com:10000/ -n scott -w password_file<br>
连接hiveserver2 ：<br>
-n 指定用户名：linux上的用户<br>
-p：指定密码： linux用户的密码<br>
bin/beeline -u jdbc:hive2://hadoop01.xningge.com:10000/ -n xningge -p 123456<br>
或者<br>
bin/beeline<br>
!connect jdbc:hive2://hadoop01.xningge.com:10000</p>
<p>hive JDBC 远程连接<br>
官网：<a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-JDBC" rel="nofollow">https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-JDBC</a><br>
**</p>
<h2><a id="Hive_849"></a>二十五、Hive的运行模式与虚拟列</h2>
<p>**<br>
hive的配置模式分为三种：<br>
–依据hive的安装和metastore的设置机器而言<br>
嵌入模式：使用自带的derby数据库<br>
本地模式：将metastore放在mysql上，且mysql与hive安装在同一台机器<br>
远程模式：将metastore放在mysql上，mysql和hive不在同一台机器上</p>
<p>对于远程调控模式，则需要让mysql向集群对外提供服务，则需要配置metastore</p>
<p>配置:   指明存放metastore的mysql所在的机器号<br>
<br>
hive.metastore.uris<br>
<br>
Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.<br>
<br>
启动服务：bin/hive --service metastore &amp;</p>
<p>fetch模式<br>
<br>
hive.fetch.task.conversion<br>
more<br>
<br>
Expects one of [none, minimal, more].<br>
Some select queries can be converted to single FETCH task minimizing latency.<br>
Currently the query should be single sourced not having any subquery and should not have<br>
any aggregations or distincts (which incurs RS), lateral views and joins.<br>
0. none : disable hive.fetch.task.conversion<br>
1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only<br>
2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)<br>
<br>
</p>
<p>none：   不管你写什么sql都会跑mapreduce，不开启fetch模式<br>
minimal：当select * 、针对分区字段进行过滤、limit不跑mapreduce<br>
more：   当select 、过滤、limit不跑mapreduce</p>
<p>虚拟列：Virtual Colums<br>
INPUT__FILE__NAME  输入文件名称，显示这行数据所在的绝对路径<br>
BLOCK__OFFSET__INSIDE__FILE	 记录数据在块中的偏移量</p>
<p>select *, BLOCK__OFFSET__INSIDE__FILE  from dept;<br>
10      ACCOUNTING      NEW YORK        0<br>
20      RESEARCH        DALLAS  23<br>
30      SALES   CHICAGO 42<br>
40      OPERATIONS      BOSTON  59</p>
<h2><a id="_897"></a><strong>二十六、【案列】日志数据文件分析</strong></h2>
<p>需求：统计24小时内的每个时段的pv和uv<br>
pv统计总的浏览量<br>
uv统计guid去重后的总量<br>
获取时间字段，日期和小时-》分区表<br>
数据清洗：获取日期和小时，获取想要字段<br>
2015-08-28 18:14:59    -》28和18  substring方式获取	<br>
数据分析<br>
hive ：select sql<br>
数据导出：<br>
sqoop：导出mysql<br>
最终结果预期：<br>
日期	小时	pv		uv<br>
日期和小时：tracktime<br>
pv：url<br>
uv：guid<br>
1、【数据收集】</p>
<p>登陆hive：<br>
启动服务端：bin/hiveserver2 &amp;<br>
启动客户端：bin/beeline -u jdbc:hive2://hadoop01.xningge.com:10000/ -n xningge -p 123456<br>
创建源表：<br>
create database track_log;<br>
create table yhd_source(<br>
id                                 string,<br>
url                                string,<br>
referer                            string,<br>
keyword                            string,<br>
type                               string,<br>
guid                               string,<br>
pageId                             string,<br>
moduleId                           string,<br>
linkId                             string,<br>
attachedInfo                       string,<br>
sessionId                          string,<br>
trackerU                           string,<br>
trackerType                        string,<br>
ip                                 string,<br>
trackerSrc                         string,<br>
cookie                             string,<br>
orderCode                          string,<br>
trackTime                          string,<br>
endUserId                          string,<br>
firstLink                          string,<br>
sessionViewNo                      string,<br>
productId                          string,<br>
curMerchantId                      string,<br>
provinceId                         string,<br>
cityId                             string,<br>
fee                                string,<br>
edmActivity                        string,<br>
edmEmail                           string,<br>
edmJobId                           string,<br>
ieVersion                          string,<br>
platform                           string,<br>
internalKeyword                    string,<br>
resultSum                          string,<br>
currentPage                        string,<br>
linkPosition                       string,<br>
buttonPosition                     string<br>
)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>load data local inpath ‘/opt/datas/2015082818’ into table yhd_source;<br>
load data local inpath ‘/opt/datas/2015082819’ into table yhd_source;</p>
<p>2、【数据清洗】<br>
create table yhd_qingxi(<br>
id string,<br>
url string,<br>
guid string,<br>
date string,<br>
hour string<br>
)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>insert into table yhd_qingxi select id,url,guid,substring(trackTime,9,2) date,substring(trackTime,12,2) hour from yhd_source;</p>
<p>select id,url,guid,date,hour from yhd_qingxi;</p>
<p>创建分区表：根据时间字段进行分区<br>
create table yhd_part(<br>
id string,<br>
url string,<br>
guid string<br>
)partitioned by (date string,hour string)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>insert into table yhd_part partition(date=‘20150828’,hour=‘18’)<br>
select id,url,guid from yhd_qingxi where date=‘28’ and hour=‘18’;</p>
<p>insert into table yhd_part partition(date=‘20150828’,hour=‘19’)<br>
select id,url,guid from yhd_qingxi where date=‘28’ and hour=‘19’;</p>
<p>select id,url,guid from yhd_part where date=‘20150828’ and hour=‘18’;<br>
select id,url,guid from yhd_part where date=‘20150828’ and hour=‘19’;</p>
<p>动态分区：<br>
表示是否开启动态分区<br>
<br>
hive.exec.dynamic.partition<br>
true<br>
Whether or not to allow dynamic partitions in DML/DDL.<br>
<br>
表示动态分区最大个数<br>
<br>
hive.exec.max.dynamic.partitions<br>
1000<br>
Maximum number of dynamic partitions allowed to be created in total.<br>
<br>
每个节点上支持动态分区的个数<br>
<br>
hive.exec.max.dynamic.partitions.pernode<br>
100<br>
Maximum number of dynamic partitions allowed to be created in each mapper/reducer node.<br>
<br>
使用动态分区，需要改变模式为非严格模式<br>
<br>
hive.exec.dynamic.partition.mode<br>
strict<br>
In strict mode, the user must specify at least one static partition in case the user accidentally overwrites all partitions.<br>
<br>
set设置：<br>
set hive.exec.dynamic.partition.mode=nonstrict;</p>
<p>create table yhd_part2(<br>
id string,<br>
url string,<br>
guid string<br>
)partitioned by (date string,hour string)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>固定写死的格式<br>
insert into table yhd_part partition(date=‘20150828’,hour=‘18’)<br>
select id,url,guid from yhd_qingxi where date=‘28’ and hour=‘18’;</p>
<p>动态分区（非常灵活）<br>
insert into table yhd_part2 partition (date,hour) select * from yhd_qingxi;</p>
<p>select id,url,guid from yhd_part2 where date=‘20150828’ and hour=‘18’;</p>
<p>3、数据分析<br>
PV：<br>
select date,hour,count(url) pv from yhd_part group by date,hour;<br>
结果：<br>
±----------±------±-------±-+<br>
|   date    | hour  |   pv   |<br>
±----------±------±-------±-+<br>
| 20150828  | 18    | 64972  |<br>
| 20150828  | 19    | 61162  |<br>
±----------±------±-------±-+</p>
<p>UV<br>
select date,hour,count(distinct guid) uv from yhd_part group by date,hour;<br>
结果：<br>
±----------±------±-------±-+<br>
|   date    | hour  |   uv   |<br>
±----------±------±-------±-+<br>
| 20150828  | 18    | 23938  |<br>
| 20150828  | 19    | 22330  |<br>
±----------±------±-------±-+</p>
<p>最终结果导入最终结果表中：<br>
create table result as select date,hour,count(url) pv,count(distinct guid) uv from yhd_part group by date,hour;<br>
结果：<br>
±-------------±-------------±-----------±-----------±-+<br>
| result.date  | result.hour  | result.pv  | result.uv  |<br>
±-------------±-------------±-----------±-----------±-+<br>
| 20150828     | 18           | 64972      | 23938      |<br>
| 20150828     | 19           | 61162      | 22330      |<br>
±-------------±-------------±-----------±-----------±-+</p>
<p>4、数据导出：</p>
<p>将最终的结果保存在mysql：<br>
在mysql里创建表：<br>
create table save(<br>
date varchar(30),<br>
hour varchar(30),<br>
pv varchar(30),<br>
uv varchar(30),<br>
primary key(date,hour)<br>
);</p>
<p>sqoop方式：<br>
hive-》mysql</p>
<p>bin/sqoop export <br>
–connect jdbc:mysql://hadoop01.xningge.com:3306/t_data_pv_uv <br>
–username xningge <br>
–password WN@950421 <br>
–table save <br>
–export-dir /user/hive/warehouse/track_log.db/result <br>
-m 1 <br>
–input-fields-terminated-by ‘\001’<br>
**</p>
<h2><a id="hive_shell__1095"></a>二十七、【案列二】hive shell脚本 自动化加载数据</h2>
<p>**<br>
效果：<br>
access_log/20180122/<br>
2018012201.log<br>
2018012202.log<br>
2018012203.log<br>
【案例实现】<br>
创建源表<br>
create database load_hive;<br>
create table load_tb(<br>
id                                 string,<br>
url                                string,<br>
referer                            string,<br>
keyword                            string,<br>
type                               string,<br>
guid                               string,<br>
pageId                             string,<br>
moduleId                           string,<br>
linkId                             string,<br>
attachedInfo                       string,<br>
sessionId                          string,<br>
trackerU                           string,<br>
trackerType                        string,<br>
ip                                 string,<br>
trackerSrc                         string,<br>
cookie                             string,<br>
orderCode                          string,<br>
trackTime                          string,<br>
endUserId                          string,<br>
firstLink                          string,<br>
sessionViewNo                      string,<br>
productId                          string,<br>
curMerchantId                      string,<br>
provinceId                         string,<br>
cityId                             string,<br>
fee                                string,<br>
edmActivity                        string,<br>
edmEmail                           string,<br>
edmJobId                           string,<br>
ieVersion                          string,<br>
platform                           string,<br>
internalKeyword                    string,<br>
resultSum                          string,<br>
currentPage                        string,<br>
linkPosition                       string,<br>
buttonPosition                     string<br>
)partitioned by(date string,hour string)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>启动服务：bin/hiveserver2 &amp;<br>
启动客户端：bin/beeline -u jdbc:hive2://hadoop01.xningge.com:10000 -n xningge -p 123456</p>
<p>1、通过hive -e 的方式：</p>
<p>创建一个脚本：load_to_hive.sh<br>
<img src="https://img-blog.csdnimg.cn/20181115125738332.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
查看分区：<br>
show partitions load_tb;</p>
<p>在linux下，可以debug运行脚本</p>
<p>sh -x load_to_hive.sh</p>
<p>2、通过hive -f的方式：</p>
<p>1、创建一个文件：vi load.sql存放sql语句</p>
<p>load data local inpath ‘<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>h</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>f</mi><mo>:</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>d</mi></msub><mi>i</mi><mi>r</mi></mrow><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">{hiveconf:log_dir}/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit">h</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.03588em;">v</span><span class="mord mathit">e</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right: 0.10764em;">f</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mord mathit">o</span><span class="mord"><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span></span><span class="mord">/</span></span></span></span></span>{hiveconf:file_path}’ into table load_log.load_tab<br>
partition(dateTime=’<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi>h</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>f</mi><mo>:</mo><mi>D</mi><mi>A</mi><mi>Y</mi></mrow><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>h</mi><mi>o</mi><mi>u</mi><mi>r</mi><msup><mo>=</mo><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">{hiveconf:DAY}&amp;#x27;,hour=&amp;#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.03067em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathit">h</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.03588em;">v</span><span class="mord mathit">e</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right: 0.10764em;">f</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathit" style="margin-right: 0.02778em;">D</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.22222em;">Y</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.836232em;"><span class="" style="top: -3.14734em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">h</span><span class="mord mathit">o</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel"><span class="mrel">=</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>{hiveconf:HOUR}’)：</p>
<p>重命名load_to_hive.sh load_to_hive_file.sh<br>
<img src="https://img-blog.csdnimg.cn/20181115125800477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
2.创建一个load.sql，把-e后面的复制过来(然后在修改)：<br>
针对变量，我们可以用–hiveconf 去传参<br>
load data local inpath ‘<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>h</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>f</mi><mo>:</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>d</mi></msub><mi>i</mi><mi>r</mi></mrow><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">{hiveconf:log_dir}/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit">h</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.03588em;">v</span><span class="mord mathit">e</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right: 0.10764em;">f</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mord mathit">o</span><span class="mord"><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span></span><span class="mord">/</span></span></span></span></span>{hiveconf:file_path}’ into table load_hive.load_h partition(date=’<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mi>h</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>f</mi><mo>:</mo><mi>D</mi><mi>A</mi><mi>Y</mi></mrow><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>h</mi><mi>o</mi><mi>u</mi><mi>r</mi><msup><mo>=</mo><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">{hiveconf:DAY}&amp;#x27;,hour=&amp;#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.03067em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathit">h</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.03588em;">v</span><span class="mord mathit">e</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right: 0.10764em;">f</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathit" style="margin-right: 0.02778em;">D</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.22222em;">Y</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.836232em;"><span class="" style="top: -3.14734em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">h</span><span class="mord mathit">o</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel"><span class="mrel">=</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>{hiveconf:HOUR}’)<br>
**</p>
<h2><a id="hive_1175"></a>二十八、hive优化</h2>
<p>**<br>
详细优化策略：</p>
<p>hive中sql优化：<a href="https://blog.csdn.net/qq_35036995/article/details/80298449" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80298449</a></p>
<p>hive整体架构优化：<a href="https://blog.csdn.net/qq_35036995/article/details/80298416" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80298416</a></p>
<p>hive数据倾斜优化：<a href="https://blog.csdn.net/qq_35036995/article/details/80298403" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80298403</a></p>
<p>hivejob中map优化：<a href="https://blog.csdn.net/qq_35036995/article/details/80298355" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80298355</a></p>
<p>1、表拆分成小表<br>
包含临时表、分区表、外部表<br>
2、sql语句：<br>
优化sql：复杂的sql-》子查询+join -》简化，拆分成多个简单的语句<br>
join、filter：先过滤再join<br>
3、设置map和reduce的个数	<br>
reduce数目：可以参数进行设置<br>
hive： set mapreduce.job.reduces=<br>
MR：job.setNumReduceTasks(tasks);<br>
**map的数量：<br>
Math.max(minSize, Math.min(maxSize, blockSize))<br>
blocksize：128M<br>
**代码块中<br>
FileInputFormat.setMaxInputSplitSize(job, size);<br>
FileInputFormat.setMinInputSplitSize(job, size);<br>
通过spilt切片的大小来改变map任务数	<br>
4、开启并行化执行，默认是false</p>
<p>hive.exec.parallel</p>
<p>5、设置同时运行job的数目，根据集群设置，默认是8<br>
hive.exec.parallel.thread.number</p>
<p>6、jvm重用	<br>
mapreduce.job.jvm.numtasks默认是1，运行一个job会启动一个jvm上运行	<br>
7、推测执行：<br>
缺点：会消耗更多的资源，一般不建议开启，有可能数据会重复写入，造成异常<br>
8、hive本地模式：	<br>
hive的本地模式hive.exec.mode.local.auto默认是false<br>
hive底层运行的hadoop集群中的资源，yarn调度，本地模式不会再集群所有机器上运行，会选一台作为本地运行，一般处理小数据量级的数据速度回很快<br>
限制：job输入的数据不能大于128M，map的个数不能超过4个，默认reduce数目 1个<br>
测试：<br>
create table result4 as<br>
select<br>
date date,<br>
sum(pv) PV,<br>
count(distinct guid) UV,<br>
count(distinct case when length(user_id)!=0 then guid else null end) login_user,<br>
count(distinct case when length(user_id)=0 then guid else null end) visitor,<br>
avg(stay_time) avg_time,<br>
(count(case when pv&gt;=2 then session_id else null end)/count(session_id)) session_jump,<br>
count(distinct ip) IP<br>
from session_info<br>
where date=‘2015082818’<br>
group by date;</p>
<p>9、数据倾斜</p>
<p>在MR程序中由于某个key值分布不均，导致某个reduce运行速度异常缓慢，严重影响了整个job的运行<br>
【根据产生原因进行解析：】	<br>
考虑分区阶段-》默认的分区采用hash值，可以自定义实现分区规避产生倾斜<br>
或者直接在key中加入随机数，相同的key呢会分到一个reduce处理，不同的key分到不同的reduce进行处理，加上随机值打乱分区</p>
<p>在hive中<br>
产生倾斜的主要语句：join、group by、distinct<br>
join：连接某个key值时，key值数据量变多</p>
<p>join：<br>
map join、reduce join、SMB join（sort merge bucket）<br>
map join适合小表join大表的场景【读取小表缓存到内存中，在map端完成reduce，减轻reduce聚合压力】<br>
reduce join适合大表join 大表的场景【加上随机数，把倾斜的数据分到不同的reduce上】	<br>
SMB join	适合大表join 大表的场景 创建桶表<br>
分区与分区join，减小join的范围<br>
桶join只适合桶与桶之间的join，适合抽样调查<br>
注意：桶表之间的join，两张表的个数要么一致，要么倍数关系<br>
A：10000万   3桶<br>
1<br>
2<br>
3<br>
b：10000万   3桶<br>
1<br>
2<br>
3<br>
4<br>
5<br>
6<br>
在join时，A表就会join B表1和4，也就说B表1 和4桶的数量等于A表的 1桶，以此类推<br>
最终就是减小join的范围，避免数据倾斜的问题</p>
<p>map join	<br>
开启map join ，默认是true，符合条件就去执行<br>
<br>
hive.auto.convert.join<br>
true<br>
Whether Hive enables the optimization about converting common join into mapjoin based on the input file size<br>
</p>
<p>执行map join 的条件，默认是10M	<br>
<br>
hive.auto.convert.join.noconditionaltask.size<br>
10000000<br>
If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. However, if it<br>
is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than this size, the join is directly<br>
converted to a mapjoin(there is no conditional task). The default is 10MB<br>
<br>
</p>
<p>如何判断是key导致的问题<br>
通过时间判断<br>
如果每个reduce的运行时间都很长，那么可能是redcue数目设置过少造成的<br>
如果大部分的reduce任务在几分钟完成了，而某一个reduce可能30分钟还没完成，可能是倾斜<br>
可能也是某个节点造成的问题<br>
可以考虑使用推测执行，如果推测执行的任务也很慢，就有可能是倾斜的问题<br>
如果推测执行的新任务在短时间内完成，可能就是节点造成的某个任务运行过慢<br>
自定义counter（context.getCounter()方法自定义计数器）<br>
判断统计查看每个任务的信息<br>
输入记录条数</p>
<pre><code>		输出的记录条数
</code></pre>
<p>**</p>
<h2><a id="hadoop__hive_1300"></a>二十九、hadoop &amp; hive压缩</h2>
<p>**<br>
MR<br>
流程：inputformat -&gt; map() -&gt;shuffle -&gt;redcue() -&gt;outputforat<br>
可以mr中实现<br>
优化一：combiner<br>
优化二：compress压缩</p>
<p>使用和配置压缩的前提，当前的hadoop环境必须支持压缩<br>
命令：bin/hadoop checknative<br>
cdh版本的native包替换到hadoop/lib/native下</p>
<p>编译：<br>
如果自己编译源码的步骤<br>
<a href="https://github.com/snappy%E4%B8%8B%E8%BD%BDsnappy%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6%EF%BC%8C%E8%AE%A9%E7%B3%BB%E7%BB%9F%E6%9C%89snappy%E5%BA%93" rel="nofollow">https://github.com/snappy下载snappy的压缩文件，让系统有snappy库</a><br>
snappy-1.1.1.tar.gz<br>
下载依赖包<br>
snappy安装: yum没法成功，必须编译snappy的源码<br>
./configure<br>
make<br>
make install<br>
下载依赖：<br>
sudo yum -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev<br>
sudo yum install snappy libsnappy-dev<br>
-》执行编译的命令<br>
mvn clean package -Pnative -Drequire.snappy -DskipTests -Dtar<br>
或者<br>
mvn clean package -Pnative,dist -DskipTests -Dtar</p>
<p>不同压缩格式牵扯到不同的压缩算法<br>
压缩能减少IO的负载（磁盘、网络）<br>
压缩要支持可分割性（Splittability must be taken into account）<br>
压缩后分为多个块，每个map解压自己的文件，不可分割是不是就不能解压自己的了</p>
<p>MR压缩设置：<br>
map<br>
mapreduce.map.output.compress=true<br>
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec</p>
<p>reduce<br>
mapreduce.output.fileoutputformat.compress=true<br>
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec</p>
<p>通过-D制定运行参数，后面参数选项的格式：key=value</p>
<p>bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar wordcount -Dmapreduce.map.output.compress=true -Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec /input/WordCount.txt /ouputhdfs11</p>
<p>hive压缩设置<br>
配置hive<br>
map：<br>
<br>
hive.exec.compress.intermediate<br>
false<br>
<br>
配置map<br>
mapreduce.map.output.compress=true<br>
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec</p>
<p>reduce：<br>
<br>
hive.exec.compress.output<br>
false<br>
<br>
mapreduce.output.fileoutputformat.compress=true<br>
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec</p>
<p>**</p>
<h2><a id="hive_1370"></a>三十、hive存储格式</h2>
<p>**<br>
file_format:<br>
: SEQUENCEFILE	  行存储，hdfs底层的存储格式，存储的二进制文件<br>
| TEXTFILE    	  行存储（磁盘开销大，数据解析开销也大）<br>
| RCFILE     		  数据按行分块，每块按照列存储（压缩快）<br>
| ORC         	  数据按行分块，每块按照列存储（压缩快，是RCfile的改良版）<br>
| PARQUET    		  行式存储，同时具有很好的压缩性能<br>
| AVRO       		<br>
| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname  自定义格式</p>
<p>【原文本数据】</p>
<p>create table file_source(<br>
id                                 string,<br>
url                                string,<br>
referer                            string,<br>
keyword                            string,<br>
type                               string,<br>
guid                               string,<br>
pageId                             string,<br>
moduleId                           string,<br>
linkId                             string,<br>
attachedInfo                       string,<br>
sessionId                          string,<br>
trackerU                           string,<br>
trackerType                        string,<br>
ip                                 string,<br>
trackerSrc                         string,<br>
cookie                             string,<br>
orderCode                          string,<br>
trackTime                          string,<br>
endUserId                          string,<br>
firstLink                          string,<br>
sessionViewNo                      string,<br>
productId                          string,<br>
curMerchantId                      string,<br>
provinceId                         string,<br>
cityId                             string,<br>
fee                                string,<br>
edmActivity                        string,<br>
edmEmail                           string,<br>
edmJobId                           string,<br>
ieVersion                          string,<br>
platform                           string,<br>
internalKeyword                    string,<br>
resultSum                          string,<br>
currentPage                        string,<br>
linkPosition                       string,<br>
buttonPosition                     string<br>
)<br>
row format delimited fields terminated by ‘\t’;</p>
<p>load data local inpath ‘/opt/datas/2015082818’ into table file_source;</p>
<p>【textfile】</p>
<p>create table file_textfile(<br>
id                                 string,<br>
url                                string,<br>
referer                            string,<br>
keyword                            string,<br>
type                               string,<br>
guid                               string,<br>
pageId                             string,<br>
moduleId                           string,<br>
linkId                             string,<br>
attachedInfo                       string,<br>
sessionId                          string,<br>
trackerU                           string,<br>
trackerType                        string,<br>
ip                                 string,<br>
trackerSrc                         string,<br>
cookie                             string,<br>
orderCode                          string,<br>
trackTime                          string,<br>
endUserId                          string,<br>
firstLink                          string,<br>
sessionViewNo                      string,<br>
productId                          string,<br>
curMerchantId                      string,<br>
provinceId                         string,<br>
cityId                             string,<br>
fee                                string,<br>
edmActivity                        string,<br>
edmEmail                           string,<br>
edmJobId                           string,<br>
ieVersion                          string,<br>
platform                           string,<br>
internalKeyword                    string,<br>
resultSum                          string,<br>
currentPage                        string,<br>
linkPosition                       string,<br>
buttonPosition                     string<br>
)<br>
row format delimited fields terminated by ‘\t’<br>
stored as textfile;</p>
<p>insert overwrite table file_textfile select * from file_source;</p>
<p>【PARQUET】</p>
<p>create table file_parquet(<br>
id                                 string,<br>
url                                string,<br>
referer                            string,<br>
keyword                            string,<br>
type                               string,<br>
guid                               string,<br>
pageId                             string,<br>
moduleId                           string,<br>
linkId                             string,<br>
attachedInfo                       string,<br>
sessionId                          string,<br>
trackerU                           string,<br>
trackerType                        string,<br>
ip                                 string,<br>
trackerSrc                         string,<br>
cookie                             string,<br>
orderCode                          string,<br>
trackTime                          string,<br>
endUserId                          string,<br>
firstLink                          string,<br>
sessionViewNo                      string,<br>
productId                          string,<br>
curMerchantId                      string,<br>
provinceId                         string,<br>
cityId                             string,<br>
fee                                string,<br>
edmActivity                        string,<br>
edmEmail                           string,<br>
edmJobId                           string,<br>
ieVersion                          string,<br>
platform                           string,<br>
internalKeyword                    string,<br>
resultSum                          string,<br>
currentPage                        string,<br>
linkPosition                       string,<br>
buttonPosition                     string<br>
)<br>
row format delimited fields terminated by ‘\t’<br>
stored as PARQUET;</p>
<p>insert overwrite table file_parquet select * from file_source;</p>
<p>【ORC】</p>
<p>create table file_orc(<br>
id                                 string,<br>
url                                string,<br>
referer                            string,<br>
keyword                            string,<br>
type                               string,<br>
guid                               string,<br>
pageId                             string,<br>
moduleId                           string,<br>
linkId                             string,<br>
attachedInfo                       string,<br>
sessionId                          string,<br>
trackerU                           string,<br>
trackerType                        string,<br>
ip                                 string,<br>
trackerSrc                         string,<br>
cookie                             string,<br>
orderCode                          string,<br>
trackTime                          string,<br>
endUserId                          string,<br>
firstLink                          string,<br>
sessionViewNo                      string,<br>
productId                          string,<br>
curMerchantId                      string,<br>
provinceId                         string,<br>
cityId                             string,<br>
fee                                string,<br>
edmActivity                        string,<br>
edmEmail                           string,<br>
edmJobId                           string,<br>
ieVersion                          string,<br>
platform                           string,<br>
internalKeyword                    string,<br>
resultSum                          string,<br>
currentPage                        string,<br>
linkPosition                       string,<br>
buttonPosition                     string<br>
)<br>
row format delimited fields terminated by ‘\t’<br>
stored as ORC;</p>
<p>insert overwrite table file_orc select * from file_source;</p>
<p>【结果比较】<br>
原始数据    37.60 MB<br>
textfile	27.48 MB<br>
parquet		16.14 MB<br>
orc			4.40 MB</p>
<p>总结：<br>
textfile 存储空间消耗空间比较大，并且压缩的text，无法分割和合并，查询的效率最低，可以直接存储，加载数据的速度最高<br>
rcfile存储空间最小，查询的效率最高，需要通过text文件转化来加载，加载的速度最低</p>
<p>**</p>
<h2><a id="hivehive_1575"></a>三十一、hive总结&amp;hive使用场景</h2>
<p>**<br>
<img src="https://img-blog.csdnimg.cn/20181115125829264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1YW5zbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<hr>
<p>作者：宁哥说<br>
来源：CSDN<br>
原文：<a href="https://blog.csdn.net/qq_35036995/article/details/80249845" rel="nofollow">https://blog.csdn.net/qq_35036995/article/details/80249845</a></p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>