---
layout:     post
title:      Spark SQL的几个里程碑！
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/rlnLo2pNEfx9c/article/details/83965704				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
    <div class="rich_media_content" id="js_content">
                    

                    

                    
                    
                    <p style="letter-spacing:.5px;">本文讲讲Spark SQL的几个里程碑的更新升级。</p><p style="letter-spacing:.5px;"><span style="color:rgb(255,104,39);"><strong><span style="font-family:'宋体';">1. spark 1.0.0诞生了Spark SQL</span></strong></span></p><p style="letter-spacing:.5px;"><span style="font-family:'宋体';">官方版本是</span>spark 1.0.0<span style="font-family:'宋体';">引入的</span>Spark SQL<span style="font-family:'宋体';">模块。当时这个模块的核心实际上就是一种新类型的</span>RDD<span style="font-family:'宋体';">，叫做</span>SchemaRDD<span style="font-family:'宋体';">。</span>SchemaRDD<span style="font-family:'宋体';">就是类型为</span>ROW<span style="font-family:'宋体';">的</span>RDD，<span style="font-family:'宋体';">但同时又包含了一个描述每一列数据类型的</span>schema<span style="font-family:'宋体';">信息。</span>SchemRDD<span style="font-family:'宋体';">也可类似于传统数据库的一张表。</span>SchemaRDD<span style="font-family:'宋体';">可以从已有的</span>RDD<span style="font-family:'宋体';">创建，可以是</span>Parquet<span style="font-family:'宋体';">文件，</span>json<span style="font-family:'宋体';">数据集或则</span>HiveQL<span style="font-family:'宋体';">生成。该版本引入是在</span>2014<span style="font-family:'宋体';">年五月</span>30<span style="font-family:'宋体';">日。</span><br></p><p style="text-align:center;letter-spacing:.5px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFVvRRHZ0jLDYwW9nRVQmTafibZmHXAHTcRktvBLtZSeMGIdqBB0h5an7ERUlo799A1aQnaovCBpriaQ/640" alt="640"></p><p style="text-align:left;letter-spacing:.5px;"><span style="color:rgb(255,104,39);"><strong>2. Spark 1.2.0诞生了ML机器学习库</strong></span></p><p style="text-align:left;letter-spacing:.5px;">Ml机器学习库是基于SchemaRDD的，后来的版本是基于Dataframe的，可以直接与Spark SQL进行交互。</p><p style="text-align:center;letter-spacing:.5px;margin-left:.5em;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFVvRRHZ0jLDYwW9nRVQmTafM8jZa9rcFEVm4AmnXZSZjANMzAlxo3xep2ia9FIMicRkgp223dGLQ6pQ/640" alt="640"></p><p style="letter-spacing:.5px;"><span style="color:rgb(255,104,39);"><strong>3. Spark 1.3.0 诞生了Dataframe</strong></span></p><p style="letter-spacing:.5px;">Spark 1.3<span style="font-family:'宋体';">的时候做了一个重大变革。就是将</span>SchemaRDD<span style="font-family:'宋体';">重命名为了</span>DataFrame<span style="font-family:'宋体';">，主要原因是</span>DataFrame<span style="font-family:'宋体';">不再直接继承自</span>RDD<span style="font-family:'宋体';">，而是自己维护和实现了自己的功能函数。但是</span>DataFrame<span style="font-family:'宋体';">可以通过调用</span> .rdd <span style="font-family:'宋体';">转化为</span>RDD<span style="font-family:'宋体';">。</span></p><p style="text-align:center;letter-spacing:.5px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFVvRRHZ0jLDYwW9nRVQmTafCJVEkmVZ3JfefTJpHT8opALYtGz76VicJ96fA1fK2kQWIGFf7ULasibQ/640" alt="640"></p><p style="letter-spacing:.5px;"><span style="color:rgb(255,104,39);"><strong>4. spark 1.6.0诞生了Dataset和SparkSession</strong></span></p><p style="letter-spacing:.5px;">Spark 1.6<span style="font-family:'宋体';">的时候也是有了重大调整，增加了</span>Dataset<span style="font-family:'宋体';">的概念</span>,<span style="font-family:'宋体';">类似</span>RDD<span style="font-family:'宋体';">，在享受</span>Spark SQL<span style="font-family:'宋体';">执行引擎性能优化的同时允许用户使用自定义对象和</span>lambda<span style="font-family:'宋体';">函数。</span></p><p style="letter-spacing:.5px;"><span style="font-family:'宋体';">在引入</span>Dataset<span style="font-family:'宋体';">的同时，也引入了</span>SparkSession<span style="font-family:'宋体';">，也即是会话管理功能，允许不同用户可以在使用不同配置和临时表的情况下共享统一的集群。</span></p><p style="text-align:center;letter-spacing:.5px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFVvRRHZ0jLDYwW9nRVQmTafcCf68KohTcExI5iacpgHe0FjjujgLNXWdY95LD5Yrc0OgMW9U8Xl5iaA/640" alt="640"></p><p style="text-align:left;letter-spacing:.5px;"><span style="color:rgb(255,104,39);"><strong>5. Spark 2.0.0诞生了Strcutured Streaming</strong></span></p><p style="text-align:left;letter-spacing:.5px;">Spark 2.0<span style="font-family:'宋体';">开始，</span>Dataset API<span style="font-family:'宋体';">和</span>Dataframe API<span style="font-family:'宋体';">统一了。</span>Scala<span style="font-family:'宋体';">版本，</span>DataFrame<span style="font-family:'宋体';">被类型定义成</span>Dataset[Row]<span style="font-family:'宋体';">，</span>java<span style="font-family:'宋体';">版本必须是要</span>Dataset[Row]<span style="font-family:'宋体';">代替</span>Dataframe<span style="font-family:'宋体';">。</span></p><p style="text-align:left;letter-spacing:.5px;"><span style="font-family:'宋体';">SparkSession已经完全替换掉了旧的SQLContext和HiveContext。SQLContext和HiveContext为了保持兼容还在被保留。</span></p><p style="text-align:left;letter-spacing:.5px;"><span style="font-family:'宋体';">上线了Structured Streaming。这个是Spark 流处理发展的主要方向，底层是基于Spark SQL 和 Catalyst 优化器，让用户像使用静态Dataset开发离线处理任务一样<span style="font-family:'宋体';">使用流Dataset</span>开发流处理业务，这个就是依赖于Catalyst 优化器自动增量的查询计划。</span></p><p style="text-align:left;letter-spacing:.5px;">从自Spark 2.x依赖的更新状态来看，Spark SQL及Catalyst 优化器已经成为Spark框架努力的方向，主要体现在：<br></p><p style="text-align:left;letter-spacing:.5px;"><span style="background-color:rgb(255,255,255);color:rgb(255,104,39);">1). 逐步废弃掉基于RDD的mllib机器学习库，着重发展基于DataFrame的ml库。目前是，基于RDD的机器学习库处于保留状态，后期会废弃。</span></p><p style="text-align:left;letter-spacing:.5px;"><span style="background-color:rgb(255,255,255);color:rgb(255,104,39);">2). Spark2.4.0 未对Spark Streaming(RDD-based)做进一步更新。Structured Streaming(dataframe-based)被大力优化更新，也有取代Spark Streaming之势头。</span></p><p style="text-align:left;letter-spacing:.5px;">所以，spark 使用及爱好者要大力掌握好Spark SQL和Structured Streaming。</p><p style="text-align:left;letter-spacing:.5px;"><span style="color:rgb(0,0,0);"><strong>那么是不是就不要深入学习Spark Core和Spark Streaming了呢？</strong></span></p><p style="text-align:left;letter-spacing:.5px;">答案是否定的！</p><p style="text-align:left;letter-spacing:.5px;"><span style="color:rgb(136,136,136);">Spark Core是Spark SQL的基石，所以很有必要掌握好Spark Core。</span></p><p style="text-align:left;letter-spacing:.5px;"><span style="color:rgb(255,104,39);">Spark SQL和Structured Streaming处理的是结构化数据，非结构化数据，还是需要Spark Core和Spark Streaming进行解析处理。</span></p><p style="text-align:left;letter-spacing:.5px;"><span style="color:rgb(136,136,136);">Structured Streaming 的功能还不够完善，限制颇多，比如多流join之后不能聚合等，所以Spark Streaming的给用户以灵活处理的接口还是有用武之地的。</span></p><p style="text-align:center;letter-spacing:.5px;"><span style="color:rgb(0,0,0);">推荐阅读：</span></p><p style="text-align:center;letter-spacing:.5px;"><span style="color:rgb(0,0,0);"><a href="http://mp.weixin.qq.com/s?__biz=MzA3MDY0NTMxOQ==&amp;mid=2247485810&amp;idx=1&amp;sn=178ce450fef932d27026dc5e2951ac3a&amp;chksm=9f38ea5aa84f634c91b518671e135a4761e9d0a0a0912488944653aac2baf66f83579540078b&amp;scene=21#wechat_redirect" rel="nofollow">大数据啊大数据！</a><br></span></p><p style="text-align:center;letter-spacing:.5px;"><a href="http://mp.weixin.qq.com/s?__biz=MzA3MDY0NTMxOQ==&amp;mid=2247485792&amp;idx=1&amp;sn=89df7bad8652930a01256fd4acce02ea&amp;chksm=9f38ea48a84f635edcd52427da14466bc0224cce3c025206c616b15cb02c3fb94eb0bcdc8f49&amp;scene=21#wechat_redirect" rel="nofollow">如何成为一个优秀的工程师？</a><br></p><p style="text-align:center;letter-spacing:.5px;"><a href="http://mp.weixin.qq.com/s?__biz=MzA3MDY0NTMxOQ==&amp;mid=2247485791&amp;idx=1&amp;sn=52ab8a2ab64c82f1c70287d0a69bf604&amp;chksm=9f38ea77a84f636155f724ad9a94486ede70ffbc3ff4f3de4a99cb8092a797a9e2fa91df707b&amp;scene=21#wechat_redirect" rel="nofollow">解惑:这个SPARK任务是数据倾斜了吗？</a><br></p><p style="text-align:center;"><img style="width:286px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFWF1rkKibTzeA8PicbicYXBsH26a9PXg2HNnlEt1thHBFxUtEjicACeaSlRWictpPziaMdibXmYq34dWfQ9w/640" alt="640"></p><p style="text-align:center;">感谢点赞，转发！</p>
                </div>
              </div>
                </div>