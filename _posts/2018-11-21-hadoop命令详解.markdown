---
layout:     post
title:      hadoop命令详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>本文转载自：<a href="http://www.cnblogs.com/linjiqin/p/3147856.html" rel="nofollow">http://www.cnblogs.com/linjiqin/p/3147856.html</a></p>
<p></p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<strong>一、用户命令</strong><br><span style="line-height:1.5;color:rgb(255,0,0);">1、archive命令</span></p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
(1).什么是Hadoop archives?<br>
Hadoop archives是特殊的档案格式。一个Hadoop archive对应一个文件系统目录。 Hadoop archive的扩展名是*.har。Hadoop archive包含元数据(形式是_index和_masterindx)和数据文件(part-*)。_index文件包含了档案中的文件的文件名和位置信息。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
(2).如何创建archive?<br>
用法：hadoop archive -archiveName NAME &lt;src&gt;* &lt;dest&gt;<br>
命令选项：<br>
-archiveName NAME 要创建的档案的名字。<br>
src 源文件系统的路径名。<br>
dest 保存档案文件的目标目录。<br>
范例：<br>
例1.将/user/hadoop/dir1和/user/hadoop/dir2归档到/user/zoo/文件系统目录下–/user/zoo/foo.har。<br>
hadoop@ubuntu:~/ hadoop archive -archiveName foo.har /user/hadoop/dir1 /user/hadoop/dir2 /user/zoo/<br>
当创建archive时，源文件foo.har不会被更改或删除。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
(3).如何查看archives中的文件?<br>
archive作为文件系统层暴露给外界。所以所有的fs shell命令都能在archive上运行，但是要使用不同的URI。另外，archive是不可改变的。所以创建、重命名和删除都会返回错误。Hadoop Archives的URI是har://scheme-hostname:port/archivepath/fileinarchive。<br>
如果没提供scheme-hostname，它会使用默认的文件系统。这种情况下URI是这种形式har:///archivepath/fileinarchive。<br>
范例：<br>
例1.archive的输入是/dir，该dir目录包含文件filea和fileb，现把/dir归档到/user/hadoop/foo.bar。<br>
hadoop@ubuntu:~/ hadoop archive -archiveName foo.har /dir /user/hadoop<br>
例2.获得创建的archive中的文件列表<br>
hadoop@ubuntu:~/hadoop dfs -lsr har:///user/hadoop/foo.har<br>
例3.查看archive中的filea文件<br>
hadoop@ubuntu:~/hadoop dfs -cat har:///user/hadoop/foo.har/dir/filea</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">2、distcp</span><br>
说明：用于集群内部或者集群之间拷贝数据的常用命令(顾名思义: dist即分布式，分布式拷贝是也。)。<br>
用法：hadoop distcp [选项] src_url dest_url <br>
命令选项：<br>
-m 表示启用多少map<br>
-delete 删除已经存在的目标文件，不会删除源文件。这个删除是通过FS Shell实现的。所以如果垃圾回收机制启动的话，删除的目标文件会进入trash。<br>
-i 忽略失败。这个选项会比默认情况提供关于拷贝的更精确的统计，同时它还将保留失败拷贝操作的日志，这些日志信息可以用于调试。最后，如果一个map失败了，但并没完成所有分块任务的尝试，这不会导致整个作业的失败。<br>
-overwrite 覆盖目标。如果一个map失败并且没有使用-i选项，不仅仅那些拷贝失败的文件，这个分块任务中的所有文件都会被重新拷贝。 所以这就是为什么要使用-i参数。<br>
范例：<br>
例1.<br>
hadoop@ubuntu:~/ hadoop distcp "hdfs://A:8020/user/foo/bar" "hdfs://B:8020/user/foo/baz"</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">3、fs</span><br>
说明：运行一个常规的文件系统客户端。<br>
用法：hadoop fs [GENERIC_OPTIONS] [COMMAND_OPTIONS]<br>
各种命令选项可以参考HDFS Shell指南。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">4、fsck</span><br>
说明：用来检查整个文件系统的健康状况，但是要注意它不会主动恢复备份缺失的block，这个是由NameNode单独的线程异步处理的。<br>
用法：hadoop fsck [GENERIC_OPTIONS] [-move|-delete|-openforwrite] [-files [-blocks [-locations | -racks]]]<br>
参数选项：<br>
&lt;path&gt; 检查这个目录中的文件是否完整<br>
-move 破损的文件移至/lost+found目录<br>
-delete 删除破损的文件<br>
-openforwrite 打印正在打开写操作的文件<br>
-files 打印正在check的文件名<br>
-blocks 打印block报告(需要和-files参数一起使用)<br>
-locations 打印每个block的位置信息(需要和-files参数一起使用)<br>
-racks 打印位置信息的网络拓扑图(需要和-files参数一起使用)<br>
范例：<br>
例1.<br>
hadoop@ubuntu:~/hadoop-1.1.1/bin$ hadoop fsck /<br>
Warning: $HADOOP_HOME is deprecated.</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
FSCK started by hadoop from /192.168.11.156 for path / at Sat Dec 29 19:33:40 PST 2012<br>
.Status: HEALTHY<br>
Total size: 4 B<br>
Total dirs: 9<br>
Total files: 1<br>
Total blocks (validated): 1 (avg. block size 4 B)<br>
Minimally replicated blocks: 1 (100.0 %)<br>
Over-replicated blocks: 0 (0.0 %)<br>
Under-replicated blocks: 0 (0.0 %)<br>
Mis-replicated blocks: 0 (0.0 %)<br>
Default replication factor: 1 #缺省的备份参数1<br>
Average block replication: 1.0<br>
Corrupt blocks: 0 #破损的block数0<br>
Missing replicas: 0 (0.0 %)<br>
Number of data-nodes: 1<br>
Number of racks: 1<br>
FSCK ended at Sat Dec 29 19:33:40 PST 2012 in 4 milliseconds</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<br>
The filesystem under path '/' is HEALTHY<br>
hadoop@ubuntu:~/hadoop-1.1.1/bin$</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">5、jar</span><br>
说明：运行jar文件。用户可以把他们的Map Reduce代码捆绑到jar文件中，使用这个命令执行。<br>
用法：hadoop jar &lt;jar&gt; [mainClass] args…<br>
范例：<br>
例1.在集群上运行Map Reduce程序，以WordCount程序为例<br>
hadoop jar /home/hadoop/hadoop-1.1.1/hadoop-examples.jar wordcount input output<br>
描述：<br>
hadoop jar：执行jar命令<br>
/home/hadoop/hadoop-1.1.1/hadoop-examples.jar: WordCount所在jar<br>
wordcount：程序主类名<br>
input output：输入输出文件夹</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">6、job</span><br>
说明：用于和Map Reduce作业交互和命令。<br>
用法：hadoop job [GENERIC_OPTIONS] [-submit ] | [-status ] | [-counter ] | [-kill ] | [-events &lt;#-of-events&gt;] | [-history [all] ] | [-list [all]] | [-kill-task ] | [-fail-task ]<br>
参数选项：<br>
-submit &lt;job-file&gt;：提交作业<br>
-status &lt;job-id&gt;：打印map和reduce完成百分比和所有计数器。<br>
-counter &lt;job-id&gt; &lt;group-name&gt; &lt;counter-name&gt;：打印计数器的值。<br>
-kill &lt;job-id&gt;：杀死指定作业。<br>
-events &lt;job-id&gt; &lt;from-event-#&gt; &lt;#-of-events&gt;：打印给定范围内jobtracker接收到的事件细节。<br>
-history [all] ：-history 打印作业的细节、失败及被杀死原因的细节。更多的关于一个作业的细节比如成功的任务，做过的任务尝试等信息可以通过指定[all]选项查看。 <br>
-list [all]：-list all显示所有作业。-list只显示将要完成的作业。<br>
-kill-task &lt;task-id&gt;：杀死任务。被杀死的任务不会不利于失败尝试。<br>
-fail-task &lt;task-id&gt;：使任务失败。被失败的任务会对失败尝试不利。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">7、pipes</span><br>
说明：运行pipes作业。<br>
用法：hadoop pipes [-conf ] [-jobconf , , ...] [-input ] [-output ] [-jar ] [-inputformat ] [-map ] [-partitioner ] [-reduce ] [-writer ] [-program ] [-reduces ]<br>
参数选项：<br>
-conf &lt;path&gt;：作业的配置<br>
-jobconf &lt;key=value&gt;, &lt;key=value&gt;, …：增加/覆盖作业的配置项<br>
-input &lt;path&gt;：输入目录<br>
-output &lt;path&gt;：输出目录<br>
-jar &lt;jar file&gt;：Jar文件名<br>
-inputformat &lt;class&gt;：InputFormat类<br>
-map &lt;class&gt;：Java Map类<br>
-partitioner &lt;class&gt;：Java Partitioner<br>
-reduce &lt;class&gt;：Java Reduce类<br>
-writer &lt;class&gt;：Java RecordWriter<br>
-program &lt;executable&gt;：可执行程序的URI<br>
-reduces &lt;num&gt;：reduce个数</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">8、version</span><br>
说明：打印版本信息。<br>
用法：hadoop version</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">9、CLASSNAME</span><br>
说明：hadoop脚本可用于调用任何类。<br>
用法：hadoop CLASSNAME<br>
描述：运行名字为CLASSNAME的类。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<strong>二、管理命令</strong><br>
hadoop集群管理员常用的命令。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">1、balancer</span><br>
说明：运行集群平衡工具。管理员可以简单的按Ctrl-C来停止平衡过程。<br>
用法：hadoop balancer [-threshold ]<br>
参数选项：<br>
-threshold &lt;threshold&gt;：磁盘容量的百分比。这会覆盖缺省的阀值。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">2、daemonlog</span><br>
说明：获取或设置每个守护进程的日志级别。<br>
用法：hadoop daemonlog -getlevel &lt;host:port&gt; &lt;name&gt;<br>
用法：hadoop daemonlog -setlevel &lt;host:port&gt; &lt;name&gt; &lt;level&gt;<br>
参数选项：<br>
-getlevel &lt;host:port&gt; &lt;name&gt;：打印运行在&lt;host:port&gt;的守护进程的日志级别。这个命令内部会连接http://&lt;host:port&gt;/logLevel?log=&lt;name&gt;<br>
-setlevel &lt;host:port&gt; &lt;name&gt; &lt;level&gt;：设置运行在&lt;host:port&gt;的守护进程的日志级别。这个命令内部会连接http://&lt;host:port&gt;/logLevel?log=&lt;name&gt;</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">3、datanode</span><br>
说明：运行一个HDFS的datanode。<br>
用法：hadoop datanode [-rollback]<br>
参数选项：<br>
-rollback：将datanode回滚到前一个版本。这需要在停止datanode，分发老的hadoop版本之后使用。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">4、dfsadmin</span><br>
说明：运行一个HDFS的dfsadmin客户端。<br>
用法：hadoop dfsadmin [GENERIC_OPTIONS] [-report] [-safemode enter | leave | get | wait] [-refreshNodes] [-finalizeUpgrade] [-upgradeProgress status | details | force] [-metasave filename] [-setQuota ...] [-clrQuota ...] [-help [cmd]]<br>
参数选项：<br>
-report：报告文件系统的基本信息和统计信息。<br>
…</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">5、jobtracker</span><br>
说明：运行MapReduce job Tracker节点。<br>
用法：hadoop jobtracker</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">6、namenode</span><br>
说明：运行namenode。<br>
用法：hadoop namenode [-format] | [-upgrade] | [-rollback] | [-finalize] | [-importCheckpoint]<br>
参数选项：<br>
-format：格式化namenode。它启动namenode，格式化namenode，之后关闭namenode。<br>
-upgrade：分发新版本的hadoop后，namenode应以upgrade选项启动。<br>
-rollback：将namenode回滚到前一版本。这个选项要在停止集群，分发老的hadoop版本后使用。 <br>
-finalize：finalize会删除文件系统的前一状态。最近的升级会被持久化，rollback选项将再不可用，升级终结操作之后，它会停掉namenode。<br>
-importCheckpoint 从检查点目录装载镜像并保存到当前检查点目录，检查点目录由fs.checkpoint.dir指定。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">7、secondarynamenode</span><br>
说明：运行HDFS的secondary namenode。<br>
用法：hadoop secondarynamenode [-checkpoint [force]] | [-geteditsize]<br>
参数选项：<br>
-checkpoint [force]：如果EditLog的大小 &gt;= fs.checkpoint.size，启动Secondary namenode的检查点过程。 如果使用了-force，将不考虑EditLog的大小。<br>
-geteditsize：打印EditLog大小。</p>
<p style="line-height:1.5;color:rgb(75,75,75);font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;">
<span style="line-height:1.5;color:rgb(255,0,0);">8、tasktracker</span><br>
说明：运行MapReduce的task Tracker节点。<br>
用法：hadoop tasktracker</p>
<br>            </div>
                </div>