---
layout:     post
title:      Spark的RDD简单操作
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：分享的快乐。。					https://blog.csdn.net/baolibin528/article/details/51405415				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<div style="font-family:'微软雅黑';font-size:14px;line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong></strong></span></div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;">
<div style="font-family:'微软雅黑';font-size:14px;line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>0、Spark的wc.note</strong></span></div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;">
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:9pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-html">
</code></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * hadoop
  * spark
  * tachyon
  * hadoop
  * hbase
  * spark
  */
/**
  * Created by Administrator on 2016/4/23.
  */
object rdd0_wc {
def main(args: Array[String]) {
//创建环境变量
val conf=new SparkConf().setMaster("local").setAppName("rdd0_wc")
//创建环境变量实例
val sc=new SparkContext(conf)
//读取文件
val data=sc.textFile("E://sparkdata//wc.txt")
    data.flatMap(_.split("\t")).map((_,1)).reduceByKey(_+_).collect().foreach(println)  //wrod计数
}
}</code></pre><br><br><br>
</div>
</td>
</tr></tbody></table><img src="https://img-blog.csdn.net/20160514131859818?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;"></div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;"><br style="background-color:inherit;"></div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;"><br style="background-color:inherit;"></div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;">
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>1、rdd1_aggregate.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div>aggregate用法：</div>
<div><br style="background-color:inherit;"></div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/23.
  */
object rdd1_aggregate {
/**
    * aggregate方法
    *  RDD是工作在Spark上，因此，parallelize方法是将内存数据读入Spark系统中，作为一个整体的数据集。
    * math.max方法用于比较数据集中数据的大小；
    * 第二个_+_方法是对传递的第一个比较方法结果进行处理。第一个比较结果是6，与中立空值相加，所以最终结果为6
    */
def main(args: Array[String]) {
val conf=new SparkConf().setMaster("local").setAppName("rdd1_aggregate")
val sc=new SparkContext(conf)
val arr=sc.parallelize(Array(1,2,3,4,5,6))
val result=arr.aggregate(0)(math.max(_, _), _ + _)  //aggregate用法
println(result)
  }

/**
    * 参数改变后
    * 这里parallelize将数据分成两个节点存储
    * math方法分别查找出两个数据集的最大值，分别是3和6.
    * 这样在调用aggregate方法的第二个计算方法时，将查找的数据值进行相加，获得最大值9
    */
//    def main(args: Array[String]) {
//      val conf=new SparkConf().setMaster("local").setAppName("rdd1_aggregate")
//      val sc=new SparkContext(conf)
//      val arr=sc.parallelize(Array(1,2,3,4,5,6),2)
//      val result=arr.aggregate(0)(math.max(_, _), _ + _)  //aggregate用法
//      println(result)
//    }

  /**
    * aggregate方法用于字符串
    */
//    def main(args: Array[String]) {
//      val conf=new SparkConf().setMaster("local").setAppName("rdd1_aggregate")
//      val sc=new SparkContext(conf)
//      val arr=sc.parallelize(Array("hadoop","spark","hive","hbase","kafka"))
//      val result=arr.aggregate("")((value,word)=&gt;value+"、"+word,_+_)  //aggregate用法
//      println(result)
//  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div>示例一结果：</div>
<div>（<span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;"> </span><span style="background-color:inherit;color:rgb(0,0,128);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;"><strong>val </strong></span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">arr=sc.parallelize(</span><span style="background-color:inherit;font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;"><em>Array</em></span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">(</span><span style="background-color:inherit;color:rgb(0,0,255);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">1</span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">,</span><span style="background-color:inherit;color:rgb(0,0,255);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">2</span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">,</span><span style="background-color:inherit;color:rgb(0,0,255);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">3</span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">,</span><span style="background-color:inherit;color:rgb(0,0,255);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">4</span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">,</span><span style="background-color:inherit;color:rgb(0,0,255);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">5</span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">,</span><span style="background-color:inherit;color:rgb(0,0,255);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">6</span><span style="font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;">))</span><span style="font-size:10.5pt;">）</span></div>
<div></div>
<div><img src="https://img-blog.csdn.net/20160514131906361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div>示例二结果：</div>
<div>（<span style="background-color:inherit;color:rgb(255,0,0);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;"><em>val arr=sc.parallelize(Array(1,2,3,4,5,6),2)</em></span><span style="font-size:10.5pt;">）</span></div>
<div><br style="background-color:inherit;"></div>
<div><img src="https://img-blog.csdn.net/20160514131910131?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div>示例三结果：</div>
<div>（<span style="background-color:inherit;color:rgb(255,0,0);font-family:Monaco, Consolas, Courier, 'Lucida Console', monospace;font-size:10.5pt;"><em>val arr=sc.parallelize(Array("hadoop","spark","hive","hbase","kafka"))</em></span><span style="font-size:10.5pt;">）</span></div>
<div></div>
<div><img src="https://img-blog.csdn.net/20160514131913740?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
</div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;">
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>2、rdd2_cache.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>cache()简单用法：</div>
<div><br style="background-color:inherit;"></div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  * cache方法的作用是将数据内容计算并保存在计算节点的内存中，这个方法的使用是针对Spark的Lazy数据处理模式。
  * 在Lazy模式中，数据在编译和使用时是不进行计算的，而仅仅保存其存储地址，只有Action方法到来时才正式计算。
  * 这样做的好处是可以极大的减少存储空间，从而提高利用率，而有时必须要求数据进行计算，此时就需要使用cache方法。
  */
object rdd2_cache {
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd2_cache") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array("hadoop","spark","hive"))  //设定数据集
println(arr)  //打印结果
val arr1=arr.cache()
println("-----------------------------")  //分隔符
arr1.foreach(println)  //专门用来打印未进行Action操作的数据的专用方法，可以对数据进行提早计算。
}
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><img src="https://img-blog.csdn.net/20160514131917095?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
</div>
<div style="font-family:'微软雅黑';font-size:14px;line-height:21px;">
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>3、rdd3_cartesian.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  * 笛卡尔积操作cartsian方法
  */
object rdd3_cartesian {
/**
    * 此方法用于对不同的数组进行笛卡尔积操作，要求是数据集的长度必须相同，结果作为一个新的数据集返回
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd3_cartesian") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,2,3,4,5))  //创建第一个数组
val arr1=sc.parallelize(Array(6,7,8,9,10))  //创建第二个数组
val result=arr.cartesian(arr1)  //进行笛卡尔积计算
result.foreach(println) //打印结果
}
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table><img src="https://img-blog.csdn.net/20160514131920724?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>4、rdd4_coalesce.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd4_coalesce {
/**
    * coalesce方法是将已经存储的数据重新分片后再进行存储
    * 第一个参数是将数据重新分成的片数，布尔型数指的是将数据分成更小的片时使用。
    */
//  def main(args: Array[String]) {
//    val conf=new SparkConf()  //创建环境变量
//      .setMaster("local") //设置本地化处理
//      .setAppName("rdd4_coalesce") //设置名称
//    val sc=new SparkContext(conf)   //创建环境变量实例
//    val arr=sc.parallelize(Array(1,2,3,4,5,6))
//    val arr2=arr.coalesce(2,true) //将数据重新分区
//    val result=arr.aggregate(0)(math.max(_,_),_+_)  //计算数据值
//    println(result)
//    val result2=arr2.aggregate(0)(math.max(_,_),_+_)  //计算重新分区数据值
//    println(result2)
//  }

  /**
    * RDD中还有一个repartition方法与这个coalesce方法类似，均是将数据重新分区组合
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd4_coalesce") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,2,3,4,5,6))
val arr2=arr.repartition(3) //将数据分区
println(arr2.partitions.length) //打印分区结果
}
}</code></pre><br><span style="background-color:inherit;color:rgb(0,0,128);"><strong>package </strong></span>RddApi<br style="background-color:inherit;"><br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(0,0,128);"><strong>import </strong></span>org.apache.spark.{SparkConf, SparkContext}<br style="background-color:inherit;"><br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(255,0,0);"><em>/**<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>  * Created by Administrator on 2016/4/24.<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>  */<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>object </strong></span>rdd4_coalesce {<br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(255,0,0);"><em>/**<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    * coalesce方法是将已经存储的数据重新分片后再进行存储<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    * 第一个参数是将数据重新分成的片数，布尔型数指的是将数据分成更小的片时使用。<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    */<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//  def main(args: Array[String]) {<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    val conf=new SparkConf()  //创建环境变量<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//      .setMaster("local") //设置本地化处理<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//      .setAppName("rdd4_coalesce") //设置名称<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    val sc=new SparkContext(conf)   //创建环境变量实例<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    val arr=sc.parallelize(Array(1,2,3,4,5,6))<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    val arr2=arr.coalesce(2,true) //将数据重新分区<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    val result=arr.aggregate(0)(math.max(_,_),_+_)  //计算数据值<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    println(result)<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    val result2=arr2.aggregate(0)(math.max(_,_),_+_)  //计算重新分区数据值<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//    println(result2)<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>//  }<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em><br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>  /**<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    * RDD中还有一个repartition方法与这个coalesce方法类似，均是将数据重新分区组合<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    */<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>def </strong></span>main(args: Array[<span style="background-color:inherit;color:rgb(32,153,157);">String</span>]) {<br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>conf=<span style="background-color:inherit;color:rgb(0,0,128);"><strong>new </strong></span>SparkConf()  <span style="background-color:inherit;color:rgb(255,0,0);"><em>//创建环境变量<br style="background-color:inherit;"></em></span>.setMaster(<span style="background-color:inherit;color:rgb(0,128,0);"><strong>"local"</strong></span>) <span style="background-color:inherit;color:rgb(255,0,0);"><em>//设置本地化处理<br style="background-color:inherit;"></em></span>.setAppName(<span style="background-color:inherit;color:rgb(0,128,0);"><strong>"rdd4_coalesce"</strong></span>) <span style="background-color:inherit;color:rgb(255,0,0);"><em>//设置名称<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>sc=<span style="background-color:inherit;color:rgb(0,0,128);"><strong>new </strong></span>SparkContext(conf)   <span style="background-color:inherit;color:rgb(255,0,0);"><em>//创建环境变量实例<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>arr=sc.parallelize(<span style="background-color:inherit;"><em>Array</em></span>(<span style="background-color:inherit;color:rgb(0,0,255);">1</span>,<span style="background-color:inherit;color:rgb(0,0,255);">2</span>,<span style="background-color:inherit;color:rgb(0,0,255);">3</span>,<span style="background-color:inherit;color:rgb(0,0,255);">4</span>,<span style="background-color:inherit;color:rgb(0,0,255);">5</span>,<span style="background-color:inherit;color:rgb(0,0,255);">6</span>))<br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>arr2=arr.repartition(<span style="background-color:inherit;color:rgb(0,0,255);">3</span>) <span style="background-color:inherit;color:rgb(255,0,0);"><em>//将数据分区<br style="background-color:inherit;"></em></span><span style="background-color:inherit;"><em>println</em></span>(arr2.partitions.length) <span style="background-color:inherit;color:rgb(255,0,0);"><em>//打印分区结果<br style="background-color:inherit;"></em></span>}<br style="background-color:inherit;">}<br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><br style="background-color:inherit;"></div>
<div>示例一：</div>
<div></div>
<div><img src="https://img-blog.csdn.net/20160514131924736?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div><img src="https://img-blog.csdn.net/20160514131928303?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div>示例2：</div>
<div></div>
<div><img src="https://img-blog.csdn.net/20160514131931455?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>5、rdd5_countByValue.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);">countByValue用法：</pre>
</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd5_countByValue {
/**
    * countByValue方法是计算数据集中某个数据出现的个数，并将其以map的形式返回
    * @param args
*/
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd5_countByValue") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,1,2,2,3,3,4,5,6))
val result=arr.countByValue() //计算个数
result.foreach(print)
  }
}</code></pre><br><span style="background-color:inherit;color:rgb(0,0,128);"><strong>package </strong></span>RddApi<br style="background-color:inherit;"><br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(0,0,128);"><strong>import </strong></span>org.apache.spark.{SparkConf, SparkContext}<br style="background-color:inherit;"><br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(255,0,0);"><em>/**<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>  * Created by Administrator on 2016/4/24.<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>  */<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>object </strong></span>rdd5_countByValue {<br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(255,0,0);"><em>/**<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    * countByValue方法是计算数据集中某个数据出现的个数，并将其以map的形式返回<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>    * </em></span><span style="background-color:inherit;"><strong>@param args<br style="background-color:inherit;"></strong></span><span style="background-color:inherit;color:rgb(255,0,0);"><em>*/<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>def </strong></span>main(args: Array[<span style="background-color:inherit;color:rgb(32,153,157);">String</span>]) {<br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>conf=<span style="background-color:inherit;color:rgb(0,0,128);"><strong>new </strong></span>SparkConf()  <span style="background-color:inherit;color:rgb(255,0,0);"><em>//创建环境变量<br style="background-color:inherit;"></em></span>.setMaster(<span style="background-color:inherit;color:rgb(0,128,0);"><strong>"local"</strong></span>) <span style="background-color:inherit;color:rgb(255,0,0);"><em>//设置本地化处理<br style="background-color:inherit;"></em></span>.setAppName(<span style="background-color:inherit;color:rgb(0,128,0);"><strong>"rdd5_countByValue"</strong></span>) <span style="background-color:inherit;color:rgb(255,0,0);"><em>//设置名称<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>sc=<span style="background-color:inherit;color:rgb(0,0,128);"><strong>new </strong></span>SparkContext(conf)   <span style="background-color:inherit;color:rgb(255,0,0);"><em>//创建环境变量实例<br style="background-color:inherit;"></em></span><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>arr=sc.parallelize(<span style="background-color:inherit;"><em>Array</em></span>(<span style="background-color:inherit;color:rgb(0,0,255);">1</span>,<span style="background-color:inherit;color:rgb(0,0,255);">1</span>,<span style="background-color:inherit;color:rgb(0,0,255);">2</span>,<span style="background-color:inherit;color:rgb(0,0,255);">2</span>,<span style="background-color:inherit;color:rgb(0,0,255);">3</span>,<span style="background-color:inherit;color:rgb(0,0,255);">3</span>,<span style="background-color:inherit;color:rgb(0,0,255);">4</span>,<span style="background-color:inherit;color:rgb(0,0,255);">5</span>,<span style="background-color:inherit;color:rgb(0,0,255);">6</span>))<br style="background-color:inherit;"><span style="background-color:inherit;color:rgb(0,0,128);"><strong>val </strong></span>result=arr.countByValue() <span style="background-color:inherit;color:rgb(255,0,0);"><em>//计算个数<br style="background-color:inherit;"></em></span>result.foreach(<span style="background-color:inherit;"><em>print</em></span>)<br style="background-color:inherit;">  }<br style="background-color:inherit;">}<br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><br style="background-color:inherit;"></div>
<div><img src="https://img-blog.csdn.net/20160514131934971?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;">
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>6、rdd6_countByKey.note</strong></span></div>
<hr><div><br></div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd6_countByKey {
/**
    * countByKey方法与countByValue方法有本质的区别。
    * countByKey是计算数组中元数据键值对Key出现的个数
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd6_countByKey") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array((1,"hadoop"),(2,"spark"),(3,"tachyon")))
val result=arr.countByKey() //进行计数
result.foreach(print)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table><img src="https://img-blog.csdn.net/20160514131938486?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>7、rdd7_distinct.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>distinct方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd7_distinct {
/**
    * distinct方法作用是去除数据集中重复项
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd7_distinct") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(("hadoop"),("spark"),("hive"),("hadoop"),("hive")))
val result=arr.distinct()
    result.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table><img src="https://img-blog.csdn.net/20160514131942225?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>8、rdd8_filter.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>filter方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd8_filter {
/**
    * filter方法用来对数据集进行过滤
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd8_filter") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,2,3,4,5))
val result=arr.filter(_&gt;=3)
    result.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></td>
</tr></tbody></table></div>
<div><br style="background-color:inherit;"></div>
<div><img src="https://img-blog.csdn.net/20160514131945018?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>9、rdd9_flatMap.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>flatMap方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd9_flatMap {
def main(args: Array[String]) {
/**
      * flatMap方法是对RDD中的数据集进行整体操作的一个特殊方法。
      */
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd9_flatMap") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,2,3,4,5,6,7))
val result=arr.flatMap(x=&gt;List(x+1)).collect()  //进行数据集计算
result.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><img src="https://img-blog.csdn.net/20160514131948084?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>10、rdd10_map .note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>map方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd10_map {
/**
    * map方法是对RDD中的数据集中的数据进行逐个处理。
    * map与flatMap不同之处在于，flatMap是将数据集中的数据作为一个整体去处理，之后再对其中的数据做计算。
    * 而map方法直接对数据集中的数据做单独的处理。
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd10_map") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,2,3,4,5,6))
val result=arr.map(x=&gt;List(x+1)).collect()
    result.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><img src="https://img-blog.csdn.net/20160514131950596?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>11、rdd11_groupBy .note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>group方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd11_groupBy {
/**
    * groupBy方法是将传入的数据进行分组
    * 传入的第一个参数是方法名，第二个参数是分组的标签
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd11_groupBy") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array(1,2,3,4,5,6))
    arr.groupBy(myFilter1).foreach(println)  //设置第一个分组
}
def myFilter1(num:Int):Int={ //自定义第一个分组
num %2
}

}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table><br style="background-color:inherit;"><img src="https://img-blog.csdn.net/20160514131953865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>12、rdd12_keyBy .note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>keyBy方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd12_keyBy {
/**
    * keyBy方法是为数据集中的每个个体数据增加一个Key，从而可以和原来的数据集形成键值对。
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd12_keyBy") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr=sc.parallelize(Array("hadoop","spark","tachyon","hive","hbase"))
val str=arr.keyBy(word=&gt;word.size)  //设置配置方法
str.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><img src="https://img-blog.csdn.net/20160514131956393?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div></div>
<div><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>13、rdd13_reduce.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>reduce方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd13_reduce {
/**
    * reduce方法主要是对传入的数据进行合并处理。
    * 第一个下划线代表数据集中的第一个参数。
    * 第二个下划线在第一次合并处理时代表空集
    */
def main(args: Array[String]) {
//    val conf=new SparkConf()  //创建环境变量
//      .setMaster("local") //设置本地化处理
//      .setAppName("rdd13_reduce") //设置名称
//    val sc=new SparkContext(conf)   //创建环境变量实例
//    val arr=sc.parallelize(Array("hadoop","spark","tachyon","hive","hbase"))
//    val result=arr.reduce(_+_)
//    result.foreach(print)


    /**
      * 寻找最长字符串
      */
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd13_reduce") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val str=sc.parallelize(Array("hadoop","spark","tachyon","hive"))
val result=str.reduce(myFun)  //进行数据拟合
result.foreach(print)
  }
def myFun(str1:String,str2:String):String={
var str=str1
if(str2.size&gt;=str.size){
      str=str2
    }
return str
  }
}
</code></pre><br><br>
</div>
</td>
</tr></tbody></table></div>
<div><br style="background-color:inherit;"></div>
<div><br style="background-color:inherit;"></div>
<div>示例一：</div>
<div></div>
<div><img src="https://img-blog.csdn.net/20160514131959850?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div>示例二：</div>
<div></div>
<div><img src="https://img-blog.csdn.net/20160514132005674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>14、rdd14_sortBy.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>sortBy方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  */
object rdd14_sortBy {
/**
    * sortBy方法是一个常用的排序方法，其功能是对已有的RDD重新排序，并将重新排序后的数据生成一个新的RDD。
    * sortBy有3个参数：
    * 第一个参数：传入方法，用以计算排序的方法
    * 第二个参数：指定排序的值按升序还是降序显示
    * 第三个参数：分片的数量
    */
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd14_sortBy") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val str=sc.parallelize(Array((1,"hadoop"),(2,"spark"),(3,"hive"),(4,"hbase"),(5,"tachyon")))
val str1=str.sortBy(word=&gt;word._1,true)  //按照第一个数据排序
val str2=str.sortBy(word=&gt;word._2,true) //按照第二个数据排序
str1.foreach(println)
    str2.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table></div>
<div><br style="background-color:inherit;"></div>
<div>
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"><span style="background-color:inherit;"><em>按照第一个数据排序：</em></span></pre>
</div>
<div><img src="https://img-blog.csdn.net/20160514132008787?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></div>
<div><br style="background-color:inherit;"></div>
<div>
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"><span style="background-color:inherit;"><em>按照第二个数据排序：</em></span></pre>
</div>
<div><img src="https://img-blog.csdn.net/20160514132011628?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></div>
<br style="background-color:inherit;"></div>
<div style="line-height:2.5;"><span style="background-color:inherit;font-size:22px;"><strong>15、rdd15_zip.note</strong></span></div>
<hr><div><br style="background-color:inherit;"></div>
<div>zip方法：</div>
<div>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse:collapse;background-color:inherit;"><tbody style="background-color:inherit;"><tr style="background-color:inherit;"><td valign="top" style="border:1px solid rgb(153,153,153);min-height:25px;min-width:25px;background-color:inherit;width:579px;">
<div style="min-width:2px;background-color:inherit;">
<pre style="font-family:'宋体';font-size:10.5pt;background-color:rgb(255,255,255);"></pre><pre><code class="language-java">package RddApi

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Administrator on 2016/4/24.
  * zip方法是常用的合并压缩算法，它将若干个RDD压缩成一个新的DD，进而形成一系列的键值对存储形式的新RDD。
  */
object rdd15_zip {
def main(args: Array[String]) {
val conf=new SparkConf()  //创建环境变量
.setMaster("local") //设置本地化处理
.setAppName("rdd15_zip") //设置名称
val sc=new SparkContext(conf)   //创建环境变量实例
val arr1=sc.parallelize(Array(1,2,3,4,5,6))
val arr2=sc.parallelize(Array("a","b","c","d","e","f"))
val arr3=sc.parallelize(Array("g","h","i","j","k","l"))
val arr4=arr1.zip(arr2).zip(arr3) //进行压缩算法
arr4.foreach(println)
  }
}</code></pre><br><br style="background-color:inherit;">
<br style="background-color:inherit;"></div>
</td>
</tr></tbody></table><img src="https://img-blog.csdn.net/20160514132015787?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="background-color:inherit;"></div>
<div></div>
<div><br></div>
</div>
</div>
</div>
</div>
            </div>
                </div>