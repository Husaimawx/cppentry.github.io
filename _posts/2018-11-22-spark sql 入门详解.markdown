---
layout:     post
title:      spark sql 入门详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/bitcarmanlee/article/details/52006718				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2 id="1spark-sql简介">1.spark sql简介</h2>

<p>spark sql是为了处理结构化数据的一个spark 模块。不同于spark rdd的基本API，spark sql接口更多关于数据结构本身与执行计划等更多信息。在spark内部，sql sql利用这些信息去更好地进行优化。有如下几种方式执行spark sql：SQL，DataFramesAPI与Datasets API。当相同的计算引擎被用来执行一个计算时，有不同的API和语言种类可供选择。这种统一性意味着开发人员可以来回轻松切换各种最熟悉的API来完成同一个计算工作。</p>



<h2 id="2dataframe">2.DataFrame</h2>

<p>首先看看官网上对DataFrames的介绍： <br>
A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. <br>
The DataFrame API is available in Scala, Java, Python, and R.</p>

<p>翻译一把 <br>
DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造。 <br>
DataFrame可以使用的API包括Scala,Java,Python,R。</p>



<h2 id="3起始点sqlcontext">3.起始点：SQLContext</h2>

<p>spark sql所有功能的入口是SQLContext类，或者SQLContext的子类。为了创建一个基本的SQLContext，需要一个SparkContext。 <br>
接下来我们所有的操作与实验都是在spark-shell里进行的。首先cd到spark的目录里，执行./bin/spark-shell，将spark-shell 跑起来。spark-shell启动过程中，会输出这么一行：</p>



<pre class="prettyprint"><code class=" hljs cs">Spark context available <span class="hljs-keyword">as</span> sc.</code></pre>

<p>其实这就是告诉我们，spark-shell启动时候，已经帮我们初始化了一个可用的spark context并且叫sc。</p>



<pre class="prettyprint"><code class=" hljs avrasm">scala&gt; val sqlContext = new org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.SQLContext</span>(sc)
<span class="hljs-label">sqlContext:</span> org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.SQLContext</span> = org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.SQLContext</span><span class="hljs-localvars">@1943</span>a343

scala&gt; import sqlContext<span class="hljs-preprocessor">.implicits</span>._
import sqlContext<span class="hljs-preprocessor">.implicits</span>._</code></pre>



<h2 id="4创建dataframes">4.创建DataFrames</h2>



<pre class="prettyprint"><code class=" hljs avrasm">val df = sqlContext<span class="hljs-preprocessor">.read</span><span class="hljs-preprocessor">.json</span>(<span class="hljs-string">"file:///data/wanglei/soft/spark-1.6.0-bin-hadoop2.4/examples/src/main/resources/people.json"</span>)</code></pre>

<p>吧啦吧啦输出一堆东西以后，最后一行：</p>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">df:</span> org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.DataFrame</span> = [age: bigint, name: string]</code></pre>

<p>可以看到一个DataFrame已经生成！</p>

<p>注意的是： <br>
官网上面这一行是这么写的：</p>



<pre class="prettyprint"><code class=" hljs avrasm">val df = sqlContext<span class="hljs-preprocessor">.read</span><span class="hljs-preprocessor">.json</span>(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)</code></pre>

<p>根据代码猜测，这样写应该是读取本地相对路径的文件。但是不知道为什么，在我的环境里这么写不行，只能用file的方式指定绝对路径。或许是哪个配置项没有配正确。</p>



<h2 id="5操作dataframe">5.操作DataFrame</h2>

<p>DataFrames提供了特定的语言来操作结构化数据，包括scala，java，python，R。 <br>
下面我们来一些基本的操作</p>



<pre class="prettyprint"><code class=" hljs lasso">scala<span class="hljs-subst">&gt;</span> val sqlContext <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> org<span class="hljs-built_in">.</span>apache<span class="hljs-built_in">.</span>spark<span class="hljs-built_in">.</span>sql<span class="hljs-built_in">.</span>SQLContext(sc)
sqlContext: org<span class="hljs-built_in">.</span>apache<span class="hljs-built_in">.</span>spark<span class="hljs-built_in">.</span>sql<span class="hljs-built_in">.</span>SQLContext <span class="hljs-subst">=</span> org<span class="hljs-built_in">.</span>apache<span class="hljs-built_in">.</span>spark<span class="hljs-built_in">.</span>sql<span class="hljs-built_in">.</span>SQLContext@<span class="hljs-number">1943</span>a343

scala<span class="hljs-subst">&gt;</span> <span class="hljs-keyword">import</span> sqlContext<span class="hljs-built_in">.</span>implicits<span class="hljs-built_in">.</span>_
<span class="hljs-keyword">import</span> sqlContext<span class="hljs-built_in">.</span>implicits<span class="hljs-built_in">.</span>_

<span class="hljs-comment">// Show the content of the DataFrame</span>
df<span class="hljs-built_in">.</span>show()
<span class="hljs-comment">// age  name</span>
<span class="hljs-comment">// null Michael</span>
<span class="hljs-comment">// 30   Andy</span>
<span class="hljs-comment">// 19   Justin</span>

<span class="hljs-comment">// Print the schema in a tree format</span>
df<span class="hljs-built_in">.</span>printSchema()
<span class="hljs-comment">// root</span>
<span class="hljs-comment">// |-- age: long (nullable = true)</span>
<span class="hljs-comment">// |-- name: string (nullable = true)</span>

<span class="hljs-comment">// Select only the "name" column</span>
df<span class="hljs-built_in">.</span><span class="hljs-keyword">select</span>(<span class="hljs-string">"name"</span>)<span class="hljs-built_in">.</span>show()
<span class="hljs-comment">// name</span>
<span class="hljs-comment">// Michael</span>
<span class="hljs-comment">// Andy</span>
<span class="hljs-comment">// Justin</span>

<span class="hljs-comment">// Select everybody, but increment the age by 1</span>
df<span class="hljs-built_in">.</span><span class="hljs-keyword">select</span>(df(<span class="hljs-string">"name"</span>), df(<span class="hljs-string">"age"</span>) <span class="hljs-subst">+</span> <span class="hljs-number">1</span>)<span class="hljs-built_in">.</span>show()
<span class="hljs-comment">// name    (age + 1)</span>
<span class="hljs-comment">// Michael null</span>
<span class="hljs-comment">// Andy    31</span>
<span class="hljs-comment">// Justin  20</span>

<span class="hljs-comment">// Select people older than 21</span>
df<span class="hljs-built_in">.</span>filter(df(<span class="hljs-string">"age"</span>) <span class="hljs-subst">&gt;</span> <span class="hljs-number">21</span>)<span class="hljs-built_in">.</span>show()
<span class="hljs-comment">// age name</span>
<span class="hljs-comment">// 30  Andy</span>

<span class="hljs-comment">// Count people by age</span>
df<span class="hljs-built_in">.</span>groupBy(<span class="hljs-string">"age"</span>)<span class="hljs-built_in">.</span>count()<span class="hljs-built_in">.</span>show()
<span class="hljs-comment">// age  count</span>
<span class="hljs-comment">// null 1</span>
<span class="hljs-comment">// 19   1</span>
<span class="hljs-comment">// 30   1</span></code></pre>



<h2 id="6根据反射推断schema">6.根据反射推断schema</h2>

<p>Spark SQL的Scala接口支持将包括case class数据的RDD转换成DataFrame。 <br>
case class定义表的schema，case class的属性会被读取并且成为列的名字，这里case class也可以被当成别的case class的属性或者是复杂的类型，比如Sequence或Array。RDD会被隐式转换成DataFrame并且被注册成一个表，这个表可以被用在查询语句中。</p>



<pre class="prettyprint"><code class=" hljs avrasm">// sc is an existing SparkContext.
val sqlContext = new org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.SQLContext</span>(sc)
// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext<span class="hljs-preprocessor">.implicits</span>._

// Define the schema using a case class.
// Note: Case classes <span class="hljs-keyword">in</span> Scala <span class="hljs-number">2.10</span> can support only up to <span class="hljs-number">22</span> fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class Person(name: String, age: Int)

// Create an RDD of Person objects <span class="hljs-keyword">and</span> register it as a table.
scala&gt; val people = sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"file:///data/wanglei/soft/spark-1.6.0-bin-hadoop2.4/examples/src/main/resources/people.txt"</span>)<span class="hljs-preprocessor">.map</span>(_<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">","</span>))<span class="hljs-preprocessor">.map</span>(p =&gt; Person(p(<span class="hljs-number">0</span>), p(<span class="hljs-number">1</span>)<span class="hljs-preprocessor">.trim</span><span class="hljs-preprocessor">.toInt</span>))<span class="hljs-preprocessor">.toDF</span>()
//执行完毕以后，最后一行会输出如下：
//people: org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.DataFrame</span> = [name: string, age: int]
people<span class="hljs-preprocessor">.registerTempTable</span>(<span class="hljs-string">"people"</span>)

// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext<span class="hljs-preprocessor">.sql</span>(<span class="hljs-string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)

// The results of SQL queries are DataFrames <span class="hljs-keyword">and</span> support all the normal RDD operations.
// The columns of a row <span class="hljs-keyword">in</span> the result can be accessed by field index:
teenagers<span class="hljs-preprocessor">.map</span>(t =&gt; <span class="hljs-string">"Name: "</span> + t(<span class="hljs-number">0</span>))<span class="hljs-preprocessor">.collect</span>()<span class="hljs-preprocessor">.foreach</span>(println)

// <span class="hljs-keyword">or</span> by field name:
teenagers<span class="hljs-preprocessor">.map</span>(t =&gt; <span class="hljs-string">"Name: "</span> + t<span class="hljs-preprocessor">.getAs</span>[String](<span class="hljs-string">"name"</span>))<span class="hljs-preprocessor">.collect</span>()<span class="hljs-preprocessor">.foreach</span>(println)

// row<span class="hljs-preprocessor">.getValuesMap</span>[T] retrieves multiple columns at once into a Map[String, T]
teenagers<span class="hljs-preprocessor">.map</span>(_<span class="hljs-preprocessor">.getValuesMap</span>[Any](List(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>)))<span class="hljs-preprocessor">.collect</span>()<span class="hljs-preprocessor">.foreach</span>(println)
// Map(<span class="hljs-string">"name"</span> -&gt; <span class="hljs-string">"Justin"</span>, <span class="hljs-string">"age"</span> -&gt; <span class="hljs-number">19</span>)</code></pre>



<h2 id="7使用programmatically的方式指定schema">7.使用Programmatically的方式指定Schema</h2>

<p>当case class不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。 <br>
1.从原来的 RDD 创建一个行的 RDD <br>
2.创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配 <br>
3.在行 RDD 上通过 applySchema 方法应用模式</p>



<pre class="prettyprint"><code class=" hljs avrasm">// Create an RDD
scala&gt; val people = sc<span class="hljs-preprocessor">.textFile</span>(<span class="hljs-string">"file:///data/wanglei/soft/spark-1.6.0-bin-hadoop2.4/examples/src/main/resources/people.txt"</span>)

// The schema is encoded <span class="hljs-keyword">in</span> a string
val schemaString = <span class="hljs-string">"name age"</span>

// Import Row.
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.Row</span><span class="hljs-comment">;</span>

// Import Spark SQL data types
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.sql</span><span class="hljs-preprocessor">.types</span>.{StructType,StructField,StringType}<span class="hljs-comment">;</span>

// Generate the schema based on the string of schema
val schema =
  StructType(
    schemaString<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">" "</span>)<span class="hljs-preprocessor">.map</span>(fieldName =&gt; StructField(fieldName, StringType, true)))

// Convert records of the RDD (people) to Rows.
val rowRDD = people<span class="hljs-preprocessor">.map</span>(_<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">","</span>))<span class="hljs-preprocessor">.map</span>(p =&gt; Row(p(<span class="hljs-number">0</span>), p(<span class="hljs-number">1</span>)<span class="hljs-preprocessor">.trim</span>))

// Apply the schema to the RDD.
val peopleDataFrame = sqlContext<span class="hljs-preprocessor">.createDataFrame</span>(rowRDD, schema)

// Register the DataFrames as a table.
peopleDataFrame<span class="hljs-preprocessor">.registerTempTable</span>(<span class="hljs-string">"people"</span>)

// SQL statements can be run by using the sql methods provided by sqlContext.
val results = sqlContext<span class="hljs-preprocessor">.sql</span>(<span class="hljs-string">"SELECT name FROM people"</span>)

// The results of SQL queries are DataFrames <span class="hljs-keyword">and</span> support all the normal RDD operations.
// The columns of a row <span class="hljs-keyword">in</span> the result can be accessed by field index <span class="hljs-keyword">or</span> by field name.
results<span class="hljs-preprocessor">.map</span>(t =&gt; <span class="hljs-string">"Name: "</span> + t(<span class="hljs-number">0</span>))<span class="hljs-preprocessor">.collect</span>()<span class="hljs-preprocessor">.foreach</span>(println)</code></pre>



<h2 id="8spark-sql整体文档结构">8.spark sql整体文档结构</h2>

<p>最后给大家截个图，可以清晰看到spark sql的整体文档结构 <br>
<img src="https://img-blog.csdn.net/20160723222735013" alt="这里写图片描述" title=""></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>