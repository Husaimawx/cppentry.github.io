---
layout:     post
title:      Spark概述
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1 id="spark-overview" style="margin-left:0px;"><strong><span style="color:#1d1f22;">Spark概述</span></strong></h1>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Apache Spark是一种快速通用的集群计算系统。它提供Java，Scala，Python和R中的高级API，以及支持通用执行图的优化引擎。它还支持一组丰富的更高级别的工具，包括Spark <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="nofollow">SQL</a>用于SQL和结构化数据的处理，<a href="http://spark.apache.org/docs/latest/ml-guide.html" rel="nofollow">MLlib</a>机器学习，<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" rel="nofollow">GraphX</a>用于图形处理和Spark Stream。</span></p>

<h1 id="downloading" style="margin-left:0px;"><strong><span style="color:#1d1f22;">下载</span></strong></h1>

<p style="margin-left:0px;"><span style="color:#1d1f22;">从项目网站的<a href="http://spark.apache.org/downloads.html" rel="nofollow">下载页面</a>获取Spark 。本文档适用于Spark版本2.3.1。Spark使用Hadoop的客户端库来实现HDFS和YARN。下载是针对少数流行的Hadoop版本预先打包的。用户还可以<a href="http://spark.apache.org/docs/latest/hadoop-provided.html" rel="nofollow">通过增加Spark的类路径</a>下载“Hadoop免费”二进制文件并使用任何Hadoop版本运行Spark 。Scala和Java用户可以使用Maven坐标在他们的项目中包含Spark，并且将来Python用户也可以从PyPI安装Spark。</span></p>

<p style="margin-left:0px;"><span style="color:#1d1f22;">如果您想从源代码构建Spark，请访问<a href="http://spark.apache.org/docs/latest/building-spark.html" rel="nofollow">Building Spark</a>。</span></p>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Spark在Windows和类UNIX系统（例如Linux，Mac OS）上运行。在一台机器上本地运行很容易 - 您只需要<code>java</code>在系统上安装<code>PATH</code>，或者<code>JAVA_HOME</code>指向Java安装的环境变量。</span></p>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Spark运行在Java 8 +，Python 2.7 + / 3.4 +和R 3.1+上。对于Scala API，Spark 2.3.1使用Scala 2.11。您需要使用兼容的Scala版本（2.11.x）。</span></p>

<p style="margin-left:0px;"><span style="color:#1d1f22;">请注意，自Spark 2.2.0起，对2.6.5之前的Java 7，Python 2.6和旧Hadoop版本的支持已被删除。自2.3.0起，对Scala 2.10的支持被删除。</span></p>

<h1 id="running-the-examples-and-shell" style="margin-left:0px;"><strong><span style="color:#1d1f22;">运行示例和Shell</span></strong></h1>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Spark附带了几个示例程序。Scala，Java，Python和R示例都在 <code>examples/src/main</code>目录中。要运行其中一个Java或Scala示例程序，请 <code>bin/run-example &lt;class&gt; [params]</code>在顶级Spark目录中使用。（在幕后，这将调用用于启动应用程序的更通用的 <a href="http://spark.apache.org/docs/latest/submitting-applications.html" rel="nofollow"><code>spark-submit</code>脚本</a>）。例如，</span></p>

<pre style="margin-left:0px;">
<span style="color:#333333;"><code>./bin/run-example SparkPi 10
</code></span></pre>

<p style="margin-left:0px;"><span style="color:#1d1f22;">您还可以通过Scala shell的修改版本以交互方式运行Spark。这是学习框架的好方法。</span></p>

<pre style="margin-left:0px;">
<span style="color:#333333;"><code>./bin/spark-shell --master local[2]
</code></span></pre>

<p style="margin-left:0px;"><span style="color:#1d1f22;">该<code>--master</code>选项指定<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" rel="nofollow">分布式集群</a>的 <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" rel="nofollow">主URL</a>，或<code>local</code>使用一个线程<code>local[N]</code>在本地运行，或使用N个线程在本地运行。您应该从使用<code>local</code>测试开始。有关选项的完整列表，请使用该<code>--help</code>选项运行Spark shell 。</span></p>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Spark还提供了Python API。要在Python解释器中以交互方式运行Spark，请使用 <code>bin/pyspark</code>：</span></p>

<pre style="margin-left:0px;">
<span style="color:#333333;"><code>./bin/pyspark --master local[2]
</code></span></pre>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Python中也提供了示例应用程序。例如，</span></p>

<pre style="margin-left:0px;">
<span style="color:#333333;"><code>./bin/spark-submit examples/src/main/python/pi.py 10
</code></span></pre>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Spark还提供了自1.4以来的实验性<a href="http://spark.apache.org/docs/latest/sparkr.html" rel="nofollow">R API</a>（仅包括DataFrames API）。要在R解释器中以交互方式运行Spark，请使用<code>bin/sparkR</code>：</span></p>

<pre style="margin-left:0px;">
<span style="color:#333333;"><code>./bin/sparkR --master local[2]
</code></span></pre>

<p style="margin-left:0px;"><span style="color:#1d1f22;">R中也提供了示例应用程序。例如，</span></p>

<pre style="margin-left:0px;">
<span style="color:#333333;"><code>./bin/spark-submit examples/src/main/r/dataframe.R
</code></span></pre>

<h1 id="launching-on-a-cluster" style="margin-left:0px;"><strong><span style="color:#1d1f22;">在群集上启动</span></strong></h1>

<p style="margin-left:0px;"><span style="color:#1d1f22;">Spark <a href="http://spark.apache.org/docs/latest/cluster-overview.html" rel="nofollow">群集模式概述</a>说明了在群集上运行的关键概念。Spark既可以自己运行，也可以通过几个现有的集群管理器运行。它目前提供了几种部署选项：</span></p>

<ul style="margin-left:25px;"><li><a href="http://spark.apache.org/docs/latest/spark-standalone.html" rel="nofollow">独立部署模式</a>：在专用群集上部署Spark的最简单方法</li>
	<li><a href="http://spark.apache.org/docs/latest/running-on-mesos.html" rel="nofollow">Apache Mesos</a></li>
	<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" rel="nofollow">Hadoop YARN</a></li>
	<li><a href="http://spark.apache.org/docs/latest/running-on-kubernetes.html" rel="nofollow">Kubernetes</a></li>
</ul>            </div>
                </div>