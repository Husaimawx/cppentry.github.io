---
layout:     post
title:      第3章 实战环境搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1>3-1课程目录</h1>

<h2>实战环境搭建</h2>

<h2>Spark 源码编译 Spark环境搭建 Spark 简单使用</h2>

<h1>3-2 -Spark源码编译</h1>

<p><strong>1、下载到官网（源码编译版本）（<a href="http://spark.apache.org/downloads.html" rel="nofollow">http://spark.apache.org/downloads.html</a>）</strong></p>

<p>wget <a href="https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0.tgz" rel="nofollow">https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0.tgz</a></p>

<p><strong>2、编译步骤</strong></p>

<p><a href="http://spark.apache.org/docs/latest/building-spark.html" rel="nofollow">http://spark.apache.org/docs/latest/building-spark.html</a></p>

<p><strong>前置要求</strong></p>

<p>1）The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.3.9 or newer and Java 8+. Note that support for Java 7 was removed as of Spark 2.2.0.</p>

<p>2） export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"</p>

<p><strong>mvn编译命令</strong></p>

<p>./build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package</p>

<p>前提：需要对maven有一定了解</p>

<p>./build/mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package</p>

<p><strong>spark源码编译</strong></p>

<p>mvn编译 make-distribution.sh</p>

<h1>3-3 补录：Spark源码编译中的坑</h1>

<p>1、</p>

<p>./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes</p>

<p>建议：阿里云的机器，可能内存不足，建议使用虚拟机2-4G</p>

<p> </p>

<h1>3-4 Spark Local模式环境搭建</h1>

<h2>Spark环境搭建</h2>

<h2>Local模式</h2>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813100230556?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813100317277?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h1>3-5 Spark Standalone模式环境搭建</h1>

<p>Spark Standalone模式架构和hadoop 、HDFS/ YARN 和类似的</p>

<p>1 master+n worker</p>

<p>spark-env.sh</p>

<p></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813100439777?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813100825688?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813101146779?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>hadoop1：master</p>

<p>hadoop2:worker</p>

<p>hadoop3:worker</p>

<p>hadoop4:worker</p>

<p></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813101220853?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h1>3-6 Spark简单使用</h1>

<p><strong>Spark简单使用</strong></p>

<p><strong>使用Spark完成wordcount统计</strong></p>

<p></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180813102355548?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODQ5MjI3Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p></p>

<p>参考文档 ：</p>

<p><a href="http://spark.apache.org/examples.html" rel="nofollow">http://spark.apache.org/examples.html</a></p>

<p>Word Count</p>

<p>In this example, we use a few transformations to build a dataset of (String, Int) pairs called counts and then save it to a file.</p>

<p>val textFile = sc.textFile("hdfs://...") val counts = textFile.flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) counts.saveAsTextFile("hdfs://...")</p>

<p>代码：</p>

<p>val textFile = sc.textFile("file:///home/hadoop/data/wc.txt") val counts = textFile.flatMap(line =&gt; line.split(",")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) counts.collect</p>

<p>在开发阶段直接用Local模式</p>

<p></p>

<p></p>            </div>
                </div>