---
layout:     post
title:      Hive (一)
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/weixin_41143582/article/details/78635994				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>Hive</h1>
<div>数据仓库，理解为hadoop的客户端，可以通过Hive来操作hadoop。</div>
<div>Hive的功能是把脚本变成MapReduce程序，方便不熟悉MapReduce的开发者来分析数据。</div>
<h1>数据存储</h1>
<div>Hive的元素存储在关系型数据库中。Hive本身不存储数据，数据存在HDFS上，Hive存储的事Hive到HDFS中数据的映射关系，通过这个映射关系Hive可以操作HDFS上的数据。</div>
<h1>端口</h1>
<div>Hive服务监听10000端口，但是只有localhost才可以访问，无法远程访问，如需远程访问，还需要修改conf/hive-site.xml中的相关配置</div>
<h1>相关操作</h1>
<div>创建软链：ln -sf /root/apache-hive-** /home/ob</div>
<div>用户授权：mysql:grant all on *.* to root@'%' identified by '123456'</div>
<div>进入mysql：mysql -u username -p password</div>
<div>vi模式进入日志，/匹配字符串，n向下匹配</div>
<div>/localhost service2<br><br><h1>启动Hive服务</h1>
<div>启动hiveserver2 ： hive --service hiveserver2 或者直接 hiveserver2</div>
<div>测试远程是否能够访问hive服务：./bin/beeline 进入命令行模式，！connect jdbc:hive2://172.16.145.124:10000 hive hive（hive 为数据库）</div>
<div>本机测试hive：直接输入hive 或者在bin目录下输入hive</div>
<h1>Hive部分指令</h1>
<div>1.在hive命令下：查看HDFS上的文件：dfs -ls 目录，或者，dfs -lsr 目录（以递归的方式查看目录）。这些都是在命令模式下执行，如果不在命令模式。用：hive -e 'show tables';<br>
2.从文件中执行hive查询：hive -f /filepath 后缀名为.q或者.hql。</div>
<div>3.创建表：</div>
<div>create table t2(<br>
tid int,</div>
<div>tname string,</div>
<div>age int<br>
)location ‘/mytable/hive/t2’    ---指定文件存储在hdfs上的文件路径<br>
row format delimited fields terminated by ‘|’ ----指定分隔符;<br>
load data local inpath 'file' （overwrite）可选 into table **  partition （num = 1）;</div>
<div>4.修改表：alter table<br>
limit:限制行<br>
sort by: sort by a.aaa ASC<br>
distribute by:保证相同的记录分发到同一个reducer中进行处理<br></div>
<div>5.使用from .. insert.. select ..where结构能够从一个数据表中抽取数据，将结果插入到不同的表和分区中<br>
UDF和UDAF</div>
<div></div>
UDF：函数的输入数据为一条数据，输出数据也为一条数据<br>
UDAF：输入数据为多行数据，输出可能为一条，可能为多行
<div>数据优化：解决数据倾斜问题。减少job数。合并小文件</div>
<div>6.hive元数据的初始化：schematool -dbType mysql -initSchema</div>
<div>7.在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。有时候只需要扫描表中关心的一部分数据，因此建表时引入了partition概念。<br>
8.分区表指的是在创建表时指定的partition的分区空间。<br>
9.如果需要创建有分区的表，需要在create表的时候调用可选参数partitioned by，详见表创建的语法结构。<br>
10.一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。<br>
11.表和列名不区分大小写。<br>
12.分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。<br>
13.建表的语法（建分区可参见PARTITIONED BY参数）：<br>
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC],
 ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path]<br>
14.分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。<br>
a、单分区建表语句：create table day_table (id int, content string) partitioned by (dt string);单分区表，按天分区，在表结构中存在id，content，dt三列。<br>
b、双分区建表语句：create table day_hour_table (id int, content string) partitioned by (dt string, hour string);双分区表，按天和小时分区，在表结构中新增加了dt和hour两列。<br>
表文件夹目录示意图（多分区表）：<br>
15.添加分区表语法（表已创建，在此基础上添加分区）：<br>
ALTER TABLE table_name ADD partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ... partition_spec: : PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)<br>
用户可以用 ALTER TABLE ADD PARTITION 来向一个表中增加分区。当分区名是字符串时加引号。例：<br>
ALTER TABLE day_table ADD PARTITION (dt='2008-08-08', hour='08') location '/path/pv1.txt' PARTITION (dt='2008-08-08', hour='09') location '/path/pv2.txt';<br>
16.删除分区语法：<br>
ALTER TABLE table_name DROP partition_spec, partition_spec,...<br>
用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。例：<br>
ALTER TABLE day_hour_table DROP PARTITION (dt='2008-08-08', hour='09');<br>
17.数据加载进分区表中语法：<br>
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]<br>
例：<br>
LOAD DATA INPATH '/user/pv.txt' INTO TABLE day_hour_table PARTITION(dt='2008-08- 08', hour='08');<br>
LOAD DATA local INPATH '/user/hua/*' INTO TABLE day_hour partition(dt='2010-07- 07');<br>
当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制至Hive表对应的位置。数据加载时在表下自动创建一个目录，文件存放在该分区下。<br>
18.基于分区的查询的语句：<br>
SELECT day_table.* FROM day_table WHERE day_table.dt&gt;= '2008-08-08';<br>
19.查看分区语句：<br>
hive&gt; show partitions day_hour_table; OK dt=2008-08-08/hour=08 dt=2008-08-08/hour=09 dt=2008-08-09/hour=09<br></div>
<div>20.在 Hive 中，表中的一个 Partition 对应于表下的一个目录，所有的 Partition 的数据都存储在最字集的目录中。<br>
21.总的说来partition就是辅助查询，缩小查询范围，加快数据的检索速度和对数据按照一定的规格和条件进行管理。<br>
22.Hive不支持等值连接 <br>
SQL中对两表内联可以写成：<br>
select * from dual a,dual b where a.key = b.key;<br>
Hive中应为<br>
select * from dual a join dual b on a.key = b.key; <br>
23.Hive从本地导入数据的时候不会删除本地的数据。<br>
Hive从HDFS文件系统上导入数据的时候：从本地文件系统中将数据导入到Hive表的过程中，其实是先将数据临时复制到HDFS的一个目录下（典型的情况是复制到上传用户的HDFS home目录下,比如/home/wyp/），然后再将数据从那个临时目录下移动（注意，这里说的是移动，不是复制！）到对应的Hive表的数据目录里面。既然如此，那么Hive肯定支持将数据直接从HDFS上的一个目录移动到相应Hive表的数据目录下。<br>
如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用STORED AS SEQUENCE。<br></div>
<div>24.UDF和UDAF<br>
分组取topN：row_number() over<br>
row_number() over ([partition col1] [order by col2])<br>
rank() over ([partition col1] [order by col2])<br>
dense_rank() over ([partition col1] [order by col2])<br>
它们都是根据col1字段分组，然后对col2字段进行排序，对排序后的每行生成一个行号，这个行号从1开始递增<br>
col1、col2都可以是多个字段，用‘,‘分隔<br>
区别：<br>
1）row_number：不管col2字段的值是否相等，行号一直递增，比如：有两条记录的值相等，但一个是第一，一个是第二<br>
2）rank：上下两条记录的col2相等时，记录的行号是一样的，但下一个col2值的行号递增N（N是重复的次数），比如：有两条并列第一，下一个是第三，没有第二<br>
3）dense_rank：上下两条记录的col2相等时，下一个col2值的行号递增1，比如：有两条并列第一，下一个是第二<br>
select * ,row_number() over(partition by provice order by people desc) from datatable group by provice,city,people;<br>
不管是外部表还是内部表，在hdfs上导入数据到表中的时候，数据都会被移动到表的目录下，但是在删除表的时候，内部表会把数据删除，外部表不会把数据删除，之后删除表原来的元数据信息。保留数据。<br>
外部表：如果删除外部表之后再重新建一个和外部表一样的表，那么系统默认将原来的数据导入到新表。（其实hdfs路径上原来表的信息都还在）<br></div>
<h1>Hive中的外部表和内部表</h1>
<div>1）创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径， 不对数据的位置做任何改变。<br><br>
2）删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 </div>
<div>注：往表插入数据的时候：</div>
<div>从本地文件系统中将数据导入到Hive表的过程中，其实是先将数据临时复制到HDFS的一个目录下（典型的情况是复制到上传用户的HDFS home目录下,比如/home/wyp/），然后再将数据从那个临时目录下移动（注意，这里说的是移动，不是复制！）到对应的Hive表的数据目录里面。<br>
如果从hdfs文件系统上直接导入数据，则原来位置上的数据被移动到新的表中，原来位置上数据不存在了。<br></div>
<div>
<h1>Hive分区和分桶</h1>
必须在表定义时创建partition<br>
a、单分区建表语句：create table day_table (id int, content string) partitioned by (dt string);单分区表，按天分区，在表结构中存在id，content，dt三列。<br>
以dt为文件夹区分。<br>
b、 双分区建表语句：create table day_hour_table (id int, content string) partitioned by (dt string, hour string);双分区表，按天和小时分区，在表结构中新增加了dt和hour两列。<br>
先以dt为文件夹，再以hour子文件夹区分。<br>
LOAD DATA INPATH '/user/pv.txt' INTO TABLE day_hour_table PARTITION(dt='2008-08- 08', hour='08'); </div>
<div>LOAD DATA local INPATH '/user/hua/*' INTO TABLE day_hour partition(dt='2010-07- 07');</div>
<div>当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制至Hive表对应的位置。数据加载时在表下自动创建一个目录。<br>
加载数据时数据中没有partition字段，自己添加partition的值，然后hdfs上以partition值为目录，目录下为表的值。<br>
查看分区信息：show partitions tablename;</div>
<div>分区表：性能优势。逻辑数据管理，高性能查询。</div>
<div><br></div>
<div>分桶：<br>
如要安装name属性分为3个桶，就是对name属性值的hash值对3取摸<br>
与分区不同的是，分区依据的不是真实数据表文件中的列，而是我们指定的伪列，但是分桶是依据数据表中真实的列而不是伪列。</div>
<h1>Hive的部分配置</h1>
<div>1.hive-site.xml配置mysql的信息：<br>
&lt;property&gt;  <br>
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;  <br>
    &lt;value&gt;jdbc:mysql://Master/hive&lt;/value&gt;  <br>
    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;  <br>
  &lt;/property&gt; <br>
&lt;property&gt;  <br>
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;  <br>
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;  <br>
    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;  <br>
  &lt;/property&gt;<br>
&lt;property&gt;  <br>
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  <br>
    &lt;value&gt;root&lt;/value&gt;  <br>
    &lt;description&gt;Username to use against metastore database&lt;/description&gt;  <br>
  &lt;/property&gt;  <br>
&lt;property&gt;  <br>
    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;  <br>
    &lt;value&gt;root&lt;/value&gt;  <br>
    &lt;description&gt;password to use against metastore database&lt;/description&gt;  <br>
  &lt;/property&gt; <br>
将hive-site.xml中${system:Java.io.tmpdir}替换为/usr/local/hive/tmp<br>
在lib下添加连接mysql的jar包。<br></div>
<div><br></div>
<div>2.初始化配置：</div>
<div>假定你已经安装好 MySQL。下面创建一个 hive 数据库用来存储 Hive 元数据，且数据库访问的用户名和密码都为 hive。<br>
mysql&gt; CREATE DATABASE hive; <br>
mysql&gt; USE hive; <br>
mysql&gt; CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';<br>
mysql&gt; GRANT ALL ON hive.* TO 'hive'@'localhost' IDENTIFIED BY 'hive'; <br>
mysql&gt; GRANT ALL ON hive.* TO 'hive'@'%' IDENTIFIED BY 'hive'; <br>
mysql&gt; FLUSH PRIVILEGES; <br>
mysql&gt; quit;<br>
mysql：grant all on *.* to root@'%' identified by '123456' </div>
<h1>Hive和HBase的区别</h1>
<div>共同点：<br>
1.hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储<br>
区别：<br>
2.Hive是建立在Hadoop之上为了减少MapReduce jobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。<br>
3.想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。<br>
4.Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。<br>
5.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。<br>
6.hive借用hadoop的MapReduce来完成一些hive中的命令的执行<br>
7.hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。<br>
8.hbase是列存储。<br>
9.hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。<br>
10.hive需要用到hdfs存储文件，需要用到MapReduce计算框架。<br></div>
<h1>问题</h1>
<div>1../bin/hive启动hive失败：Exception in thread "main" java.lang.RuntimeException: Couldn't create directory /usr/local/hive/tmp</div>
<div>权限问题，在root下执行成功。</div>
2.Hive中的日志：<br>
1. 系统日志，记录了hive的运行情况，错误状况。<br>
2. Job 日志，记录了Hive 中job的执行的历史过程。<br>
3.系统日志存储在什么地方呢 ？<br>
在hive/conf/ hive-log4j.properties 文件中记录了Hive日志的存储情况，<br>
默认的存储情况：<br>
hive.root.logger=WARN,DRFA<br>
hive.log.dir=/tmp/${user.name} # 默认的存储位置<br>
hive.log.file=hive.log  # 默认的文件名</div>
<div>3.mysql</div>
<div>报错：Fatal error: Can't open and lock privilege tables: Table 'mysql.user' doesn'：<br>
解决方法：mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data<br>
修改登录密码：update user set authentication_string=password('95279527') where user='root';<br></div>
<div><br></div>
<div><br></div>
<div><br></div>
<div><br></div>
<div><br></div>
            </div>
                </div>