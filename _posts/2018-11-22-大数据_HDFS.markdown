---
layout:     post
title:      大数据_HDFS
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/weixin_37243717/article/details/78921606				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<pre class="prettyprint"><code class=" hljs http">

<span class="r">第五章：HDFS

一、操作HDFS
    <span class="hljs-number">1</span>、Web Console：端口<span class="hljs-number">50070</span>
    <span class="hljs-number">2</span>、命令行：有两种类型
        （<span class="hljs-number">1</span>）普通操作命令： hdfs dfs ******
            命令
            -mkdir：在HDFS上创建目录
                    hdfs dfs -mkdir /aaa
                    hdfs dfs -mkdir -p /bbb/ccc
                    如果父目录不存在，使用-p参数先创建父目录

            -ls      查看HDFS的某个目录
            -ls -R   查看HDFS的某个目录，包含子目录
                     简写： -lsr
                     hdfs dfs -ls -R /bb

            -put            上传数据
            -copyFromLocal  上传数据
            -moveFromLocal  上传数据，相当于ctrl+x(剪切)

            -copyToLocal   下载数据
            -get           下载数据
                 举例: hdfs dfs -get /input/data.txt .

            -rm： 删除目录
            -rmr: 删除目录，包括子目录
                   hdfs dfs -rmr /bbb
                   日志：
                   <span class="hljs-number">17</span>/<span class="hljs-number">12</span>/<span class="hljs-number">08</span> <span class="hljs-number">20</span>:<span class="hljs-number">32</span>:<span class="hljs-number">10</span> INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = <span class="hljs-number">0</span> minutes, Emptier interval = <span class="hljs-number">0</span> minutes.
                   Deleted /bbb

            -getmerge：把某个目录下的文件，合并后再下载
            hdfs dfs -getmerge /student ~/allstudent.txt



            -cp：拷贝   hdfs dfs -cp /input/data.txt /input/data2.txt
            -mv：移动   hdfs dfs -mv /input/data.txt /aaa/a.txt

            -count: 举例：hdfs dfs -count /students

            -du: 类似-count，信息更详细
                 hdfs dfs -du /students

            例子：
                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfs -count /students</span>
                           <span class="hljs-number">1</span>            <span class="hljs-number">2</span>                 <span class="hljs-number">29</span> /students
                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfs -ls /students</span>
                Found <span class="hljs-number">2</span> items
                -rw-r--r--   <span class="hljs-number">1</span> root supergroup         <span class="hljs-number">19</span> <span class="hljs-number">2017</span>-<span class="hljs-number">12</span>-<span class="hljs-number">08</span> <span class="hljs-number">20</span>:<span class="hljs-number">35</span> /students/student01.txt
                -rw-r--r--   <span class="hljs-number">1</span> root supergroup         <span class="hljs-number">10</span> <span class="hljs-number">2017</span>-<span class="hljs-number">12</span>-<span class="hljs-number">08</span> <span class="hljs-number">20</span>:<span class="hljs-number">35</span> /students/student02.txt
                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfs -du /students</span>
                <span class="hljs-number">19</span>  /students/student01.txt
                <span class="hljs-number">10</span>  /students/student02.txt         

            -text、-cat： 查看文本的内容
                      hdfs dfs -cat /input/data.txt
            balancer：平衡操作
                      hdfs  balancer        

      （<span class="hljs-number">2</span>）管理命令：hdfs dfsadmin ******
            -report: 打印HDFS的报告
                     举例：hdfs dfsadmin -report

            -safemode：安全模式
                hdfs dfsadmin -safemode
                Usage: hdfs dfsadmin [-safemode enter | leave | get | wait] 

                enter 表示手动禁用安全模式
                leave 表示手动退出安全模式
                get 表示获取当前安全模式的状态
                wait 等待安全模式执行完成

                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfsadmin -safemode get</span>
                Safe mode is OFF
                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfsadmin -safemode enter</span>
                Safe mode is ON
                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfs -mkdir /dddd</span>
                mkdir: Cannot create directory /dddd. Name node is <span class="hljs-keyword">in</span> safe mode.
                [root@bigdata11 ~]<span class="hljs-comment"># hdfs dfsadmin -safemode leave</span>
                Safe mode is OFF    

    <span class="hljs-number">3</span>、Java API
        （<span class="hljs-number">1</span>）创建一个目录：mkdir ----&gt; 告诉：权限的问题
        （<span class="hljs-number">2</span>）上传数据、下载数据
        （<span class="hljs-number">3</span>）查询数据的元信息

        依赖的jar包：
          /root/training/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.3</span>/share/hadoop/common
          /root/training/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.3</span>/share/hadoop/common/lib

          /root/training/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.3</span>/share/hadoop/hdfs
          /root/training/hadoop-<span class="hljs-number">2.7</span><span class="hljs-number">.3</span>/share/hadoop/hdfs/lib



二、HDFS输出数据的原理（画图）：比较重要(见另外一篇博客)
    <span class="hljs-number">1</span>、数据上传的原理（过程）
    <span class="hljs-number">2</span>、数据下载的原理（过程）

三、HDFS的高级特性
    <span class="hljs-number">1</span>、回收站 recyclebin
        日志
        -rmr: 删除目录，包括子目录
                hdfs dfs -rmr /bbb
                日志：
                <span class="hljs-number">17</span>/<span class="hljs-number">12</span>/<span class="hljs-number">08</span> <span class="hljs-number">20</span>:<span class="hljs-number">32</span>:<span class="hljs-number">10</span> INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = <span class="hljs-number">0</span> minutes, Emptier interval = <span class="hljs-number">0</span> minutes.
                Deleted /bbb
                （*）默认，HDFS的回收站是关闭   
                （*）启用回收站：参数---&gt; core-site.xml
             本质：删除数据的时候，实际是一个ctrl+x操作

            &lt;property&gt;
               &lt;name&gt;fs.trash.interval&lt;/name&gt;
               &lt;value&gt;<span class="hljs-number">1440</span>&lt;/value&gt;
            &lt;/property&gt;
            由于修改了配置文件，所以需要重新启动hadoop。

            日志：
            hdfs dfs -rmr /folder1
            rmr: DEPRECATED: Please use <span class="hljs-string">'rm -r'</span> instead.
            <span class="hljs-number">17</span>/<span class="hljs-number">12</span>/<span class="hljs-number">11</span> <span class="hljs-number">21</span>:<span class="hljs-number">05</span>:<span class="hljs-number">57</span> INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = <span class="hljs-number">1440</span> minutes, Emptier interval = <span class="hljs-number">0</span> minutes.
            Moved: <span class="hljs-string">'hdfs://bigdata11:9000/folder1'</span> to trash at: hdfs://bigdata11:<span class="hljs-number">9000</span>/user/root/.Trash/Current
                （*）恢复：实际就是cp，拷贝

                     hdfs dfs -cp /user/root/.Trash/Current/input/data.txt /input

                     清空：hdfs dfs -expunge

                （*）补充：Oracle数据库也有回收站
                        SQL&gt; select * from tab;

                        TNAME                          TABTYPE  CLUSTERID
                        ------------------------------ ------- ----------
                        BIN$WBSNMvxJpWvgUAB/AQBygg==$<span class="hljs-number">0</span> TABLE
                        BONUS                          TABLE
                        DEPT                           TABLE
                        EMP                            TABLE
                        RESULT                         TABLE
                        SALGRADE                       TABLE

                        <span class="hljs-number">6</span> rows selected.

                        SQL&gt; -- drop table mydemo1;
                        SQL&gt; show recyclebin;
                        ORIGINAL NAME    RECYCLEBIN NAME                OBJECT TYPE  DROP TIME
                        ---------------- ------------------------------ ------------ -------------------
                        MYDEMO1          BIN$WBSNMvxJpWvgUAB/AQBygg==$<span class="hljs-number">0</span> TABLE        <span class="hljs-number">2017</span>-<span class="hljs-number">09</span>-<span class="hljs-number">01</span>:<span class="hljs-number">06</span>:<span class="hljs-number">56</span>:<span class="hljs-number">15</span>
                        SQL&gt; select count(*) from mydemo1;
                        select count(*) from mydemo1
                                             *
                        ERROR at line <span class="hljs-number">1</span>:
                        ORA-<span class="hljs-number">00942</span>: table or view does not exist


                        SQL&gt; select count(*) from BIN$WBSNMvxJpWvgUAB/AQBygg==$<span class="hljs-number">0</span>;
                        select count(*) from BIN$WBSNMvxJpWvgUAB/AQBygg==$<span class="hljs-number">0</span>
                                                                *
                        ERROR at line <span class="hljs-number">1</span>:
                        ORA-<span class="hljs-number">00933</span>: SQL command not properly ended

                        sql语句这样子写：
                        SQL&gt; select count(*) from <span class="hljs-string">"BIN$WBSNMvxJpWvgUAB/AQBygg==$0"</span>;

                          COUNT(*)
                        ----------
                                <span class="hljs-number">30</span>
                        通过oracle的闪回技术恢复这张表。
                        SQL&gt; flashback table mydemo1 to before drop;

                        Flashback complete.

                        SQL&gt; show recyclebin;
                        SQL&gt; select count(*) from mydemo1;

                          COUNT(*)
                        ----------
                                <span class="hljs-number">30</span>
    <span class="hljs-number">2</span>、快照snapshot：备份 ---&gt; 一般来说：不建议使用快照
        （*）默认：HDFS的快照是禁用的
        （*）第一步：管理员开启某个目录的快照功能
            [-allowSnapshot &lt;snapshotDir&gt;]
            [-disallowSnapshot &lt;snapshotDir&gt;]   

            hdfs dfsadmin -allowSnapshot /mydir1

        （*）第二步：使用HDFS的操作命令，创建快照
            [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]
            [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]  
            [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] 

            hdfs dfs -createSnapshot /mydir1 mydir1_backup_01
            日志：Created snapshot /mydir1/.snapshot/mydir1_backup_01
            本质：将数据拷贝一份到当前目录的一个隐藏目录下

        （*）继续试验
            hdfs dfs -put student02.txt /mydir1
            hdfs dfs -createSnapshot /mydir1 mydir1_backup_02

            对比快照： hdfs snapshotDiff /mydir1 mydir1_backup_01 mydir1_backup_02
            Difference between snapshot mydir1_backup_01 and snapshot mydir1_backup_02 under directory /mydir1:
            M       .
            +       ./student02.txt

    <span class="hljs-number">3</span>、配额quota：（<span class="hljs-number">1</span>）名称配额: 规定某个目录下，存放文件（目录）的个数
                        实际的个数：N-<span class="hljs-number">1</span>个
                    [-setQuota &lt;quota&gt; &lt;dirname&gt;<span class="hljs-keyword">...</span>&lt;dirname&gt;]
                    [-clrQuota &lt;dirname&gt;<span class="hljs-keyword">...</span>&lt;dirname&gt;]

                    hdfs dfs -mkdir /quota1
                    设置该目录的名称配额：<span class="hljs-number">3</span>
                    hdfs dfsadmin -setQuota <span class="hljs-number">3</span> /quota1

                    当我们放第三个文件的时候,会报错。
                    hdfs dfs -put data.txt /quota1
                    put: The NameSpace quota (directories and files) of directory /quota1 is exceeded: quota=<span class="hljs-number">3</span> file count=<span class="hljs-number">4</span>
                 （<span class="hljs-number">2</span>）空间配额: 规定某个目录下，文件的大小
                    [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;<span class="hljs-keyword">...</span>&lt;dirname&gt;]
                    [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;<span class="hljs-keyword">...</span>&lt;dirname&gt;]

                    hdfs dfs -mkdir /quota2
                    设置该目录的空间配额是：10M
                    hdfs dfsadmin -setSpaceQuota 10M /quota2

                    正确的做法：hdfs dfsadmin -setSpaceQuota 130M /quota2

                    放一个小于10M的文件，会出错
                    Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /quota2 is exceeded: quota = <span class="hljs-number">10485760</span> B = <span class="hljs-number">10</span> MB but diskspace consumed = <span class="hljs-number">134217728</span> B = <span class="hljs-number">128</span> MB

                    注意：尽管数据不到128M，但是占用的数据块依然是128M
                    切记：当设置空间配额的时候，这个值不能小于128M

    <span class="hljs-number">4</span>、HDFS安全模式: safemode  ---&gt; HDFS只读
        命令: hdfs dfsadmin -safemode get|wait|leave|enter
        作用：检查数据块的副本率，如果副本率不满足要求，就会进行水平复制
    <span class="hljs-number">5</span>、HDFS的权限
        chmod [R] mode file <span class="hljs-keyword">...</span>
    <span class="hljs-number">6</span>、HDFS的集群：开个头
                    集群的两大功能：负载均衡，高可用(失败迁移)
                    （<span class="hljs-number">1</span>）NameNode联盟（Federation） ----&gt; HDFS

                    （<span class="hljs-number">2</span>）HA: HDFS、Yarn、HBase、Storm、Spark ---&gt; 都需要ZooKeeper

四、HDFS底层的原理：Java程序
    <span class="hljs-number">1</span>、Java的动态代理
    <span class="hljs-number">2</span>、RPC：remote procedure call 远程过程调用</span></code></pre>

<blockquote>
  <p>HDFS_WebConsole</p>
</blockquote>

<p><img src="https://img-blog.csdn.net/20171228141908486?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VpeGluXzM3MjQzNzE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<blockquote>
  <p>HDFS_StartupProgress</p>
</blockquote>

<p><img src="https://img-blog.csdn.net/20171228141930675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VpeGluXzM3MjQzNzE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<blockquote>
  <p>HDFS的集群简介</p>
</blockquote>

<p><img src="https://img-blog.csdn.net/20171228234504754?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VpeGluXzM3MjQzNzE3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">package</span> demo;

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.junit.Test;

<span class="hljs-comment">/*
 * 使用HDFS的Java API创建一个目录
 * 
 * 权限的问题：
 * org.apache.hadoop.security.AccessControlException: Permission denied: 
 * user=lenovo, access=WRITE, inode="/folder1":root:supergroup:drwxr-xr-x
 * 
 * 四种办法解决：
 * 1、设置执行程序的用户是：root（HADOOP_USER_NAME）
 * 2、使用Java的-D参数: HADOOP_USER_NAME
 * 3、使用命令改变目录的权限：hdfs dfs -chmod 777 /folder2
 * 4、参数：dfs.permissions  ---&gt; false(不进行权限检查，首先要把hadoop停下来，修改为false之后，需要重新启动hadoop)
 */</span>
<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestDemo1</span> {</span>

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test1</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//办法一：设置执行程序的用户是：root</span>
        System.setProperty(<span class="hljs-string">"HADOOP_USER_NAME"</span>, <span class="hljs-string">"root"</span>);

        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        <span class="hljs-comment">//如果要使用主机名，需要配置Windows的host文件</span>
        <span class="hljs-comment">//C:\Windows\System32\drivers\etc\hosts文件</span>
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">/*
         * 还有一种写法：IP地址
         * conf.set("fs.defaultFS", "hdfs://192.168.157.11:9000");
         */</span>

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);
        <span class="hljs-comment">//创建目录</span>
        client.mkdirs(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1"</span>));

        <span class="hljs-comment">//关闭客户端</span>
        client.close();

    }

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test2</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        <span class="hljs-comment">//如果要使用主机名，需要配置Windows的host文件</span>
        <span class="hljs-comment">//C:\Windows\System32\drivers\etc\hosts文件</span>
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">/*
         * 还有一种写法：IP地址
         * conf.set("fs.defaultFS", "hdfs://192.168.157.11:9000");
         */</span>

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);
        <span class="hljs-comment">//创建目录</span>
        client.mkdirs(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder2"</span>));

        <span class="hljs-comment">//关闭客户端</span>
        client.close();
    }

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test3</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        <span class="hljs-comment">//如果要使用主机名，需要配置Windows的host文件</span>
        <span class="hljs-comment">//C:\Windows\System32\drivers\etc\hosts文件</span>
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">/*
         * 还有一种写法：IP地址
         * conf.set("fs.defaultFS", "hdfs://192.168.157.11:9000");
         */</span>

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);
        <span class="hljs-comment">//创建目录</span>
        client.mkdirs(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder2/folder3"</span>));

        <span class="hljs-comment">//关闭客户端</span>
        client.close();
    }

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test4</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        <span class="hljs-comment">//如果要使用主机名，需要配置Windows的host文件</span>
        <span class="hljs-comment">//C:\Windows\System32\drivers\etc\hosts文件</span>
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">/*
         * 还有一种写法：IP地址
         * conf.set("fs.defaultFS", "hdfs://192.168.157.11:9000");
         */</span>

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);
        <span class="hljs-comment">//创建目录</span>
        client.mkdirs(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder4"</span>));

        <span class="hljs-comment">//关闭客户端</span>
        client.close();
    }
}
</code></pre>

<blockquote>
  <p>java -D</p>
</blockquote>



<pre class="prettyprint"><code class="language-java hljs ">
<span class="hljs-comment">//java -Dname=Tom -Dage=24 TestD</span>
<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestD</span>{</span>

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args){
        test1();
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test1</span>(){
        String name = System.getProperty(<span class="hljs-string">"name"</span>);
        String age = System.getProperty(<span class="hljs-string">"age"</span>);

        System.out.println(name+<span class="hljs-string">"\t"</span>+age);
    }

}

</code></pre>

<blockquote>
  <p>文件上传操作</p>
</blockquote>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">package</span> demo;

<span class="hljs-keyword">import</span> java.io.FileInputStream;
<span class="hljs-keyword">import</span> java.io.InputStream;
<span class="hljs-keyword">import</span> java.io.OutputStream;

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.IOUtils;
<span class="hljs-keyword">import</span> org.junit.Test;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestUpload</span> {</span>

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testUpload1</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);   

        <span class="hljs-comment">//构造一个输出流，指向HDFS</span>
        OutputStream out = client.create(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1/a.tar.gz"</span>));

        <span class="hljs-comment">//构造一个输入流，从本地文件读入数据</span>
        InputStream in = <span class="hljs-keyword">new</span> FileInputStream(<span class="hljs-string">"d:\\temp\\hadoop-2.7.3.tar.gz"</span>);

        <span class="hljs-comment">//下面是Java的基本IO</span>
        <span class="hljs-comment">//缓冲区</span>
        <span class="hljs-keyword">byte</span>[] buffer = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">1024</span>];
        <span class="hljs-comment">//长度</span>
        <span class="hljs-keyword">int</span> len = <span class="hljs-number">0</span>;
        <span class="hljs-keyword">while</span>((len=in.read(buffer)) &gt; <span class="hljs-number">0</span>){
            <span class="hljs-comment">//表示读入了数据，再输出</span>
            out.write(buffer,<span class="hljs-number">0</span>,len);
        }

        out.flush();

        out.close();
        in.close();
    }

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testUpload2</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//使用HDFS的工具类来简化程序</span>
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);   

        <span class="hljs-comment">//构造一个输出流，指向HDFS</span>
        OutputStream out = client.create(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1/b.tar.gz"</span>));

        <span class="hljs-comment">//构造一个输入流，从本地文件读入数据</span>
        InputStream in = <span class="hljs-keyword">new</span> FileInputStream(<span class="hljs-string">"d:\\temp\\hadoop-2.7.3.tar.gz"</span>);

        <span class="hljs-comment">//使用HDFS的工具类来简化程序</span>
        IOUtils.copyBytes(in, out, <span class="hljs-number">1024</span>);
    }
}
</code></pre>

<blockquote>
  <p>文件下载</p>
</blockquote>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">package</span> demo;

<span class="hljs-keyword">import</span> java.io.FileOutputStream;
<span class="hljs-keyword">import</span> java.io.InputStream;
<span class="hljs-keyword">import</span> java.io.OutputStream;

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.IOUtils;
<span class="hljs-keyword">import</span> org.junit.Test;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestDownload</span> {</span>

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testDownload</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);   

        <span class="hljs-comment">//从HDFS获取一个输入流</span>
        InputStream in = client.open(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1/a.tar.gz"</span>));

        <span class="hljs-comment">//创建一个输出流，指向本地进行下载</span>
        OutputStream out = <span class="hljs-keyword">new</span> FileOutputStream(<span class="hljs-string">"d:\\temp\\aaa.tar.gz"</span>);

        <span class="hljs-comment">//下面是Java的基本IO</span>
        <span class="hljs-comment">//缓冲区</span>
        <span class="hljs-keyword">byte</span>[] buffer = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">1024</span>];
        <span class="hljs-comment">//长度</span>
        <span class="hljs-keyword">int</span> len = <span class="hljs-number">0</span>;
        <span class="hljs-keyword">while</span>((len=in.read(buffer)) &gt; <span class="hljs-number">0</span>){
            <span class="hljs-comment">//表示读入了数据，再输出</span>
            out.write(buffer,<span class="hljs-number">0</span>,len);
        }

        out.flush();

        out.close();
        in.close();     
    }

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testDownload1</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);   

        <span class="hljs-comment">//从HDFS获取一个输入流</span>
        InputStream in = client.open(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1/a.tar.gz"</span>));

        <span class="hljs-comment">//创建一个输出流，指向本地进行下载</span>
        OutputStream out = <span class="hljs-keyword">new</span> FileOutputStream(<span class="hljs-string">"d:\\temp\\bbb.tar.gz"</span>);

        <span class="hljs-comment">//使用HDFS的工具类来简化程序</span>
        IOUtils.copyBytes(in, out, <span class="hljs-number">1024</span>);
    }

}

</code></pre>

<blockquote>
  <p>获取文件信息</p>
</blockquote>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">package</span> demo;

<span class="hljs-keyword">import</span> java.util.Arrays;

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.BlockLocation;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileStatus;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.junit.Test;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestMetaData</span> {</span>

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test1</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//获取HDFS的目录信息</span>
        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);   

        FileStatus[] fsList =  client.listStatus(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1"</span>));
        <span class="hljs-keyword">for</span>(FileStatus s: fsList){
            System.out.println(<span class="hljs-string">"文件还是目录？"</span> + (s.isDirectory()?<span class="hljs-string">"目录"</span>:<span class="hljs-string">"文件"</span>));
            System.out.println(s.getPath().toString());
        }
    }

    <span class="hljs-annotation">@Test</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test2</span>() <span class="hljs-keyword">throws</span> Exception{
        <span class="hljs-comment">//获取文件的数据块信息</span>

        <span class="hljs-comment">//指定NameNode地址</span>
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">"fs.defaultFS"</span>, <span class="hljs-string">"hdfs://bigdata11:9000"</span>);

        <span class="hljs-comment">//创建一个HDFS的客户端</span>
        FileSystem client = FileSystem.get(conf);   

        <span class="hljs-comment">//获取文件：/folder1/a.tar.gz数据块的信息</span>
        FileStatus fs = client.getFileStatus(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">"/folder1/a.tar.gz"</span>));

        <span class="hljs-comment">//通过文件的FileStatus获取相应的数据块信息</span>
        <span class="hljs-comment">/*
         * 0: 表示从头开始获取
         * fs.getLen(): 表示文件的长度
         * 返回值：数据块的数组
         */</span>
        BlockLocation[] blkLocations = client.getFileBlockLocations(fs, <span class="hljs-number">0</span>, fs.getLen());
        <span class="hljs-keyword">for</span>(BlockLocation b: blkLocations){
            <span class="hljs-comment">//数据块的主机信息: 数组，表示同一个数据块的多个副本（冗余）被 保存到了不同的主机上</span>
            System.out.println(Arrays.toString(b.getHosts()));

            <span class="hljs-comment">//获取的数据块的名称</span>
            System.out.println(Arrays.toString(b.getNames()));
        }

        client.close();
    }
}

</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>