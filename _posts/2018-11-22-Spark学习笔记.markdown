---
layout:     post
title:      Spark学习笔记
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><h3>文章目录</h3><ul><li><a href="#Spark_1" rel="nofollow">Spark简介</a></li><ul><li><a href="#Spark_2" rel="nofollow">Spark的历史</a></li><li><a href="#spark_26" rel="nofollow">spark开发者</a></li><li><a href="#sparkMR_30" rel="nofollow">spark比MR快的原因</a></li><li><a href="#Spark_36" rel="nofollow">Spark的四种运行模式</a></li><li><a href="#Spark_42" rel="nofollow">开发Spark的语言</a></li></ul><li><a href="#RDD_53" rel="nofollow">RDD（弹性分布式数据集）</a></li><ul><li><a href="#RDD_55" rel="nofollow">RDD简介</a></li><li><a href="#RDD_59" rel="nofollow">RDD五大特性</a></li><li><a href="#RDD_61" rel="nofollow">RDD的三种算子</a></li><ul><li><a href="#Transformation_66" rel="nofollow">Transformation类算子</a></li><li><a href="#Action_80" rel="nofollow">Action类算子</a></li><li><a href="#_93" rel="nofollow">控制类算子</a></li></ul><li><a href="#Spark_108" rel="nofollow">Spark在集群中的大体运行流程</a></li><li><a href="#Application_117" rel="nofollow">提交Application的两种方式</a></li><li><a href="#WordCount_142" rel="nofollow">WordCount案例</a></li></ul><li><a href="#Spark_147" rel="nofollow">Spark集群搭建</a></li></ul></div><p></p>
<h1><a id="Spark_1"></a>Spark简介</h1>
<h2><a id="Spark_2"></a>Spark的历史</h2>
<p>Spark在2012开源，距今长达6年时间，hadoop已经有12年的历史了。Spark使用Scala语言进行实现，它是一种面向对象、函数式编程语言，能够像操作本地集合对象一样轻松地操作分布式数据集，在Spark官网上介绍，它具有运行速度快、易用性好、通用性强和随处运行等特点。</p>
<p><strong>spark特点</strong></p>
<ul>
<li>运行速度快</li>
</ul>
<p>spark在内存中对数据进行迭代计算如果数据由内存读取是hadoop MapReduce的100倍。Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小.</p>
<ul>
<li>易用性好</li>
</ul>
<p>支持Scala编程java编程 Python等语言（Scala是一种高效可扩展语言，使用简洁）</p>
<ul>
<li>一次编译，随意执行</li>
</ul>
<p>spark运行在Hadoop,cloud上,能够读取HDFS,HBase Cassandra等数据源</p>
<ul>
<li>通用性强</li>
</ul>
<p>spark生态圈(BDAS)包括spark Core、spark SQL Spark Streaming等组件，这些组件提供spark Core处理内存计算框架</p>
<h2><a id="spark_26"></a>spark开发者</h2>
<p>加州大学伯克利分校的AMP实验室开发的。</p>
<h2><a id="sparkMR_30"></a>spark比MR快的原因</h2>
<p>Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。</p>
<p><font color="red">总结来说就是：spark是基于内存迭代的，而MR是基于磁盘迭代的。</font></p>
<h2><a id="Spark_36"></a>Spark的四种运行模式</h2>
<ul>
<li>local</li>
<li>standalone</li>
<li>yarn</li>
<li>mesos</li>
</ul>
<h2><a id="Spark_42"></a>开发Spark的语言</h2>
<ul>
<li>Java</li>
<li>Scala</li>
<li>Python</li>
<li>R</li>
</ul>
<p>1，首先Spark计算框架是由Scala来写，所以Scala作为Spark的开发语言，兼容性和效率上都是毫无疑问的。<br>
2，Java和Scala都是基于JVM的编程语言，所以Java和Scala在开发Spark上兼容性和效率都是一样的。<br>
3，Python是解释性语言，他要在解释中运行，那么他要开发Spark应用程序，就要运行在JVM上，也就是解释器要和JVM之间要进行通信，所以它的效率要比Java和Scala差。<br>
4，R语言，它的应用并不是很多，这里不做详细介绍。</p>
<h1><a id="RDD_53"></a>RDD（弹性分布式数据集）</h1>
<h2><a id="RDD_55"></a>RDD简介</h2>
<p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p>
<h2><a id="RDD_59"></a>RDD五大特性</h2>
<p><img src="https://img-blog.csdnimg.cn/20181103171800630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvbWRr,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="RDD_61"></a>RDD的三种算子</h2>
<ul>
<li>Transformation类算子</li>
<li>Action类算子</li>
<li>控制类算子</li>
</ul>
<h3><a id="Transformation_66"></a>Transformation类算子</h3>
<ul>
<li>map：一对一</li>
<li>flatMap：一对多</li>
<li>filter：一对N（0,1）</li>
<li>join：inner（内连接）</li>
<li>leftouter join</li>
<li>rightouter join</li>
<li>fullouter join</li>
<li>sortByKey</li>
<li>groupByKey</li>
<li>reduceByKey</li>
<li>sortBy</li>
<li>sample</li>
</ul>
<h3><a id="Action_80"></a>Action类算子</h3>
<p>1、<strong>count</strong><br>
2、<strong>collect</strong>:将task的计算结果拉回到Driver端<br>
3、<strong>foreach</strong>:不会回收所有task的计算结果，原理：将用户传入的函数传递到 各个节点上去执行，只能去各个节点找计算结果。</p>
<p>查看结果的方式：<br>
① web UI<br>
② 去各个节点的Worker工作目录查看 SPARK_WORKER_DIR去各个节点的Worker工作目录查看 SPARK_WORKER_DIR</p>
<p>4、<strong>saveAsTextFile(path)</strong> path:本地、HDFS路径<br>
5、<strong>reduce</strong></p>
<h3><a id="_93"></a>控制类算子</h3>
<ol>
<li><strong>cache</strong></li>
<li><strong>persist</strong><br>
MEMORY_ONLY<br>
DISK_ONLY<br>
MEMORY_AND_DISK</li>
</ol>
<p><font color="red">注意点：<br>
① 控制类算子后不能立即紧跟action类算子<br>
② 缓存单元是Partition<br>
③懒执行，需要action类算子触发执行<br>
如果你的Application中只要一个job，没有必要使用控制类算子<br>
</font></p>
<h2><a id="Spark_108"></a>Spark在集群中的大体运行流程</h2>
<ol>
<li>Driver分发task到节点运行（计算找数据）。</li>
<li>task执行结果拉回到Driver（有可能发生OOM）。<br>
Driver的作用：<br>
1）、分发任务到计算节点运行。<br>
2）、监控task（thread）的运行情况。<br>
3）、如果task失败，会重新发送（有限制）。<br>
4）、可以拉回结果到Driver进程。<br>
结论：Driver进程会和集群频繁通信。</li>
</ol>
<h2><a id="Application_117"></a>提交Application的两种方式</h2>
<ol>
<li>
<p><strong>Client</strong><br>
提交方式：spark-submit --deploy-mode client --class jarPath args<br>
特点：Driver进程在客户端节点启动<br>
适用场景：测试环境<br>
大概运行流程：<br>
1）、在Client本地启动Driver进程。<br>
2）、Driver会向Master为当前Application申请资源。<br>
3）、Master接收到请求后，会在资源充足的节点上启动Executor进程。<br>
4）、Driver分发task到Executor执行。</p>
</li>
<li>
<p><strong>Cluster</strong><br>
提交方式：spark-submit --deploy-mode cluster --class jarPath args<br>
特点：每次启动application，Driver进程在随机一台节点启动<br>
适用场景：生产环境<br>
大概运行流程：<br>
1）、客户端执行spark-submit --deploy-mode cluster --class jarPath args命令，启动一个sparksubmit进程。<br>
2）、为Driver向Master申请资源。Driver进程默认需要1G内存，1core。<br>
3）、master会随机找一台Worker节点启动Driver进程。<br>
4）、Driver进程启动成功后，spark-submit进程关闭，然后Driver会向Master为当前Application申请资源。<br>
5）、Master接收到请求后，会在资源充足的节点上启动Executor进程。<br>
6）、Driver分发task到Executor执行。</p>
</li>
</ol>
<h2><a id="WordCount_142"></a>WordCount案例</h2>
<p><img src="https://img-blog.csdnimg.cn/20181103173836883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvbWRr,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>如图即是wordCount小案例的大体流程，相比较于MR应用程序简单了许多，并且执行效率上也更快。</p>
<h1><a id="Spark_147"></a>Spark集群搭建</h1>
<p>请参考下一篇博客Spark集群的搭建。</p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>