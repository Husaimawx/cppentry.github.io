---
layout:     post
title:      Ubuntu 14.04 搭建单机版 hadoop 2.6.0 环境
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/u013652219/article/details/43027783				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><span style="font-size:14px;">最近需要hadoop,再加上 hadoop这么流行， 就打算开始学习hadoop， 关于hadoop的简介 就不再赘述了，只需先记住， HDFS(Hadoop分布式文件系统), MapReduce（计算模型) 是hadoop的 核心即可。<br>
 首先  hadoop 平台的搭建有三种方式：</span></p>
<ul><li><span style="font-size:14px;">     单机版</span></li><li><span style="font-size:14px;">     伪分布式版</span></li><li><span style="font-size:14px;">     完全集群分布式          <br></span></li></ul><p><span style="font-size:14px;">首先先试着练习来安装 单机版hadoop</span></p>
<p><br></p>
<p><strong><span style="font-size:14px;">一、安装环境</span></strong></p>
<p>       <span style="font-size:14px;">   Ubuntu 14.04 <br></span></p>
<p><span style="font-size:14px;">         hadoop 2.6.0</span></p>
<p><span style="font-size:14px;">         java jdk 1.7</span></p>
<p><span style="font-size:14px;">        openssh</span></p>
<p><span style="font-size:14px;"><br></span></p>
<p><span style="font-size:14px;"><strong>二、配置用户、组</strong></span></p>
<p><span style="font-size:14px;"><strong>     1， 创建hadoop组      <br></strong></span></p>
<p><span style="font-size:14px;"><strong>           </strong> 在终端执行：</span></p>
<p><span style="font-size:14px;"><strong>            sudo addgroup hadoop</strong></span></p>
<p><span style="font-size:14px;"><strong>           </strong>如下图</span></p>
<p><span style="font-size:14px;"><strong><img src="https://img-blog.csdn.net/20150122222427687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></strong></span></p>
<p><span style="font-size:14px;"><strong>        2，创建hadoop用户</strong></span></p>
<p><span style="font-size:14px;"><strong>             </strong>在hadooop用户组添加hadoop用户</span></p>
<p><span style="font-size:14px;"><strong>             sudo adduser -ingroup hadoop hadoop</strong>表示在hadoop 组 添加hadoop 用户</span></p>
<p><span style="font-size:14px;">  <img src="https://img-blog.csdn.net/20150122222610656?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></span></p>
<p><span style="font-size:14px;">     <strong> 3，为hadoop增加root权限</strong></span></p>
<p><span style="font-size:14px;"><strong>             </strong>赋予hadoop  root权限，就相当于每次执行权限指令时候，只需sudo 就可以，方法如下，修改/etc/sudoers， 如下图，增加              hadoop    ALL=(ALL:ALL) ALL，  执行  sudo  gedit /etc/sudoers  修改之后，如下图</span></p>
<p><span style="font-size:14px;">         <img src="https://img-blog.csdn.net/20150122223416421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></span></p>
<p><span style="font-size:14px;">         保存退出即可</span></p>
<p><span style="font-size:14px;"><br></span></p>
<p><span style="font-size:14px;"><strong>三、安装SSH 服务</strong></span></p>
<p><span style="font-size:14px;"><strong>        </strong>首先SSH是一个安全的传输协议，可以进行远程登录，hadoop使用ssh 协议来进行数据通信</span></p>
<p><span style="font-size:14px;">        建议先  切换到 刚刚 建立的 hadoop 用户下进行操作：  <strong>
su hadoop</strong><br></span></p>
<p><span style="font-size:14px;"><strong>       </strong>1，安装ssh 服务</span></p>
<p><span style="font-size:14px;"><strong>             sudo apt-get install open-ssh-server</strong></span></p>
<p><span style="font-size:14px;"><strong>         </strong>2,  建立ssh无密码登录本机</span></p>
<p><span style="font-size:14px;"><strong>            </strong>执行<strong>  ssh-keygen </strong>生成密钥  中间需要几个回车<br></span></p>
<p><span style="font-size:14px;"><strong>           <img src="https://img-blog.csdn.net/20150122230804731?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></strong></span></p>
<p><span style="font-size:14px;">        然后可以试试 ssh localhost 需要输入密码， 比较麻烦。如果把 key 拷贝到远程机器上，下次ssh链接的时候就无需输入密码登陆了   ：  执行  <strong>ssh-copy-id  localhost   之后，</strong> 在继续<strong>ssh localhost</strong>就无需输入密码了。 然后在 执行exit 返回到 hadoop用户</span></p>
<p><span style="font-size:14px;">如下：</span></p>
<p><span style="font-size:14px;"> <img src="https://img-blog.csdn.net/20150123102153606?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></span></p>
<p><span style="font-size:14px;"><br></span></p>
<p><span style="font-size:14px;"><strong>四、安装java jdk</strong></span></p>
<p>       <strong>  </strong><span style="font-size:14px;">运行hadoop ，jdk 是必需 的。 安装jdk 也比较容易；可以 自己选择版本下载手动安装， 也可以直接apt-get   ，这里为了方便，就直接安装apt-get的openjdk了</span></p>
<p><span style="font-size:14px;">        1，直接 执行  <strong>sudo apt-get openjdk-7-jdk</strong></span></p>
<p><span style="font-size:14px;"><strong>       </strong> 2，查看jdk是否安装完成</span>  <span style="font-size:14px;"><strong> java -version</strong></span></p>
<p>  <img src="https://img-blog.csdn.net/20150123103213582?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="">                <br></p>
<p><br></p>
<p><span style="font-size:14px;"><strong>五、配置安装hadoop</strong></span></p>
<p><span style="font-size:14px;"><strong>       </strong>  hadoop 各个版本差别挺大， 但是作为练习 就直接用了最新版 2.6.0<br></span></p>
<p><span style="font-size:14px;"><strong>        1，安装解压</strong></span></p>
<p><span style="font-size:14px;"><strong>           </strong>官网直接下载就好<strong>：<a href="http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.0/" rel="nofollow">http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.0/</a></strong></span></p>
<p><span style="font-size:14px;"><strong>        </strong>  一般是安装到 /usr/local ， 假设下载的tar.gz 在downloads ，将其移动到 /usr/local :<br></span></p>
<p><span style="font-size:14px;"><strong>          sudo cp hadoop-2.6.0.tar.gz  /usr/local</strong></span></p>
<p><span style="font-size:14px;"><strong>       </strong>    之后进入/usr/local 目录下： 解压<strong>sudo tar -zxvf haddoop-2.6.0.tar.gz   
<br></strong></span></p>
<p><span style="font-size:14px;"><strong>          </strong>解压之后 在该目录下 会多出一个 hadoop-2.6.0的目录，为了方便，一般 是重命名为hadoop：<strong><br>
          sudo  mv  hadoop-2.6.0  hadoop</strong></span></p>
<p><span style="font-size:14px;"><strong>        </strong> 这就算安装好了，下面就是修改配置文件了</span></p>
<p><span style="font-size:14px;">     <strong> 2，修改配置文件</strong></span></p>
<p><span style="font-size:14px;"><strong>           </strong>首先修改 hadoop文件夹的权限用户：<strong>  sudo chown -R hadoop：hadoop  hadoop</strong></span></p>
<p><span style="font-size:14px;"><strong>           </strong>修改<code>/.bashrc</code>文件，添加环境变量</span></p>
<p><span style="font-size:14px;"><strong>            </strong>首先查看自己的jdk目录：<strong>update-alternatives --config java</strong>如下：</span></p>
<p><span style="font-size:14px;"><strong>       <img src="https://img-blog.csdn.net/20150123111147501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="">      <br></strong></span></p>
<p><span style="font-size:14px;"><strong>          </strong>我们只需要到根目录 也就是 ：/usr/lib/jvm/jdk1.7.0_71 <br></span></p>
<p><span style="font-size:14px;">          然后就是修改 hadoop的 hadoop-env.sh <br></span></p>
<p><span style="font-size:14px;">        <strong> sudo gedit /etc/hadoop/hadoop-env.sh</strong></span></p>
<p><span style="font-size:14px;"><strong>         修改export JAVA_HOME=<span style="font-size:14px;">/usr/lib/jvm/jdk1.7.0_71<br></span></strong></span></p>
<p><span style="font-size:14px;"><strong><span style="font-size:14px;">         如下：</span></strong></span></p>
<p><span style="font-size:14px;"><strong><span style="font-size:14px;">  <img src="https://img-blog.csdn.net/20150123111556593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="">
          <br></span></strong></span></p>
<p><span style="font-size:14px;">保存退出  ，至此，单机版的配置 就基本完成了。 下面就是测试了：<br>
  <br></span></p>
<p><span style="font-size:14px;"><strong>六、测试单机版hadoop        </strong><br><code><strong></strong></code></span></p>
<p><span style="font-size:14px;">    1，首先格式化</span></p>
<p><span style="font-size:14px;"><strong>           bin/hadoop  namenode -format</strong></span></p>
<p><span style="font-size:14px;"><strong>        </strong>之后出现： 表示格式化成功<br></span></p>
<p><strong><span style="font-size:14px;">   <img src="https://img-blog.csdn.net/20150123112153265?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="">     <br></span></strong></p>
<p><strong>   </strong><span style="font-size:14px;"><br></span></p>
<p><span style="font-size:14px;">  2、运行测试程序</span></p>
<p><strong>        </strong><span style="font-size:14px;">在hadoop目录下，<code>hadoop-examples-2.6.0.jar</code>这个是测试程序，里面包含了很多测试用的代码，我们运行一个经典的，wordcount<strong></strong>程序： 就是 统计单词频率的一个程序， 首先我们需要建立一个input 文件夹，作为输入</span></p>
<p><span style="font-size:14px;">    <strong>sudo mkdir input </strong> <br></span></p>
<p><span style="font-size:14px;">    然后随便复制文本进去： 这里就用hadoop目录下的  README.txt 作为测试样本。 <br></span></p>
<p><span style="font-size:14px;">   <strong>sudo  cp README.txt input</strong></span></p>
<p><span style="font-size:14px;">   然后执行jar 包 运行样例：</span></p>
<p><span style="font-size:14px;">    <strong>bin/hadoop jar share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.6.0-sources.jar  org.apache.hadoop.examples.WordCount input output</strong></span></p>
<p><span style="font-size:14px;"><strong>   </strong><br></span></p>
<p><span style="font-size:14px;">   结果如下：</span></p>
<p><span style="font-size:14px;"><strong>   <img src="https://img-blog.csdn.net/20150123193253515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></strong></span></p>
<p><span style="font-size:14px;"><strong>  </strong>稍等运行一会，会在/home/hadoop/下生成一个output目录，里面有<code>part-r-00000</code>和<code>_SUCCESS</code>两个文件，看到<code>_SUCCESS</code>就知道已经成功了，打开<code>part-r-00000</code>看看，每个单词的出现次数都给你统计好了吧。</span></p>
<p><span style="font-size:14px;">    执行 cat output/* 可以查看 单词的出现次数～ </span></p>
<p>   <br><img src="https://img-blog.csdn.net/20150123193337171?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hvbXlfbGl1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></p>
<p><span style="font-size:14px;">  <br></span></p>
<p><span style="font-size:14px;">   至此， 单机版的hadoop 已经完成， 可以写自己的程序； 虽然单机版并不能体现云计算的优势 , 在实际应用中并没有什么意义 , 但是在程序的测试与调试过程中 , 它们还是很有意义的 。</span></p>
<p><span style="font-size:14px;">  后面还会相继搭建 伪分布式和 完全集群式的环境</span></p>
<p><span style="font-size:14px;"><br></span></p>
<p><br></p>
<p><span style="font-size:14px;"><strong><br></strong></span></p>
            </div>
                </div>