---
layout:     post
title:      flume的概述
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><strong>   一、Flume的概念</strong></p><p>     Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。<br></p><p>     由三部分组成：Source/Channel/Sink，Source 相当于数据录入源，是 生产者 的角色； Channel 相当于数据传输通道；Sink 相当于数据接收端，是消费者的角色。在 Flume-ng 中，数据流向是 Source--&gt;Channel--&gt;Sink。<br></p><div class="para">    Source:从数据发生器接收数据,并将接收的数据以Flume的event格式传递给一个或者多个通道channal,Flume提供多种数据接收的方式,比如Avro,Thrift,twitter1%等</div><div class="para">    Channel:channel是一种短暂的存储容器,它将从source处接收到的event格式的数据缓存起来,直到它们被sinks消费掉,它在source和sink间起着一共桥梁的作用,channal是一个完整的事务,这一点保证了数据在收发的时候的一致性. 并且它可以和任意数量的source和sink链接. 支持的类型有: JDBC channel , File System channel , Memort channel等.</div><div class="para">    sink:sink将数据存储到集中存储器比如Hbase和HDFS,它从channals消费数据(events)并将其传递给目标地. 目标地可能是另一个sink,也可能HDFS,HBase.</div><p></p><p><strong>二、Flume的架构</strong></p><p><img src="https://img-blog.csdn.net/20180423123448603?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3llem9uZ2dhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>    flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到图中的HDFS，简单来说flume就是收集日志的<br></p><p><strong>2、Event的概念 </strong><br>    在这里有必要先介绍一下flume中event的相关概念：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 <br>    在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？—–event将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，event也是事务的基本单位。event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers(头信息)信息。event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 <br>为了方便大家理解，给出一张event的数据流向图：<br></p><p><img src="https://img-blog.csdn.net/20180423123805523?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3llem9uZ2dhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>一个完整的event包括：event headers、event body、event信息(即文本文件中的单行记录)，如下所以： <br><img src="https://img-blog.csdn.net/20160530163629374" alt="这里写图片描述" title=""><br></p><p>其中event信息就是flume收集到的日记记录。</p><p><strong>3、flume架构介绍 </strong><br>    flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent，agent本身是一个java进程，运行在服务器节点。 <br>    agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。 <br>source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。 <br>channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 <br>sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、hbase、solr、自定义。 <br><strong>4、flume的运行机制 </strong><br>    flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据的输入——source，一个是数据的输出sink，sink负责将数据发送到外部指定的目的地。source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方—-例如HDFS等，注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。 <br><strong>5、flume的广义用法 </strong><br>    flume之所以这么神奇—-其原因也在于flume可以支持多级flume的agent，即flume可以前后相继，例如sink可以将数据写到下一个agent的source中，这样的话就可以连成串了，可以整体处理了。flume还支持扇入(fan-in)、扇出(fan-out)。所谓扇入就是source可以接受多个输入，所谓扇出就是sink可以将数据输出多个目的地destination中。<br></p><p><img src="https://img-blog.csdn.net/20180423124130305?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3llem9uZ2dhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p><strong>三、Flume的应用</strong><br></p><p>           对于flume的原理其实很容易理解，我们更应该掌握flume的具体使用方法，flume提供了大量内置的Source、Channel和Sink类型。而且不同类型的Source、Channel和Sink可以自由组合—–组合方式基于用户设置的配置文件，非常灵活。比如：Channel可以把事件暂存在内存里，也可以持久化到本地硬盘上。Sink可以把日志写入HDFS, HBase，甚至是另外一个Source等等。下面我将用具体的案例详述flume的具体用法。 <br></p><p></p><h3>flume+kafka+storm+mysql构建大数据实时系统</h3><p><img src="https://images2015.cnblogs.com/blog/273387/201705/273387-20170525171421466-858666716.png" alt="" height="371" width="724"></p><p> </p><h3>Flume+HDFS+KafKa+Strom实现实时推荐，反爬虫服务等服务在美团的应用</h3><p><img src="https://images2015.cnblogs.com/blog/273387/201705/273387-20170525165702044-537142790.png" alt="" height="372" width="598"></p><h3>Flume+Hadoop+Hive的离线分析网站用户浏览行为路径</h3><p><img src="https://images2015.cnblogs.com/blog/273387/201705/273387-20170525165418357-2121145996.png" alt="" height="294" width="566"></p><h3> Flume+Logstash+Kafka+Spark Streaming进行实时日志处理分析</h3><p><img src="https://images2015.cnblogs.com/blog/273387/201705/273387-20170525170124247-1728611754.png" alt="" height="389" width="630"></p><h3>Flume+Spark + ELK新浪数据系统实时监控平台</h3><p><img src="https://images2015.cnblogs.com/blog/273387/201705/273387-20170525171014044-990665665.png" alt="" height="313" width="633"></p><p>           其实flume的用法很简单—-书写一个配置文件，在配置文件当中描述source、channel与sink的具体实现，而后运行一个agent实例，在运行agent实例的过程中会读取配置文件的内容，这样flume就会采集到数据。 <br></p><p>       配置文件的编写原则：</p><p></p><pre class="prettyprint"><code class="hljs avrasm has-numbering">    <span class="hljs-preprocessor"># Name the components on this agent</span>
    a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
    a1<span class="hljs-preprocessor">.sinks</span> = k1
    a1<span class="hljs-preprocessor">.channels</span> = c1</code></pre><p>2&gt;详细描述agent中每一个source、sink与channel的具体实现：即在描述source的时候，需要 <br>   指定source到底是什么类型的，即这个source是接受文件的、还是接受http的、还是接受thrift <br>   的；对于sink也是同理，需要指定结果是输出到HDFS中，还是Hbase中啊等等；对于channel <br>   需要指定是内存啊，还是数据库啊，还是文件啊等等。</p><pre class="prettyprint"><code class="hljs avrasm has-numbering">    <span class="hljs-preprocessor"># Describe/configure the source</span>
    a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = netcat
    a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.bind</span> = localhost
    a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.port</span> = <span class="hljs-number">44444</span>

    <span class="hljs-preprocessor"># Describe the sink</span>
    a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = logger

    <span class="hljs-preprocessor"># Use a channel which buffers events in memory</span>
    a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = memory
    a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.capacity</span> = <span class="hljs-number">1000</span>
    a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.transactionCapacity</span> = <span class="hljs-number">100</span></code></pre><p>3&gt;通过channel将source与sink连接起来</p><pre class="prettyprint"><code class="hljs avrasm has-numbering">    <span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
    a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
    a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><ul class="pre-numbering"><li>1</li><li>2</li><li>3</li></ul><p>启动agent的shell操作：</p><pre class="prettyprint"><code class="hljs lasso has-numbering">    flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span>  <span class="hljs-built_in">..</span>/conf   <span class="hljs-attribute">-f</span>  <span class="hljs-built_in">..</span>/conf/example<span class="hljs-built_in">.</span>file  
    <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console  </code></pre><p>参数说明：    -n  指定agent名称(与配置文件中代理的名字相同) <br>                      -c  指定flume中配置文件的目录 <br>                      -f  指定配置文件 <br>                      -Dflume.root.logger=DEBUG,console   设置日志等级</p><p>具体案例： <br>案例1：  NetCat   Source：监听一个指定的网络端口，即只要应用程序向这个端口里面写数据，这个source组件就可以获取到信息。 其中     Sink：logger     Channel：memory <br>flume官网中NetCat  Source描述：</p><pre class="prettyprint"><code class="hljs vhdl has-numbering"><span class="hljs-keyword">Property</span> Name <span class="hljs-keyword">Default</span>     Description
channels       –     
<span class="hljs-keyword">type</span>           –     The <span class="hljs-keyword">component</span> <span class="hljs-keyword">type</span> name, needs <span class="hljs-keyword">to</span> be netcat
bind           –  日志需要发送到的主机名或者Ip地址，该主机运行着netcat类型的source在监听          
<span class="hljs-keyword">port</span>           –  日志需要发送到的端口号，该端口号要有netcat类型的source在监听      </code></pre><p>a) 编写配置文件：</p><pre class="prettyprint"><code class="hljs avrasm has-numbering"><span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.sinks</span> = k1
a1<span class="hljs-preprocessor">.channels</span> = c1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = netcat
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.bind</span> = <span class="hljs-number">192.168</span><span class="hljs-number">.80</span><span class="hljs-number">.80</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.port</span> = <span class="hljs-number">44444</span>

<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = logger

<span class="hljs-preprocessor"># Use a channel which buffers events in memory</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = memory
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.capacity</span> = <span class="hljs-number">1000</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.transactionCapacity</span> = <span class="hljs-number">100</span>

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><p>b) 启动flume agent  a1 服务端</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span> <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-f</span> <span class="hljs-built_in">..</span>/conf/netcat<span class="hljs-built_in">.</span>conf   <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><ul class="pre-numbering"><li>1</li></ul><p>c) 使用telnet发送数据</p><pre class="prettyprint"><code class="hljs haskell has-numbering"><span class="hljs-title">telnet</span>  <span class="hljs-number">192.168</span><span class="hljs-number">.80</span><span class="hljs-number">.80</span>  <span class="hljs-number">44444</span>  big <span class="hljs-typedef"><span class="hljs-keyword">data</span> world！（windows中运行的）</span></code></pre><p>d) 在控制台上查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160530211114709" alt="这里写图片描述" title=""></p><p>案例2：NetCat   Source：监听一个指定的网络端口，即只要应用程序向这个端口里面写数据，这个source组件就可以获取到信息。 其中     Sink：hdfs     Channel：file (相比于案例1的两个变化) <br>flume官网中HDFS Sink的描述： <br><img src="https://img-blog.csdn.net/20160530213143487" alt="这里写图片描述" title=""><br>a) 编写配置文件：</p><pre class="prettyprint"><code class="hljs avrasm has-numbering"><span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.sinks</span> = k1
a1<span class="hljs-preprocessor">.channels</span> = c1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = netcat
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.bind</span> = <span class="hljs-number">192.168</span><span class="hljs-number">.80</span><span class="hljs-number">.80</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.port</span> = <span class="hljs-number">44444</span>

<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = hdfs
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.path</span> = hdfs://hadoop80:<span class="hljs-number">9000</span>/dataoutput
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.writeFormat</span> = Text
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.fileType</span> = DataStream
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollInterval</span> = <span class="hljs-number">10</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollSize</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollCount</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.filePrefix</span> = %<span class="hljs-built_in">Y</span>-%m-%d-%H-%M-%S
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.useLocalTimeStamp</span> = true

<span class="hljs-preprocessor"># Use a channel which buffers events in file</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = file
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.checkpointDir</span> = /usr/flume/checkpoint
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.dataDirs</span> = /usr/flume/data

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><p>b) 启动flume agent  a1 服务端</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span> <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-f</span> <span class="hljs-built_in">..</span>/conf/netcat<span class="hljs-built_in">.</span>conf   <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><p>c) 使用telnet发送数据</p><pre class="prettyprint"><code class="hljs haskell has-numbering"><span class="hljs-title">telnet</span>  <span class="hljs-number">192.168</span><span class="hljs-number">.80</span><span class="hljs-number">.80</span>  <span class="hljs-number">44444</span>  big <span class="hljs-typedef"><span class="hljs-keyword">data</span> world！（windows中运行的）</span></code></pre><p>d) 在HDFS中查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160530214141812" alt="这里写图片描述" title=""><br>案例3：Spooling Directory Source：监听一个指定的目录，即只要应用程序向这个指定的目录中添加新的文件，source组件就可以获取到该信息，并解析该文件的内容，然后写入到channle。写入完成后，标记该文件已完成或者删除该文件。其中     Sink：logger     Channel：memory <br>flume官网中Spooling Directory Source描述：</p><pre class="prettyprint"><code class="hljs oxygene has-numbering"><span class="hljs-keyword">Property</span> Name       <span class="hljs-keyword">Default</span>      Description
channels              –  
<span class="hljs-keyword">type</span>                  –          The component <span class="hljs-keyword">type</span> name, needs <span class="hljs-keyword">to</span> be spooldir.
spoolDir              –          Spooling Directory Source监听的目录
fileSuffix         .COMPLETED    文件内容写入到channel之后，标记该文件
deletePolicy       never         文件内容写入到channel之后的删除策略: never <span class="hljs-keyword">or</span> immediate
fileHeader         <span class="hljs-keyword">false</span>         Whether <span class="hljs-keyword">to</span> <span class="hljs-keyword">add</span> a header storing the absolute path filename.
ignorePattern      ^$           Regular expression specifying which files <span class="hljs-keyword">to</span> ignore (<span class="hljs-keyword">skip</span>)
interceptors          –          指定传输中<span class="hljs-keyword">event</span>的head(头信息)，常用timestamp</code></pre><p>Spooling Directory Source的两个注意事项：</p><pre class="prettyprint"><code class="hljs applescript has-numbering">①If a <span class="hljs-type">file</span> <span class="hljs-keyword">is</span> written <span class="hljs-keyword">to</span> <span class="hljs-keyword">after</span> being placed <span class="hljs-keyword">into</span> <span class="hljs-keyword">the</span> spooling directory, Flume will print an <span class="hljs-keyword">error</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">its</span> <span class="hljs-command">log</span> <span class="hljs-type">file</span> <span class="hljs-keyword">and</span> stop processing.
即：拷贝到spool目录下的文件不可以再打开编辑
②If a <span class="hljs-type">file</span> <span class="hljs-property">name</span> <span class="hljs-keyword">is</span> reused <span class="hljs-keyword">at</span> a later <span class="hljs-property">time</span>, Flume will print an <span class="hljs-keyword">error</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">its</span> <span class="hljs-command">log</span> <span class="hljs-type">file</span> <span class="hljs-keyword">and</span> stop processing.
即：不能将具有相同文件名字的文件拷贝到这个目录下</code></pre><p>a) 编写配置文件：</p><pre class="prettyprint"><code class="hljs avrasm has-numbering"><span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.sinks</span> = k1
a1<span class="hljs-preprocessor">.channels</span> = c1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = spooldir
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.spoolDir</span> = /usr/local/datainput
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.fileHeader</span> = true
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.interceptors</span> = i1
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.interceptors</span><span class="hljs-preprocessor">.i</span>1<span class="hljs-preprocessor">.type</span> = timestamp

<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = logger

<span class="hljs-preprocessor"># Use a channel which buffers events in memory</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = memory
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.capacity</span> = <span class="hljs-number">1000</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.transactionCapacity</span> = <span class="hljs-number">100</span>

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><p>b) 启动flume agent  a1 服务端</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span> <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-f</span> <span class="hljs-built_in">..</span>/conf/spool<span class="hljs-built_in">.</span>conf   <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><p>c) 使用cp命令向Spooling Directory 中发送数据</p><pre class="prettyprint"><code class="hljs haskell has-numbering"> cp datafile  /usr/local/datainput   (注：datafile中的内容为：big <span class="hljs-typedef"><span class="hljs-keyword">data</span> world！)</span></code></pre><p>d) 在控制台上查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160531092947740" alt="这里写图片描述" title=""><br>从控制台显示的结果可以看出event的头信息中包含了时间戳信息。 <br>同时我们查看一下Spooling Directory中的datafile信息—-文件内容写入到channel之后，该文件被标记了：</p><pre class="prettyprint"><code class="hljs ruby has-numbering">[root<span class="hljs-variable">@hadoop80</span> datainput]<span class="hljs-comment"># ls</span>
datafile.<span class="hljs-constant">COMPLETED</span></code></pre><p>案例4：Spooling Directory Source：监听一个指定的目录，即只要应用程序向这个指定的目录中添加新的文件，source组件就可以获取到该信息，并解析该文件的内容，然后写入到channle。写入完成后，标记该文件已完成或者删除该文件。 其中     Sink：hdfs     Channel：file (相比于案例3的两个变化)</p><p>a) 编写配置文件：</p><pre class="prettyprint"><code class="hljs avrasm has-numbering"><span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.sinks</span> = k1
a1<span class="hljs-preprocessor">.channels</span> = c1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = spooldir
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.spoolDir</span> = /usr/local/datainput
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.fileHeader</span> = true
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.interceptors</span> = i1
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.interceptors</span><span class="hljs-preprocessor">.i</span>1<span class="hljs-preprocessor">.type</span> = timestamp

<span class="hljs-preprocessor"># Describe the sink</span>
<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = hdfs
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.path</span> = hdfs://hadoop80:<span class="hljs-number">9000</span>/dataoutput
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.writeFormat</span> = Text
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.fileType</span> = DataStream
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollInterval</span> = <span class="hljs-number">10</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollSize</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollCount</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.filePrefix</span> = %<span class="hljs-built_in">Y</span>-%m-%d-%H-%M-%S
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.useLocalTimeStamp</span> = true

<span class="hljs-preprocessor"># Use a channel which buffers events in file</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = file
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.checkpointDir</span> = /usr/flume/checkpoint
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.dataDirs</span> = /usr/flume/data

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><p>b) 启动flume agent  a1 服务端</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span> <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-f</span> <span class="hljs-built_in">..</span>/conf/spool<span class="hljs-built_in">.</span>conf   <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><p>c) 使用cp命令向Spooling Directory 中发送数据</p><pre class="prettyprint"><code class="hljs haskell has-numbering"> cp datafile  /usr/local/datainput   (注：datafile中的内容为：big <span class="hljs-typedef"><span class="hljs-keyword">data</span> world！)</span></code></pre><p>d) 在控制台上可以参看sink的运行进度日志： <br><img src="https://img-blog.csdn.net/20160531094446417" alt="这里写图片描述" title=""><br>d) 在HDFS中查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160531094639919" alt="这里写图片描述" title=""><br><img src="https://img-blog.csdn.net/20160531094652974" alt="这里写图片描述" title=""><br>从案例1与案例2、案例3与案例4的对比中我们可以发现：flume的配置文件在编写的过程中是非常灵活的。</p><p>案例5：Exec Source：监听一个指定的命令，获取一条命令的结果作为它的数据源 <br>常用的是tail  -F file指令，即只要应用程序向日志(文件)里面写数据，source组件就可以获取到日志(文件)中最新的内容 。  其中     Sink：hdfs     Channel：file <br>这个案列为了方便显示Exec Source的运行效果，结合Hive中的external table进行来说明。</p><p>a) 编写配置文件：</p><pre class="prettyprint"><code class="hljs avrasm has-numbering"><span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.sinks</span> = k1
a1<span class="hljs-preprocessor">.channels</span> = c1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = exec
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.command</span> = tail -F /usr/local/log<span class="hljs-preprocessor">.file</span>

<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = hdfs
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.path</span> = hdfs://hadoop80:<span class="hljs-number">9000</span>/dataoutput
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.writeFormat</span> = Text
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.fileType</span> = DataStream
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollInterval</span> = <span class="hljs-number">10</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollSize</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollCount</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.filePrefix</span> = %<span class="hljs-built_in">Y</span>-%m-%d-%H-%M-%S
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.useLocalTimeStamp</span> = true

<span class="hljs-preprocessor"># Use a channel which buffers events in file</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = file
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.checkpointDir</span> = /usr/flume/checkpoint
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.dataDirs</span> = /usr/flume/data

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><p>b)在hive中建立外部表—–hdfs://hadoop80:9000/dataoutput的目录，方便查看日志捕获内容</p><pre class="prettyprint"><code class="hljs oxygene has-numbering">hive&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> table t1(infor  string)
    &gt; row format delimited
    &gt; fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">'\t'</span>
    &gt; location <span class="hljs-string">'/dataoutput/'</span>;
OK
Time taken: <span class="hljs-number">0.284</span> seconds</code></pre><p>c) 启动flume agent  a1 服务端</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span> <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-f</span> <span class="hljs-built_in">..</span>/conf/exec<span class="hljs-built_in">.</span>conf   <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><p>d) 使用echo命令向/usr/local/datainput 中发送数据</p><pre class="prettyprint"><code class="hljs haskell has-numbering"> echo  big <span class="hljs-typedef"><span class="hljs-keyword">data</span> &gt; log.file</span></code></pre><p>d) 在HDFS和Hive分别中查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160531103741650" alt="这里写图片描述" title=""></p><pre class="prettyprint"><code class="hljs cs has-numbering">hive&gt; <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> t1;
OK
big data
Time taken: <span class="hljs-number">0.086</span> seconds</code></pre><p>e)使用echo命令向/usr/local/datainput 中在追加一条数据</p><pre class="prettyprint"><code class="hljs haskell has-numbering"><span class="hljs-title">echo</span> big <span class="hljs-typedef"><span class="hljs-keyword">data</span> world! &gt;&gt; log.file</span></code></pre><p>d) 在HDFS和Hive再次分别中查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160531104040589" alt="这里写图片描述" title=""><br><img src="https://img-blog.csdn.net/20160531104108195" alt="这里写图片描述" title=""></p><pre class="prettyprint"><code class="hljs haskell has-numbering"><span class="hljs-title">hive</span>&gt; select * from t1;
<span class="hljs-type">OK</span>
<span class="hljs-title">big</span> <span class="hljs-typedef"><span class="hljs-keyword">data</span></span>
<span class="hljs-title">big</span> <span class="hljs-typedef"><span class="hljs-keyword">data</span> world!</span>
<span class="hljs-type">Time</span> taken: <span class="hljs-number">0.511</span> seconds</code></pre><p>总结Exec source：Exec source和Spooling Directory Source是两种常用的日志采集的方式，其中Exec source可以实现对日志的实时采集，Spooling Directory Source在对日志的实时采集上稍有欠缺，尽管Exec source可以实现对日志的实时采集，但是当Flume不运行或者指令执行出错时，Exec source将无法收集到日志数据，日志会出现丢失，从而无法保证收集日志的完整性。</p><p>案例6：Avro  Source：监听一个指定的Avro 端口，通过Avro 端口可以获取到Avro client发送过来的文件 。即只要应用程序通过Avro 端口发送文件，source组件就可以获取到该文件中的内容。  其中     Sink：hdfs     Channel：file <br>(注：Avro和Thrift都是一些序列化的网络端口–通过这些网络端口可以接受或者发送信息，Avro可以发送一个给定的文件给Flume，Avro 源使用AVRO RPC机制) <br>Avro  Source运行原理如下图： <br><img src="https://img-blog.csdn.net/20160531111151304" alt="这里写图片描述" title=""><br>flume官网中Avro Source的描述：</p><pre class="prettyprint"><code class="hljs vhdl has-numbering"><span class="hljs-keyword">Property</span>     Name   <span class="hljs-keyword">Default</span> Description
channels      –  
<span class="hljs-keyword">type</span>          –     The <span class="hljs-keyword">component</span> <span class="hljs-keyword">type</span> name, needs <span class="hljs-keyword">to</span> be avro
bind          –     日志需要发送到的主机名或者ip，该主机运行着ARVO类型的source
<span class="hljs-keyword">port</span>          –     日志需要发送到的端口号，该端口要有ARVO类型的source在监听</code></pre><p>1)编写配置文件</p><pre class="prettyprint"><code class="hljs avrasm has-numbering"><span class="hljs-preprocessor"># Name the components on this agent</span>
a1<span class="hljs-preprocessor">.sources</span> = <span class="hljs-built_in">r1</span>
a1<span class="hljs-preprocessor">.sinks</span> = k1
a1<span class="hljs-preprocessor">.channels</span> = c1

<span class="hljs-preprocessor"># Describe/configure the source</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.type</span> = avro
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.bind</span> = <span class="hljs-number">192.168</span><span class="hljs-number">.80</span><span class="hljs-number">.80</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.port</span> = <span class="hljs-number">4141</span>

<span class="hljs-preprocessor"># Describe the sink</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.type</span> = hdfs
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.path</span> = hdfs://hadoop80:<span class="hljs-number">9000</span>/dataoutput
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.writeFormat</span> = Text
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.fileType</span> = DataStream
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollInterval</span> = <span class="hljs-number">10</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollSize</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.rollCount</span> = <span class="hljs-number">0</span>
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.filePrefix</span> = %<span class="hljs-built_in">Y</span>-%m-%d-%H-%M-%S
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.hdfs</span><span class="hljs-preprocessor">.useLocalTimeStamp</span> = true

<span class="hljs-preprocessor"># Use a channel which buffers events in file</span>
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.type</span> = file
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.checkpointDir</span> = /usr/flume/checkpoint
a1<span class="hljs-preprocessor">.channels</span><span class="hljs-preprocessor">.c</span>1<span class="hljs-preprocessor">.dataDirs</span> = /usr/flume/data

<span class="hljs-preprocessor"># Bind the source and sink to the channel</span>
a1<span class="hljs-preprocessor">.sources</span><span class="hljs-preprocessor">.r</span>1<span class="hljs-preprocessor">.channels</span> = c1
a1<span class="hljs-preprocessor">.sinks</span><span class="hljs-preprocessor">.k</span>1<span class="hljs-preprocessor">.channel</span> = c1</code></pre><p>b) 启动flume agent  a1 服务端</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span>  agent <span class="hljs-attribute">-n</span> a1  <span class="hljs-attribute">-c</span> <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-f</span> <span class="hljs-built_in">..</span>/conf/avro<span class="hljs-built_in">.</span>conf   <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><p>c)使用avro-client发送文件</p><pre class="prettyprint"><code class="hljs lasso has-numbering">flume<span class="hljs-attribute">-ng</span> avro<span class="hljs-attribute">-client</span> <span class="hljs-attribute">-c</span>  <span class="hljs-built_in">..</span>/conf  <span class="hljs-attribute">-H</span> <span class="hljs-number">192.168</span><span class="hljs-number">.80</span><span class="hljs-number">.80</span>  <span class="hljs-attribute">-p</span> <span class="hljs-number">4141</span> <span class="hljs-attribute">-F</span> /usr/<span class="hljs-built_in">local</span>/<span class="hljs-keyword">log</span><span class="hljs-built_in">.</span>file</code></pre><p>注：log.file文件中的内容为：</p><pre class="prettyprint"><code class="hljs haskell has-numbering">[root@hadoop80 local]# more log.file
<span class="hljs-title">big</span> <span class="hljs-typedef"><span class="hljs-keyword">data</span></span>
<span class="hljs-title">big</span> <span class="hljs-typedef"><span class="hljs-keyword">data</span> world!</span></code></pre><p>d) 在HDFS中查看flume收集到的日志数据： <br><img src="https://img-blog.csdn.net/20160531112942467" alt="这里写图片描述" title=""><br><img src="https://img-blog.csdn.net/20160531122918121" alt="这里写图片描述" title=""><br><img src="https://img-blog.csdn.net/20160531122951552" alt="这里写图片描述" title=""></p><p>通过上面的几个案例，我们可以发现：flume配置文件的书写是相当灵活的—-不同类型的Source、Channel和Sink可以自由组合！</p><p>最后对上面用的几个flume source进行适当总结： <br>①  NetCat   Source：监听一个指定的网络端口，即只要应用程序向这个端口里面写数据，这个source组件 <br>就可以获取到信息。 <br>②Spooling Directory Source：监听一个指定的目录，即只要应用程序向这个指定的目录中添加新的文 <br>件，source组件就可以获取到该信息，并解析该文件的内容，然后写入到channle。写入完成后，标记 <br>该文件已完成或者删除该文件。 <br>③Exec Source：监听一个指定的命令，获取一条命令的结果作为它的数据源 <br>常用的是tail  -F file指令，即只要应用程序向日志(文件)里面写数据，source组件就可以获取到日志(文件)中最新的内容 。 <br>④Avro  Source：监听一个指定的Avro 端口，通过Avro 端口可以获取到Avro client发送过来的文件 。即只要应用程序通过Avro 端口发送文件，source组件就可以获取到该文件中的内容。</p><p>如有问题，欢迎留言指正！</p>            </div>
                </div>