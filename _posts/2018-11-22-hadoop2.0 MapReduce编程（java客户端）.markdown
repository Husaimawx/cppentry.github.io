---
layout:     post
title:      hadoop2.0 MapReduce编程（java客户端）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><strong>1  通用开发步骤</strong></p>
<p>       创建java工程</p>
<p>       引入相应的hadoop相关jar <br></p>
<p>               share/hadoop/mapreduce下面的全部jar<br>
               share/hadoop/common/hadoop-common-2.7.1.jar<br>
               share/hadoop/common/lib全部jar</p>
<p>      编写mapreduce程序</p>
<p>      导出为jar包</p>
<p>      将jar包拷贝到hadoop集群环境上<strong><br></strong></p>
<p><strong>2  wordcount案例</strong></p>
<p>用pig客户端<strong>将文件上传到HDFS</strong></p>
<p><strong></strong></p>
<pre><code class="language-html">grunt&gt; copyFromLocal /usr/local/input.txt /input
grunt&gt; cat /input
hello
world
hadoop
hbase
hive
hadoop</code></pre>
<p></p>
<p><strong></strong></p>
<pre><code class="language-html">import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.examples.WordCount.IntSumReducer;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount {

	public static class WordCountMapper 
		extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
		
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();
		
		@Override
		protected void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)
				throws IOException, InterruptedException {
			StringTokenizer itr = new StringTokenizer(value.toString());
			while(itr.hasMoreTokens()){
				word.set(itr.nextToken());
				context.write(word, one);
			}
		}
		
	}
	
	public static class WordCountReduce
		extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{
		
		private IntWritable result = new IntWritable();
		
		@Override
		protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,
				Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException {
			int sum = 0;
			for (IntWritable val: values) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key, result);
		}
	}
	
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
		if(otherArgs.length != 2){
			System.out.println("Usage:wordcount &lt;in&gt; &lt;out&gt;");
			System.exit(2);
		}
		Job job = new Job(conf,"wodr count");
		job.setJarByClass(WordCount.class);
		job.setMapperClass(WordCountMapper.class);
		job.setCombinerClass(IntSumReducer.class);
		job.setReducerClass(IntSumReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		System.exit(job.waitForCompletion(true)? 0 :1);
	}
}
</code></pre>
<p>运行并查看结果</p>
<p></p>
<pre><code class="language-html">root@localhost hadoop-2.7.1]# bin/hadoop jar /usr/local/wordcount.jar WordCount /input /output
grunt&gt; cat /output
hadoop	2
hbase	1
hello	1
hive	1
world	1</code></pre><br><br><p></p>
<br><p></p>
            </div>
                </div>