---
layout:     post
title:      hbase入库几种方式
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：原创作品，允许转载，转载时请务必以超链接形式标明文章 原始出处 、作者信息和本声明。否则将追究法律责任。http://blog.csdn.net/qq_34291777					https://blog.csdn.net/qq_34291777/article/details/56009549				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="hbase入库几种方式">hbase入库几种方式</h1>

<p></p><div class="toc">
<ul>
<li><a href="#hbase%E5%85%A5%E5%BA%93%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F" rel="nofollow">hbase入库几种方式</a><ul>
<li><ul>
<li><a href="#1-%E7%AC%AC%E4%B8%80%E7%A7%8D%E5%88%A9%E7%94%A8client-api-%E8%BF%9B%E8%A1%8C%E5%AF%B9hbase%E6%93%8D%E4%BD%9C" rel="nofollow">第一种利用client API 进行对hbase操作</a></li>
<li><a href="#2-%E7%AC%AC%E4%BA%8C%E7%A7%8D%E5%88%A9%E7%94%A8hbase%E6%8F%90%E4%BE%9B%E7%9A%84importtsv%E5%B0%86csv%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5%E5%88%B0hbase%E7%94%A8-importtsv-%E5%92%8C-bulk-load%E5%B0%86tsv%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5hbase" rel="nofollow">第二种利用Hbase提供的ImportTsv将csv文件导入到HBase用 importtsv 和 bulk load将TSV数据文件导入HBase</a></li>
<li><a href="#3-%E7%AC%AC%E4%B8%89%E7%A7%8D%E5%88%A9%E7%94%A8hbase%E6%8F%90%E4%BE%9B%E7%9A%84completebulkload%E5%B0%86%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0hbase" rel="nofollow">第三种利用HBase提供的completebulkload将数据导入到HBase</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>


<h3 id="1-第一种利用client-api-进行对hbase操作">1 . 第一种利用client API 进行对hbase操作</h3>

<hr>

<p>使用HBase的API中的Put是最直接的方法，用法也很容易学习。但针对大部分情况，它并非都是最高效的方式。 <br>
    下面例子的数据是通过我书写输入，当然也可以通过mysql数据里得到数据后批量put到hbase，或者MapReduce 读取hdfs上的文件，以put的方式在map中完成数据写入。在这个项目中，也可以在网上爬到数据后，直接put到hbase。 不过在我看来，如果把获取数据（就是爬虫爬数据阶段），录入数据库。这两步一起做会比较繁琐，错了一个地方，可能每个地方都要变动。而把两步分开，这样更模块化，易管理易维护。</p>



<pre class="prettyprint"><code class="language-java hljs ">    <span class="hljs-keyword">import</span> java.io.IOException;
    <span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
    <span class="hljs-keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;
    <span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PutDemo</span> {</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span>(String[] args)<span class="hljs-keyword">throws</span> IOException {
        Configuration conf = HBaseConfiguration.create();
        conf.set(<span class="hljs-string">"hbase.zookeeper.quorum"</span>, <span class="hljs-string">"192.168.6.250,192.168.6.251,192.168.6.252"</span>);
        HBHelper helper = <span class="hljs-keyword">new</span> HBHelper(conf);
        helper.dropTable(<span class="hljs-string">"person3"</span>);
        helper.createTable(<span class="hljs-string">"person3"</span>,  <span class="hljs-string">"info"</span>);
        helper.insterRow(<span class="hljs-string">"person3"</span>, <span class="hljs-string">"row1"</span>, <span class="hljs-string">"info"</span>, <span class="hljs-string">"name"</span>, <span class="hljs-string">"小明"</span>);
        helper.insterRow(<span class="hljs-string">"person3"</span>, <span class="hljs-string">"row1"</span>, <span class="hljs-string">"info"</span>, <span class="hljs-string">"majar"</span>, <span class="hljs-string">"软件技术"</span>);
        helper.insterRow(<span class="hljs-string">"person3"</span>, <span class="hljs-string">"row1"</span>, <span class="hljs-string">"info"</span>, <span class="hljs-string">"name"</span>, <span class="hljs-string">"Malik"</span>);
        helper.scanData(<span class="hljs-string">"person3"</span>);
        helper.close();
    }
}</code></pre>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-keyword">import</span> java.io.IOException;
<span class="hljs-keyword">import</span> java.util.ArrayList;
<span class="hljs-keyword">import</span> java.util.List;

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.Cell;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.CellUtil;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.HColumnDescriptor;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.HTableDescriptor;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.TableName;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Admin;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Connection;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Get;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Put;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Result;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.ResultScanner;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Scan;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Table;
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.util.Bytes;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBHelper</span> {</span>
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> Connection connection = <span class="hljs-keyword">null</span>;
      <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> Admin admin = <span class="hljs-keyword">null</span>;

      <span class="hljs-keyword">public</span> <span class="hljs-title">HBHelper</span> (Configuration conf) {
          <span class="hljs-keyword">try</span> {
              connection = HBHelper.getConnInstance(conf);
            } <span class="hljs-keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
      } 

      <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">synchronized</span> Connection <span class="hljs-title">getConnInstance</span>(Configuration conf)<span class="hljs-keyword">throws</span> IOException{
          <span class="hljs-keyword">if</span>(connection==<span class="hljs-keyword">null</span>){
              <span class="hljs-keyword">try</span>{
                  connection = ConnectionFactory.createConnection(conf);
                   admin = connection.getAdmin();
              }
              <span class="hljs-keyword">catch</span> (IOException e) {
            e.printStackTrace();
        } 

        System.out.println(<span class="hljs-string">"连接数据库成功"</span>);
          }
            <span class="hljs-keyword">return</span> connection;
      }
      <span class="hljs-keyword">public</span>  <span class="hljs-keyword">void</span> <span class="hljs-title">close</span>(){  
            <span class="hljs-keyword">try</span> {  
                <span class="hljs-keyword">if</span>(<span class="hljs-keyword">null</span> != admin)  
                    admin.close();  
                <span class="hljs-keyword">if</span>(<span class="hljs-keyword">null</span> != connection)  
                    connection.close();  
                System.out.println(<span class="hljs-string">"关闭数据库成功"</span>);
            } <span class="hljs-keyword">catch</span> (IOException e) {  
                e.printStackTrace();  
            }  

        }  


      <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">createTable</span>(String table, String... colfams)
              <span class="hljs-keyword">throws</span> IOException {
                createTable(table, <span class="hljs-keyword">null</span>, colfams);
              }
      <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">createTable</span>(String table, <span class="hljs-keyword">byte</span>[][] splitKeys, String... colfams)
              <span class="hljs-keyword">throws</span> IOException {
                HTableDescriptor desc = <span class="hljs-keyword">new</span> HTableDescriptor(TableName.valueOf(table));
                <span class="hljs-keyword">for</span> (String cf : colfams) {
                  HColumnDescriptor coldef = <span class="hljs-keyword">new</span> HColumnDescriptor(cf);
                  desc.addFamily(coldef);
                }
                <span class="hljs-keyword">if</span> (splitKeys != <span class="hljs-keyword">null</span>) {
                  admin.createTable(desc, splitKeys);
                } <span class="hljs-keyword">else</span> {
                  admin.createTable(desc);
                }
              }

              <span class="hljs-javadoc">/**
             *<span class="hljs-javadoctag"> @Title</span> disableTable
             *<span class="hljs-javadoctag"> @Description</span> 禁用table
             *<span class="hljs-javadoctag"> @param</span> table
             *<span class="hljs-javadoctag"> @throws</span> IOException
             */</span>
            <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">disableTable</span>(String table) <span class="hljs-keyword">throws</span> IOException {
<span class="hljs-comment">//              admin.disableTable(table);</span>
                admin.disableTable(TableName.valueOf(table));
              }
            <span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">existsTable</span>(String table)
                      <span class="hljs-keyword">throws</span> IOException {
                        <span class="hljs-keyword">return</span> admin.tableExists(TableName.valueOf(table));
                      }
              <span class="hljs-javadoc">/**
             *<span class="hljs-javadoctag"> @Title</span> dropTable
             *<span class="hljs-javadoctag"> @Description</span> dropTable
             *<span class="hljs-javadoctag"> @param</span> table
             *<span class="hljs-javadoctag"> @throws</span> IOException
             */</span>
            <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">dropTable</span>(String table) <span class="hljs-keyword">throws</span> IOException {
                <span class="hljs-keyword">if</span> (existsTable(table)) {
                  disableTable(table);
                  admin.deleteTable(TableName.valueOf(table));
                }
              }
            <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">insterRow</span>(String tableName,String rowkey,String colFamily,String col,String val) <span class="hljs-keyword">throws</span> IOException {  

                Table table = connection.getTable(TableName.valueOf(tableName));  
                Put put = <span class="hljs-keyword">new</span> Put(Bytes.toBytes(rowkey));  
                put.addColumn(Bytes.toBytes(colFamily), Bytes.toBytes(col), Bytes.toBytes(val));  
                table.put(put);  

                <span class="hljs-comment">//批量插入  </span>
               <span class="hljs-comment">/* List&lt;Put&gt; putList = new ArrayList&lt;Put&gt;(); 
                puts.add(put); 
                table.put(putList);*/</span>  
                table.close();  

            }  
              <span class="hljs-comment">//格式化输出  </span>
            <span class="hljs-keyword">public</span>  <span class="hljs-keyword">void</span> <span class="hljs-title">showCell</span>(Result result){  
                Cell[] cells = result.rawCells(); 

                <span class="hljs-keyword">for</span>(Cell cell:cells){  
                      System.out.println(<span class="hljs-string">"RowName:"</span>+<span class="hljs-keyword">new</span> String(CellUtil.cloneRow(cell))+<span class="hljs-string">", "</span>
                        +<span class="hljs-string">"Timetamp:"</span>+cell.getTimestamp()+<span class="hljs-string">", "</span>
                        +<span class="hljs-string">"column Family:"</span>+<span class="hljs-keyword">new</span> String(CellUtil.cloneFamily(cell))+<span class="hljs-string">", "</span>
                        +<span class="hljs-string">"列修饰符:"</span>+<span class="hljs-keyword">new</span> String(CellUtil.cloneQualifier(cell))+<span class="hljs-string">", "</span>
                        +<span class="hljs-string">"value:"</span>+<span class="hljs-keyword">new</span> String(CellUtil.cloneValue(cell))+<span class="hljs-string">" "</span>);  
                }  
            }  

            <span class="hljs-comment">//批量查找数据  </span>
            <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">scanData</span>(String tableName/*,String startRow,String stopRow*/)<span class="hljs-keyword">throws</span> IOException{  

                Table table = connection.getTable(TableName.valueOf(tableName));  
                Scan scan = <span class="hljs-keyword">new</span> Scan();  
                <span class="hljs-comment">//scan.setStartRow(Bytes.toBytes(startRow));  </span>
                <span class="hljs-comment">//scan.setStopRow(Bytes.toBytes(stopRow));  </span>
                ResultScanner resultScanner = table.getScanner(scan);  
                <span class="hljs-keyword">for</span>(Result result : resultScanner){  
                    showCell(result);  
                }  
                table.close();  

            }  
            }</code></pre>

<hr>



<h3 id="2-第二种利用hbase提供的importtsv将csv文件导入到hbase用-importtsv-和-bulk-load将tsv数据文件导入hbase">2 .第二种利用Hbase提供的ImportTsv将csv文件导入到HBase（用 importtsv 和 bulk load将TSV数据文件导入HBase）</h3>

<hr>

<p>importtsv 通过运行一个MapReduce Job，将数据从TSV文件（在数据库里导出的csv或tsv文件）中直接写入HBase的表或者写入一个HBase的自有格式数据文件。 <br>
HBase提供importtsv工具支持从TSV文件中将数据导入HBase。使用该工具将文本数据加载至HBase十分高效，因为它是通过MapReduce Job来实施导入的。哪怕是要从现有的关系型数据库中加载数据，也可以先将数据导入文本文件中，然后使用importtsv 工具导入HBase。在导入海量数据时，这个方式运行的很好，因为导出数据比在关系型数据库中执行SQL快很多。</p>

<p>命令：</p>

<p>格式：hbase [类] [分隔符] [行键，列族] [表] [导入文件] <br>
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator=”,”  <br>
-Dimporttsv.columns=HBASE_ROW_KEY,cf hbase-tb1-001 /data.csv <br>
data.csv数据如下</p>

<pre><code>1,"Tony"
2,"Ivy"
3,"Tom"
4,"Spark"
5,"Storm
</code></pre>

<p>把data.csv上传到hdfs上</p>

<pre><code>[hadoop@master ~]$ hdfs dfs -put /tmp/hbase-input
</code></pre>

<p>建表  </p>

<pre><code>hbase(main):001:0&gt; create 'hbase-tb1-001','cf'

0 row(s) in 3.1120 seconds
</code></pre>

<p>=&gt; Hbase::Table - hbase-tb1-001 <br>
运行命令</p>

<pre><code>[hadoop@master ~]$ hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator="," 
-Dimporttsv.columns=HBASE_ROW_KEY,cf hbase-tb1-001 /tmp/hbase-input/data.csv
</code></pre>

<p>查看结果</p>

<pre><code>hbase(main):003:0&gt; scan 'hbase-tb1-001'

ROW                  COLUMN+CELL
 1                   column=cf:, timestamp=1436152834178, value="Tony"
 2                   column=cf:, timestamp=1436152834178, value="Ivy"
 3                   column=cf:, timestamp=1436152834178, value="Tom"
 4                   column=cf:, timestamp=1436152834178, value="Spark"
 5                   column=cf:, timestamp=1436152834178, value="Storm"
5 row(s) in 0.1490 seconds
</code></pre>

<hr>



<h3 id="3-第三种利用hbase提供的completebulkload将数据导入到hbase">3 . 第三种利用HBase提供的completebulkload将数据导入到HBase</h3>

<hr>

<p>HBase支持bulkload的入库方式，它是利用hbase的数据信息按照特定格式存储在hdfs内这一原理，直接在HDFS中生成持久化的HFile数据格式文件，然后上传至合适位置，即完成巨量数据快速入库的办法。配和mapreduce完成，高效便捷，而且不占用region资源，增添负载，在大数据量写入时，能极大的提高写入效率，并降低对HBase节点的写入压力。  <br>
通过使用先生成HFile，然后再BulkLoad到HBase的方式来替代之前直接调用HTableOutputFormat的方法有如下的好处：  <br>
1、消除了对HBase集群的插入压力  <br>
2、提高了Job的运行速度，降低了Job的执行时间</p>

<p>利用completebulkload将数据导入到HBase  <br>
运行过程发现出现错误，解决办法是在hadoo-env.sh 里给hadoopclass添加hbase下lib的jar路径。</p>

<pre><code>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/home/hadoop/hbase-1.2.4/lib/*
</code></pre>



<hr>

<p>data.csv数据如下 <br>
1,”Tony” <br>
2,”Ivy” <br>
3,”Tom” <br>
4,”Spark” <br>
5,”Storm <br>
把data.csv上传到hdfs上 <br>
        [hadoop@master ~]$ hdfs dfs -put /tmp/hbase-input <br>
 1. 先通过lmportTsv生成HFile</p>

<pre><code>    [hadoop@master hadoop]$hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.bulk.output=/chenglianlong/hfile_tmp -Dimporttsv.columns=HBASE_ROW_KEY,cf hbase-tbl-002 /tmp/hbase-input/data.csv
</code></pre>

<p>命令：hbase [类] [分隔符] [输出存储路径] [行键,列族] [表] [导入原始数据文件] <br>
以上的指令，它会主动创建表hbase-tbl-002和文件夹hfile_tmp。 <br>
2. 通过completebulkload将数据导入表hbase-tbl-002</p>

<pre><code>    [hadoop@master hadoop]$ hadoop jar /home/hadoop/hbase-1.2.4/lib/hbase-server-1.2.4.jar completebulkload /chenglianlong/hfile_tmp hbase-tbl-002
</code></pre>

<p>3.  查看结果</p>

<pre><code>    hbase(main):001:0&gt; scan 'hbase-tbl-002'
    ROW                      COLUMN+CELL                                                                      
    1                           column=cf:, timestamp=1487579771424, value="Tony"                                
    2                           column=cf:, timestamp=1487579771424, value="Ivy"                                 
    3                           column=cf:, timestamp=1487579771424, value="Tom"                                 
    4                           column=cf:, timestamp=1487579771424, value="Spark"                               
    5                           column=cf:, timestamp=1487579771424, value="Storm"                               
            5 row(s) in 0.6490 seconds
</code></pre>



<hr>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>