---
layout:     post
title:      Hadoop2.7集群环境搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p id="main-toc"><strong>目录</strong></p>

<p id="-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Hadoop%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87" rel="nofollow">一、安装Hadoop前的准备</a></p>

<p id="%E4%BA%8C%E3%80%81%E5%AE%89%E8%A3%85Hadoop-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E5%AE%89%E8%A3%85Hadoop" rel="nofollow">二、安装Hadoop</a></p>

<p id="1%E3%80%81%E5%AE%89%E8%A3%85Hadoop-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E5%AE%89%E8%A3%85Hadoop" rel="nofollow">1、安装Hadoop</a></p>

<p id="2%E3%80%81%E9%85%8D%E7%BD%AEHadoop%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E9%85%8D%E7%BD%AEHadoop%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" rel="nofollow">2、配置Hadoop的环境变量</a></p>

<p id="3%E3%80%81%E4%BF%AE%E6%94%B9Hadoop%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E4%BF%AE%E6%94%B9Hadoop%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">3、修改Hadoop的配置文件</a></p>

<p id="4%E3%80%81%E5%90%AF%E5%8A%A8-toc" style="margin-left:40px;"><a href="#4%E3%80%81%E5%90%AF%E5%8A%A8" rel="nofollow">4、启动</a></p>

<p id="1%EF%BC%89%E3%80%81%E5%90%AF%E5%8A%A8HDFS-toc" style="margin-left:80px;"><a href="#1%EF%BC%89%E3%80%81%E5%90%AF%E5%8A%A8HDFS" rel="nofollow">1）、启动HDFS</a></p>

<p id="2%EF%BC%89%E3%80%81%E5%90%AF%E5%8A%A8yarn-toc" style="margin-left:80px;"><a href="#2%EF%BC%89%E3%80%81%E5%90%AF%E5%8A%A8yarn" rel="nofollow">2）、启动yarn</a></p>

<p id="3%E3%80%81%E5%90%AF%E5%8A%A8%E5%85%A8%E9%83%A8-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E5%90%AF%E5%8A%A8%E5%85%A8%E9%83%A8" rel="nofollow">3、启动全部</a></p>

<p id="5%E3%80%81Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E-toc" style="margin-left:40px;"><a href="#5%E3%80%81Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E" rel="nofollow">5、Hadoop常用命令说明</a></p>

<p id="%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98" rel="nofollow">三、安装过程可能遇到的问题</a></p>

<hr id="hr-toc"><h1 id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85Hadoop%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87">一、安装Hadoop前的准备</h1>

<p>服务器目录约定：</p>

<p>所有的按照文件均放在/export下</p>

<p>/export/package  放置下载的安装包（只在主节点下，其他几点通过scp命令安装）</p>

<p>/export/java安装jdk</p>

<p> </p>

<p>备注：</p>

<p>/etc/hostname 作用：给服务器起一个名称</p>

<p>/etc/hosts 作用： 类似Windows下的hosts文件，配置节点的ip和域名对应关系</p>

<p><strong>1、在VMware中构建三台centos7服务器，每个节点对应的hostname和IP如下</strong></p>

<p>node1 192.168.254.101</p>

<p>node2  192.168.254.102</p>

<p>node3 192.168.254.103</p>

<p>用户名和密码：</p>

<p>root   111111</p>

<p><strong>2、设置node1和node2、node3免密钥登录，便于在主节点的配置快速复制到从节点</strong></p>

<p>这里node1设置为master节点，node2和node3位从节点，详细的免密钥登录参考</p>

<p><a href="https://blog.csdn.net/l1394049664/article/details/82528372" rel="nofollow">centos7多个节点之间实现免密钥登录</a></p>

<p><strong>3、在主节点node1安装jdk然后同步到其他节点</strong></p>

<p>首先下载jdk-7u80-linux-i586.tar.gz包上传到服务器，然后使用tar命令解压安装包</p>

<p>tar zxvf jdk-7u80-linux-x64.tar.gz -C /export/java</p>

<p>配置环境变量 vi /etc/profile，把下面的配置添加到最后</p>

<pre class="has">
<code># jdk
export JAVA_HOME=/export/java/jdk1.7.0_80
export JRE_HOME=/$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</code></pre>

<p>然后使用：source /etc/profile</p>

<p>刷新环境变量配置，最后使用java -version</p>

<p><img alt="" class="has" height="107" src="https://img-blog.csdn.net/20180908155010472?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="572"></p>

<p>然后使用scp命令，把安装包和环境变量配置同步到其他节点</p>

<p>同步复制安装包：</p>

<p>scp -r /export/java/jdk1.7.0_80 node2:/export/java/jdk1.7.0_80</p>

<p>scp -r /export/java/jdk1.7.0_80 node3:/export/java/jdk1.7.0_80</p>

<p>同步复制环境变量文件：</p>

<p>scp /etc/profile node2:/etc/profile</p>

<p>scp /etc/profile node3:/etc/profile</p>

<p>在node2和node3查看jdk的按照情况</p>

<p><img alt="" class="has" height="114" src="https://img-blog.csdn.net/20180908155313627?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="534"></p>

<p><img alt="" class="has" height="122" src="https://img-blog.csdn.net/20180908155333706?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="536"></p>

<p><strong>4、关闭每个节点的防火墙</strong></p>

<p>查看防火墙状态   firewall-cmd    --state<br>
关闭防火墙    systemctl  stop   firewalld.service</p>

<p>开启防火墙   systemctl  start   firewalld.service<br>
禁止开机启动启动防火墙   systemctl   disable   firewalld.service</p>

<p> </p>

<h1 id="%E4%BA%8C%E3%80%81%E5%AE%89%E8%A3%85Hadoop">二、安装Hadoop</h1>

<p>约定目录结构</p>

<p>/export/data               HDFS的数据目录</p>

<p>/export/data/hadoop/dfs/name         存放元数据，在hdfs-site.xml中配置</p>

<p>/export/data/hadoop/dfs/data           存放数据目录，在hdfs-site.xml中配置</p>

<p>/export/temp  Hadoop的临时文件的目录，在core-site.xml中配置</p>

<h2 id="1%E3%80%81%E5%AE%89%E8%A3%85Hadoop">1、安装Hadoop</h2>

<p>首先下载hadoop-2.7.3.tar.gz安装包，并且上传到服务器上，然后使用下面的命令解压安装包</p>

<p>tar zxvf hadoop-2.7.3.tar.gz -C /export/</p>

<h2 id="2%E3%80%81%E9%85%8D%E7%BD%AEHadoop%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">2、配置Hadoop的环境变量</h2>

<pre class="has">
<code>export    HADOOP_HOME=/export/hadoop-2.7.3
export PATH=$PATH:$HADOOP_HOME/bin </code></pre>

<h2 id="3%E3%80%81%E4%BF%AE%E6%94%B9Hadoop%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">3、修改Hadoop的配置文件</h2>

<p>这里要涉及到的配置文件有7个：<br>
/export/hadoop-2.7.3/etc/hadoop/hadoop-env.sh        <span style="color:#f33b45;">Hadoop环境变量</span><br>
/export/hadoop-2.7.3/etc/hadoop/yarn-env.sh<br>
/export/hadoop-2.7.3/etc/hadoop/slaves                     <span style="color:#f33b45;">配置从节点</span><br>
/export/hadoop-2.7.3/etc/hadoop/core-site.xml           <span style="color:#f33b45;">Hadoop核心全局配置文件，其他文件的配置项可以覆盖它的配置项</span><br>
/export/hadoop-2.7.3/etc/hadoop/hdfs-site.xml           <span style="color:#f33b45;">HDFS配置文件，该模板的属性继承于core-site.xml</span><br>
/export/hadoop-2.7.3/etc/hadoop/mapred-site.xml      <span style="color:#f33b45;">MapReduce的配置文件，该模板的属性继承于core-site.xml</span><br>
/export/hadoop-2.7.3/etc/hadoop/yarn-site.xml<br>
其中mapred-site.xml默认不存在的，可以通过复制mapred-site.xml.template文件获得。</p>

<p>配置文件1：hadoop-env.sh</p>

<pre class="has">
<code>export JAVA_HOME=/export/java/jdk1.7.0_80</code></pre>

<p>配置文件2：yarn-env.sh</p>

<pre class="has">
<code>export JAVA_HOME=/export/java/jdk1.7.0_80</code></pre>

<p>配置文件3：slaves （这个文件里面保存所有slave节点）</p>

<pre class="has">
<code>node2
node3</code></pre>

<p>配置文件4：core-site.xml</p>

<pre class="has">
<code>&lt;configuration&gt;
//指定hdfs的主端口 namenode要放在哪台机器上
     &lt;property&gt;
             &lt;name&gt;fs.defaultFS&lt;/name&gt;
             &lt;value&gt;hdfs://node1:9000&lt;/value&gt;
    &lt;/property&gt;
//流缓冲区大小 128MB 
    &lt;property&gt;
             &lt;name&gt;io.file.buffer.size&lt;/name&gt;
             &lt;value&gt;131072&lt;/value&gt;
    &lt;/property&gt;
&lt;!--用来指定使用hadoop时产生文件的存放目录--&gt;
   &lt;property&gt;
             &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
             &lt;value&gt;file:/export/temp&lt;/value&gt;
              &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
    &lt;/property&gt;

&lt;/configuration&gt;</code></pre>

<p>配置文件5：hdfs-site.xml</p>

<pre class="has">
<code>&lt;configuration&gt;
//Namenode HTTP服务器地址和端口。
    &lt;property&gt;
                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
               &lt;value&gt;node1:9001&lt;/value&gt;
        &lt;/property&gt;
//存贮在本地的名字节点数据镜象的目录,作为名字节点的冗余备份
     &lt;property&gt;
             &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
             &lt;value&gt;file:/export/data/hadoop/dfs/name&lt;/value&gt;
    &lt;/property&gt;
//数据节点的块本地存放目录
     &lt;property&gt;
             &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
             &lt;value&gt;file:/export/data/hadoop/dfs/data&lt;/value&gt;
    &lt;/property&gt;
//备份数
    &lt;property&gt;
             &lt;name&gt;dfs.replication&lt;/name&gt;
             &lt;value&gt;3&lt;/value&gt;
    &lt;/property&gt;
//使WebHDFS（REST API）在Namenodes和数据节点
     &lt;property&gt;
                     &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
                     &lt;value&gt;true&lt;/value&gt;
         &lt;/property&gt;
&lt;/configuration&gt;</code></pre>

<p>配置文件6：mapred-site.xml</p>

<pre class="has">
<code>&lt;configuration&gt;
//告诉hadoop以后MR(Map/Reduce)运行在YARN上
   &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
          &lt;value&gt;node1:10020&lt;/value&gt;
   &lt;/property&gt;

   &lt;property&gt;
               &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
               &lt;value&gt;node1:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>

<p>配置文件7：yarn-site.xml</p>

<pre class="has">
<code>&lt;configuration&gt;
        &lt;property&gt;
               &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
               &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;

        &lt;property&gt;
               &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
               &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
        &lt;/property&gt;

        &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
               &lt;value&gt;node1:8032&lt;/value&gt;
       &lt;/property&gt;

       &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
               &lt;value&gt;node1:8030&lt;/value&gt;
         &lt;/property&gt;

         &lt;property&gt;
                       &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
                        &lt;value&gt; node1:8031&lt;/value&gt;

           &lt;/property&gt;

           &lt;property&gt;
                       &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
                        &lt;value&gt; node1:8033&lt;/value&gt;
          &lt;/property&gt;
&lt;/configuration&gt;</code></pre>

<p> </p>

<h2 id="4%E3%80%81%E5%90%AF%E5%8A%A8">4、启动</h2>

<p><span style="color:#f33b45;"><strong>在启动之前记得格式化namenode空间：hadoop namenode -format</strong></span></p>

<p>首先切换到安装目录/export/hadoop-2.7.3</p>

<p>由于Hadoop2的架构，数据存放HDFS和计算框架解耦，所以可以分别启动</p>

<h3 id="1%EF%BC%89%E3%80%81%E5%90%AF%E5%8A%A8HDFS">1）、启动HDFS</h3>

<pre class="has">
<code>./sbin/start-dfs.sh</code></pre>

<p><img alt="" class="has" height="102" src="https://img-blog.csdn.net/20180908175832444?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="299"></p>

<p><img alt="" class="has" height="91" src="https://img-blog.csdn.net/20180908175849537?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="304"></p>

<p><img alt="" class="has" height="85" src="https://img-blog.csdn.net/20180908175921289?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="342"></p>

<p>访问HDFS管理界面：<a href="http://192.168.254.101:50070" rel="nofollow">http://192.168.254.101:50070</a></p>

<p><img alt="" class="has" height="909" src="https://img-blog.csdn.net/20180908181004510?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<p> </p>

<h3 id="2%EF%BC%89%E3%80%81%E5%90%AF%E5%8A%A8yarn">2）、启动yarn</h3>

<pre class="has">
<code>./sbin/start-yarn.sh</code></pre>

<p><img alt="" class="has" height="82" src="https://img-blog.csdn.net/20180908181251719?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="316"></p>

<p><img alt="" class="has" height="83" src="https://img-blog.csdn.net/20180908181308122?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="275"></p>

<p><img alt="" class="has" height="88" src="https://img-blog.csdn.net/20180908181324262?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="288"></p>

<p>访问MR管理界面：<a href="http://192.168.254.101:8088" rel="nofollow">http://192.168.254.101:8088</a></p>

<p><img alt="" class="has" height="510" src="https://img-blog.csdn.net/20180908181437200?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<h3 id="3%E3%80%81%E5%90%AF%E5%8A%A8%E5%85%A8%E9%83%A8">3、启动全部</h3>

<p><img alt="" class="has" height="123" src="https://img-blog.csdn.net/20180908185354898?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="329"></p>

<p><img alt="" class="has" height="106" src="https://img-blog.csdn.net/20180908185414634?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="258"></p>

<p><img alt="" class="has" height="102" src="https://img-blog.csdn.net/20180908185430982?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMzk0MDQ5NjY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="256"> </p>

<h2 id="5%E3%80%81Hadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E">5、Hadoop常用命令说明</h2>

<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td style="width:159px;">文件名称 </td>
			<td style="width:689px;">说明</td>
		</tr><tr><td style="width:159px;">hadoop-daemon.sh    </td>
			<td style="width:689px;">通过执行hadoop命令来启动/停止一个守护进程(daemon)；该命令会被bin目录下面所有以start或stop开头的所有命令调用来执行命令，hadoop-daemons.sh也是通过调用hadoop-daemon.sh来执行命令的，而hadoop-daemon.sh本身就是通过调用hadoop命令来执行任务。</td>
		</tr><tr><td style="width:159px;">start-all.sh </td>
			<td style="width:689px;">全部启动，它会调用start-dfs.sh及start-yarn.sh</td>
		</tr><tr><td style="width:159px;">start-dfs.sh</td>
			<td style="width:689px;">启动NameNode、DataNode以及SecondaryNameNode</td>
		</tr><tr><td style="width:159px;">start-yarn.sh</td>
			<td style="width:689px;">启动MapReduce</td>
		</tr><tr><td style="width:159px;">stop-all.sh</td>
			<td style="width:689px;">全部停止，它会调用stop-dfs.sh及start-yarn.sh</td>
		</tr><tr><td style="width:159px;">stop-balancer.sh </td>
			<td style="width:689px;">停止balancer</td>
		</tr><tr><td style="width:159px;">stop-dfs.sh</td>
			<td style="width:689px;">停止NameNode、DataNode及SecondaryNameNode</td>
		</tr><tr><td style="width:159px;">stop-yarn.sh</td>
			<td style="width:689px;">
			<p>停止MapReduce</p>
			</td>
		</tr><tr><td style="width:159px;"> </td>
			<td style="width:689px;"> </td>
		</tr></tbody></table><h1 id="%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98">三、安装过程可能遇到的问题</h1>

<p><span style="color:#f33b45;"><strong>1、启动时候出现</strong></span></p>

<p>guard.: ssh: Could not resolve hostname guard.: Name or service not known</p>

<p>now.: ssh: Could not resolve hostname now.: Name or service not known</p>

<p>etc/profile配置文件添加</p>

<pre class="has">
<code>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS=-Djava.library.path=$HADOOP_HOME/lib</code></pre>

<p><span style="color:#f33b45;"><strong>2、datanode节点起来了，但是没有namenode节点和secondarynode节点，并且启动界面有下面的信息</strong></span></p>

<p>Starting namenodes on [Java HotSpot(TM) Client VM warning: You have loaded library /export/hadoop-2.7.3/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.</p>

<p>这个问题的错误原因会发生在64位的操作系统上，原因是从官方下载的hadoop使用的本地库文件(例如lib/native/libhadoop.so.1.0.0)都是基于32位编译的，运行在64位系统上就会出现上述错误。解决方法之一是在64位系统上重新编译hadoop，另一种方法是在hadoop-env.sh和yarn-env.sh中添加如下两行：</p>

<pre class="has">
<code>export HADOOP_COMMON_LIB_NATIVE_DIR=/export/hadoop-2.7.3/lib/native  
export HADOOP_OPTS="-Djava.library.path=/export/hadoop-2.7.3/lib"  
</code></pre>

<p><span style="color:#f33b45;"><strong>3、启动hadoop时，namenode无法启动，log中出现：java.io.IOException: NameNode is not formatted</strong></span></p>

<p>解决方案：hadoop namenode -format</p>

<p> </p>

<p> </p>

<p> </p>            </div>
                </div>