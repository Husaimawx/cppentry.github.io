---
layout:     post
title:      Hadoop单机版安装步骤
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h2>1.创建组</h2>
<p>groupadd -g 301 hadoop</p>
<h2>2.添加hadoop用户</h2>
<p>useradd –g hadoop -d /home/hadoop –m hadoop</p>
<p>passwd hadoop</p>
<h2>3.修改hosts文件</h2>
<p>su - root。在root用户下操作：</p>
<p>vi /etc/hosts</p>
<p>在127.0.0.1       localhost的上面一行添加：</p>
<p> </p>
<p><span style="color:#0070C0;">10.16.11.253(</span><span style="color:#0070C0;">该机器的</span><span style="color:#0070C0;">ip</span><span style="color:#0070C0;">地址</span><span style="color:#0070C0;">)       hadoop(</span><span style="color:#0070C0;">主机名</span><span style="color:#0070C0;">)</span></p>
<p> </p>
<h2>4.修改机器名</h2>
<p>su - root。在root用户下操作：</p>
<p>vi /etc/hostname:添加主机名</p>
<p><span style="color:#0070C0;">hadoop</span></p>
<p> </p>
<p><strong><span style="color:#FF0000;">以下步骤，全部使用</span><span style="color:#FF0000;">hadoop</span><span style="color:#FF0000;">用户操作：</span></strong></p>
<h2>5.配置SSH：</h2>
<p>修改/etc/ssh/sshd_config找到以下内容并去掉“#”：</p>
<p>vi <a name="OLE_LINK17"></a><a name="OLE_LINK16">/etc/ssh/sshd_config</a></p>
<p>RSAAuthentication yes</p>
<p>PubkeyAuthentication yes</p>
<p>AuthorizedKeysFile .ssh/authorized_keys</p>
<p>添加允许帐户：</p>
<p>AllowUsers root hadoop</p>
<p>最后，重启SSH服务：</p>
<p>service sshd restart</p>
<p> </p>
<h2>6.下载JAVA和Hadoop</h2>
<p>下载java1.7。放到/usr目录下。JAVA_HOME为：/usr/jdk1.7.0_10</p>
<p>下载hadoop，解压后，放到/home/software目录下。</p>
<p> </p>
<h2>7.修改profile文件</h2>
<p>在hadoop的.profile文件中添加下列内容：</p>
<p>JAVA_HOME=/home/software/jdk1.7.0_45</p>
<p>CLASSPATH=$JAVA_HOME/lib:.$JAVA_HOME/jre/lib:.</p>
<p>PATH=$JAVA_HOME/bin:$PATH</p>
<p>export JAVA_HOME CLASSPATH PATH</p>
<p> </p>
<p>HADOOP_HOME=/home/software/hadoop-1.2.1</p>
<p>PATH=$HADOOP_HOME/bin:$PATH</p>
<p>export HADOOP_HOME PATH</p>
<p> </p>
<p>export HADOOP_CONF_DIR=$HADOOP_HOME/conf</p>
<p> </p>
<h2>8.修改hdfs-site.xml</h2>
<p>cd $HADOOP_HOME/conf。使用vi命令修改。</p>
<p><a name="OLE_LINK4"></a><a name="OLE_LINK3">&lt;property&gt;</a></p>
<p>         &lt;name&gt;dfs.name.dir&lt;/name&gt;</p>
<p>         &lt;value&gt;/home/hadoop/hadoopNAME&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;dfs.data.dir&lt;/name&gt;</p>
<p>         &lt;value&gt;/home/hadoop/hadoopDATA&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;dfs.replication&lt;/name&gt;</p>
<p>         &lt;value&gt;1&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;dfs.support.append&lt;/name&gt;</p>
<p>         &lt;value&gt;true&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<h2>9.修改core-site.xml</h2>
<p><a name="OLE_LINK6"></a><a name="OLE_LINK5">&lt;property&gt;</a></p>
<p>         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</p>
<p>         &lt;value&gt;/home/hadoop/hadoopTMP&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;fs.default.name&lt;/name&gt;</p>
<p>         &lt;value&gt;hdfs://<span style="color:#FF0000;">10.16.11.253</span>:9000&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt; </p>
<p> &lt;name&gt;<a name="OLE_LINK8"></a><a name="OLE_LINK7">hadoop.native.lib</a>&lt;/name&gt;</p>
<p> &lt;value&gt;false&lt;/value&gt; </p>
<p>&lt;/property&gt;</p>
<p> </p>
<h2>10.修改mapred-site.xml</h2>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;mapred.job.tracker&lt;/name&gt;</p>
<p>         &lt;value&gt;<span style="color:#FF0000;">192.168.1.103</span>:9001&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;mapred.system.dir&lt;/name&gt;</p>
<p>         &lt;value&gt;/home/hadoop/hadoopDATA/mapred.system.dir&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;mapred.local.dir&lt;/name&gt;</p>
<p>         &lt;value&gt;/home/hadoop/hadoopDATA/mapred.local.dir&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p>&lt;property&gt;</p>
<p>         &lt;name&gt;mapred.child.java.opts&lt;/name&gt;</p>
<p>         &lt;value&gt;-d64-Xmx256m&lt;/value&gt;</p>
<p>&lt;/property&gt;</p>
<p> </p>
<h2>11.修改masters文件和slaves文件</h2>
<p>cd $HADOOP_HOME/conf</p>
<p>vi slaves:写入主机名：</p>
<p><span style="color:#0070C0;">hadoop</span></p>
<p> </p>
<p>vi masters:写入主机名：</p>
<p><span style="color:#0070C0;">hadoop</span></p>
<h2>12.在.profile末尾中添加</h2>
<p>export HADOOP_HOME_WARN_SUPPRESS=1</p>
<p> </p>
<h2>13.生成密钥，形成ssh互信机制</h2>
<p>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</p>
<p>cd .ssh</p>
<p>cp id_rsa.pub authorized_keys</p>
<p> </p>
<p>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa</p>
<p>cd .ssh</p>
<p>cat id_dsa.pub &gt;&gt; authorized_keys</p>
<p> </p>
<p><a name="OLE_LINK10"></a><a name="OLE_LINK9">chmod 600 authorized_keys</a></p>
<p> </p>
<p> </p>
<p> </p>
<h2>14.conf/hadoop-env.sh末尾添加</h2>
<p>export JAVA_HOME=/usr/jdk1.7.0_10</p>
<p> </p>
<h2>15.格式化namenode</h2>
<p><a name="OLE_LINK13"></a><a name="OLE_LINK12"></a><a name="OLE_LINK11">hadoop namenode -format</a></p>
<p> </p>
<h2>16.启动hdfs</h2>
<p><a name="OLE_LINK15"></a><a name="OLE_LINK14">start-dfs.sh</a></p>
<p> </p>
<h2>17.启动mapreduce</h2>
<p><a name="OLE_LINK21"></a><a name="OLE_LINK20">start-mapred.sh</a></p>
<p> </p>
<h2>18.查看集群状态</h2>
<p><a name="OLE_LINK19"></a><a name="OLE_LINK18">hadoop dfsadmin -report</a></p>
<p> </p>
<h2>19.测试</h2>
<p>echo "Hello world Bye world" &gt;/home/hadoop/testhadoopbysunyt/f1</p>
<p>echo "Hello world Bye world" &gt;/home/hadoop/testhadoopbysunyt/f2</p>
<p>hadoop fs -mkdir/home/hadoop/hadoopTMP/tmp/input</p>
<p>hadoop fs -put/home/hadoop/testhadoopbysunyt/* /home/hadoop/hadoopTMP/tmp/input/</p>
<p>hadoop fs -ls/home/hadoop/hadoopTMP/tmp/input/</p>
<p> </p>
<p>cd $HADOOP_HOME</p>
<p>1.0.4的测试办法：</p>
<p>hadoop jar hadoop-examples-1.0.4.jarwordcount /home/hadoop/hadoopTMP/tmp/input/ /output</p>
<p>1.1.1的运行办法：</p>
<p>hadoop jar hadoop-examples-1.1.1.jarwordcount /home/hadoop/hadoopTMP/tmp/input/ /output</p>
<p>hadoop fs -ls /output/</p>
<p>hadoop fs -cat /output/part-r-00000</p>
<p> </p>
<h2>备注：如何使用ant重新编译native库</h2>
<p>1.      下载ant;</p>
<p>2.      修改.profile</p>
<p>在.profile中添加如下内容：</p>
<p>exportANT_HOME=/home/hadoop/software/apache-ant-1.8.4</p>
<p>exportPATH=$ANT_HOME/bin:$PATH</p>
<p>3.      执行如下命令，让配置生效：</p>
<p>source .profile</p>
<p>4.      编译</p>
<p>在$HADOOP_HOME目录下，使用如下命令：</p>
<p>ant compile-native</p>
<p>5．移动编译好的文件：</p>
<p>把build/native/Linux-amd64-64/lib的所有文件，拷贝到HADOOP HOME的： </p>
<p>lib/native/Linux-amd64-64/下。</p>
<p>6. 将build目录移走或删除。否则有些意想不到的结果</p>
<p>7. start-all.sh后，查看日志是否报错。日志位置：$HADOOP_HOME/logs</p>
<p>8. 运行jps命令，查看是不是所有服务都已经启动。</p>
            </div>
                </div>