---
layout:     post
title:      大数据-spark概述
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/xiaoqiang17/article/details/77432189				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>1.  Spark概述</h1>
<h2><a name="_Toc19173"></a><a name="_Toc32712"></a><a name="_Toc1922"></a><a name="_Toc421117888"></a><a name="_Toc21707"></a>1.1.  
 什么是Spark（官网：<a href="http://spark.apache.org" rel="nofollow">http://spark.apache.org</a>）</h2>
<p>Spark是一种快速、通用、可扩展的大数据分析引擎。目前，Spark生态系统已经包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark是基于<strong>内存计算的</strong>大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下<strong>数据处理的实时性</strong>，同时保证了<strong>高容错性和高可伸缩性</strong>，允许用户将Spark部署在大量廉价硬件之上，形成集群。</p>
<h2><a name="_Toc23663"></a><a name="_Toc12440"></a>1.2.   为什么要学Spark</h2>
<p><span style="color:#FF0000;">中间结果输出</span>： <span style="color:#FF0000;">Spark</span><span style="color:#FF0000;">是</span><span style="color:#FF0000;">MapReduce</span><span style="color:#FF0000;">的替代方案，而且兼容</span><span style="color:#FF0000;">HDFS</span><span style="color:#FF0000;">、</span><span style="color:#FF0000;">Hive</span><span style="color:#FF0000;">，可融入</span><span style="color:#FF0000;">Hadoop</span><span style="color:#FF0000;">的生态系统，以弥补</span><span style="color:#FF0000;">MapReduce</span><span style="color:#FF0000;">的不足。</span></p>
<h2><a name="_Toc1566"></a>1.3.  Spark特点</h2>
<h3><a name="_Toc2395"></a>1.3.1.  快</h3>
<p>DAG（<span style="color:rgb(85,85,85);">计算路径的有向无环图</span>）执行引擎，可以通过基于内存来高效处理数据流。</p>
<h3><a name="_Toc27960"></a>1.3.2.  易用</h3>
<p>Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</p>
<h3><a name="_Toc28050"></a>1.3.3.  通用</h3>
<p>Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（SparkSQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。<a name="_Toc19107"></a></p>
<h1><a name="_Toc421117891"></a><a name="_Toc13118"></a>2.  Spark集群安装</h1>
<h2><a name="_Toc421117892"></a><a name="_Toc27509"></a>2.1.  安装</h2>
<h3><a name="_Toc421117893"></a><a name="_Toc22634"></a>2.1.1.   机器部署</h3>
<p>准备两台以上Linux服务器，安装好JDK</p>
<h3><a name="_Toc19885"></a>2.1.2.  下载Spark安装包 </h3>
<p>上传spark-安装包到Linux上</p>
<p>解压安装包到指定位置</p>
<p><span style="color:rgb(51,51,51);">tar -zxvf spark-2.1.0-bin-hadoop2.6.tgz -C/usr/local</span></p>
<h3><a name="_Toc691"></a>2.1.3.  配置Spark</h3>
<p>进入到Spark安装目录</p>
<p><span style="color:rgb(51,51,51);">cd /usr/local/spark-2.1.0-bin-hadoop2.6</span></p>
<p>进入conf目录并重命名并修改spark-env.sh.template文件</p>
<p><span style="color:rgb(51,51,51);">cd conf/</span></p>
<p><span style="color:rgb(51,51,51);">mv spark-env.sh.template spark-env.sh</span></p>
<p><span style="color:rgb(51,51,51);">vi spark-env.sh</span></p>
<p>在该配置文件中添加如下配置</p>
<p><span style="color:#FF0000;">export JAVA_HOME=/usr/java/jdk1.8.0_111</span></p>
<p><span style="color:#FF0000;">export SPARK_MASTER_IP=node1.edu360.cn</span></p>
<p><span style="color:#FF0000;">export SPARK_MASTER_PORT=7077</span></p>
<p>保存退出</p>
<p>重命名并修改slaves.template文件</p>
<p><span style="color:rgb(51,51,51);">mv slaves.template slaves</span></p>
<p><span style="color:rgb(51,51,51);">vi slaves</span></p>
<p>在该文件中添加子节点所在的位置（Worker节点）</p>
<p><span style="color:#FF0000;">node2.edu360.cn</span></p>
<p><span style="color:#FF0000;">node3.edu360.cn</span></p>
<p><span style="color:#FF0000;">node4.edu360.cn</span></p>
<p>保存退出</p>
<p>将配置好的Spark拷贝到其他节点上</p>
<p><span style="color:rgb(51,51,51);">scp -r spark-2.1.0-bin-hadoop2.6/node2.edu360.cn:/usr/local/</span></p>
<p><span style="color:rgb(51,51,51);">scp -r spark-2.1.0-bin-hadoop2.6/node3.edu360.cn:/usr/local/</span></p>
<p><span style="color:rgb(51,51,51);">scp -r spark-2.1.0-bin-hadoop2.6/node4.edu360.cn:/usr/local/</span><strong></strong></p>
<p>Spark集群配置完毕，目前是1个Master，3个Work，在node1.edu360.cn上启动Spark集群</p>
<p><span style="color:rgb(51,51,51);">/usr/local/spark-2.1.0-bin-hadoop2.6/sbin/start-all.sh</span></p>
<p>启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：<a href="http://node1.itcast.cn:8080/" rel="nofollow">http://node1.edu360.cn:8080/</a></p>
<p>到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：</p>
<p>Spark集群规划：node1，node2是Master；node3，node4，node5是Worker</p>
<p>安装配置zk集群，并启动zk集群</p>
<p>停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置</p>
<p><span style="color:#FF0000;">exportSPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=zk1,zk2,zk3-Dspark.deploy.zookeeper.dir=/spark"</span></p>
<p>1.在node1节点上修改slaves配置文件内容指定worker节点</p>
<p>2.在node1上执行<strong>sbin/start-all.sh</strong>脚本，然后在node2上执行<strong>sbin/start-master.sh</strong>启动第二个Master</p>
<h1><a name="_Toc2470"></a>3.  执行Spark程序</h1>
<h2><a name="_Toc5486"></a>3.1.  执行第一个spark程序</h2>
<p><span style="color:rgb(51,51,51);">/usr/local/spark-2.1.0-bin-hadoop2.6/bin/spark-submit\</span></p>
<p><span style="color:rgb(51,51,51);">--class org.apache.spark.examples.SparkPi \</span></p>
<p><span style="color:rgb(51,51,51);">--master spark://node1.edu360.cn:7077 \</span></p>
<p><span style="color:rgb(51,51,51);">--executor-memory 1G \</span></p>
<p><span style="color:rgb(51,51,51);">--total-executor-cores 2 \</span></p>
<p><span style="color:rgb(51,51,51);">/usr/local/spark-2.1.0-bin-hadoop2.6/lib/spark-examples-2.1.0-hadoop2.6.0.jar\</span></p>
<p><span style="color:rgb(51,51,51);">100</span></p>
<p>该算法是利用蒙特·卡罗算法求PI</p>
<h2><a name="_Toc6177"></a>3.2.  启动Spark Shell</h2>
<p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p>
<h3><a name="_Toc17940"></a>3.2.1.  启动spark shell</h3>
<p><span style="color:rgb(51,51,51);">/usr/local/spark-2.1.0-bin-hadoop2.6/bin/spark-shell\</span></p>
<p><span style="color:rgb(51,51,51);">--master spark://node1.edu360.cn:7077 \</span></p>
<p><span style="color:rgb(51,51,51);">--executor-memory 2g \</span></p>
<p><span style="color:rgb(51,51,51);">--total-executor-cores 2</span></p>
<p><span style="color:#FF0000;">参数说明：</span></p>
<p><span style="color:#FF0000;">--masterspark://node1.edu360.cn:7077 </span>指定Master的地址</p>
<p><span style="color:#FF0000;">--executor-memory 2g </span>指定每个worker可用内存为2G</p>
<p><span style="color:#FF0000;">--total-executor-cores 2 </span>指定整个集群使用的cup核数为2个</p>
<p><span style="color:#FF0000;">注意：</span></p>
<p><span style="color:#FF0000;">如果启动</span><span style="color:#FF0000;">spark shell</span><span style="color:#FF0000;">时没有指定</span><span style="color:#FF0000;">master</span><span style="color:#FF0000;">地址，但是也可以正常启动</span><span style="color:#FF0000;">spark shell</span><span style="color:#FF0000;">和执行</span><span style="color:#FF0000;">spark
 shell</span><span style="color:#FF0000;">中的程序，其实是启动了</span><span style="color:#FF0000;">spark</span><span style="color:#FF0000;">的</span><span style="color:#0070C0;">local</span><span style="color:#0070C0;">模式</span><span style="color:#FF0000;">，该模式仅在本机启动一个进程，没有与集群建立联系。</span></p>
<p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p>
<h3><a name="_Toc24968"></a>3.2.2.  在spark shell中编写WordCount程序</h3>
<p>1.首先启动hdfs</p>
<p>2.向hdfs上传一个文件到hdfs://node1.edu360.cn:9000/words.txt</p>
<p>3.在spark shell中用scala语言编写spark程序</p>
<p><span style="color:rgb(51,51,51);">sc.textFile("hdfs://node1.edu360.cn:9000/words.txt").flatMap(_.split(""))</span></p>
<p><span style="color:rgb(51,51,51);">.map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://node1.edu360.cn:9000/out")</span></p>
<p> </p>
<p>4.使用hdfs命令查看结果</p>
<p><span style="color:rgb(51,51,51);">hdfs dfs -ls hdfs://node1.edu360.cn:9000/out/p*</span></p>
<p><span style="color:#FF0000;">说明：</span></p>
<p><span style="color:#FF0000;">sc</span>是SparkContext对象，该对象时提交spark程序的入口</p>
<p><span style="color:#FF0000;">textFile(hdfs://node1.edu360.cn:9000/words.txt)</span>是hdfs中读取数据</p>
<p><span style="color:#FF0000;">flatMap(_.split(" "))</span>先map在压平</p>
<p><span style="color:#FF0000;">map((_,1))</span>将单词和1构成元组</p>
<p><span style="color:#FF0000;">reduceByKey(_+_)</span>按照key进行reduce，并将value累加</p>
<p><span style="color:#FF0000;">saveAsTextFile("hdfs://node1.edu360.cn:9000/out")</span>将结果写入到hdfs中</p>
<h2><a name="_Toc2290"></a>3.3.  在IDEA中编写WordCount程序</h2>
<p>spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。</p>
<p>1.创建一个项目 </p>
<p>2.选择Maven项目，然后点击next</p>
<p>3.填写maven的GAV，然后点击next</p>
<p>4.填写项目名称，然后点击finish</p>
<p>5.创建好maven项目后，点击Enable Auto-Import </p>
<p>6.配置Maven的pom.xml</p>
<table border="1" cellspacing="0" cellpadding="0" width="568"><tbody><tr><td valign="top">
<p style="background:#2B2B2B;"><span style="color:#E8BF6A;">&lt;properties&gt;<br>
    &lt;maven.compiler.source&gt;</span><span style="color:#A9B7C6;">1.8</span><span style="color:#E8BF6A;">&lt;/maven.compiler.source&gt;<br>
    &lt;maven.compiler.target&gt;</span><span style="color:#A9B7C6;">1.8</span><span style="color:#E8BF6A;">&lt;/maven.compiler.target&gt;<br>
    &lt;scala.version&gt;</span><span style="color:#A9B7C6;">2.11.8</span><span style="color:#E8BF6A;">&lt;/scala.version&gt;<br>
    &lt;spark.version&gt;</span><span style="color:#A9B7C6;">2.2.0</span><span style="color:#E8BF6A;">&lt;/spark.version&gt;<br>
    &lt;hadoop.version&gt;</span><span style="color:#A9B7C6;">2.8.0</span><span style="color:#E8BF6A;">&lt;/hadoop.version&gt;<br>
    &lt;encoding&gt;</span><span style="color:#A9B7C6;">UTF-8</span><span style="color:#E8BF6A;">&lt;/encoding&gt;<br>
&lt;/properties&gt;<br><br>
&lt;dependencies&gt;<br>
    </span><span style="color:#808080;">&lt;!-- </span><span style="color:#808080;">导入</span><span style="color:#808080;">scala</span><span style="color:#808080;">的依赖</span><span style="color:#808080;">--&gt;<br>
    </span><span style="color:#E8BF6A;">&lt;dependency&gt;<br>
        &lt;groupId&gt;</span><span style="color:#A9B7C6;">org.scala-lang</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
        &lt;artifactId&gt;</span><span style="color:#A9B7C6;">scala-library</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
        &lt;version&gt;</span><span style="color:#A9B7C6;">${scala.version}</span><span style="color:#E8BF6A;">&lt;/version&gt;<br>
    &lt;/dependency&gt;<br><br>
    </span><span style="color:#808080;">&lt;!-- </span><span style="color:#808080;">导入</span><span style="color:#808080;">spark</span><span style="color:#808080;">的依赖</span><span style="color:#808080;">--&gt;<br>
    </span><span style="color:#E8BF6A;">&lt;dependency&gt;<br>
        &lt;groupId&gt;</span><span style="color:#A9B7C6;">org.apache.spark</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
        &lt;artifactId&gt;</span><span style="color:#A9B7C6;">spark-core_2.11</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
        &lt;version&gt;</span><span style="color:#A9B7C6;">${spark.version}</span><span style="color:#E8BF6A;">&lt;/version&gt;<br>
    &lt;/dependency&gt;<br><br>
    </span><span style="color:#808080;">&lt;!-- </span><span style="color:#808080;">指定</span><span style="color:#808080;">hadoop-client API</span><span style="color:#808080;">的版本</span><span style="color:#808080;">--&gt;<br>
    </span><span style="color:#E8BF6A;">&lt;dependency&gt;<br>
        &lt;groupId&gt;</span><span style="color:#A9B7C6;">org.apache.hadoop</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
        &lt;artifactId&gt;</span><span style="color:#A9B7C6;">hadoop-client</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
        &lt;version&gt;</span><span style="color:#A9B7C6;">${hadoop.version}</span><span style="color:#E8BF6A;">&lt;/version&gt;<br>
    &lt;/dependency&gt;<br><br>
&lt;/dependencies&gt;<br><br>
&lt;build&gt;<br>
    &lt;pluginManagement&gt;<br>
        &lt;plugins&gt;<br>
            </span><span style="color:#808080;">&lt;!-- </span><span style="color:#808080;">编译</span><span style="color:#808080;">scala</span><span style="color:#808080;">的插件</span><span style="color:#808080;">--&gt;<br>
            </span><span style="color:#E8BF6A;">&lt;plugin&gt;<br>
                &lt;groupId&gt;</span><span style="color:#A9B7C6;">net.alchim31.maven</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
                &lt;artifactId&gt;</span><span style="color:#A9B7C6;">scala-maven-plugin</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
                &lt;version&gt;</span><span style="color:#A9B7C6;">3.2.2</span><span style="color:#E8BF6A;">&lt;/version&gt;<br>
            &lt;/plugin&gt;<br>
            </span><span style="color:#808080;">&lt;!-- </span><span style="color:#808080;">编译</span><span style="color:#808080;">java</span><span style="color:#808080;">的插件</span><span style="color:#808080;">--&gt;<br>
            </span><span style="color:#E8BF6A;">&lt;plugin&gt;<br>
                &lt;groupId&gt;</span><span style="color:#A9B7C6;">org.apache.maven.plugins</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
                &lt;artifactId&gt;</span><span style="color:#A9B7C6;">maven-compiler-plugin</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
                &lt;version&gt;</span><span style="color:#A9B7C6;">3.5.1</span><span style="color:#E8BF6A;">&lt;/version&gt;<br>
            &lt;/plugin&gt;<br>
        &lt;/plugins&gt;<br>
    &lt;/pluginManagement&gt;<br>
    &lt;plugins&gt;<br>
        &lt;plugin&gt;<br>
            &lt;groupId&gt;</span><span style="color:#A9B7C6;">net.alchim31.maven</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
            &lt;artifactId&gt;</span><span style="color:#A9B7C6;">scala-maven-plugin</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
            &lt;executions&gt;<br>
                &lt;execution&gt;<br>
                    &lt;id&gt;</span><span style="color:#A9B7C6;">scala-compile-first</span><span style="color:#E8BF6A;">&lt;/id&gt;<br>
                    &lt;phase&gt;</span><span style="color:#A9B7C6;">process-resources</span><span style="color:#E8BF6A;">&lt;/phase&gt;<br>
                    &lt;goals&gt;<br>
                        &lt;goal&gt;</span><span style="color:#A9B7C6;">add-source</span><span style="color:#E8BF6A;">&lt;/goal&gt;<br>
                        &lt;goal&gt;</span><span style="color:#A9B7C6;">compile</span><span style="color:#E8BF6A;">&lt;/goal&gt;<br>
                    &lt;/goals&gt;<br>
                &lt;/execution&gt;<br>
                &lt;execution&gt;<br>
                    &lt;id&gt;</span><span style="color:#A9B7C6;">scala-test-compile</span><span style="color:#E8BF6A;">&lt;/id&gt;<br>
                    &lt;phase&gt;</span><span style="color:#A9B7C6;">process-test-resources</span><span style="color:#E8BF6A;">&lt;/phase&gt;<br>
                    &lt;goals&gt;<br>
                        &lt;goal&gt;</span><span style="color:#A9B7C6;">testCompile</span><span style="color:#E8BF6A;">&lt;/goal&gt;<br>
                    &lt;/goals&gt;<br>
                &lt;/execution&gt;<br>
            &lt;/executions&gt;<br>
        &lt;/plugin&gt;<br><br>
        &lt;plugin&gt;<br>
            &lt;groupId&gt;</span><span style="color:#A9B7C6;">org.apache.maven.plugins</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
            &lt;artifactId&gt;</span><span style="color:#A9B7C6;">maven-compiler-plugin</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
            &lt;executions&gt;<br>
                &lt;execution&gt;<br>
                    &lt;phase&gt;</span><span style="color:#A9B7C6;">compile</span><span style="color:#E8BF6A;">&lt;/phase&gt;<br>
                    &lt;goals&gt;<br>
                        &lt;goal&gt;</span><span style="color:#A9B7C6;">compile</span><span style="color:#E8BF6A;">&lt;/goal&gt;<br>
                    &lt;/goals&gt;<br>
                &lt;/execution&gt;<br>
            &lt;/executions&gt;<br>
        &lt;/plugin&gt;<br><br><br>
        </span><span style="color:#808080;">&lt;!-- </span><span style="color:#808080;">打</span><span style="color:#808080;">jar</span><span style="color:#808080;">插件</span><span style="color:#808080;">--&gt;<br>
        </span><span style="color:#E8BF6A;">&lt;plugin&gt;<br>
            &lt;groupId&gt;</span><span style="color:#A9B7C6;">org.apache.maven.plugins</span><span style="color:#E8BF6A;">&lt;/groupId&gt;<br>
            &lt;artifactId&gt;</span><span style="color:#A9B7C6;">maven-shade-plugin</span><span style="color:#E8BF6A;">&lt;/artifactId&gt;<br>
            &lt;version&gt;</span><span style="color:#A9B7C6;">2.4.3</span><span style="color:#E8BF6A;">&lt;/version&gt;<br>
            &lt;executions&gt;<br>
                &lt;execution&gt;<br>
                    &lt;phase&gt;</span><span style="color:#A9B7C6;">package</span><span style="color:#E8BF6A;">&lt;/phase&gt;<br>
                    &lt;goals&gt;<br>
                        &lt;goal&gt;</span><span style="color:#A9B7C6;">shade</span><span style="color:#E8BF6A;">&lt;/goal&gt;<br>
                    &lt;/goals&gt;<br>
                    &lt;configuration&gt;<br>
                        &lt;filters&gt;<br>
                            &lt;filter&gt;<br>
                                &lt;artifact&gt;</span><span style="color:#A9B7C6;">*:*</span><span style="color:#E8BF6A;">&lt;/artifact&gt;<br>
                                &lt;excludes&gt;<br>
                                    &lt;exclude&gt;</span><span style="color:#A9B7C6;">META-INF/*.SF</span><span style="color:#E8BF6A;">&lt;/exclude&gt;<br>
                                    &lt;exclude&gt;</span><span style="color:#A9B7C6;">META-INF/*.DSA</span><span style="color:#E8BF6A;">&lt;/exclude&gt;<br>
                                    &lt;exclude&gt;</span><span style="color:#A9B7C6;">META-INF/*.RSA</span><span style="color:#E8BF6A;">&lt;/exclude&gt;<br>
                                &lt;/excludes&gt;<br>
                            &lt;/filter&gt;<br>
                        &lt;/filters&gt;<br>
                    &lt;/configuration&gt;<br>
                &lt;/execution&gt;<br>
            &lt;/executions&gt;<br>
        &lt;/plugin&gt;<br>
    &lt;/plugins&gt;<br>
&lt;/build&gt;</span></p>
</td>
</tr></tbody></table><p> </p>
<p>7.新建一个scala class，类型为Object</p>
<p align="center">  </p>
<p>8.编写spark程序</p>
<table border="1" cellspacing="0" cellpadding="0" width="568"><tbody><tr><td valign="top">
<p style="background:#2B2B2B;"><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">conf =</span><strong><span style="color:#CC7832;">new
</span></strong><span style="color:#A9B7C6;">SparkConf().setAppName(</span><span style="color:#6A8759;">"WordCount"</span><span style="color:#A9B7C6;">).setMaster(</span><span style="color:#6A8759;">"local[4]"</span><span style="color:#A9B7C6;">)<br></span><span style="color:#808080;">//sparkContext</span><span style="color:#808080;">是</span><span style="color:#808080;">spark</span><span style="color:#808080;">程序执行的入口<br></span><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">sc =</span><strong><span style="color:#CC7832;">new
</span></strong><span style="color:#A9B7C6;">SparkContext(conf)<br></span><span style="color:#808080;">//</span><span style="color:#808080;">使用</span><span style="color:#808080;">SparkContext</span><span style="color:#808080;">创建</span><span style="color:#808080;">RDD<br>
//</span><span style="color:#808080;">告诉</span><span style="color:#808080;">spark</span><span style="color:#808080;">以后从哪里读取数据（不会立即读取数据，是</span><span style="color:#808080;">lazy</span><span style="color:#808080;">）<br></span><span style="color:#808080;">//sc.textFile(args(0)).flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).sortBy(_._2, false).saveAsTextFile(args(1))<br></span><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">lines: RDD[</span><span style="color:#4E807D;">String</span><span style="color:#A9B7C6;">] = sc.textFile(args(</span><span style="color:#6897BB;">0</span><span style="color:#A9B7C6;">))<br><br></span><span style="color:#808080;">//</span><span style="color:#808080;">切分压平数据<br></span><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">words: RDD[</span><span style="color:#4E807D;">String</span><span style="color:#A9B7C6;">] = lines.flatMap(_.split(</span><span style="color:#6A8759;">" "</span><span style="color:#A9B7C6;">))<br></span><span style="color:#808080;">//</span><span style="color:#808080;">将单词和一组合在一起<br></span><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">wordAndOne: RDD[(</span><span style="color:#4E807D;">String</span><span style="color:#CC7832;">, Int</span><span style="color:#A9B7C6;">)] = words.map((_</span><span style="color:#CC7832;">,</span><span style="color:#6897BB;">1</span><span style="color:#A9B7C6;">))<br></span><span style="color:#808080;">//</span><span style="color:#808080;">按</span><span style="color:#808080;">key</span><span style="color:#808080;">进行聚合<br></span><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">reduced: RDD[(</span><span style="color:#4E807D;">String</span><span style="color:#CC7832;">, Int</span><span style="color:#A9B7C6;">)] = wordAndOne.reduceByKey(_+_)<br></span><span style="color:#808080;">//</span><span style="color:#808080;">排序<br></span><strong><span style="color:#CC7832;">val </span></strong><span style="color:#A9B7C6;">sorted: RDD[(</span><span style="color:#4E807D;">String</span><span style="color:#CC7832;">, Int</span><span style="color:#A9B7C6;">)] = reduced.sortBy(_._2</span><span style="color:#CC7832;">,<strong>false</strong></span><span style="color:#A9B7C6;">)<br></span><span style="color:#808080;">//</span><span style="color:#808080;">产生结果（将数据保存到</span><span style="color:#808080;">hdfs</span><span style="color:#808080;">中）<br></span><span style="color:#A9B7C6;">sorted.saveAsTextFile(args(</span><span style="color:#6897BB;">1</span><span style="color:#A9B7C6;">))<br></span><span style="color:#808080;">//</span><span style="color:#808080;">释放资源<br></span><span style="color:#A9B7C6;">sc.stop()</span></p>
<pre style="background:#FFFFFF;">9. </pre>
</td>
</tr></tbody></table><p> </p>
<p>10.使用Maven打包：首先修改pom.xml中的main class </p>
<p>点击idea右侧的MavenProject选项 </p>
<p>点击Lifecycle,选择clean和package，然后点击<span style="color:#FF0000;">Run Maven Build</span></p>
<p>11.选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上</p>
<p align="center"> </p>
<p>12.首先启动hdfs和Spark集群</p>
<p>启动hdfs</p>
<p><span style="color:rgb(51,51,51);">/usr/local/hadoop-2.6.5/sbin/start-dfs.sh</span></p>
<p>启动spark</p>
<p><span style="color:rgb(51,51,51);">/usr/local/spark-2.1.0-bin-hadoop2.6/sbin/start-all.sh</span></p>
<p>13.使用spark-submit命令提交Spark应用（注意参数的顺序）</p>
<p><span style="color:rgb(51,51,51);">/usr/local/spark-2.1.0-bin-hadoop2.6/bin/spark-submit\</span></p>
<p><span style="color:rgb(51,51,51);">--class cn.itcast.spark.WordCount \</span></p>
<p><span style="color:rgb(51,51,51);">--master spark://node1.edu360.cn:7077 \</span></p>
<p><span style="color:rgb(51,51,51);">--executor-memory 2G \</span></p>
<p><span style="color:rgb(51,51,51);">--total-executor-cores 4 \</span></p>
<p><span style="color:rgb(51,51,51);">/root/spark-mvn-1.0-SNAPSHOT.jar \</span></p>
<p><span style="color:rgb(51,51,51);">hdfs://node1.edu360.cn:9000/words.txt \</span></p>
<p><span style="color:rgb(51,51,51);">hdfs://node1.edu360.cn:9000/out</span></p>
<p>查看程序执行结果</p>
<p><span style="color:rgb(51,51,51);">hdfs dfs -cathdfs://node1.edu360.cn:9000/out/part-00000</span></p>
            </div>
                </div>