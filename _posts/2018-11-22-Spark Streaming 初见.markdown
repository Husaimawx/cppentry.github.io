---
layout:     post
title:      Spark Streaming 初见
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/Trigl/article/details/81949091				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<blockquote>
  <p>本文内容是对 <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="nofollow">Spark Streaming</a> 官方文档的总结，用一个简单的例子来入门 Spark Streaming。</p>
</blockquote>

<p>Spark Streaming 是用来处理实时流数据的，所以必然有一个输入和一个输出：</p>

<p><img src="https://spark.apache.org/docs/latest/img/streaming-arch.png" alt="" title=""></p>

<p>Spark Streaming 的内部实现其实还是 Spark core，将接收到的实时流数据分成一个一个很小的批数据进行处理：</p>

<p><img src="https://spark.apache.org/docs/latest/img/streaming-flow.png" alt="" title=""></p>

<p>Spark Streaming 基本的数据结构是 DStream（discretized stream)，这是一个连续的数据流，其底层是 RDD 的集合。</p>

<h2 id="先上个栗子">先上个栗子</h2>

<p>在我们正式讲解 Spark Streaming 的细节之前，先通过一个简单的栗子来明白它到底是怎么工作的。我们要计算接收到的文本中相同单词出现的次数，数据源是一个 TCP 端口，Spark Streaming 通过监听这个端口来获取数据。</p>

<p>首先我们可以使用 <code>Netcat</code> 创建这个 TCP 数据源，这是大部分类 Unix 系统中都会有的一个小工具，在终端直接输入下面的命令：</p>



<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span></code></pre>

<p>然后开始写 Spark Streaming 程序，引入下面 SBT：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-string">"org.apache.spark"</span> %% <span class="hljs-string">"spark-streaming"</span> % <span class="hljs-number">2.3</span><span class="hljs-number">.1</span>,</code></pre>

<p>第一步我们需要创建一个 <code>StreamingContext</code>，它是所有 streaming 程序的入口。这里我们创建了一个名称是 <code>NetworkWordCount</code>，有两个运行线程（一个用于启动监听，一个用于数据处理），批次间隔是 1 秒的本地 StreamingContext。</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-comment">// Create the context with a 1 second batch size</span>
<span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> SparkConf().setMaster(<span class="hljs-string">"local[2]"</span>).setAppName(<span class="hljs-string">"NetworkWordCount"</span>)
<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(sparkConf, Seconds(<span class="hljs-number">1</span>))</code></pre>

<p>然后我们可以从 TCP 源创建一个 <code>DStream</code>，指定其主机名和端口号：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-comment">// Create a DStream on target ip:port and count the</span>
<span class="hljs-comment">// words in input stream of \n delimited text (eg. generated by 'nc')</span>
<span class="hljs-comment">// Note that no duplication in storage level only for running locally.</span>
<span class="hljs-comment">// Replication necessary in distributed scenario for fault tolerance.</span>
<span class="hljs-keyword">val</span> lines = ssc.socketTextStream(args(<span class="hljs-number">0</span>), args(<span class="hljs-number">1</span>).toInt, StorageLevel.MEMORY_AND_DISK_SER)</code></pre>

<p>通过空格把文本切割成多个单词：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-comment">// Split each line into words</span>
<span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">" "</span>))</code></pre>

<p>之后我们对单词计数并打印出来：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>)).reduceByKey(_ + _)
wordCounts.print()</code></pre>

<p>上面就是实时流转换的流程，只有启动以后这些转换才会实际执行：</p>



<pre class="prettyprint"><code class="language-scala hljs ">ssc.start()             <span class="hljs-comment">// Start the computation</span>
ssc.awaitTermination()  <span class="hljs-comment">// Wait for the computation to terminate</span></code></pre>

<p>OK，上面就是 Streaming 处理的代码，现在我们执行这个程序，然后在 TCP 数据源终端那输入一些内容：</p>



<pre class="prettyprint"><code class=" hljs vala"><span class="hljs-preprocessor"># TERMINAL 1:</span>
<span class="hljs-preprocessor"># Running Netcat</span>
$ nc -lk <span class="hljs-number">9999</span>
哈哈 哈哈 哈哈 呵呵 呵呵 嘿嘿</code></pre>

<p>程序输出结果如下：</p>



<pre class="prettyprint"><code class="language-scala hljs ">(哈哈,<span class="hljs-number">3</span>)
(呵呵,<span class="hljs-number">2</span>)
(嘿嘿,<span class="hljs-number">1</span>)</code></pre>

<p>以上程序在本地 IntelliJ IDEA 就可以运行，源码见 <a href="https://github.com/Trigl/spark-learning/blob/master/src/main/scala/ink/baixin/spark/examples/streaming/NetworkWordCount.scala" rel="nofollow">Spark Streaming Word Count</a></p>



<h2 id="基本概念">基本概念</h2>



<h4 id="sbt">SBT</h4>

<p>想要写自己的 Spark Streaming 程序，首先将下面内容加入 SBT：</p>



<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-string">"org.apache.spark"</span> <span class="hljs-string">%% "spark-streaming_2.11" %</span> <span class="hljs-string">"2.3.1"</span></code></pre>

<p>如果还需要从 Kafka、Flume 或者 AWS Kinesis 中接收数据，还需要加入与之相关的包，例如对于 Kinesis，需要在 SBT 中加入以下内容：</p>



<pre class="prettyprint"><code class=" hljs ruby"><span class="hljs-string">"org.apache.spark"</span> <span class="hljs-string">%% "spark-streaming-kinesis-asl_2.11" %</span> <span class="hljs-string">"2.3.1"</span></code></pre>



<h4 id="初始化-streamingcontext">初始化 StreamingContext</h4>

<p>可以通过 <code>SparkConf</code> 创建一个 <code>StreamingContext</code> 对象：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-keyword">import</span> org.apache.spark._
<span class="hljs-keyword">import</span> org.apache.spark.streaming._

<span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> SparkConf().setAppName(appName).setMaster(master)
<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(conf, Seconds(<span class="hljs-number">1</span>))</code></pre>

<p><code>appName</code> 是显示在集群上的应用名称，<code>master</code> 是 Spark、Mesos 或 YARN 集群的 URL，或者是 <code>local[*]</code>。当运行在集群上时，我们实际上很少把 <code>master</code> 硬编码到程序代码中，而是在执行 <code>spark-submit</code> 把它作为一个参数输入。但是在本地或者单元测试时，我们可以写入 <code>local[*]</code> 使 Spark Streaming 运行在本地。注意创建 <code>StreamingContext</code> 其内部还是创建了一个 <code>SparkContext</code>，我们可以通过 <code>ssc.sparkContext</code> 访问到。</p>

<p>我们也可以通过一个已存在的 <code>SparkContext</code> 对象创建 <code>StreamingContext</code> 对象：</p>



<pre class="prettyprint"><code class="language-scala hljs "><span class="hljs-keyword">import</span> org.apache.spark.streaming._

<span class="hljs-keyword">val</span> sc = ...                <span class="hljs-comment">// existing SparkContext</span>
<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">1</span>))</code></pre>

<p>当定义好 context 以后，还需要进行下面的步骤：</p>

<ol>
<li>通过创建输入 <code>DStreams</code> 来定义数据源</li>
<li>定义一系列 Streaming 的转化和行动操作</li>
<li>使用 <code>streamingContext.start()</code> 开始接收和处理数据</li>
<li>使用 <code>streamingContext.awaitTermination()</code> 等待进程的停止（人为或者某些异常导致）</li>
<li>进程也可以通过 <code>streamingContext.stop()</code> 人为停止</li>
</ol>

<p><strong>注意以下几点：</strong></p>

<ul>
<li>一旦 context 开始以后，就不能建立或者添加新的 streaming 操作了</li>
<li>context 停止以后不能再重启</li>
<li>同一时间一个 JVM 内只能有一个活跃的 StreamingContext</li>
<li><code>streamingContext.stop()</code> 同时也会把 SparkContext 停止，可以通过设置该方法中参数 <code>stopSparkContext</code> 为 false 来避免</li>
<li>SparkContext 可以用来多次创建 StreamingContext，只要保证在新的 StreamingContext 建立之前停掉旧的</li>
</ul>



<h4 id="离散流dstreams">离散流（DStreams）</h4>

<p><code>DStream</code> 是 Spark Streaming 的基本抽象，它代表连续的数据流，不管是源数据还是经过转换处理后的数据。本质上它是一系列 RDD，每一个 DStream 都是确定间隔大小的 RDD 集合，如下图所示。</p>

<p><img src="https://spark.apache.org/docs/latest/img/streaming-dstream.png" alt="" title=""></p>

<p>Streaming 做的转换操作也都是基于 RDD：</p>

<p><img src="https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png" alt="" title=""></p>



<h4 id="输入流和接收器">输入流和接收器</h4>

<p>当在本地执行 Spark Streaming 程序时，不要使用 <code>local</code> 或者 <code>local[1]</code> 作为 master 的 URL，因为这样就只有一个线程。但是一个 Streaming 程序可能需要两个线程，一个用于接收输入流，一个用于处理数据。当然有接收器的才需要相应的线程来支持，如 Kafka、Flume 等，对于 HDFS 就不需要接收器，也就不需要另开线程。</p>

<p>对于文件系统（HDFS，S3 等），可以使用如下方式创建 <code>DStream</code>：</p>



<pre class="prettyprint"><code class="language-scala hljs ">streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)</code></pre>

<p>对于简单的文本文件，下面这种方式更加简单：</p>



<pre class="prettyprint"><code class="language-scala hljs ">streamingContext.textFileStream(dataDirectory)</code></pre>

<p>那么 Spark Streaming 如何监控文件系统中的目录呢？</p>

<p>可以直接监控一个目录，例如 <code>hdfs://namenode:8040/logs/</code>，这个目录下的所有文件（注意不能包含多级目录）都能被处理。也可以使用正则匹配如 <code>hdfs://namenode:8040/logs/2016-*</code>，注意这个正则匹配是指符合这个规则的目录而不是文件，例如有个目录是 <code>hdfs://namenode:8040/logs/2016-08-12</code>，那么这个目录下的所有文件都能被监控到。了解了这个规则以后，我们还可以通过改目录名的方式把其下面的文件加入到监控中。而且注意所有的文件都应该保持相同的数据格式，监控一个文件是监控它的修改时间而非创建时间，只有文件的修改时间在 Spark Streaming 执行的时间窗口之内，这个文件才会被处理，所以我们可以通过调用 <code>FileSystem.setTimes()</code> 来人为设置文件的修改时间从而使其可以被处理。</p>

<p>不过 Spark Streaming 直接监控对象存储系统中的目录还是有一些坑，可能会造成丢数据的问题，原因这里给出了详解：<a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#using-object-stores-as-a-source-of-data" rel="nofollow">using-object-stores-as-a-source-of-data</a>，所以一般还是使用 <a href="http://flume.apache.org" rel="nofollow">Flume</a> 监控目录比较好。</p>

<p>除了上面所说的文件流创建 DStream， 还可以通过接收器接收来自 Kafka、Flume 或 Kinesis 这些第三方组件的数据，由于这些组件各自有其对应的版本，所以可能会有版本依赖冲突的问题，Spark 就把这部分拆分出来作为专门的 Jar 包，所以对应到不同的第三方组件，我们需要引入不同的包。</p>



<h2 id="总结">总结</h2>

<p>本文是 Spark Streaming 的入门篇，通过一个简单的例子讲解了如何写 streaming 应用，并且介绍了 streaming 基本的数据结构 DStream，后面会介绍更多详细内容。</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>