---
layout:     post
title:      hadoop学习(五)
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><strong><span style="font-size:medium;">HDFS Details for Multimachine Clusters(2nd)</span>
</strong>
<br><br><strong>Checking the NameNodes</strong>
<br>
    ${JAVA_HOME}/bin/jps 结果第一行为java进程的pid<br><strong>Checking the DataNodes</strong>
<br>
    bin/slaves.sh jps | grep Datanode | sort<br>
    在查看过程中，如果有slave失败，则需要去那台机器上查看他们的日志文件。这样会不会造成管理员压力太大的问题？<br>
    In fact, I had half of a new cluster fail to start, and it took some time to realize that the newly installed machines had a default firewall that         blocked the HDFS port.<br>
    bin/hadoop dfsadmin -report  可以查看当前在线的datanode的部分信息<br><strong>Tuning Factors</strong>
<br>
    most important factors are network bandwidth and disk throughput. Memory use and CPU overhead for thread handling may also be issues.<br>
    The large input-split size reduces the ratio of task setup time to task run time.<br>
    Set the maximum number of requests in progress. the more requests in progress, the more contention there is for storage operations and network bandwidth, with a corresponding increase in memory requirements and CPU overhead for handling all of the outstanding requests. <br>
    Different factors per cluster.<br><strong>File Descriptors</strong>
(<a href="http://en.wikipedia.org/wiki/File_descriptor" rel="nofollow">http://en.wikipedia.org/wiki/File_descriptor</a>
)<br>
    Any user that runs processes that access HDFS should have a large limit on file descriptor access, and all applications that open files need careful         checking to make sure that the files are explicitly closed.</p>            </div>
                </div>