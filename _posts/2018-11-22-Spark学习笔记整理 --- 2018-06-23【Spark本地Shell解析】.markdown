---
layout:     post
title:      Spark学习笔记整理 --- 2018-06-23【Spark本地Shell解析】
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：学习交流为主，未经博主同意禁止转载，禁止用于商用。					https://blog.csdn.net/u012965373/article/details/80781627				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                在 Spark Shell 中运行代码<br>学习Spark程序开发，建议首先通过spark-shell交互式学习，加深Spark程序开发的理解。<br>这里介绍Spark Shell 的基本使用。Spark shell 提供了简单的方式来学习 API，并且提供了交互的方式来分析数据。你可以输入一条语句，Spark shell会立即执行语句并返回结果，这就是我们所说的REPL（Read-Eval-Print Loop，交互式解释器），为我们提供了交互式执行环境，表达式计算完成就会输出结果，而不必等到整个程序运行完毕，因此可即时查看中间结果，并对程序进行修改，这样可以在很大程度上提升开发效率。<br>Spark Shell 支持 Scala 和 Python，这里使用 Scala 来进行介绍。<br><br>现在我们直接开始使用Spark。<br>spark-shell命令及其常用的参数如下：<br><br>./bin/spark-shell --master &lt;master-url&gt;<br>Spark的运行模式取决于传递给SparkContext的Master URL的值。Master URL可以是以下任一种形式：<br>* local 使用一个Worker线程本地化运行SPARK(完全不并行)<br>* local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark<br>* local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定）<br>* spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077.<br>* yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。<br>* yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。<br>* mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050。<br><br><br>需要强调的是，这里我们采用“本地模式”（local）运行Spark<br>在Spark中采用本地模式启动Spark Shell的命令主要包含以下参数：<br>–master：这个参数表示当前的Spark Shell要连接到哪个master，如果是local[*]，就是使用本地模式启动spark-shell，其中，中括号内的星号表示需要使用几个CPU核心(core)；<br>–jars： 这个参数用于把相关的JAR包添加到CLASSPATH中；如果有多个jar包，可以使用逗号分隔符连接它们；<br><br><br>比如，要采用本地模式，在4个CPU核心上运行spark-shell：<br><br><br>cd /usr/local/spark<br>./bin/spark-shell --master local[4]<br><br><br>或者，可以在CLASSPATH中添加code.jar，命令如下：<br><br><br>cd /usr/local/spark<br>./bin/spark-shell --master local[4] --jars code.jar <br><br><br>可以执行“spark-shell –help”命令，获取完整的选项列表，具体如下：<br><br><br>cd /usr/local/spark<br>./bin/spark-shell --help<br><br><br>上面是命令使用方法介绍，下面正式使用命令进入spark-shell环境，可以通过下面命令启动spark-shell环境：<br><br><br>bin/spark-shell<br><br>            </div>
                </div>