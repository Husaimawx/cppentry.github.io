---
layout:     post
title:      HDFS初级编程实践
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p id="main-toc"><strong>目录</strong></p>

<p id="0.%E7%A0%81%E4%BB%99%E5%8A%B1%E5%BF%97-toc" style="margin-left:0px;"><a href="#0.%E7%A0%81%E4%BB%99%E5%8A%B1%E5%BF%97" rel="nofollow">0.码仙励志</a></p>

<p id="1.%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E8%A6%81%E6%B1%82-toc" style="margin-left:0px;"><a href="#1.%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E8%A6%81%E6%B1%82" rel="nofollow">1.开发环境要求</a></p>

<p id="2.%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE-toc" style="margin-left:0px;"><a href="#2.%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE" rel="nofollow">2.创建项目</a></p>

<p id="3.%E7%BC%96%E5%86%99jar%E5%8C%85%E7%9A%84%E4%BE%9D%E8%B5%96%E4%BF%A1%E6%81%AF-toc" style="margin-left:0px;"><a href="#3.%E7%BC%96%E5%86%99jar%E5%8C%85%E7%9A%84%E4%BE%9D%E8%B5%96%E4%BF%A1%E6%81%AF" rel="nofollow">3.编写jar包的依赖信息</a></p>

<p id="4.%E8%BF%9C%E7%A8%8B%E6%93%8D%E4%BD%9CHDFS%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#4.%E8%BF%9C%E7%A8%8B%E6%93%8D%E4%BD%9CHDFS%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">4.远程操作HDFS的代码实现</a></p>

<p id="(1)import%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E7%B1%BB-toc" style="margin-left:40px;"><a href="#(1)import%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E7%B1%BB" rel="nofollow">(1)import一些需要用到的类</a></p>

<p id="(2)%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5HDFS%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(2)%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5HDFS%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(2)远程连接HDFS系统的代码实现</a></p>

<p id="(3)%E5%88%9B%E5%BB%BAhdfs%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(3)%E5%88%9B%E5%BB%BAhdfs%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(3)创建hdfs文件夹的代码实现</a></p>

<p id="(4)%E5%AE%9E%E7%8E%B0%E5%BE%80HDFS%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#(4)%E5%AE%9E%E7%8E%B0%E5%BE%80HDFS%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81" rel="nofollow">(4)实现往HDFS上传文件的代码</a></p>

<p id="(5)%E4%BB%8EHDFS%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(5)%E4%BB%8EHDFS%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(5)从HDFS下载文件到本地代码实现</a></p>

<p id="(6)%E5%9C%A8%E8%BF%90%E8%A1%8C%E4%B8%AD%E8%AF%BB%E5%8F%96HDFS%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(6)%E5%9C%A8%E8%BF%90%E8%A1%8C%E4%B8%AD%E8%AF%BB%E5%8F%96HDFS%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(6)在运行中读取HDFS文件的代码实现</a></p>

<p id="(7)HDFS%E6%96%87%E4%BB%B6%E9%87%8D%E5%91%BD%E5%90%8D%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(7)HDFS%E6%96%87%E4%BB%B6%E9%87%8D%E5%91%BD%E5%90%8D%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(7)HDFS文件重命名代码实现</a></p>

<p id="(8)%E5%9C%A8%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(8)%E5%9C%A8%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(8)在程序中创建文件的代码实现</a></p>

<p id="(9)%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9%E5%88%B0%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(9)%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9%E5%88%B0%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(9)追加内容到某个文件的代码实现</a></p>

<p id="(10)%E5%88%A0%E9%99%A4HDFS%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#(10)%E5%88%A0%E9%99%A4HDFS%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">(10)删除HDFS文件的代码实现</a></p>

<p style="margin-left:40px;"><a href="#(11)%E6%95%B4%E7%AF%87%E4%BB%A3%E7%A0%81" rel="nofollow">(11)整篇代码</a></p>

<hr id="hr-toc"><h1 id="0.%E7%A0%81%E4%BB%99%E5%8A%B1%E5%BF%97">0.码仙励志</h1>

<p><strong>你害怕一件事，可还是要去做，那才是勇敢</strong></p>

<h1 id="1.%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E8%A6%81%E6%B1%82">1.开发环境要求</h1>

<ol><li>java  jdk 1.8</li>
	<li>eclipse 2018</li>
	<li>maven 2.7.5</li>
	<li>远程HDFS服务支持</li>
</ol><h1 id="2.%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE">2.创建项目</h1>

<p>首先打开eclipse，创建maven项目</p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113195645115.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113195847775.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113195956831.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113200114274.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181114091703610.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p>创建完毕后检查下jdk版本是否正确，在鼠标移动到项目上点击鼠标右键-&gt;Build  Path-&gt; Configure  Build  Path,如下图操作</p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181114091852698.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113201414260.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113201600742.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113201817324.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181113202035732.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<h1 id="3.%E7%BC%96%E5%86%99jar%E5%8C%85%E7%9A%84%E4%BE%9D%E8%B5%96%E4%BF%A1%E6%81%AF">3.编写jar包的依赖信息</h1>

<p>我们开始进入编码工作，由于我们创建的是maven工程，所以要编写pom.xml文件，下面是我编写好的pom.xml文件</p>

<pre class="has">
<code class="language-html">&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
	&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

	&lt;groupId&gt;com.maxian&lt;/groupId&gt;
	&lt;artifactId&gt;hdfs&lt;/artifactId&gt;
	&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
	&lt;packaging&gt;jar&lt;/packaging&gt;

	&lt;name&gt;hdfs&lt;/name&gt;
	&lt;url&gt;http://maven.apache.org&lt;/url&gt;

	&lt;properties&gt;
		&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
		&lt;hadoop.version&gt;2.7.5&lt;/hadoop.version&gt;
	&lt;/properties&gt;

	&lt;dependencies&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;junit&lt;/groupId&gt;
			&lt;artifactId&gt;junit&lt;/artifactId&gt;
			&lt;version&gt;3.8.1&lt;/version&gt;
			&lt;scope&gt;test&lt;/scope&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt;
			&lt;version&gt;${hadoop.version}&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt;
			&lt;version&gt;${hadoop.version}&lt;/version&gt;
			&lt;scope&gt;provided&lt;/scope&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
			&lt;version&gt;${hadoop.version}&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt;
			&lt;version&gt;${hadoop.version}&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
			&lt;version&gt;${hadoop.version}&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
			&lt;version&gt;${hadoop.version}&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;jdk.tools&lt;/groupId&gt;
			&lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;
			&lt;version&gt;1.8&lt;/version&gt;
			&lt;scope&gt;system&lt;/scope&gt;
			&lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;
		&lt;/dependency&gt;
	&lt;/dependencies&gt;
	&lt;build&gt;
		&lt;plugins&gt;
			&lt;plugin&gt;
				&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
				&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
				&lt;configuration&gt;
					&lt;source&gt;1.8&lt;/source&gt;
					&lt;target&gt;1.8&lt;/target&gt;
				&lt;/configuration&gt;
			&lt;/plugin&gt;
		&lt;/plugins&gt;
	&lt;/build&gt;
&lt;/project&gt;</code></pre>

<p>编写完毕后保存，在工程根目录点鼠标右键-&gt;Maven -&gt;Update Project。</p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181114090415655.jpg"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181114092515889.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<h1 id="4.%E8%BF%9C%E7%A8%8B%E6%93%8D%E4%BD%9CHDFS%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">4.远程操作HDFS的代码实现</h1>

<p>首先先在com.maxian.hdfs包下创建一个叫TestHdfs的类</p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181114092955395.jpg"></p>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/2018111409352655.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<h2 id="(1)import%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E7%B1%BB">(1)import一些需要用到的类</h2>

<p>现在我们可以开始编写JAVA代码了，打开com.maxian.hdfs下的TestHdfs.java类，import一些需要用到的类：</p>

<pre class="has">
<code class="language-java">import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.security.PrivilegedExceptionAction;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.security.UserGroupInformation;</code></pre>

<h2 id="(2)%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5HDFS%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">(2)远程连接HDFS系统的代码实现</h2>

<p>之后我们实例化Configuration和定义FileSystem 这两个静态常量，FileSystem是一个实现了文件系统的抽象类，继承自org.apache.hadoop.conf.Configured，并实现了 Closeable接口，可以适用于多种文件系统，如本地文件系统 file://、ftp、hdfs等。 代码如下：</p>

<pre class="has">
<code class="language-java">static Configuration conf = new Configuration();
static FileSystem hdfs;</code></pre>

<p>接着编写远程连接HDFS系统的一些配置信息，代码如下：</p>

<pre class="has">
<code class="language-java">static {
	UserGroupInformation ugi = UserGroupInformation.createRemoteUser("root");
	try {
		ugi.doAs(new PrivilegedExceptionAction&lt;Void&gt;() {
			public Void run() throws Exception {
				Configuration conf = new Configuration();
				conf.set("fs.defaultFS", "hdfs://192.168.56.110:9000/");
				// 以下两行是支持 hdfs的追加功能的：hdfs.append()
				conf.set("dfs.client.block.write.replace-datanode-on-failure.policy", "NEVER");
				conf.set("dfs.client.block.write.replace-datanode-on-failure.enable", "true");
				Path path = new Path("hdfs://192.168.56.110:9000/");
				hdfs = FileSystem.get(path.toUri(), conf);
				// hdfs = path.getFileSystem(conf); // 这个也可以
				return null;
			}
		});
	} catch (IOException e) {
		e.printStackTrace();
	} catch (InterruptedException e) {
		e.printStackTrace();
	}
}</code></pre>

<h2 id="(3)%E5%88%9B%E5%BB%BAhdfs%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">(3)<span style="color:#000000;">创建hdfs文件夹的代码实现</span></h2>

<pre class="has">
<code class="language-java">// 创建文件夹方法
public static void createDir(String dir) throws IOException {
	// String dir = "/test3/";
	Path path = new Path(dir);
	// 判断文件夹是否已存在，如果已存在就不再创建。
	if (hdfs.exists(path)) {
		System.out.println("文件夹 \t" + dir + "\t 已存在");
		return;
	}
	// 开始创建文件夹
	hdfs.mkdirs(path);
	System.out.println("新建文件夹 \t" + dir);
}</code></pre>

<p>代码编写完毕后，我们在入口方法main调用createDir方法进行验证，代码如下：</p>

<pre class="has">
<code class="language-java">public static void main( String[] args ) throws IOException
{
    createDir("/test1");
}</code></pre>

<p>现在我们可以开始来验证代码是否可行，首先确认集群的hadoop的dfs服务是否已经启动，确认dfs已经正常启动后，我们在该类的编辑界面上右击鼠标，在弹出的菜单中选中Run As -&gt; Java Application开始运行该类。</p>

<p><img alt="" class="has" height="267" src="https://img-blog.csdnimg.cn/20181114201628697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70" width="553"></p>

<p style="margin-left:0in;"><span style="color:#404040;">执行完后我们会在Console中看到相应的执行结果。也可以在集群的master主机中验证是否执行成功，在master中输入命令“hadoop fs -ls /”查看hdfs根目录的文件，如果得到如下图结果证明文件夹创建成功。</span></p>

<p style="margin-left:0in;"><span style="color:#404040;"><img alt="" class="has" height="177" src="https://img-blog.csdnimg.cn/20181114201736785.jpg" width="723"></span></p>

<p style="margin-left:0in;"><span style="color:#404040;"><img alt="" class="has" height="50" src="https://img-blog.csdnimg.cn/20181114201819670.jpg" width="566"></span></p>

<h2 id="(4)%E5%AE%9E%E7%8E%B0%E5%BE%80HDFS%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81" style="margin-left:0in;">(4)<span style="color:#404040;">实现往HDFS上传文件的代码</span></h2>

<pre class="has">
<code class="language-java">// 复制本地文件到HDFS
public static void copyFile(String localSrc, String hdfsDst, String fileName) throws IOException {
	if ("".equals(localSrc)) {
		localSrc = "D:/wordcount/input/myfile.txt";
	}
	if ("".equals(hdfsDst)) {
		hdfsDst = "/test/";
	}
	Path src = new Path(localSrc);
	Path dst = new Path(hdfsDst);
	// 本地文件不存在
	if (!(new File(localSrc)).exists()) {
		System.out.println("Error: 本地文件 \t" + localSrc + "\t 不存在。");
		return;
	}
	// hdfs路径不存在
	if (!hdfs.exists(dst)) {
		System.out.println("Error: hdfs目录 \t" + dst.toUri() + "\t 不存在。");
		return;
	}
	if ("".equals(fileName)) {
		fileName = src.getName();
	}
	String dstPath = dst.toUri() + "/" + fileName;
	System.out.println(dstPath); // "/test2/myfile.txt"
	Path targetPath = new Path(dstPath);
	// 判断上传的文件 hdfs的目录下是否存在
	if (hdfs.exists(targetPath)) {
		System.out.println("Warn: 文件 \t" + dstPath + "\t 已存在。");
	} else {
		// 本地文件上传hdfs
		hdfs.copyFromLocalFile(src, targetPath);
		// 遍历该目录文件
		FileStatus files[] = hdfs.listStatus(dst);
		System.out.println("上传到 \t" + conf.get("fs.defaultFS") + hdfsDst);
		for (FileStatus file : files) {
			System.out.println(file.getPath());
		}
	}
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们也来运行验证下，在main中注释掉之前的createDir方法，将copyFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	//createDir("/test1");
	copyFile("D:/wordcount/input/myfile.txt","/test1/","myfile.txt");
}</code></pre>

<p style="margin-left:0in;"><span style="color:#000000;">编写完毕后一样在该类的编辑界面上右击鼠标，在弹出的菜单中选中 </span><span style="color:#000000;">Run As -&gt; Java Application</span><span style="color:#000000;">开始运行该类。执行完后我们会在</span><span style="color:#000000;">Console</span><span style="color:#000000;">中看到相应的执行结果。也可以在集群的</span><span style="color:#000000;">master</span><span style="color:#000000;">主机中验证是否执行成功，在</span><span style="color:#000000;">master</span><span style="color:#000000;">中输入命令“</span><span style="color:#000000;">hadoop fs -ls /test1/”</span><span style="color:#000000;">查看</span><span style="color:#000000;">test1</span><span style="color:#000000;">目录的文件，如果得到如下图结果证明文件上传成功。</span></p>

<p style="margin-left:0in;"><span style="color:#000000;"><img alt="" class="has" height="55" src="https://img-blog.csdnimg.cn/20181114202457644.jpg" width="629"></span></p>

<h2 id="(5)%E4%BB%8EHDFS%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%AC%E5%9C%B0%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" style="margin-left:0in;">(5)<span style="color:#404040;">从HDFS下载文件到本地代码实现</span></h2>

<pre class="has">
<code class="language-java">// 下载文件方法
public static void downloadFile() throws IllegalArgumentException, IOException {
	String hdfsDst = "/test1/myfile.txt";
	String localSrc = "D:/test";
	Path dst = new Path(hdfsDst);
	Path src = new Path(localSrc);
	String localFile = localSrc + "/" + dst.getName(); // 本地的路径 + hdfs下载的文件名
	if (!hdfs.exists(dst.getParent())) { // 如果HDFS路径不存在
		System.out.println("Error : HDFS路径:\t" + dst.getParent() + "\t 不存在!");
		return;
	}
	if (!new File(localSrc).exists()) { // 如果本地目录不存在，则创建
		new File(localSrc).mkdirs();
		System.out.println("Warn : 本地目录已创建!");
	}
	if (new File(localFile).exists()) { // 如果本地文件存在
		System.out.println("Error : 本地文件: \t" + localFile + "\t 已存在.");
		return;
	}
	if (!hdfs.exists(new Path(hdfsDst))) { // 如果HDFS文件不存在
		System.out.println("Error : HDFS文件: \t" + hdfsDst + "\t 不存在.");
	} else {
		// HDFS下载文件到本地
		hdfs.copyToLocalFile(false, dst, src, true);
		System.out.println("successful ：下载成功! 请查看: \t" + localSrc);
	}
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们也来运行验证下，在main中注释掉之前的copyFile方法，将downloadFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	downloadFile();
}</code></pre>

<p><img alt="" class="has" height="92" src="https://img-blog.csdnimg.cn/2018111420304958.jpg" width="622"></p>

<h2 id="(6)%E5%9C%A8%E8%BF%90%E8%A1%8C%E4%B8%AD%E8%AF%BB%E5%8F%96HDFS%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">(6)<span style="color:#404040;">在运行中读取HDFS文件的代码实现</span></h2>

<pre class="has">
<code class="language-java">// 读取文件方法
public static void readFile() throws IOException {
	String uri = "/test1/myfile.txt";
	// 判断文件是否存在
	if (!hdfs.exists(new Path(uri))) {
		System.out.println("Error ; 文件不存在.");
		return;
	}
	InputStream in = null;
	try {
		in = hdfs.open(new Path(uri));
		// 复制到标准输出流
		IOUtils.copyBytes(in, System.out, 4096, false);
	} catch (Exception e) {
		e.printStackTrace();
	} finally {
		IOUtils.closeStream(in);
	}
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们也来运行验证下，在main中注释掉之前的downloadFile方法，将readFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	readFile();
}</code></pre>

<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20181114203545762.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rzd2NfYnl5,size_16,color_FFFFFF,t_70"></p>

<h2 id="(7)HDFS%E6%96%87%E4%BB%B6%E9%87%8D%E5%91%BD%E5%90%8D%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">(7)<span style="color:#404040;">HDFS文件重命名代码实现</span></h2>

<pre class="has">
<code class="language-java">// 重命名方法
public static void renameFile() throws IOException {
	String oldName = "/test1/myfile.txt";
	String newName = "/test1/readme_1.txt";
	Path oldPath = new Path(oldName);
	Path newPath = new Path(newName);
	if (hdfs.exists(oldPath)) {
		hdfs.rename(oldPath, newPath);
		System.out.println("rename成功！");
	} else {
		System.out.println("文件不存在!rename失败!");
	}
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们也来运行验证下，在main中注释掉之前的readFile方法，将renameFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	renameFile();
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">编写完毕后一样在该类的编辑界面上右击鼠标，在弹出的菜单中选中Run As -&gt; Java Application开始运行该类。执行完后我们会在Console中看到相应的执行结果。也可以在集群的master主机中验证是否执行成功，在master中输入命令“hadoop fs -ls /test1/”查看test1目录的文件，查看文件名是否已经重命名，如果得到如下图结果证明文件重命名成功。</span></p>

<p style="margin-left:0in;"><span style="color:#404040;"><img alt="" class="has" height="52" src="https://img-blog.csdnimg.cn/20181114204006611.jpg" width="633"></span></p>

<h2 id="(8)%E5%9C%A8%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" style="margin-left:0in;">(8)<span style="color:#404040;">在程序中创建文件的代码实现</span></h2>

<pre class="has">
<code class="language-java">// 创建文件方法
public static void createFile() throws IOException {
	String fileName = "/test1/file1.txt";
	String fileContent = "this is new file.";
	Path dst = new Path(fileName);
	// 判断 新建的文件在hdfs上是否存在
	if (hdfs.exists(dst)) {
		System.out.println("Error : 文件已存在.");
	} else {
		// 将文件内容转成字节数组
		byte[] bytes = fileContent.getBytes();
		FSDataOutputStream output = hdfs.create(dst);
		output.write(bytes);
		output.close();
		System.out.println("创建文件 \t" + fileName);
	}
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们也来运行验证下，在main中注释掉之前的renameFile方法，将createFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	createFile();
}</code></pre>

<p><img alt="" class="has" height="68" src="https://img-blog.csdnimg.cn/2018111420432797.jpg" width="641"></p>

<p>接着看一下该文件里面的内容</p>

<p><img alt="" class="has" height="44" src="https://img-blog.csdnimg.cn/20181114204524145.jpg" width="547"></p>

<h2 id="(9)%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9%E5%88%B0%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">(9)<span style="color:#404040;">追加内容到某个文件的代码实现</span></h2>

<pre class="has">
<code class="language-java">// 追加内容方法
public static void appendFile() throws IOException {
	String fileName = "/test1/file1.txt";
	String fileContent = "Here is an additional content";
	Path dst = new Path(fileName);
	byte[] bytes = fileContent.getBytes();
	// 如果文件不存在
	if (!hdfs.exists(dst)) {
		System.out.println("Error : 文件不存在。");
		return;
	}
	FSDataOutputStream output = hdfs.append(dst);
	output.write(bytes);
	output.close();
	System.out.println("successful: 追加内容到 \t" + fileName);
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们一样也来运行验证下，在main中注释掉之前的createFile方法，将appendFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	appendFile();
}</code></pre>

<p><img alt="" class="has" height="42" src="https://img-blog.csdnimg.cn/20181114205320300.jpg" width="547"></p>

<h2 id="(10)%E5%88%A0%E9%99%A4HDFS%E6%96%87%E4%BB%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">(10)<span style="color:#404040;">删除HDFS文件的代码实现</span></h2>

<pre class="has">
<code class="language-java">//删除HDFS文件的代码实现
public static void deleteFile(String fileName) throws IOException {
    if("".equals(fileName)) {
    fileName = "/test2/file1.txt";
    }
        Path f = new Path(fileName);
        boolean isExists = hdfs.exists(f);
        if (isExists) { // if exists, delete  
            boolean isDel = hdfs.delete(f, true);
            System.out.println(fileName + "  删除状态：  " + isDel);
        } else {
            System.out.println(fileName + "  文件不存在。");
        }
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">代码编写完成后，我们一样也来运行验证下，在main中注释掉之前的appendFile方法，将deleteFile方法写进去，代码如下：</span></p>

<pre class="has">
<code class="language-java">public static void main(String[] args) throws IOException {
	deleteFile("/test1/file1.txt");
}</code></pre>

<p style="margin-left:0in;"><span style="color:#404040;">编写完毕后一样在该类的编辑界面上右击鼠标，在弹出的菜单中选中Run As -&gt; Java Application开始运行该类。执行完后我们会在Console中看到相应的执行结果。也可以在集群的master主机中验证是否执行成功，在master中输入命令“hadoop fs -ls /test1/”查看test1目录的file1.txt文件是否已经被删除。</span></p>

<p style="margin-left:0in;"><span style="color:#404040;"><img alt="" class="has" height="55" src="https://img-blog.csdnimg.cn/20181115082809315.jpg" width="645"></span></p>

<h2 id="(11)%E6%95%B4%E7%AF%87%E4%BB%A3%E7%A0%81" style="margin-left:0in;"><span style="color:#404040;">(11)整篇代码</span></h2>

<pre class="has">
<code class="language-java">package com.maxian.hdfs;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.security.PrivilegedExceptionAction;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.security.UserGroupInformation;

public class TestHdfs {

	static Configuration conf = new Configuration();
	static FileSystem hdfs;
	static {
		UserGroupInformation ugi = UserGroupInformation.createRemoteUser("root");
		try {
			ugi.doAs(new PrivilegedExceptionAction&lt;Void&gt;() {
				public Void run() throws Exception {
					Configuration conf = new Configuration();
					conf.set("fs.defaultFS", "hdfs://192.168.56.110:9000/");
					// 以下两行是支持 hdfs的追加功能的：hdfs.append()
					conf.set("dfs.client.block.write.replace-datanode-on-failure.policy", "NEVER");
					conf.set("dfs.client.block.write.replace-datanode-on-failure.enable", "true");
					Path path = new Path("hdfs://192.168.56.110:9000/");
					hdfs = FileSystem.get(path.toUri(), conf);
					// hdfs = path.getFileSystem(conf); // 这个也可以
					return null;
				}
			});
		} catch (IOException e) {
			e.printStackTrace();
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
	}

	// 创建文件夹方法
	public static void createDir(String dir) throws IOException {
		// String dir = "/test3/";
		Path path = new Path(dir);
		// 判断文件夹是否已存在，如果已存在就不再创建。
		if (hdfs.exists(path)) {
			System.out.println("文件夹 \t" + dir + "\t 已存在");
			return;
		}
		// 开始创建文件夹
		hdfs.mkdirs(path);
		System.out.println("新建文件夹 \t" + dir);
	}

	// 复制本地文件到HDFS
	public static void copyFile(String localSrc, String hdfsDst, String fileName) throws IOException {
		if ("".equals(localSrc)) {
			localSrc = "D:/wordcount/input/myfile.txt";
		}
		if ("".equals(hdfsDst)) {
			hdfsDst = "/test/";
		}
		Path src = new Path(localSrc);
		Path dst = new Path(hdfsDst);
		// 本地文件不存在
		if (!(new File(localSrc)).exists()) {
			System.out.println("Error: 本地文件 \t" + localSrc + "\t 不存在。");
			return;
		}
		// hdfs路径不存在
		if (!hdfs.exists(dst)) {
			System.out.println("Error: hdfs目录 \t" + dst.toUri() + "\t 不存在。");
			return;
		}
		if ("".equals(fileName)) {
			fileName = src.getName();
		}
		String dstPath = dst.toUri() + "/" + fileName;
		System.out.println(dstPath); // "/test2/myfile.txt"
		Path targetPath = new Path(dstPath);
		// 判断上传的文件 hdfs的目录下是否存在
		if (hdfs.exists(targetPath)) {
			System.out.println("Warn: 文件 \t" + dstPath + "\t 已存在。");
		} else {
			// 本地文件上传hdfs
			hdfs.copyFromLocalFile(src, targetPath);
			// 遍历该目录文件
			FileStatus files[] = hdfs.listStatus(dst);
			System.out.println("上传到 \t" + conf.get("fs.defaultFS") + hdfsDst);
			for (FileStatus file : files) {
				System.out.println(file.getPath());
			}
		}
	}

	// 下载文件方法
	public static void downloadFile() throws IllegalArgumentException, IOException {
		String hdfsDst = "/test1/myfile.txt";
		String localSrc = "D:/test";
		Path dst = new Path(hdfsDst);
		Path src = new Path(localSrc);
		String localFile = localSrc + "/" + dst.getName(); // 本地的路径 + hdfs下载的文件名
		if (!hdfs.exists(dst.getParent())) { // 如果HDFS路径不存在
			System.out.println("Error : HDFS路径:\t" + dst.getParent() + "\t 不存在!");
			return;
		}
		if (!new File(localSrc).exists()) { // 如果本地目录不存在，则创建
			new File(localSrc).mkdirs();
			System.out.println("Warn : 本地目录已创建!");
		}
		if (new File(localFile).exists()) { // 如果本地文件存在
			System.out.println("Error : 本地文件: \t" + localFile + "\t 已存在.");
			return;
		}
		if (!hdfs.exists(new Path(hdfsDst))) { // 如果HDFS文件不存在
			System.out.println("Error : HDFS文件: \t" + hdfsDst + "\t 不存在.");
		} else {
			// HDFS下载文件到本地
			hdfs.copyToLocalFile(false, dst, src, true);
			System.out.println("successful ：下载成功! 请查看: \t" + localSrc);
		}
	}

	// 读取文件方法
	public static void readFile() throws IOException {
		String uri = "/test1/myfile.txt";
		// 判断文件是否存在
		if (!hdfs.exists(new Path(uri))) {
			System.out.println("Error ; 文件不存在.");
			return;
		}
		InputStream in = null;
		try {
			in = hdfs.open(new Path(uri));
			// 复制到标准输出流
			IOUtils.copyBytes(in, System.out, 4096, false);
		} catch (Exception e) {
			e.printStackTrace();
		} finally {
			IOUtils.closeStream(in);
		}
	}

	// 重命名方法
	public static void renameFile() throws IOException {
		String oldName = "/test1/myfile.txt";
		String newName = "/test1/readme_1.txt";
		Path oldPath = new Path(oldName);
		Path newPath = new Path(newName);
		if (hdfs.exists(oldPath)) {
			hdfs.rename(oldPath, newPath);
			System.out.println("rename成功！");
		} else {
			System.out.println("文件不存在!rename失败!");
		}
	}

	// 创建文件方法
	public static void createFile() throws IOException {
		String fileName = "/test1/file1.txt";
		String fileContent = "this is new file.";
		Path dst = new Path(fileName);
		// 判断 新建的文件在hdfs上是否存在
		if (hdfs.exists(dst)) {
			System.out.println("Error : 文件已存在.");
		} else {
			// 将文件内容转成字节数组
			byte[] bytes = fileContent.getBytes();
			FSDataOutputStream output = hdfs.create(dst);
			output.write(bytes);
			output.close();
			System.out.println("创建文件 \t" + fileName);
		}
	}

	// 追加内容方法
	public static void appendFile() throws IOException {
		String fileName = "/test1/file1.txt";
		String fileContent = "Here is an additional content";
		Path dst = new Path(fileName);
		byte[] bytes = fileContent.getBytes();
		// 如果文件不存在
		if (!hdfs.exists(dst)) {
			System.out.println("Error : 文件不存在。");
			return;
		}
		FSDataOutputStream output = hdfs.append(dst);
		output.write(bytes);
		output.close();
		System.out.println("successful: 追加内容到 \t" + fileName);
	}

	// 删除HDFS文件的代码实现
	public static void deleteFile(String fileName) throws IOException {
		if ("".equals(fileName)) {
			fileName = "/test2/file1.txt";
		}
		Path f = new Path(fileName);
		boolean isExists = hdfs.exists(f);
		if (isExists) { // if exists, delete
			boolean isDel = hdfs.delete(f, true);
			System.out.println(fileName + "  删除状态：  " + isDel);
		} else {
			System.out.println(fileName + "  文件不存在。");
		}
	}

	public static void main(String[] args) throws IOException {
//		createDir("/test1");
//		copyFile("D:/wordcount/input/myfile.txt", "/test1/", "myfile.txt");
//		downloadFile();
//		readFile();
//		renameFile();
//		createFile();
//		appendFile();
		deleteFile("/test1/file1.txt");
	}
}</code></pre>

<p> </p>            </div>
                </div>