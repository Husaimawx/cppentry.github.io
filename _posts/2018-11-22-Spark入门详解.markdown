---
layout:     post
title:      Spark入门详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <ul><li><a href="https://blog.csdn.net/c391183914/article/details/78672555#%E4%B8%80-spark%E6%A6%82%E8%BF%B0" rel="nofollow">一 Spark概述</a>
	<ul><li><a href="https://blog.csdn.net/c391183914/article/details/78672555#11-11-%E4%BB%80%E4%B9%88%E6%98%AFspark" rel="nofollow">1 11 什么是Spark</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#12-spark%E7%89%B9%E7%82%B9" rel="nofollow">2 Spark特点</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#13-spark%E7%9A%84%E7%94%A8%E6%88%B7%E5%92%8C%E7%94%A8%E9%80%94" rel="nofollow">3 Spark的用户和用途</a></li>
	</ul></li>
	<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#%E4%BA%8C-spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85" rel="nofollow">二 Spark集群安装</a>
	<ul><li><a href="https://blog.csdn.net/c391183914/article/details/78672555#21-%E9%9B%86%E7%BE%A4%E8%A7%92%E8%89%B2" rel="nofollow">1 集群角色</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#22-%E6%9C%BA%E5%99%A8%E5%87%86%E5%A4%87" rel="nofollow">2 机器准备</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#23-%E4%B8%8B%E8%BD%BDspark%E5%AE%89%E8%A3%85%E5%8C%85" rel="nofollow">3 下载Spark安装包</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#24-%E9%85%8D%E7%BD%AEsparkstandalone" rel="nofollow">4 配置SparkStandalone</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#25-%E9%85%8D%E7%BD%AEjob-history-serverstandalone" rel="nofollow">5 配置Job History ServerStandalone</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#26-%E9%85%8D%E7%BD%AEspark-hastandalone" rel="nofollow">6 配置Spark HAStandalone</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#27-%E9%85%8D%E7%BD%AEsparkyarn" rel="nofollow">7 配置SparkYarn</a></li>
	</ul></li>
	<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#%E4%B8%89-%E6%89%A7%E8%A1%8Cspark%E7%A8%8B%E5%BA%8F" rel="nofollow">三 执行Spark程序</a>
	<ul><li><a href="https://blog.csdn.net/c391183914/article/details/78672555#31-%E6%89%A7%E8%A1%8C%E7%AC%AC%E4%B8%80%E4%B8%AAspark%E7%A8%8B%E5%BA%8Fstandalone" rel="nofollow">1 执行第一个spark程序standalone</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#32-%E6%89%A7%E8%A1%8C%E7%AC%AC%E4%B8%80%E4%B8%AAspark%E7%A8%8B%E5%BA%8Fyarn" rel="nofollow">2 执行第一个spark程序yarn</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#33-spark%E5%BA%94%E7%94%A8%E6%8F%90%E4%BA%A4" rel="nofollow">3 Spark应用提交</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#34-%E5%90%AF%E5%8A%A8spark-shell" rel="nofollow">4 启动Spark Shell</a>
		<ul><li><a href="https://blog.csdn.net/c391183914/article/details/78672555#341-%E5%90%AF%E5%8A%A8spark-shell" rel="nofollow">41 启动Spark shell</a></li>
			<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#342-%E5%9C%A8spark-shell%E4%B8%AD%E7%BC%96%E5%86%99wordcount%E7%A8%8B%E5%BA%8F" rel="nofollow">42 在Spark shell中编写WordCount程序</a></li>
		</ul></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#35-%E5%9C%A8idea%E4%B8%AD%E7%BC%96%E5%86%99wordcount%E7%A8%8B%E5%BA%8F" rel="nofollow">5 在IDEA中编写WordCount程序</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#36-%E5%9C%A8idea%E4%B8%AD%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95wordcount%E7%A8%8B%E5%BA%8F" rel="nofollow">6 在IDEA中本地调试WordCount程序</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#37-%E5%9C%A8idea%E4%B8%AD%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95wordcount%E7%A8%8B%E5%BA%8F" rel="nofollow">7 在IDEA中远程调试WordCount程序</a></li>
		<li><a href="https://blog.csdn.net/c391183914/article/details/78672555#38-spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5" rel="nofollow">8 Spark核心概念</a></li>
	</ul></li>
</ul><p> </p>

<h1 id="一-spark概述"><a name="t0"></a>一、 Spark概述</h1>

<h2 id="11-11-什么是spark"><a name="t1"></a>1.1 1.1 什么是Spark</h2>

<p>官网：<a href="http://spark.apache.org/" rel="nofollow">http://spark.apache.org</a></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130094904341"><br><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130094925791"></p>

<p>Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。</p>

<p>目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分 别管理的负担。</p>

<p>大一统的软件栈，各个组件关系密切并且可以相互调用，这种设计有几个好处：1、软件栈中所有的程序库和高级组件 都可以从下层的改进中获益。2、运行整个软件栈的代价变小了。不需要运 行 5 到 10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。3、能够构建出无缝整合不同处理模型的应用。</p>

<p>Spark的内置项目如下：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130095053466"></p>

<p><strong>Spark Core：</strong> 实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义。 <br><strong>Spark SQL：</strong> 是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。 <br><strong>Spark Streaming：</strong> 是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。 <br><strong>Spark MLlib：</strong> 提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 <br><strong>集群管理器：</strong> Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度 器，叫作独立调度器。</p>

<p>Spark得到了众多大数据公司的支持，这些公司包括Hortonworks、IBM、Intel、Cloudera、MapR、Pivotal、百度、阿里、腾讯、京东、携程、优酷土豆。当前百度的Spark已应用于凤巢、大搜索、直达号、百度大数据等业务；阿里利用GraphX构建了大规模的图计算和图挖掘系统，实现了很多生产系统的推荐算法；腾讯Spark集群达到8000台的规模，是当前已知的世界上最大的Spark集群。</p>

<h2 id="12-spark特点"><a name="t2"></a>1.2 Spark特点</h2>

<p><strong>快</strong> <br>
与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130095500684"></p>

<p><strong>易用</strong> <br>
Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130095535195"></p>

<p><strong>通用</strong> <br>
Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</p>

<p><strong>兼容性</strong> <br>
Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130095624942"></p>

<h2 id="13-spark的用户和用途"><a name="t3"></a>1.3 Spark的用户和用途</h2>

<p>我们大致把Spark的用例分为两类：数据科学应用和数据处理应用。也就对应的有两种人群：数据科学家和工程师。</p>

<p><strong>数据科学任务</strong> <br>
主要是数据分析领域，数据科学家要负责分析数据并建模，具备 SQL、统计、预测建模(机器学习)等方面的经验，以及一定的使用 Python、 Matlab 或 R 语言进行编程的能力。</p>

<p><strong>数据处理应用</strong> <br>
工程师定义为使用 Spark 开发 生产环境中的数据处理应用的软件开发者，通过对接Spark的API实现对处理的处理和转换等任务。</p>

<h1 id="二-spark集群安装"><a name="t4"></a>二、 Spark集群安装</h1>

<h2 id="21-集群角色"><a name="t5"></a>2.1 集群角色</h2>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130095802130"></p>

<p>从物理部署层面上来看，Spark主要分为两种类型的节点，Master节点和Worker节点，Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。</p>

<p>从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。</p>

<h2 id="22-机器准备"><a name="t6"></a>2.2 机器准备</h2>

<p>准备两台以上Linux服务器，安装好JDK1.8</p>

<h2 id="23-下载spark安装包"><a name="t7"></a>2.3 下载Spark安装包</h2>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130095918141"></p>

<p>上传解压安装包 <br>
上传spark-2.1.1-bin-hadoop2.7.tgz安装包到Linux上 <br>
解压安装包到指定位置 <br><code>tar -xf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/modules</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130100219463"></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130100342480"></p>

<h2 id="24-配置sparkstandalone"><a name="t8"></a>2.4 配置Spark【Standalone】</h2>

<p>Spark的部署模式有Local、Local-Cluster、Standalone、Yarn、Mesos，我们选择最具代表性的Standalone集群部署模式。</p>

<p>进入到Spark安装目录 <br><code>cd /home/bigdata/hadoop/spark-2.1.1-bin-hadoop2.7/conf</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130100532085"></p>

<p>将slaves.template复制为slaves <br>
将spark-env.sh.template复制为spark-env.sh <br>
修改slave文件，将work的hostname输入：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130100653781"></p>

<p>修改spark-env.sh文件，添加如下配置：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130100858034"></p>

<p>将配置好的Spark文件拷贝到其他节点上</p>

<p>Spark集群配置完毕，目前是1个Master，2个Work，linux01上启动Spark集群 <br><code>/opt/modules/spark-2.1.1-bin-hadoop2.7/sbin/start-all.sh</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130101200956"></p>

<p>启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：<a href="http://linux01:8080/" rel="nofollow">http://linux01:8080/</a></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130101316885"></p>

<p>到此为止，Spark集群安装完毕.</p>

<p><em>注意：如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置：</em> <br><code>export JAVA_HOME=XXXX</code></p>

<h2 id="25-配置job-history-serverstandalone"><a name="t9"></a>2.5 配置Job History Server【Standalone】</h2>

<p>进入到Spark安装目录 <br><code>cd /opt/modules/spark-2.1.1-bin-hadoop2.7/conf</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130100532085"></p>

<p>将spark-default.conf.template复制为spark-default.conf</p>

<p>修改spark-default.conf文件，开启Log：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130101643251"></p>

<p><em>【注意：HDFS上的目录需要提前存在】</em></p>

<p>修改spark-env.sh文件，添加如下配置：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130101847394"></p>

<p>在HDFS上创建好你所指定的eventLog日志目录。</p>

<p><strong>spark-defaults.conf</strong></p>

<pre class="has">
<code>spark.eventLog.enabled  true
spark.eventLog.dir       hdfs://master01:9000/directory
spark.eventLog.compress true</code></pre>

<p><strong>spark-env.sh</strong></p>

<pre class="has">
<code>export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000
-Dspark.history.retainedApplications=3
-Dspark.history.fs.logDirectory=hdfs://linux01:9000/directory"</code></pre>

<p>参数描述： <br>
spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下；</p>

<p>spark.history.ui.port=4000 调整WEBUI访问的端口号为4000</p>

<p>spark.history.fs.logDirectory=hdfs://master01:9000/directory 配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息</p>

<p>spark.history.retainedApplications=3 指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p>

<p>将配置好的Spark文件拷贝到其他节点上</p>

<p><code>/opt/modules/spark-2.1.1-bin-hadoop2.7/sbin/start-all.sh</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130101200956"></p>

<p>启动后执行：【别忘了启动HDFS】</p>

<p><code>/opt/modules/spark-2.1.1-bin-hadoop2.7/sbin/start-history-server.sh</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130102641299?"></p>

<p>到此为止，Spark History Server安装完毕.</p>

<p>如果遇到Hadoop HDFS的写入权限问题： <br>
org.apache.hadoop.security.AccessControlException <br>
解决方案： 在hdfs-site.xml中添加如下配置，关闭权限验证</p>

<pre class="has">
<code>&lt;property&gt;
            &lt;name&gt;dfs.permissions&lt;/name&gt;
            &lt;value&gt;false&lt;/value&gt;
</code></pre>

<h2 id="26-配置spark-hastandalone"><a name="t10"></a>2.6 配置Spark HA【Standalone】</h2>

<p>集群部署完了，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130102849483"></p>

<p>Spark集群规划：master01，master02是Master；slave01，slave02，slave03是Worker</p>

<p>安装配置Zookeeper集群，并启动Zookeeper集群 <br>
停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130102944530"></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130103056310"></p>

<pre class="has">
<code>export SPARK_DAEMON_JAVA_OPTS="
 -Dspark.deploy.recoveryMode=ZOOKEEPER
 -Dspark.deploy.zookeeper.url=zk1,zk2,zk3
 -Dspark.deploy.zookeeper.dir=/spark"</code></pre>

<p>1.在 master01节点上修改spark-env.sh配置文件 <br>
2.将配置文件同步到所有节点。 <br>
3.在master01上执行sbin/start-all.sh脚本，启动集群并启动第一个master节点，然后在master02上执行sbin/start-master.sh启动第二个master节点。 <br>
4.程序中spark集群的访问地址需要改成： <br>
spark://master01:port1,master02:port2</p>

<h2 id="27-配置sparkyarn"><a name="t11"></a>2.7 配置Spark【Yarn】</h2>

<p>修改Hadoop配置下的yarn-site.xml:</p>

<pre class="has">
<code>&lt;configuration&gt;

&lt;!-- Site specific YARN configuration properties --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
                &lt;value&gt;master01&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
        &lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
        &lt;/property&gt;
        &lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>修改Spark-env.sh 添加：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130103427025"></p>

<p>让Spark能够发现Hadoop配置文件</p>

<pre class="has">
<code>HADOOP_CONF_DIR=/opt/modules/hadoop-2.7.3/etc/hadoop
YARN_CONF_DIR=/opt/modules/hadoop-2.7.3/etc/hadoop
</code></pre>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130103616682"></p>

<p>启动spark history server：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130103908721"></p>

<p>可以查看日志。</p>

<h1 id="三-执行spark程序"><a name="t12"></a>三、 执行Spark程序</h1>

<h2 id="31-执行第一个spark程序standalone"><a name="t13"></a>3.1 执行第一个spark程序（standalone）</h2>

<pre class="has">
<code>/opt/modules/spark-2.1.1-bin-hadoop2.7/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux01:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
/opt/modules/spark-2.1.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.1.jar \
100</code></pre>

<p>参数说明： <br>
–master spark://master01:7077 指定Master的地址 <br>
–executor-memory 1G 指定每个executor可用内存为1G <br>
–total-executor-cores 2 指定每个executor使用的cup核数为2个 <br>
该算法是利用蒙特·卡罗算法求PI</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130104236981"></p>

<h2 id="32-执行第一个spark程序yarn"><a name="t14"></a>3.2 执行第一个spark程序（yarn）</h2>

<pre class="has">
<code>/opt/modules/spark-2.1.1-bin-hadoop2.7/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
/opt/modules/spark-2.1.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.1.jar \
100</code></pre>

<h2 id="33-spark应用提交"><a name="t15"></a>3.3 Spark应用提交</h2>

<p>一旦打包好,就可以使用bin/spark-submit脚本启动应用了. 这个脚本负责设置spark使用的classpath和依赖,支持不同类型的集群管理器和发布模式:</p>

<pre class="has">
<code>./bin/spark-submit \
  --class &lt;main-class&gt;
  --master &lt;master-url&gt; \
  --deploy-mode &lt;deploy-mode&gt; \
  --conf &lt;key&gt;=&lt;value&gt; \
  ... # other options
  &lt;application-jar&gt; \
  [application-arguments]</code></pre>

<p>一些常用选项: <br>
1) –class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi) <br>
2) –master: 集群的master URL (如 spark://23.195.26.187:7077) <br>
3) –deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)* <br>
4) –conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value”. 缺省的Spark配置 <br>
5) application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar. <br>
6) application-arguments: 传给main()方法的参数</p>

<p>Master URL 可以是以下格式： <br>
查看Spark-submit全部参数：</p>

<table><thead><tr><th>参数</th>
			<th>解释</th>
		</tr></thead><tbody><tr><td>local</td>
			<td>本地以一个worker线程运行(例如非并行的情况).</td>
		</tr><tr><td>local[K]</td>
			<td>本地以K worker 线程 (理想情况下, K设置为你机器的CPU核数).</td>
		</tr><tr><td>local[*]</td>
			<td>本地以本机同样核数的线程运行.</td>
		</tr><tr><td>spark://HOST:PORT</td>
			<td>连接到指定的Spark standalone cluster master. 端口是你的master集群配置的端口，缺省值为7077.</td>
		</tr><tr><td>mesos://HOST:PORT</td>
			<td>连接到指定的Mesos 集群. Port是你配置的mesos端口， 缺省是5050. 或者如果Mesos使用ZOoKeeper,格式为 mesos://zk://….</td>
		</tr><tr><td>yarn-client</td>
			<td>以client模式连接到YARN cluster. 集群的位置基于HADOOP_CONF_DIR 变量找到.</td>
		</tr><tr><td>yarn-cluster</td>
			<td>以cluster模式连接到YARN cluster. 集群的位置基于HADOOP_CONF_DIR 变量找到.</td>
		</tr></tbody></table><h2 id="34-启动spark-shell"><a name="t16"></a>3.4 启动Spark Shell</h2>

<p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p>

<h3 id="341-启动spark-shell"><a name="t17"></a>3.4.1 启动Spark shell</h3>

<p>/opt/modules/spark-2.1.1-bin-hadoop2.7/bin/spark-shell \ <br>
–master spark://master01:7077 \ <br>
–executor-memory 2g \ <br>
–total-executor-cores 2 <br>
注意： <br>
如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。</p>

<p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p>

<h3 id="342-在spark-shell中编写wordcount程序"><a name="t18"></a>3.4.2 在Spark shell中编写WordCount程序</h3>

<p>首先启动hdfs</p>

<p>将Spark目录下的RELEASE文件上传一个文件到hdfs://linux01:9000/RELEASE <br>
~/opt/modules/hadoop-2.7.3/bin/hdfs dfs -put ./RELEASE /</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130104915233"></p>

<p>在Spark shell中用scala语言编写spark程序</p>

<p><code>sc.textFile("hdfs://linux01:9000/RELEASE").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://linux01:9000/out")</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105251500"></p>

<p>使用hdfs命令查看结果</p>

<p><code>hdfs dfs -cat hdfs://master01:9000/out/p*</code></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105324781"></p>

<p>说明： <br>
sc是SparkContext对象，该对象时提交spark程序的入口 <br>
textFile(hdfs://master01:9000/RELEASE)是hdfs中读取数据 <br>
flatMap(_.split(” “))先map在压平 <br>
map((_,1))将单词和1构成元组 <br>
reduceByKey(<em>+</em>)按照key进行reduce，并将value累加 <br>
saveAsTextFile(“hdfs:// master01:9000/out”)将结果写入到hdfs中</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105352556"></p>

<h2 id="35-在idea中编写wordcount程序"><a name="t19"></a>3.5 在IDEA中编写WordCount程序</h2>

<p>spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。</p>

<p>1.创建一个项目</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105456764"></p>

<p>2.选择Maven项目，然后点击next</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105537782"></p>

<p>3.填写maven的GAV，然后点击next</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105610228"></p>

<p>4.填写项目名称，然后点击finish</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105636457"></p>

<p>6.配置Maven的pom.xml</p>

<pre class="has">
<code>    &lt;?xml version="1.0" encoding="UTF-8"?&gt;
    &lt;project xmlns="http://maven.apache.org/POM/4.0.0"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
        &lt;parent&gt;
            &lt;artifactId&gt;spark&lt;/artifactId&gt;
            &lt;groupId&gt;com.atguigu&lt;/groupId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/parent&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;artifactId&gt;wordcount&lt;/artifactId&gt;

        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
                &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
                &lt;version&gt;${scala.version}&lt;/version&gt;
                &lt;scope&gt;provided&lt;/scope&gt;
            &lt;/dependency&gt;

            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
                &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
                &lt;version&gt;${spark.version}&lt;/version&gt;
                &lt;scope&gt;provided&lt;/scope&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
                &lt;version&gt;${hadoop.version}&lt;/version&gt;
                &lt;scope&gt;provided&lt;/scope&gt;
            &lt;/dependency&gt;

            &lt;!-- Logging --&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt;
                &lt;version&gt;${slf4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
                &lt;version&gt;${slf4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
                &lt;version&gt;${slf4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;log4j&lt;/groupId&gt;
                &lt;artifactId&gt;log4j&lt;/artifactId&gt;
                &lt;version&gt;${log4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;!-- Logging End --&gt;
        &lt;/dependencies&gt;
        &lt;build&gt;
            &lt;finalName&gt;wordcount&lt;/finalName&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
                    &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
                    &lt;version&gt;3.2.2&lt;/version&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;compile&lt;/goal&gt;
                                &lt;goal&gt;testCompile&lt;/goal&gt;
                            &lt;/goals&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                &lt;/plugin&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                    &lt;version&gt;3.0.0&lt;/version&gt;
                    &lt;configuration&gt;
                        &lt;archive&gt;
                            &lt;manifest&gt;
                                &lt;mainClass&gt;com.atguigu.spark.WordCount&lt;/mainClass&gt;
                            &lt;/manifest&gt;
                        &lt;/archive&gt;
                        &lt;descriptorRefs&gt;
                            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                        &lt;/descriptorRefs&gt;
                    &lt;/configuration&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;id&gt;make-assembly&lt;/id&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;single&lt;/goal&gt;
                            &lt;/goals&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                &lt;/plugin&gt;
            &lt;/plugins&gt;
        &lt;/build&gt;


    &lt;/project&gt;</code></pre>

<p>7.将src/main/scala设置成源代码目录。</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105908666"></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105931118"></p>

<p>8.添加IDEA Scala（执行此操作后，pom文件中不用添加scala依赖，应为已经以lib库的方式加入）</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130105952859"></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110017584?"></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110043712"></p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110101361"></p>

<p>9.新建一个Scala class，类型为Object</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110125251"></p>

<p>10.编写spark程序</p>

<pre class="has">
<code>package com.atguigu.spark

import org.apache.spark.{SparkConf, SparkContext}
import org.slf4j.LoggerFactory

/**
  * Created by wuyufei on 31/07/2017.
  */
object WordCount {

  val logger = LoggerFactory.getLogger(WordCount.getClass)

  def main(args: Array[String]) {
    //创建SparkConf()并设置App名称
    val conf = new SparkConf().setAppName("WC")
    //创建SparkContext，该对象是提交spark App的入口
    val sc = new SparkContext(conf)
    //使用sc创建RDD并执行相应的transformation和action
    sc.textFile(args(0)).flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1))
    //停止sc，结束该任务

    logger.info("complete!")

    sc.stop()
  }

}</code></pre>

<p>11.使用Maven打包：首先修改pom.xml中的main class</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110403754"></p>

<p>12.点击idea右侧的Maven Project选项，点击Lifecycle,选择clean和package，然后点击Run Maven Build</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110426165"></p>

<p>13.选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110448354"></p>

<p>14.首先启动hdfs和Spark集群 <br>
启动hdfs <br><code>/opt/modules/hadoop-2.7.3/sbin/start-dfs.sh</code> <br>
启动spark <br><code>/opt/modules/spark-2.1.1-bin-hadoop2.7/sbin/start-all.sh</code></p>

<p>15.使用spark-submit命令提交Spark应用（注意参数的顺序）</p>

<pre class="has">
<code>/opt/modules/spark-2.1.1-bin-hadoop2.7/bin/spark-submit\
   --class WordCount\
   --master spark://linux01:7077\
   --executor-memory 1G \
   --total-executor-cores 2 \
   wordcount-jar-with-dependencies.jar\
   hdfs://linux01:9000/calllog.csv\
   hdfs://linux01:9000/out</code></pre>

<p>16.查看程序执行结果 <br><code>hdfs dfs -cat hdfs://linux01:9000/out/part-*</code></p>

<h2 id="36-在idea中本地调试wordcount程序"><a name="t20"></a>3.6 在IDEA中本地调试WordCount程序</h2>

<p>本地Spark程序调试需要使用local提交模式，即将本机当做运行环境，Master和Worker都为本机。运行时直接加断点调试即可。如下：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110636294"></p>

<p>如果本机操作系统是windows，如果在程序中使用了hadoop相关的东西，比如写入文件到HDFS，则会遇到如下异常：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110701112"></p>

<p>出现这个问题的原因，并不是程序的错误，而是用到了hadoop相关的服务，解决办法是将附加里面的hadoop-common-bin-2.7.3-x64.zip解压到任意目录。</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110727281"></p>

<p>在IDEA中配置Run Configuration，添加HADOOP_HOME变量</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110748289"></p>

<h2 id="37-在idea中远程调试wordcount程序"><a name="t21"></a>3.7 在IDEA中远程调试WordCount程序</h2>

<p>通过IDEA进行远程调试，主要是将IDEA作为Driver来提交应用程序，配置过程如下：</p>

<p>修改sparkConf，添加最终需要运行的Jar包、Driver程序的地址，并设置Master的提交地址：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110813519"></p>

<p>然后加入断点，直接调试即可：</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110831229"></p>

<h2 id="38-spark核心概念"><a name="t22"></a>3.8 Spark核心概念</h2>

<p>每个Spark应用都由一个驱动器程序(driver program)来发起集群上的各种 并行操作。驱动器程序包含应用的 main 函数，并且定义了集群上的分布式数据集，还对这 些分布式数据集应用了相关操作。  </p>

<p>驱动器程序通过一个 SparkContext 对象来访问 Spark。这个对象代表对计算集群的一个连 接。shell 启动时已经自动创建了一个 SparkContext 对象，是一个叫作 sc 的变量。</p>

<p>驱动器程序一般要管理多个执行器(executor)节点。</p>

<p><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20171130110911446"></p>

<p> </p>            </div>
                </div>