---
layout:     post
title:      Hive初学笔记（一） ---hive环境搭建与安装
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>  从初学hadoop后，参照相关文档对hadoop生态环境相关项目hive也初学了下。</p>
<p>      1）hive相关参考文档：</p>
<p>         1. http://wiki.apache.org/hadoop/Hive</p>
<p>         2. HQL文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual<br>
         3. https://cwiki.apache.org/confluence/display/Hive/Home<br>
         4. hsql dml:https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-InsertingdataintoHiveTablesfromqueries<br>
         5.hive sql ddl:https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables<br>
         6 hive UDF( User Defined Functions):https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF<br>
         7 hive 中数据统计和数据挖掘：https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining</p>
<p><br></p>
<p>    2）hive的安装</p>
<p>         hive 安装文档：<br>
前提环境： java1.6及以上； hadoop0.20.x及以上；hive安装包<br><br>
1.解压hive安装包<br>
$ tar -xzvf hive-x.y.z.tar.gz<br>
设置环境变量HIVE——HOME指向hive安装目录<br>
$ cd hive-x.y.z<br>
$ export HIVE_HOME={{pwd}}<br>
增加hive——home/bin到PATH路径<br>
$ export PATH=$HIVE_HOME/bin:$PATH<br><br>
2.运行HIVE<br>
（1）hive是运行在hdfs之上的。因此<br>
a 必需有hadoop在path中 <br>
b export HADOOP_HOME=&lt;hadoop-install-dir&gt;<br>
c 必须创建/tmp 和/user/hive/warehouse（在配置文件中通过参数hive.metastore.warehouse.dir设置），并且设置其在hdfs中读写权限为：chmod g+w ；命令行如下：<br>
$ $HADOOP_HOME/bin/hadoop fs -mkdir /tmp<br>
$ $HADOOP_HOME/bin/hadoop fs -mkdir /user/hive/warehouse<br>
$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp<br>
$ $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse<br>
设置hive——home但非必需：<br>
$ export HIVE_HOME=&lt;hive-install-dir&gt;<br>
（2）从shell中使用hive命令行接口（cli）<br>
$ $HIVE_HOME/bin/hive<br><br>
3 配置管理<br>
a. hive默认的配置文件保存在&lt;install-dir&gt;/conf/hive-default.xml ，用户可以通过改变&lt;install-dir&gt;/conf/hive-site.xml中的变量值来改变配置变量<br>
b. hive的配置目录文件位置能够通过设置the HIVE_CONF_DIR environment variable来改变<br>
c. log4j配置保存在&lt;install-dir&gt;/conf/hive-log4j.properties中<br>
d. hive配置能够默认继承hadoop的配置变量，当在hive中改变相关变量，可能会覆盖hadoop配置文件中的变量值<br>
e. 配置hive环境变量的方式：<br>
编辑hive-site.xml ，在该文件中定义相关变量（包括hadoop的相关变量）；<br>
在cli中通过命令设置；如$ bin/hive -hiveconf x1=y1 -hiveconf x2=y2 表示将值y1,y2赋给变量x1,x2。通过设置HIVE_OPTS环境变量“"-hiveconf x1=y1 -hiveconf x2=y2”同样可以达到相同效果<br><br>
4 运行配置<br>
hive查询是通过mapreduce查询实现的，因此hive查询能够通过hadoop配置变量来控制。<br>
在命令行中使用“SET”能够设置任何hadoop（或hive）配置变量，例如：<br>
hive&gt; SET mapred.job.tracker=myhost.mycompany.com:50030;<br>
hive&gt; SET -v;<br>
SET -v 显示所有当前设置，去掉-v 则只显示与hadoop配置不相同的配置变量<br><br>
5 HIVE，Map-Reduce和本地模式<br>
hive大多数查询同会编译成mapreduce jobs，jobs然后被提交到集群中执行。<br>
a ）hive执行支持本地模式和集群模式：通过变量mapred.job.tracker来设置<br>
通常情况下mapred.job.tracker值指向多节点的集群，此时，适合处理大数据集<br>
mapred.job.tracker支持本地模式，在用户空间中运行mapreduce jobs，本地模式对于查询小数据集来讲很有效，由于本地模式只运行一个reducer 所有对大数据集处理很慢。设置本地执行命令如下： hive&gt; SET mapred.job.tracker=local;设置本地执行时，mapred.local.dir 应该指向一个本机的可用目录，否则将报分配本地磁盘空间异常。<br>
自动设置本地模式:SET hive.exec.mode.local.auto=false; 变量默认值为false，如果设置为true，hive将通过分析查询编译生成的每个map-reduce job的大小来决定是否启动本地模式，在满足如下临界值时时运行本地模式：<br>
1）job的总输入大小低于hive.exec.mode.local.auto.inputbytes.max (默认128m）设定值时；<br>
2）map-tasks 的总数量低于hive.exec.mode.local.auto.tasks.max（默认4）设定值时；<br>
3）reduce tasks总数量为0或1时；<br>
b）注意：<br>
Note that there may be differences in the runtime environment of hadoop server nodes and the machine running the hive client (because of different jvm versions or different software libraries). This can cause unexpected behavior/errors while running in local
 mode. Also note that local mode execution is done in a separate, child jvm (of the hive client). If the user so wishes, the maximum amount of memory for this child jvm can be controlled via the option hive.mapred.local.mem. By default, it's set to zero, in
 which case Hive lets Hadoop determine the default memory limits of the child jvm.<br><br>
6. hive 错误日志<br>
a hive默认错误日志不线索在console中，日志保存在/tmp/&lt;user.name&gt;/hive.log文件夹中，<br>
b 如需要日志显示在console中，可通过如下声明：bin/hive -hiveconf hive.root.logger=INFO,console，用户也能改变log的level：bin/hive -hiveconf hive.root.logger=INFO,DRFA。注：通过设置hive.root.logger不会改变logging的属性，应为其生效只在初始化时生效。<br>
c hive能够存储查询日志在/tmp/&lt;user.name&gt;/目录下，也可通过设置hive-site.xml中的hive.querylog.location变量值来设置存储路径<br>
d 在hive执行期间，集群中的写日志由hadoop配置属性控制。通常每个map和reduce task产生一个日志，该日志存储在集群中task执行的位置，该日志能够在hadoop jobtracker web UI界面中点击相应的Task details来获取。<br>
e 使用本地模式时，hadoop/hive执行日志产生在the client machine机器上。从v-0.6开始-hive使用hive-exec-log4j.properties文件定义日志默认保存位置。（当该文件丢失或不存在时则重新使用hive-log4j.properties）。默认的配置在本地模式，。每个查询执行生成一个日志文件。并保存在/tmp/&lt;user.name&gt;。使用分开的两个日志配置文件的目的是有助于管理员能够集中的捕获hive执行日志（比如在NFS文件服务器上）。hive执行日志对于调试运行错误（run-time
 errors）也是很有帮助的。<br><br>
6. 元数据存储<br>
a）hive元数据被嵌入到Derby database中，该database的磁盘存储位置通过hive配置变量javax.jdo.option.ConnectionURL来定义。默认位置在./metastore_db（见conf/hive-default.xml配置文件）。<br>
b）默认配置下，metadata数据一次只能够供一个user查看。<br>
c）hive元数据能够存储在任何支持JPOX的database中。存储位置和RDBMS能够通过javax.jdo.option.ConnectionURL和javax.jdo.option.ConnectionDriverName这两个变量来控制。关于database支持的JDO（或JPOX）详细信息参见其jdo文件。在src/contrib/hive/metastore/src/model中JDO元数据注释文件package.jdo定义了database schema。<br>
d）未来，metastore能够成为一个独立的服务器。<br>
e）如果你想在网络服务器上运行metastore，能够从multiple nodes尝试HiveDerbyServerMode来访问<br><br>
7.HIVE DDL 操作<br>
1）创建一个名为pokes的表，该表有两列，第一列数据为int，第二列数据为string类型：<br>
hive&gt; CREATE TABLE pokes (foo INT, bar STRING);<br>
2）创建一个带有两列的invites表，且切分列（a partition column）为ds。切分列为一个虚列，其并不是数据本身的一部分。但从切分列可以提取一个特定的数据集。默认情况下，表建议以text格式输入，且分割符默认为^A(ctrl-a)。<br>
hive&gt; CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);<br>
3）列出以存在的表。<br>
hive&gt; SHOW TABLES;<br>
4）列出所有以s结尾的表。格式匹配遵循java表达式。匹配类详见：http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html<br>
hive&gt; SHOW TABLES '.*s';<br>
5）显示invites的所有列<br>
hive&gt; DESCRIBE invites;<br>
6）更改表，改变表名，增加列<br>
hive&gt; ALTER TABLE pokes ADD COLUMNS (new_col INT);<br>
hive&gt; ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');<br>
hive&gt; ALTER TABLE events RENAME TO 3koobecaf;<br>
7）删除表<br>
hive&gt; DROP TABLE pokes;<br>
8. DML操作<br>
从文件中加载数据到hive<br>
1）加载一个包含两列且列间以ctrl-a进行分割的文件到pokes表中。‘local’表明输入文件是本地文件系统，如果local文件不存在，将转向HDFS文件中寻找。关键字‘overwrite’表明表中已存在的数据被删除，如果‘overwrite’关键词忽略，数据文件将添加到已存在的数据集中。如果加载文件在hdfs，该文件将被移动（并非复制）到hive控制的文件系统名空间中。hive目录的root路径可以通过hive-default.xml文件中的hive.metastore.warehouse.dir设定。<br>
hive&gt; LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;<br>
2）两个文件数据分别加载到invites表的两个不同的部分，只有到表已经创建的切分key‘ds’后加载才成功。<br>
hive&gt; LOAD DATA LOCAL INPATH './examples/files/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');<br>
hive&gt; LOAD DATA LOCAL INPATH './examples/files/kv3.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-08');<br>
3）从HDFS中加载数据，加载后，数据加载过程中导致hdfs文件/目录移动<br>
hive&gt; LOAD DATA INPATH '/user/myname/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');<br><br>
9.SQL操作<br>
1）查询和过滤<br>
a.从invites表中切分列ds=2008-08-15所有行中查询名为‘foo‘列,查询结果不进行存储，只显示在console上<br>
hive&gt; SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';<br>
b.插入数据，将invites表中ds='2008-08-15'的所有行数据插入到hdfs的/tmp/hdfs_out目录中（insert关键字可选），结果数据可以是该目录下多个文件（具体根据mapper的数量决定）<br>
hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15';<br>
c.从pokes表中查询所有行插入到本地目录中<br>
hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/local_out' SELECT a.* FROM pokes a;<br>
d.将查询的求和，平均，最大最小值，部分列等结果插入到相关目录或表中<br>
hive&gt; INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a;<br>
hive&gt; INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a WHERE a.key &lt; 100;<br>
hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/reg_3' SELECT a.* FROM events a;<br>
hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_4' select a.invites, a.pokes FROM profiles a;<br>
hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT COUNT(*) FROM invites a WHERE a.ds='2008-08-15';<br>
hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT a.foo, a.bar FROM invites a;<br>
hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/sum' SELECT SUM(a.pc) FROM pc1 a;<br>
2）分组（GROUP BY）<br>
hive&gt; FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo &gt; 0 GROUP BY a.bar;<br>
hive&gt; INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo &gt; 0 GROUP BY a.bar;<br>
3）连接（join）<br>
hive&gt; FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo;<br>
4）多表插入（MULTITABLE INSERT）<br>
FROM src<br>
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key &lt; 100<br>
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key &gt;= 100 and src.key &lt; 200<br>
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key &gt;= 200 and src.key &lt; 300<br>
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/dest4.out' SELECT src.value WHERE src.key &gt;= 300;<br>
5）流操作（STREAMING）<br>
hive&gt; FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds &gt; '2008-08-09';<br><br>
10.实例<br>
MovieLens User Ratings<br>
1）创建表，如下：<br>
CREATE TABLE u_data (<br>
userid INT,<br>
movieid INT,<br>
rating INT,<br>
unixtime STRING)<br>
ROW FORMAT DELIMITED<br>
FIELDS TERMINATED BY '\t'<br>
STORED AS TEXTFILE;<br>
2）下载和解压数据文件<br>
wget http://www.grouplens.org/system/files/ml-data.tar+0.gz<br>
tar xvzf ml-data.tar+0.gz<br>
3）加载数据到表中<br>
LOAD DATA LOCAL INPATH 'ml-data/u.data'<br>
OVERWRITE INTO TABLE u_data;<br>
4）统计表中的行数<br>
SELECT COUNT(*) FROM u_data;<br>
注：注意版本，在版本HIVE-287,需要使用COUNT(1)替换 COUNT(*)<br>
5）对数据进行复杂分析。创建pathoy脚本weekday_mapper.py：<br>
import sys<br>
import datetime<br><br>
for line in sys.stdin:<br>
line = line.strip()<br>
userid, movieid, rating, unixtime = line.split('\t')<br>
weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()<br>
print '\t'.join([userid, movieid, rating, str(weekday)])<br>
6）使用mapper脚本：<br>
CREATE TABLE u_data_new (<br>
userid INT,<br>
movieid INT,<br>
rating INT,<br>
weekday INT)<br>
ROW FORMAT DELIMITED<br>
FIELDS TERMINATED BY '\t';<br><br>
add FILE weekday_mapper.py;<br><br>
INSERT OVERWRITE TABLE u_data_new<br>
SELECT<br>
TRANSFORM (userid, movieid, rating, unixtime)<br>
USING 'python weekday_mapper.py'<br>
AS (userid, movieid, rating, weekday)<br>
FROM u_data;<br><br>
SELECT weekday, COUNT(*)<br>
FROM u_data_new<br>
GROUP BY weekday;<br>
注：hive0.5.0或更早版本需要使用COUNT(1) 替换 COUNT(*)<br><br>
11.Apache Weblog Data<br>
apache 网络日志格式为定制的，许多webmasters使用默认的格式。对于默认的weblog日志，我们能够使用如下命令创建一个对应的表。（更多详情：http://issues.apache.org/jira/browse/HIVE-662）：<br><br>
add jar ../build/contrib/hive_contrib.jar;<br><br>
CREATE TABLE apachelog (<br>
host STRING,<br>
identity STRING,<br>
user STRING,<br>
time STRING,<br>
request STRING,<br>
status STRING,<br>
size STRING,<br>
referer STRING,<br>
agent STRING)<br>
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'<br>
WITH SERDEPROPERTIES (<br>
"input.regex" = "([^]*) ([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?",<br>
"output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"<br>
)<br>
STORED AS TEXTFILE;<br><br>
12 使用hive web interface（hwi）接口访问操作数据<br>
a 查看hive-default.xml中的hive.hwi.listen.host,hive.hwi.listen.port,hive.hwi.war.file 中设置的属性信息，即hwi接受的访问的主机名，端口等信息。<br>
b 在hive安装目录下 启动bin/hive --service hwi 服务<br>
c 通过浏览器访问 http：//masterIP：PORT/hwi<br><br>
13.通过jdbc/odbc访问hive<br>
操作时注意几点：<br>
a. 在连接程序运行前要导入hive文件下面的所有的包<br>
b. 运行时间可能较长，这是正常的，只要耐心等待。运行hive --service hiveserver --help 可查看相应命令的使用<br>
c. 输入hive --service hiveserver之后，命令框卡住不动是正常情况，如果要使其成为后台程序，只要改输入为 hive --service hiveserver &amp;即可<br>
d. 真正应用是需要将metadata另外放置在一台机器上，并且导入到mysql中是比较通用的方法。<br>
e.注意hive数据库和hive元数据存储数据库区别；启动hiveservice后，其hive的驱动名（固定），注意：如果使用mysql数据库存储元数据，此驱动名并非mysql的启动名：com.mysql.jdbc.Driver，否则报错；存储hive元数据的数据库名在hive-site。xml文件中的javax.jdo.option.ConnectionURL属性中有定义。<br>
f.对于嵌入模式，url仅是“jdbc：hive：//”，对于单服务器模式，url为"jdbc:hive://host:port/dbname"，主机，端口分别为hive服务器运行宿主机的主机和相应端口，url中的dbname为hive数据库数据库名而非元数据库名。<br>
g.hive支持的客户端有jdbc,odbc,python,php,thrift java client,thrift c++ client<br><br>
14.启动hiveservice <br>
WARNING!<br>
HiveServer can not handle concurrent requests from more than one client. This is actually a limitation imposed by the Thrift interface that HiveServer exports, and can't be resolved by modifying the HiveServer code. We're currently working on a rewrite of HiveServer
 that addresses these problems. Progress on this issue is being tracked in HIVE-2935.<br>
Hive server and clients communicates through Thrift and FB303 services. In some distributions, both the Hadoop and Hive distributions have different versions of libthrift.jar and libfb303.jar. If they are incompatible, it may cause Thrift connection error when
 running the unit test on standalone mode. The solution is to remove the Hadoop's version of libthrift.jar and libfb303.jar.<br><br>
Once Hive has been built using steps in Getting Started, the thrift server can be started by running the following:<br><br>
$ build/dist/bin/hive --service hiveserver --help<br>
usage HIVE_PORT=xxxx ./hive --service hiveserver<br>
HIVE_PORT : Specify the server port<br><br>
$ bin/hive --service hiveserver<br><br>
After starting the server, to test if the server is working well, run the hiveserver and jdbc tests in 'standalone' mode. The HIVE_PORT is assumed to be 10000 on localhost for this case.<br><br>
$ ant test -Dtestcase=TestJdbcDriver -Dstandalone=true<br>
$ ant test -Dtestcase=TestHiveServer -Dstandalone=true<br><br>
The service supports clients in multiple languages. For more details see Hive Client<br></p>
<p><br></p>
            </div>
                </div>