---
layout:     post
title:      [2.4]以row_number为例解读spark sql的窗口函数
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，转载请标明出处					https://blog.csdn.net/PENGYUCHENG32109/article/details/51785749				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h3 id="参考">参考</h3>

<p><a href="http://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.sql.functions%24" rel="nofollow">spark官网</a> <br>
<a href="http://weibo.com/ilovepains" rel="nofollow">王家林DT大数据梦工厂</a></p>



<h3 id="场景">场景</h3>

<p>将本地文件toNGroup.txt中的内容：</p>



<pre class="prettyprint"><code class=" hljs ruby">hadoop<span class="hljs-variable">@master</span><span class="hljs-symbol">:~/resource</span><span class="hljs-variable">$ </span>cat toNGroup.txt 
hadoop <span class="hljs-number">29</span>
hadoop <span class="hljs-number">87</span>
hadoop <span class="hljs-number">39</span>
hadoop <span class="hljs-number">27</span>
hadoop <span class="hljs-number">88</span>
spark <span class="hljs-number">29</span>
spark <span class="hljs-number">90</span>
spark <span class="hljs-number">27</span>
spark <span class="hljs-number">84</span>
spark <span class="hljs-number">92</span>
hadoop<span class="hljs-variable">@master</span><span class="hljs-symbol">:~/resource</span><span class="hljs-variable">$ </span></code></pre>

<p>按照第一个字段分组，然后按照第二个字段降序排序，取前4位。即正常结果如下显示：</p>



<pre class="prettyprint"><code class=" hljs ">spark   92
spark   90
spark   84
spark   29
hadoop  88
hadoop  87
hadoop  39
hadoop  29</code></pre>



<h3 id="分析">分析</h3>

<ul>
<li>将本地数据导入到hive表中。spark SQL 通过HiveContext可以直接操作 hive仓库表中的数据</li>
<li>通过窗口函数生成一个数字序列，取该序列的前4条数据即可 <br>
spark sql中提供了很多内置的函数，这个与mysql中内置的函数类型相似，大致分为： <br>
Aggregate functions、Collection functions、Date time functions、Math functions、String functions、UDF functions以及Window functions - 具体内容可以参看[参考]中的相关链接。通过这些函数，数据分析人员可以很方便的对数据进行各种丰富的挖掘。本文主要以row_number函数实现分组排序为例子，体验窗口函数的使用。row_number函数说明如下：</li>
</ul>



<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">row_number</span><span class="hljs-params">()</span>:</span> Column

Window function: returns a sequential number starting at <span class="hljs-number">1</span> within a window partition. </code></pre>

<p>嘿，分组、排序在各大电商网站的应用是有多常见啊！</p>



<h3 id="实验">实验</h3>



<pre class="prettyprint"><code class=" hljs sql">package main.scala

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

<span class="hljs-comment">/**
 * 窗口函数实战
 * 
 * 1、Spark可以通过HiveContext直接操作Hive中的数据，基于HiveContext我们可以使用sql/hql两种方式编写SQL语句对Hive
 * 进行操作：创建、删除表，往表中导入数据，以及CRUD
 * 2、通过saveAsTable方式把DataFrame中的数据保存到Hive数据仓库中
 * 3、通过 HiveContext.table方式直接加载Hive中的表而生成DataFrame
*/</span>
object SparkSQLWindowFunctionOps {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("SparkSQLWindowFunctionOps")
    val sc = new SparkContext(conf)

    val hiveContext = new HiveContext(sc)
    hiveContext.sql("use hive")
    hiveContext.sql("<span class="hljs-operator"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">IF</span> <span class="hljs-keyword">EXISTS</span> scores<span class="hljs-string">")
    hiveContext.sql("</span><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span>  <span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> scores(name STRING,score <span class="hljs-keyword">INT</span>) <span class="hljs-keyword">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">' '</span> LINES TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">'\\n'</span><span class="hljs-string">")

    //把要处理的数据导入到Hive表中
    hiveContext.sql("</span><span class="hljs-keyword">LOAD</span> DATA <span class="hljs-keyword">LOCAL</span> INPATH <span class="hljs-string">'/home/hadoop/resource/toNGroup.txt'</span> <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">TABLE</span> scores<span class="hljs-string">" )

    /*
     * 使用自查询完成目标数据的提取，在目标数据内使用窗口函数row_number来进行分组排序：
     * PARTITION BY ：指定窗口函数分组的Key
     * ORDER BY :分组后进行排序
     */
    val result =  hiveContext.sql("</span><span class="hljs-keyword">SELECT</span> name,score <span class="hljs-string">"
        + "</span><span class="hljs-keyword">FROM</span> (<span class="hljs-string">"
            + "</span><span class="hljs-keyword">SELECT</span> <span class="hljs-string">"
               + "</span>name,<span class="hljs-string">"
               + "</span>score, <span class="hljs-string">"
               + "</span>row_number() OVER (PARTITION <span class="hljs-keyword">BY</span> name <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> score <span class="hljs-keyword">DESC</span> ) rank<span class="hljs-string">"
               + "</span> <span class="hljs-keyword">FROM</span> scores<span class="hljs-string">"
        + "</span>)  sub_scores <span class="hljs-string">"
        + "</span> <span class="hljs-keyword">WHERE</span> rank &lt;= <span class="hljs-number">4</span><span class="hljs-string">")

     result.show(); //在Driver的控制台上打印出结果

    hiveContext.sql("</span><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">IF</span> <span class="hljs-keyword">EXISTS</span> sortedResultScores<span class="hljs-string">")
    result.saveAsTable("</span>sortedResultScores<span class="hljs-string">")
  }
}</span></span></code></pre>



<h4 id="执行结果">执行结果</h4>



<pre class="prettyprint"><code class=" hljs asciidoc">16/06/01 23:22:19 INFO DAGScheduler: ResultStage 3 (show at SparkSQLWindowFunctionOps.scala:46) finished in 6.969 s
<span class="hljs-header">16/06/01 23:22:19 INFO DAGScheduler: Job 1 finished: show at SparkSQLWindowFunctionOps.scala:46, took 7.284524 s
+------+-----+</span>
<span class="hljs-header">|  name|score|
+------+-----+</span>
| spark|   92|
| spark|   90|
| spark|   84|
| spark|   29|
|hadoop|   88|
|hadoop|   87|
|hadoop|   39|
<span class="hljs-header">|hadoop|   29|
+------+-----+</span>

16/06/01 23:22:19 INFO ParseDriver: Parsing command: DROP TABLE IF EXISTS sortedResultScores
16/06/01 23:22:19 INFO ParseDriver: Parse Completed</code></pre>



<h3 id="总结">总结</h3>

<p>1、为什么称作 <code>Window function</code>呢？</p>



<pre class="prettyprint"><code class=" hljs sql">"<span class="hljs-operator"><span class="hljs-keyword">SELECT</span> name,score <span class="hljs-string">"
        + "</span><span class="hljs-keyword">FROM</span> (<span class="hljs-string">"
            + "</span><span class="hljs-keyword">SELECT</span> <span class="hljs-string">"
               + "</span>name,<span class="hljs-string">"
               + "</span>score, <span class="hljs-string">"
               + "</span>row_number() OVER (PARTITION <span class="hljs-keyword">BY</span> name <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> score <span class="hljs-keyword">DESC</span> ) rank<span class="hljs-string">"
               + "</span> <span class="hljs-keyword">FROM</span> scores<span class="hljs-string">"
        + "</span>)  sub_scores <span class="hljs-string">"
        + "</span> <span class="hljs-keyword">WHERE</span> rank &lt;= <span class="hljs-number">4</span><span class="hljs-string">"</span></span></code></pre>

<p>row_number函数作用于一个分区（本例中就是 spark与hadoop形成的两个分区），并为该分区中的每条记录生成一个序列号，这样在外层循环就可以通过过滤该序列号（eg、rank&lt;4）而取特定的数据。从功能上来看，row_number为外层查询操作里面的记录，打开了一扇窗户（写不下去了，这个说法实在有点勉强 ～～～），暂时就这么理解吧！</p>

<p>2、spark sql 通过hiveContext直接操作 hive仓库中的数据 - 这点实在太棒了啊啊啊！</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>