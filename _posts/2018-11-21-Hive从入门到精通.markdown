---
layout:     post
title:      Hive从入门到精通
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章,请评论后转载，谢谢合作！！！					https://blog.csdn.net/qq_33247435/article/details/84023246				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><h3>文章目录</h3><ul><li><a href="#1Hive_1" rel="nofollow">1、Hive是什么</a></li><li><a href="#2Hive_9" rel="nofollow">2、Hive架构</a></li><li><a href="#3Hive_11" rel="nofollow">3、Hive中数据类型</a></li><li><a href="#3Hive_30" rel="nofollow">3、Hive中的表</a></li><ul><li><a href="#emsp_31_37" rel="nofollow">  3.1、内部表</a></li><li><a href="#emsp_32_84" rel="nofollow">  3.2、外部表</a></li><li><a href="#emsp_33_95" rel="nofollow">  3.3、临时表</a></li><li><a href="#emsp_34_101" rel="nofollow">  3.4、分区表</a></li><li><a href="#emsp_35_194" rel="nofollow">  3.5、分桶表</a></li></ul><li><a href="#4Hive__232" rel="nofollow">4、Hive 视图</a></li><li><a href="#5Hive__249" rel="nofollow">5、Hive 索引</a></li><li><a href="#6Hive_273" rel="nofollow">6、操作Hive的方式</a></li><ul><li><a href="#emsp61beeline_276" rel="nofollow"> 6.1、beeline</a></li><li><a href="#emsp62JDBC_359" rel="nofollow"> 6.2、JDBC</a></li></ul></ul></div><p></p>
<h1><a id="1Hive_1"></a>1、Hive是什么</h1>
<p><br>  hive是基于Hadoop的一个数据仓库<mark>工具</mark>，可以将结构化的数据文件映射为一张<mark>数据库表</mark>，并提供的sql查询功能，可以<mark>将sql语句转换为MapReduce任务</mark>进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。<br></p>
<p>  Hive本身并不提供存储服务,使用<mark>HDFS</mark>做数据存储。Hive本身并不提供分布式计算功能，而是基于<mark>MapReduce</mark>计算框架。Hive本身也并不提供资源调度系统，而是使用Hadoop的<mark>Yarn</mark>集群调度。Hive运行时，元数据存储在关系型数据库里面。</p>
<p>  Hive中支持使用SQL语句来进行数据处理，使用SQL语句的过程如下图：<br>
  <img src="https://img-blog.csdnimg.cn/20181117131538598.png" alt="在这里插入图片描述"><br>
<br></p>
<h1><a id="2Hive_9"></a>2、Hive架构</h1>
<p>  <img src="https://img-blog.csdnimg.cn/20181117134421682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjQ3NDM1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1><a id="3Hive_11"></a>3、Hive中数据类型</h1>
<p>  下面简单介绍一下Hive中允许使用的数据类型：</p>

<table>
<thead>
<tr>
<th align="left">数据类型</th>
<th align="left">注释</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">TINYINT</td>
<td align="left">-128 to 127</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">-32,768 to 32,767</td>
</tr>
<tr>
<td align="left">INT</td>
<td align="left">-2,147,483,648 to 2,147,483,647</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">4-byte 单精度浮点数</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">8-byte 双精度浮点数</td>
</tr>
<tr>
<td align="left">DECIMAL</td>
<td align="left">precision of 38 digits（38位数精度）</td>
</tr>
<tr>
<td align="left">TIMESTAMP</td>
<td align="left">时间戳</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">时间</td>
</tr>
<tr>
<td align="left">STRING</td>
<td align="left">字符串</td>
</tr>
<tr>
<td align="left">VARCHAR</td>
<td align="left">可变长度字符串</td>
</tr>
<tr>
<td align="left">CHAR</td>
<td align="left">字符</td>
</tr>
<tr>
<td align="left">arrays</td>
<td align="left">数组类型</td>
</tr>
<tr>
<td align="left">maps</td>
<td align="left">键值对集合类型</td>
</tr>
<tr>
<td align="left">structs</td>
<td align="left">结构化类型</td>
</tr>
</tbody>
</table><h1><a id="3Hive_30"></a>3、Hive中的表</h1>
<p>  Hive中的表有5种：<br>
    1）、内部表（受控表）：当删除内部表的时候，HDFS上的数据和元数据都会被删除。<br>
    2）、外部表：当删除外部表的时候，元数据会被删除，但是HDFS上的数据不会被删除。<br>
    3）、临时表（测试）：当前会话期间内存在，会话结束自动消失，<br>
    4）、分区表：将一批数据分成多个目录来存储，防止暴力扫描全表。<br>
    5）、分桶表：按照数据的hash值与桶的个数取模，根据取模得到的值进行分桶。这种方式有利于join的合并与随机抽样。对于hive中每一个表、分区都可以进一步进行分桶。</p>
<h2><a id="emsp_31_37"></a>  3.1、内部表</h2>
<p>  假设我们的数据格式为：</p>
<blockquote>
<p>001 zhang 19 Angelababy,Dilireba,zhangxinyi Angelababy:qingdao,Dilireba:xinjiang,zhangxinyi:no zhongguo,shandong,qingdao<br>
002 zhang2 20 fengjie,yujie fengjie:American,yujie:长沙 zhongguo,shandong,qingdao</p>
</blockquote>
<p>  <strong>创建表方式一：</strong></p>
<pre><code>CREATE TABLE gfstbl(
  id INT,
  name STRING,
  age INT,
  gfs ARRAY&lt;STRING&gt;,
  address MAP&lt;STRING,STRING&gt;,
  info STRUCT&lt;country:String,province:String,shi:String&gt;
)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ' ' 
COLLECTION ITEMS TERMINATED BY ','   //元素分隔符
MAP KEYS TERMINATED BY ':'  		 //键值对分隔符
LINES TERMINATED BY '\n';    		 //行分隔符
LOCATION "/test" //可以设置源数据的位置，若不设置默认就在Hive的工作目录区
</code></pre>
<p>  <strong>创建表方式二：</strong></p>
<pre><code>create table gfstbl1 like gfstbl; 	 //只是创建表结构
</code></pre>
<p>  <strong>创建表方式三：</strong></p>
<pre><code>create table gfstbl2 AS SELECT id,name,gfs,address from gfstbl;  会创建相应的表结构，并且插入数据
</code></pre>
<p>  <strong>插入数据的方式有三种：</strong><br>
  1、insert 新数据：insert into tablename values(a,a,a)<br>
  2、load：load data local inpath ‘/root/gfs.txt’ into table gfstbl;<br>
  3、查询其他表数据 insert 到新表中：insert into rest select count(*) from tablename;</p>
<p>  <strong>习惯写法</strong>： from提前  减少SQL代码的冗余<br>
    from tablename<br>
    insert into rest select count(*) ;<br><br>
  <strong>查看表描述信息：</strong></p>
<pre><code>DESCRIBE [EXTENDED|FORMATTED] table_name
EXTENDED极简的方式显示
FORMATTED格式化方式来显示
DESCRIBE EXTENDED gfstbl;默认就是EXTENDED
DESCRIBE FORMATTED gfstbl;
</code></pre>
<h2><a id="emsp_32_84"></a>  3.2、外部表</h2>
<p>  <strong>建表语句</strong></p>
<pre><code>create external table wc_external 
(word1 STRING, 
word2 STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ' ' 
location '/test/external'; location可加可不加，不加location默认是在hive的工作目录区
</code></pre>
<h2><a id="emsp_33_95"></a>  3.3、临时表</h2>
<p>  进入hive shell 创建一张表，关闭shell后，表就会丢失。（临时表不支持分区）<br><br>
  <strong>建表语句：</strong></p>
<pre><code>create TEMPORARY table ttabc(id Int,name String)
</code></pre>
<h2><a id="emsp_34_101"></a>  3.4、分区表</h2>
<p>  创建分区表的原因：防止暴力扫描全表，提升查询效率。<br><br>
  <strong>3.4.1、静态分区表</strong><br><br>
  <strong><mark>单分区表</mark></strong><br><br>
  <strong>创建单分区表：</strong></p>
<pre><code>create table day_table (id int, content string) partitioned by (dt string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ;
</code></pre>
<p>  <strong>加载数据</strong>的方式：<br>
  1）、insert单条插入的方式往分区表中插入数据：</p>
<pre><code>insert into day_table partition (dt = "9-26") values(1,"anb");
</code></pre>
<p>  2）、load批量插入的方式往分区表中插入数据：</p>
<pre><code>load data local inpath "/root/ceshi" into table day_table partition (dt="9-27");
</code></pre>
<p>  <strong>删除Hive分区表中的分区</strong></p>
<pre><code>ALTER TABLE day_table DROP PARTITION (dt="9-27");
</code></pre>
<p>  <strong><mark>多分区表</mark></strong><br><br>
  <strong>创建多分区表：</strong></p>
<pre><code>create table day_hour_table (id int, content string) partitioned by (dt int,hour int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ;
</code></pre>
<p>  <strong>加载数据</strong>的方式：<br>
  1）、insert单条插入的方式往分区表中插入数据：</p>
<pre><code>insert into day_hour_table partition(dt=9,hour=1) values(1,"a2 bc");
insert into day_hour_table partition(dt=9,hour=2) values(3,"a2 bc");
insert into day_hour_table partition(dt=8,hour=1) values(3,"a2 bc");
insert into day_hour_table partition(dt=8,hour=2) values(3,"a2 bc");
</code></pre>
<p>  2）、load批量插入的方式往分区表中插入数据：</p>
<pre><code>load data local inpath "/root/ceshi" into table day_table partition (dt=10,hour=10);
</code></pre>
<p>  <strong>删除Hive分区表中的分区</strong></p>
<pre><code>ALTER TABLE day_table DROP PARTITION (dt=10,hour=10);
</code></pre>
<p>  <strong>添加/创建分区</strong><br><br>
  创建一个空分区：</p>
<pre><code>ALTER TABLE day_hour_table ADD PARTITION (dt=10000, hour=2000);
</code></pre>
<p>  然后将数据上传到空分区对应的目录下，分区表中就会显示数据</p>
<pre><code>hdfs dfs -put  数据  上传到哪
</code></pre>
<p>  创建一个空分区并且将空分区指向数据位置：</p>
<pre><code>ALTER TABLE day_hour_table ADD PARTITION (dt=10000, hour=2000) location "/test" 
</code></pre>
<p>  总结：往分区中<strong>添加数据的五种方式</strong>：</p>
<blockquote>
<p>（1）insert 指定分区<br>
（2）load data 指定分区<br>
（3）查询已有表的数据，insert到新表中<br>
from day_hour_table insert into table newt partition(dt=01,hour=9898) select id,content<br>
（4）alter table add partition创建空分区，然后使用HDFS命令往空分区目录中上传数据<br>
（5）创建分区，并且指定分区数据的位置</p>
</blockquote>
<p>  <strong>3.4.2、动态分区表</strong><br><br>
  刚才分区表示静态分区表，一个文件数据只能导入到某一个分区中，并且分区是用户指定的。这种方式不够灵活，业务场景比较局限。执行配置信息：<code>set hive.exec.dynamic.partition=true;</code>之后创建的分区就是动态分区表了。<br>
  动态分区可以根据数据本身的特征自动来划分分区，比如我们可以指定按照数据中的年龄、性别来动态分区。<br>
<br>  静态分区与动态分区<strong>创建表</strong>的语句是一模一样的：</p>
<pre><code>CREATE TABLE gfstbl_dynamic(
  id INT,
  name STRING,
  gfs ARRAY&lt;STRING&gt;,
  address MAP&lt;STRING,STRING&gt;,
  info STRUCT&lt;country:String,province:String,shi:String&gt;
)
partitioned by (sex string,age INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ' ' 
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':' 
LINES TERMINATED BY '\n';
</code></pre>
<p>  load data只是将数据上传到HDFS指定目录中。我们之前使用load data往分区表导入数据的时候，都是要指定partition分区的，这样他才会知道将数据上传到HDFS的哪一个分区。但是如果我们还是采用load data指定分区的话，那就不是动态分区表，还依然是静态分区表。所以得采用 from insert的方式<strong>插入数据</strong>：</p>
<pre><code>from gfstbl_pt
insert into gfstbl_dynamic partition(sex='man',age&gt;15)
select id,name,gfs,address,info,sex,age;
</code></pre>
<p>  <strong>查看分区数</strong>：</p>
<pre><code>show partitions gfstbl_dynamic;
</code></pre>
<h2><a id="emsp_35_194"></a>  3.5、分桶表</h2>
<p>  分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。由<mark>列的哈希值</mark>除以<mark>桶的个数</mark>来决定每条数据划分在哪个桶中。对于hive中每一个表、分区都可以进一步进行分桶。<br><br>
  使用分桶表也需要设置<strong>配置参数</strong>：<code>set hive.enforce.bucketing=true;</code><br><br>
  分桶表的<strong>作用</strong>：有利于join和随机抽样。<br></p>
<p>  下面<strong>创建</strong>分桶表进行演示。首先创建一个<strong>原始表</strong>：</p>
<pre><code>CREATE TABLE original( id INT, name STRING, age INT,height DOUBLE) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
</code></pre>
<p>  加载数据至原始表中：</p>
<pre><code>LOAD DATA LOCAL INPATH "/root/bucketData" into table original;
</code></pre>
<p>  <strong>创建分桶表</strong>：</p>
<pre><code>CREATE TABLE psnbucket( id INT, name STRING, age INT) 
CLUSTERED BY (age) INTO 4 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
</code></pre>
<p>  <strong>插入数据</strong>：（根据上面创建分桶表的逻辑进行数据分桶，每一个小文件对应一个桶）</p>
<pre><code>insert into table psnbucket select id, name, age from original;
</code></pre>
<p>  还可以在分区表的基础上创建分桶表：</p>
<pre><code>CREATE TABLE psnbucket_partition( id INT, name STRING, age INT) 
PARTITIONED BY(height DOUBLE) 
CLUSTERED BY (age) INTO 4 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
</code></pre>
<p>  插入数据：（多个分区中的对应的位置的小文件组成一个桶）</p>
<pre><code>insert into table psnbucket_partition partition(height) select id, name, age,height from original;
</code></pre>
<p>  <strong>抽样</strong>：</p>
<pre><code>select * from psnbucket tablesample(bucket 1 out of 4 on age);
</code></pre>
<h1><a id="4Hive__232"></a>4、Hive 视图</h1>
<p>  我们在进行sql查询的时候，可能会写的sql语句很长。如果后期要经常使用这个sql语句，每次都写显得比较麻烦。可以将这条长的sql语句与视图进行映射。<br><br>
  <mark>每次执行视图就是执行了这个长的sql语句</mark>。视图不存储数据，只是存储这个SQL语句的逻辑。<br><br>
  <strong>视图的特点：</strong><br>
    1）、不支持物化视图<br>
    2）、只能查询，不能做加载数据操作  load data into<br>
    3）、视图的创建，只是保存一份元数据，查询视图时才执行对应的子查询<br>
    4）、view定义中若包含了ORDER BY/LIMIT语句，当查询视图时也进行ORDER BY/LIMIT语句操作，view当中定义的优先级更高<br>
    5）、view支持迭代视图<br>
    6）、一旦创建成功，无法修改<br><br>
  <strong>视图的创建：</strong></p>
<pre><code>CREATE VIEW  IF NOT EXISTS  view1 AS SELECT * FROM logtbl order by age;
</code></pre>
<p>  查看视图：<code>show tables</code><br>
  删除视图：<code>drop view view1</code><br>
  创建视图的时候不会启动MR任务，但是在查询视图的时候会启动MR任务。因为视图的创建，只是保存一份元数据，查询视图时才执行对应的子查询。</p>
<h1><a id="5Hive__249"></a>5、Hive 索引</h1>
<p>  索引的存在就是为了<strong>优化查询性能</strong>。<br><br>
  这里举个例子来理解一下，假设某个表数据非常大，如果使用<code>select * from table where age = 10;</code>查询的性能就会比较低。我们可以创建一个目录（索引）来提高查询效率：<br></p>
<blockquote>
<p>索引1（age &gt; 10） block1(100,200)   block2(200,389)<br>
索引2（age = 10） block1(101,220)   block2(200,389)</p>
</blockquote>
<p>  <strong>创建索引库</strong>，用于存放索引（索引库中只是保存一些元数据，比如 对哪个字段创建索引，对哪个表创建索引等）。</p>
<pre><code>create index t2_index on table psnbucket_partition(age) 
as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' with deferred rebuild 
in table t2_index_table;
</code></pre>
<p>  这一步是真正的<strong>创建索引信息</strong>，并且存储到索引库中，若数据库有新增数据，也可以使用以上语句重建索引。</p>
<pre><code>alter index t2_index on psnbucket_partition rebuild; 
</code></pre>
<p>  <strong>查询索引</strong>：</p>
<pre><code>show index on psnbucket_partition;
</code></pre>
<p>  <strong>删除索引</strong>（删除索引的同时  索引库也会被删除）：</p>
<pre><code>drop index t2_index on psnbucket_partition; 
</code></pre>
<h1><a id="6Hive_273"></a>6、操作Hive的方式</h1>
<h2><a id="emsp61beeline_276"></a> 6.1、beeline</h2>
<p>  之前在操作hive的是，直接通过hive命令进入hive cli进行数据分析以及处理，这种方式既不安全又不规范。<br><br>
  beeline是一个新兴的cli客户端  类似jdbc/odbc  可以解决一切的问题，并且还能够很好的解耦合。<br><br>
  hive client直接连接HDFS、yarn。beeline需要先与<strong>thriftserver</strong>连接，thriftserver能够进行安全认证、可靠认证、提高客户端的并发。</p>
<p>  beeline默认链接hiveserver2的时候，不需要用户名 密码,默认方式也是不安全，我们可以设置hiveserver2用户名、密码。</p>
<p><strong>设置用户名、密码的步骤：</strong><br>
在hive-site.xml中添加一下信息：</p>
<pre><code>&lt;property&gt;
       &lt;name&gt;hive.server2.authentication&lt;/name&gt;
        &lt;value&gt;CUSTOM&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;hive.jdbc_passwd.auth.zhangsan&lt;/name&gt;
        &lt;value&gt;123456789&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;hive.server2.custom.authentication.class&lt;/name&gt;
        &lt;value&gt;com.hoe.hive.authoriz.UserPasswdAuth&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>写代码：</p>
<pre><code>package com.hoe.hive.authoriz;

import javax.security.sasl.AuthenticationException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hive.service.auth.PasswdAuthenticationProvider;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class UserPasswdAuth implements PasswdAuthenticationProvider {
	Logger logger = LoggerFactory.getLogger(UserPasswdAuth.class);

	private static final String USER_PASSWD_AUTH_PREFIX = "hive.jdbc_passwd.auth.%s";

	private Configuration conf = null;

	@Override
	public void Authenticate(String userName, String passwd) throws AuthenticationException {
		logger.info("user: " + userName + " try login.");
		String passwdConf = getConf().get(String.format(USER_PASSWD_AUTH_PREFIX, userName));
		if (passwdConf == null) {
			String message = "沒有發現密碼 " + userName;
			logger.info(message);
			throw new AuthenticationException(message);
		}
		if (!passwd.equals(passwdConf)) {
			String message = "用戶名密碼不匹配 " + userName;
			throw new AuthenticationException(message);
		}
	}

	public Configuration getConf() {
		if (conf == null) {
			this.conf = new Configuration(new HiveConf());
		}
		return conf;
	}

	public void setConf(Configuration conf) {
		this.conf = conf;
	}
}
</code></pre>
<p><strong>beeline 连接方式</strong>：<br>
1、</p>
<pre><code>./beeline -u jdbc:hive2://node01:10000/test -n zhangsan -p123456789
</code></pre>
<p>2、</p>
<pre><code>./beeline
!connect jdbc:hive2://node01:10000/test
username
password
</code></pre>
<h2><a id="emsp62JDBC_359"></a> 6.2、JDBC</h2>
<p>  因为通过JDBC链接hive 也是链接hiveserver2服务，链接成功才能操作hive。所以JDBC连接的时候也是需要用户名和密码的。</p>
<pre><code>package com.hpe.hive.jdbc;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

public class ConnectHive {

	public static String driverName = "org.apache.hive.jdbc.HiveDriver";

	public static void main(String[] args) {

		try {
			Class.forName(driverName);
		} catch (ClassNotFoundException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		String url = "jdbc:hive2://node01:10000";
		String userName = "zhangsan";
		String passwd = "123456789";
		Connection conn = null;
		try {
			conn = DriverManager.getConnection(url, userName, passwd);
			Statement statement = conn.createStatement();
			String sql = "select * from test.logtbl limit 10";
			ResultSet resultSet = statement.executeQuery(sql);
			while (resultSet.next()) {
				System.out.println(resultSet.getString(1) + "-" + resultSet.getString(2));
			}
		} catch (SQLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

	}
}
</code></pre>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>