---
layout:     post
title:      spark实践
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h2 id="spark是什么" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"></h2><div class="toc" style="line-height:24px;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><div class="toc" style="line-height:24px;"><ul style="list-style:none;"><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E5%85%A8%E8%A7%A3%E6%9E%90" rel="nofollow">SPARK全解析</a><ul style="list-style:none;"><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E6%98%AF%E4%BB%80%E4%B9%88" rel="nofollow">Spark是什么？</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91" rel="nofollow">Spark 源码编译</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E5%8F%8Aspark-shell%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8" rel="nofollow">Spark本地模式安装配置及Spark Shell基本使用</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E9%9B%86%E7%BE%A4" rel="nofollow">Spark集群</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-application%E5%BC%80%E5%8F%91%E8%BF%90%E8%A1%8C%E5%8F%8A%E7%9B%91%E6%8E%A7idea" rel="nofollow">Spark Application开发、运行及监控（IDEA）</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7historyserver%E9%85%8D%E7%BD%AE" rel="nofollow">Spark 日志监控（HistoryServer）配置</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-rdd" rel="nofollow">Spark RDD</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F" rel="nofollow">Spark共享变量</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-%E5%86%85%E6%A0%B8" rel="nofollow">Spark 内核</a><ul style="list-style:none;"><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-%E4%BE%9D%E8%B5%96" rel="nofollow">Spark 依赖</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-shuffle" rel="nofollow">Spark Shuffle</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-application%E6%B7%BB%E5%8A%A0jar%E5%8C%85%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95" rel="nofollow">Spark Application添加jar包的三种方法</a></li></ul></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-sql" rel="nofollow">Spark SQL</a><ul style="list-style:none;"><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-sql%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B" rel="nofollow">Spark SQL的发展历程</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#dataframe" rel="nofollow">DataFrame</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#sparksql%E6%A1%88%E4%BE%8B" rel="nofollow">SparkSQL案例</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-%E9%9B%86%E6%88%90" rel="nofollow">Spark 集成</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E4%B8%AD%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0" rel="nofollow">SPARK中聚合函数</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark%E4%B8%AD%E5%AE%9A%E4%B9%89udfudaf" rel="nofollow">SPARK中定义UDF、UDAF</a></li></ul></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming" rel="nofollow">Spark Streaming</a><ul style="list-style:none;"><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86" rel="nofollow">Spark Streaming工作原理</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#dstream" rel="nofollow">DStream</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B" rel="nofollow">Spark Streaming编程模型</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming-%E8%AF%BB%E5%8F%96hdfs%E6%95%B0%E6%8D%AE" rel="nofollow">Spark Streaming 读取HDFS数据</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming%E7%9A%84%E9%9B%86%E6%88%90" rel="nofollow">Spark Streaming的集成</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming%E5%B8%B8%E7%94%A8api%E8%A7%A3%E6%9E%90" rel="nofollow">Spark Streaming常用API解析</a></li><li style="margin-left:24px;list-style-type:none;"><a href="https://blog.csdn.net/vinfly_li/article/details/79396821#spark-streaming%E7%9A%84%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" rel="nofollow">Spark Streaming的窗口函数</a></li></ul></li></ul></li></ul></div></div><h2 style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t1"></a>Spark是什么？</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Spark是Apache的一个顶级项目，是一个快速、通用的大规模数据处理引擎。以下是它的几个特点 ：</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>Speed <br>存储在内存中的数据，Spark比Hadoop的MapReduce快100多倍，存储在磁盘中的数据要快10多倍。</li><li>Easy of Use <br>开发Spark应用程序可以使用Java、Scala、Python、R等编程语言</li><li>Generality <br>Spark提供了SparkSQL、Streaming、MLlib、GraphX，功能强大。一站式解决需求。</li><li>Runs Everywhere <br>Spark可以运行在Hadoop的Yarn上、Mesos上、以及它自身的standalone上，处理的文件系统包括HDFS、Cassandra、HBase、S3. <br><span>以上部分摘自官网： <a href="http://spark.apache.org/" rel="nofollow">http://spark.apache.org/</a></span></li></ul><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-源码编译" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t2"></a>Spark 源码编译</h2><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><span>本文以 spark1.6.1版本为例</span></p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>（1）下载源码包 <br><img src="http://static.zybuluo.com/vin123456/sjyt6ufvxb8j412etp6tu3ld/image_1arfncu4k17rfmqs15b51q47gs99.png" alt="image_1arfncu4k17rfmqs15b51q47gs99.png-62.4kB" title=""> <br><ul style="list-style:none;"><li>（2）准备环境 <br>Spark1.6.1版本编译需要Maven 3.3.3 or newer and Java 7+ 环境 <br><img src="http://static.zybuluo.com/vin123456/g6v74xzge1crsgbleqvu8b6q/image_1arfndoeh1chn2mu143c16b51bp0m.png" alt="image_1arfndoeh1chn2mu143c16b51bp0m.png-102.1kB" title=""></li><li>（3）编译 <br>–1 解压spark源码 <br>–2 在执行编译前修改$SPARK_HOME下的make-distribution.sh文件如下 <br><img src="http://static.zybuluo.com/vin123456/0t0p7comiaybbydbiq8i0hyl/image_1arfneage1ufi6flqgakrf15qb13.png" alt="image_1arfneage1ufi6flqgakrf15qb13.png-8.6kB" title=""> <br>–3 编译apache hadoop，需要配置镜像文件 <br><span>路径</span>： <code style="font-size:14px;line-height:22px;">/opt/modules/apache-maven-3.3.3/conf/settings.xml</code> <br><span>配置内容</span>： <br><img src="http://static.zybuluo.com/vin123456/gp5qhne3pepx5rttm3sjwjwk/image_1arfnes1f32h19pp1eie1lfi14ta1g.png" alt="image_1arfnes1f32h19pp1eie1lfi14ta1g.png-103.9kB" title=""> <br><span>如果是cdh版本hadoop，则必须去掉该镜像</span> <br>–配置域名解析服务器 <br><code style="font-size:14px;line-height:22px;"># vi /etc/resolv.conf</code> <br>内容： <br><code style="font-size:14px;line-height:22px;">nameserver 8.8.8.8 <br>nameserver 8.8.4.4</code> <br>–4 执行编译（根据所使用的Hadoop版本进行编译）</li></ul></li></ul><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">—-针对APACH HADOOP <br><code style="font-size:14px;line-height:22px;">./make-distribution.sh --tgz -Phadoop-2.4 -Dhadoop.version=2.5.0 -Phive -Phive-thriftserver -Pyarn</code> <br>—- 针对CDH HADOOP <br><code style="font-size:14px;line-height:22px;">./make-distribution.sh --tgz -Phadoop-2.4 -Dhadoop.version=2.5.0-cdh5.3.6 -Phive -Phive-thriftserver -Pyarn</code> <br><img src="http://static.zybuluo.com/vin123456/2l9veaog09ojf536s7nibc2c/image_1arfng4qo1hjh1agsf8efmhd4p1t.png" alt="image_1arfng4qo1hjh1agsf8efmhd4p1t.png-24.7kB" title=""></p><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark本地模式安装配置及spark-shell基本使用" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t3"></a>Spark本地模式安装配置及Spark Shell基本使用</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">1、Spark安装环境准备：</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>JAVA</li><li>HDFS（<span>HDFS是否脱离了安全模式</span>）</li><li>SCALA <br><img src="http://static.zybuluo.com/vin123456/v9pz3p36nlpi8ylx9j48isct/image_1arfp3qag39r1l7l10n9c5kias2n.png" alt="image_1arfp3qag39r1l7l10n9c5kias2n.png-117.1kB" title=""></li></ul><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">2、Spark安装</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li><p>将编译好的cdh版本的spark赋予执行权限，解压至指定目录 <br><img src="http://static.zybuluo.com/vin123456/1k4ddm5er36gzk6slex6w29u/image_1arfp7n5n4ufkqa2pv9fsfb034.png" alt="image_1arfp7n5n4ufkqa2pv9fsfb034.png-23.7kB" title=""></p></li><li><p>通过notepad++配置$SPARK_HOME目录下conf下的配置文件 <br>①日志配置 <br>更改log4j.properties.template文件名为log4j.properties <br>②配置spark-env.sh <br><img src="http://static.zybuluo.com/vin123456/4h2084g3lzk75m3qw1pw3fi9/image_1arfp9m3d57o4fl1eoapdank13h.png" alt="image_1arfp9m3d57o4fl1eoapdank13h.png-96kB" title=""></p></li><li><p>配置完成</p></li></ul><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">3、测试Spark Shell命令行 <br>使用Spark RDD进行简单测试：</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li><p>启动spark交互式命令行：bin/spark-shell 并编程测试wordcount</p><ul style="list-style:none;"><li>HDFS上的数据源</li></ul><p><img src="http://static.zybuluo.com/vin123456/l9vd539yju65ru8anklgfwye/image_1arfqlusteiq6ob1c681bav1rv23u.png" alt="image_1arfqlusteiq6ob1c681bav1rv23u.png-11.6kB" title=""></p><ul style="list-style:none;"><li>定义rdd读取数据源</li></ul><p><img src="http://static.zybuluo.com/vin123456/k3ssp12tzdiakprbbcyzl5pc/image_1arfqnbb67m5jv3rg11838uni4b.png" alt="image_1arfqnbb67m5jv3rg11838uni4b.png-36.2kB" title=""></p><ul style="list-style:none;"><li>使用rdd.map(line =&gt; line.split(“ ”)) 可以将文件按空格进行分割，分割之后会变成数组</li></ul><p><img src="http://static.zybuluo.com/vin123456/uxrwwx6gbw9oeg2fg01qscrd/image_1arfqs38afrolgd168b8mk69i55.png" alt="image_1arfqs38afrolgd168b8mk69i55.png-18.9kB" title=""></p><ul style="list-style:none;"><li>再在其后面加上.collect之后查看输出</li></ul><p><img src="http://static.zybuluo.com/vin123456/723z1ubzeio10ijbfexp0foi/image_1arfqv0nq99l12821es5na499t5i.png" alt="image_1arfqv0nq99l12821es5na499t5i.png-29.8kB" title=""></p><ul style="list-style:none;"><li><p>这里需使用flatMap代替map对该数组进行一个压平的操作，即： rdd.flatMap(line =&gt; line.split(” “)).collect，输出的为压平后的一个个单词 <br><img src="http://static.zybuluo.com/vin123456/x7zrtxjqo3jqx384gtbkhpxo/image_1arfr0lo7jum1pcbu100b1m25v.png" alt="image_1arfr0lo7jum1pcbu100b1m25v.png-25.5kB" title=""></p></li><li><p>再使用map操作将其变为元组对，即：rdd.flatMap(line =&gt; line.split(” “)).map(word =&gt; (word,1)).collect <br>输出结果： <br><img src="http://static.zybuluo.com/vin123456/3n4mijtjoy7up0ptozkw4ct0/image_1arfr1qrfri21ds01tqk1do0b3p6c.png" alt="image_1arfr1qrfri21ds01tqk1do0b3p6c.png-25.6kB" title=""></p></li><li><p>进行到这一步，再使用reduceByKey()就可完成wordcount了，reduceByKey中的ByKey使数组中的元组对按key进行排序，reduce进行相加。 <br>即： <br><code style="font-size:14px;line-height:22px;">rdd.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word,1)).reduceByKey((a,b) =&gt; (a + b))</code> <br><img src="http://static.zybuluo.com/vin123456/up0q4d17cr16g8idrzhy5aac/image_1arfr4hh81dka15t5u8uq4r1f2p6p.png" alt="image_1arfr4hh81dka15t5u8uq4r1f2p6p.png-37.1kB" title=""></p></li><li><p>上一步即完成了对数据的处理，再对其赋值之后保存，即可完成wordcount <br>赋值： <br><code style="font-size:14px;line-height:22px;">val wordcountRDD = rdd.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word,1)).reduceByKey((a,b) =&gt; (a + b))</code> <br><img src="http://static.zybuluo.com/vin123456/m0cvcdq6neg6kyerj830c2jm/image_1arfr5u91o24g2f176s181p16d476.png" alt="image_1arfr5u91o24g2f176s181p16d476.png-44.3kB" title=""></p></li><li><p>保存： <code style="font-size:14px;line-height:22px;">wordcountRDD.saveAsTextFile("/user/vin/wordcount/output")</code> <br><img src="http://static.zybuluo.com/vin123456/6pijc0y65msrr28tskgptj1w/image_1arfr6v7o1q5uutr1kmg15rahcb7j.png" alt="image_1arfr6v7o1q5uutr1kmg15rahcb7j.png-15.9kB" title=""></p></li></ul></li></ul><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark集群" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t4"></a>Spark集群</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Spark Cluster 可以运行在Yarn上，由Yarn进行资源管理和任务调度，还可以运行在其自带的资源管理调度框架Standalone上，分为主节点Master（类似于yarn的resourcemanager）和从节点Work（类似于yarn的nodemanager），不同的是一台机器上可以运行多个Work。 <br>Spark架构原理图： <br><img src="http://static.zybuluo.com/vin123456/51t0qnucnka6zir7niq1fc9g/image_1arg10ugj1lc09nr174n17cqhde80.png" alt="image_1arg10ugj1lc09nr174n17cqhde80.png-109.7kB" title=""><br>说明：Job：包含了由Spark Action催生的多个Tasks，是一个并行计算 <br>Stage：一个Job分为了多个彼此依赖的Stage，每个Stage又包含了一系列的Task，类似于MapReduce的Map阶段和reduce阶段。 <br>- Spark集群安装部署 <br>- 配置$SPARK_HOME目录下conf下的配置文件 <br>1 配置spark-env.sh <br>参考官网：<a href="http://spark.apache.org/docs/1.6.1/spark-standalone.html#installing-spark-standalone-to-a-cluster" rel="nofollow">http://spark.apache.org/docs/1.6.1/spark-standalone.html#installing-spark-standalone-to-a-cluster</a> <br>2 配置slaves <br>配置运行Work的主机名 <br>3 启动 <br>在sbin目录里使用 <br><code style="font-size:14px;line-height:22px;">start-master.sh <br>start-slaves.sh</code> //启动所有的从节点，使用此命令时，运行此命令的机器，必须要配置与其他机器的SSH无密钥登录，否则启动的时候会出现一些问题 <br><img src="http://static.zybuluo.com/vin123456/cig8fw5gcwuu1po3cnmk0tq3/image_1arg1qjvueat11c54qs17i317ka8d.png" alt="image_1arg1qjvueat11c54qs17i317ka8d.png-25.1kB" title=""> <br>4 Spark的的web监控端口为8080，URL为7070，Job监控4040，都是自动增长 <br><img src="http://static.zybuluo.com/vin123456/do0tiiz90mz4u5tc2abd5u8x/image_1arg1tn8o1fs81rceoh21vj42358q.png" alt="image_1arg1tn8o1fs81rceoh21vj42358q.png-58.3kB" title=""> <br>5 测试Spark集群 <br>spark-shell是spark的一个application，将其运行在spark standalone上，通过输入： bin/spark-shell –help 查看其运行方法 <br><img src="http://static.zybuluo.com/vin123456/smigey1dfdff6rlrl5g8sx4n/image_1arg22ohv6l61ql41u1p1hbod2897.png" alt="image_1arg22ohv6l61ql41u1p1hbod2897.png-35.5kB" title=""> <br>启动： bin/spark-shell –master spark://vin01:7077 <br><img src="http://static.zybuluo.com/vin123456/ln7xfxzzglrskf23rdqdm68z/image_1arg243mjhgqjth1dqm19qdqfk9k.png" alt="image_1arg243mjhgqjth1dqm19qdqfk9k.png-34kB" title=""></p><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-application开发运行及监控idea" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t5"></a>Spark Application开发、运行及监控（IDEA）</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>在IDEA中创建scala Project，并添加spark依赖包 <br>步骤：File -&gt; Project Structure -&gt; Libraries -&gt; +号 -&gt; java -&gt; 选择编译好的spark目录下的lib依赖包 <br><img src="http://static.zybuluo.com/vin123456/89lewizg4cp1j91wp9atas3g/image_1arhk7l25c38kkn1l1idl21vvsb8.png" alt="image_1arhk7l25c38kkn1l1idl21vvsb8.png-83.5kB" title=""></li><li>导入依赖包之后即可进行程序开发，新建包、在包中创建Scala class之SparkApp</li><li>编程</li><li><p>配置resurces（在新建scala Project时，创建resources） <br>由于程序中需要读hdfs上的数据文件，所以需要将hadoop的配置文件hdfs-site.xml 与core-site.xml 文件拷贝到scala project的resources中 <br><img src="http://static.zybuluo.com/vin123456/b5w1njyltpoxw22qq5mv0st5/image_1arhkvlcm1fic1reg12h813ktmroc2.png" alt="image_1arhkvlcm1fic1reg12h813ktmroc2.png-81.8kB" title=""></p></li><li><p>本地运行 <br>使用Idea工具可以直接运行在本地模式，无需插件 <br>运行查看输出： <br><img src="http://static.zybuluo.com/vin123456/l4fosbrzdvrtf5pqu66mm1or/image_1arhl0reh2pb1fqt1sbleapf5ccf.png" alt="image_1arhl0reh2pb1fqt1sbleapf5ccf.png-61.5kB" title=""></p></li><li><p>打包在Spark shell上提交运行（bin/spark-submit …） <br>步骤：</p><blockquote style="border-left:8px solid rgb(221,223,228);background:rgb(238,240,244);"><blockquote style="border-left:8px solid rgb(221,223,228);"><p style="font-size:14px;color:rgb(153,153,153);line-height:22px;">1 打包 <br>File -&gt; project structure -&gt; Artifacts -&gt; + -&gt; jar <br><img src="http://static.zybuluo.com/vin123456/zogfvb7c9n1pbh8bp9crzajb/image_1arhl4fer95oje1e881hmktm8cs.png" alt="image_1arhl4fer95oje1e881hmktm8cs.png-76.9kB" title=""><br>2 选择类 <br><img src="http://static.zybuluo.com/vin123456/q5lp8xuca86ikzzk3ah8hwyg/image_1arhl4v5l14a8gsd1f4d1qsohctd9.png" alt="image_1arhl4v5l14a8gsd1f4d1qsohctd9.png-48.6kB" title=""> <br>3 去除依赖包（因为集群上本身有） <br><img src="http://static.zybuluo.com/vin123456/lmbdu1wnh08ckrqudiqzm9ju/image_1arhl5br4ervkue1bafrrd97idm.png" alt="image_1arhl5br4ervkue1bafrrd97idm.png-82.8kB" title=""> <br>4 上述步骤设置了打哪个包，还需要build进行打包 <br><img src="http://static.zybuluo.com/vin123456/w4uxb0ikqoa285if428p8nde/image_1arhl678a1l371p5isapi991h6ce3.png" alt="image_1arhl678a1l371p5isapi991h6ce3.png-28.3kB" title=""> <br>5 将jar包上传至linux下并赋予执行权限：此处为方便上传到Spark主目录下 <br><img src="http://static.zybuluo.com/vin123456/7xbm3dtndavsld848ys2leon/image_1arhl73581prnlls1r3l1tkf14ndeg.png" alt="image_1arhl73581prnlls1r3l1tkf14ndeg.png-33.7kB" title=""><br>6 提交任务 <br><img src="http://static.zybuluo.com/vin123456/ivmz09ye01ssl5ubf741c2tk/image_1arhl85lpl9dai01at1hsm1hvcet.png" alt="image_1arhl85lpl9dai01at1hsm1hvcet.png-21.9kB" title=""><br>7 先测试本地 <br><img src="http://static.zybuluo.com/vin123456/e0e6d9atxes1kpcjebrngof5/image_1arhl9f5r1o7v1kr41cabc6f2h7fa.png" alt="image_1arhl9f5r1o7v1kr41cabc6f2h7fa.png-51.1kB" title=""><br>8 测试集群：此时需要将代码中的master注释掉再重新打包，重新打包直接用rebuild即可 <br><img src="http://static.zybuluo.com/vin123456/d8r504j1h5jgv4ezjhdma9kf/image_1arhlab9919gn1e7l1qr41ti71gthfn.png" alt="image_1arhlab9919gn1e7l1qr41ti71gthfn.png-69.4kB" title=""><br>9 启动spark Standalone <br>查看8080端口是否有资源 <br>提交任务 <br><code style="font-size:14px;line-height:22px;">bin/spark-sumit --master spark://vin01:7077 scalaProject.jar</code> <br><img src="http://static.zybuluo.com/vin123456/od300xy9xxo26kcmalm2qfpv/image_1arhlb2f71ug718ho13q69p212jkg4.png" alt="image_1arhlb2f71ug718ho13q69p212jkg4.png-32.7kB" title=""><br>10 监控 <br><img src="http://static.zybuluo.com/vin123456/xj13ler9brrgmqy507cjugqw/image_1arhlbfofetu1aja1339efv4l1gh.png" alt="image_1arhlbfofetu1aja1339efv4l1gh.png-47.9kB" title=""></p></blockquote></blockquote></li></ul><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-日志监控historyserver配置" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t6"></a>Spark 日志监控（HistoryServer）配置</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Spark HistoryServer配置分为两个部分：</p><blockquote style="border-left:8px solid rgb(221,223,228);background:rgb(238,240,244);color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;"><blockquote style="border-left:8px solid rgb(221,223,228);"><p style="font-size:14px;color:rgb(153,153,153);line-height:22px;">第一、设置SparkApplicaiton在运行时，需要记录日志信息 <br>配置：配置$SPARK_HOME目录下conf下spark-defaults.conf文件 <br><img src="http://static.zybuluo.com/vin123456/1rv3pzckizhmsn3pmn4f353o/image_1arhja89fq9a1kqkabk1gfrjdea1.png" alt="image_1arhja89fq9a1kqkabk1gfrjdea1.png-23.7kB" title=""><br>第二、启动HistoryServer，通过界面查看 <br>配置Spark主目录下conf下spark-env.sh文件 <br><code style="font-size:14px;line-height:22px;">SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://vin01/user/vin/sparkEventLogs"</code> <br>配置完成启动服务 <br><img src="http://static.zybuluo.com/vin123456/c5vq0uaxtkr04fkgn432to3g/image_1arhjcvbo1lou192m18kgpomv1cae.png" alt="image_1arhjcvbo1lou192m18kgpomv1cae.png-95.2kB" title=""> <br>它端口号是18080 <br><img src="http://static.zybuluo.com/vin123456/0zi98mbaazsk5zpofji1nemv/image_1arhjh0ub1cj8a1mf291q3dupar.png" alt="image_1arhjh0ub1cj8a1mf291q3dupar.png-51.1kB" title=""></p></blockquote></blockquote><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-rdd" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t7"></a>Spark RDD</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><ol style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li><p>RDD是什么 <br>官方解释： <br>RDD是Spark的基本抽象，是一个弹性分布式数据集，代表着不可变的，分区（partition）的集合，能够进行并行计算。也即是说：</p><ul style="list-style:none;"><li>它是一系列的分片、比如说128M一片，类似于Hadoop的split；</li><li>在每个分片上都有一个函数去执行/迭代/计算它</li><li>它也是一系列的依赖，比如RDD1转换为RDD2，RDD2转换为RDD3，那么RDD2依赖于RDD1，RDD3依赖于RDD2。</li><li>对于一个Key-Value形式的RDD，可以指定一个partitioner，告诉它如何分片，常用的有hash、range</li><li>可选择指定分区最佳计算位置</li></ul></li><li><p>创建RDD的两种方式</p><ul style="list-style:none;"><li>方式一： <br>将集合进行并行化操作 <br>List\Seq\Array <br>演示： <br><img src="http://static.zybuluo.com/vin123456/q9lsaama4cl1ilhm0c1h0saa/image_1arhmhtoq4pa1k5717bk1ntc15d6gu.png" alt="image_1arhmhtoq4pa1k5717bk1ntc15d6gu.png-38.8kB" title=""></li><li>方式二： <br>外部存储系统 <br>HDFS, HBase, or any data source offering a Hadoop InputFormat. <br><img src="http://static.zybuluo.com/vin123456/7m72cp7any2rp83814u2l6u1/image_1arhmrv6k1tkfp9h9731r11eukhb.png" alt="image_1arhmrv6k1tkfp9h9731r11eukhb.png-117.9kB" title=""></li></ul></li><li>RDD的三大Operations <br><ul style="list-style:none;"><li>Transformation <br>从原有的一个RDD进行操作创建一个新的RDD，通常是一个lazy过程，例如map（func） 、filter（func），直到有Action算子执行的时候</li><li>Action <br>返回给驱动program一个值，或者将计算出来的结果集导出到存储系统中，例如count（） reduce（func）</li><li>Persist <br>将数据存储在内存中，或者存储在硬盘中 <br>例如： cache（） persist（） unpersist（） <br>合理使用persist()和cache()持久化操作能大大提高spark性能，但是其调用是有原则的，必须在transformation或者textFile后面直接调用persist()或cache()，如果先创建的RDD，然后再起一行调用这两个方法，则会报错</li></ul></li><li>RDD的常用Transformation <br>– map(func) :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成 <br>spark shell本地测试：</li></ol><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="language-scala hljs has-numbering">    <span class="hljs-keyword">val</span> numbers = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)
    <span class="hljs-keyword">val</span> numberRDD = sc.parallelize(numbers, <span class="hljs-number">1</span>)  
    <span class="hljs-keyword">val</span> multipleNumberRDD = numberRDD.map ( num =&gt; num * <span class="hljs-number">2</span> )
    multipleNumberRDD.foreach ( num =&gt; println(num) ) </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/mrz0qr9u3rica6hzo6lt5q44/image_1arhvvo24sjq5921hk81ile1ob19.png" alt="image_1arhvvo24sjq5921hk81ile1ob19.png-29.7kB" title=""></p><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">– filter(func) : 返回一个新的数据集，由经过func函数后返回值为true的原元素组成</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs livecodeserver has-numbering">    val numbers = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
    val numberRDD = sc.parallelize(numbers, <span class="hljs-number">1</span>)
    val evenNumberRDD = numberRDD.<span class="hljs-built_in">filter</span> { <span class="hljs-built_in">num</span> =&gt; <span class="hljs-built_in">num</span> % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> }
    evenNumberRDD.foreach { <span class="hljs-built_in">num</span> =&gt; println(<span class="hljs-built_in">num</span>) }  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/ikbuws1xkbi6vdhuf74j9xfw/image_1ari04fp91q8sdfb1b8r66tj4mm.png" alt="image_1ari04fp91q8sdfb1b8r66tj4mm.png-22.4kB" title=""></p><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">– flatMap(func) : 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs livecodeserver has-numbering">    val lineArray = Array(<span class="hljs-string">"hello you"</span>, <span class="hljs-string">"hello me"</span>, <span class="hljs-string">"hello world"</span>)  
    val <span class="hljs-keyword">lines</span> = sc.parallelize(lineArray, <span class="hljs-number">1</span>)
    val <span class="hljs-keyword">words</span> = <span class="hljs-keyword">lines</span>.flatMap { <span class="hljs-built_in">line</span> =&gt; <span class="hljs-built_in">line</span>.<span class="hljs-built_in">split</span>(<span class="hljs-string">" "</span>) }   
    <span class="hljs-keyword">words</span>.foreach { <span class="hljs-built_in">word</span> =&gt; println(<span class="hljs-built_in">word</span>) }</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/uqmf8vs8dno2bu1crrjeo3ry/image_1ari0mqic1qmq1qvl1q2f1sq91dl13.png" alt="image_1ari0mqic1qmq1qvl1q2f1sq91dl13.png-24.3kB" title=""> <br>– union(otherDataset) : 返回一个新的数据集，由原数据集和参数联合而成 <br>– groupByKey([numTasks]) :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs php has-numbering">    val scoreList = <span class="hljs-keyword">Array</span>(Tuple2(<span class="hljs-string">"class1"</span>, <span class="hljs-number">80</span>), Tuple2(<span class="hljs-string">"class2"</span>, <span class="hljs-number">75</span>),Tuple2(<span class="hljs-string">"class1"</span>, <span class="hljs-number">90</span>), Tuple2(<span class="hljs-string">"class2"</span>, <span class="hljs-number">60</span>))
    val scores = sc.parallelize(scoreList, <span class="hljs-number">1</span>)  
    val groupedScores = scores.groupByKey() 
     groupedScores.<span class="hljs-keyword">foreach</span>(score =&gt; { 
      println(score._1); 
      score._2.<span class="hljs-keyword">foreach</span> { singleScore =&gt; println(singleScore) };
      println(<span class="hljs-string">"============================="</span>)  })</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/23xwqkvss02ofslckj0mngxb/image_1ari0pnrr31b8pkgn141f1p661g.png" alt="image_1ari0pnrr31b8pkgn141f1p661g.png-32.7kB" title=""> <br>– reduceByKey(func, [numTasks]) : 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。在实际开发中，能使reduceByKey实现的就不用groupByKey</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering"><span class="hljs-keyword">val</span> scoreList = Array(Tuple2(<span class="hljs-string">"class1"</span>, <span class="hljs-number">80</span>), Tuple2(<span class="hljs-string">"class2"</span>, <span class="hljs-number">75</span>),Tuple2(<span class="hljs-string">"class1"</span>, <span class="hljs-number">90</span>), Tuple2(<span class="hljs-string">"class2"</span>, <span class="hljs-number">60</span>))
    <span class="hljs-keyword">val</span> scores = sc.parallelize(scoreList, <span class="hljs-number">1</span>)  
    <span class="hljs-keyword">val</span> totalScores = scores.reduceByKey(_ + _)  
    totalScores.foreach(classScore =&gt; println(classScore._1 + <span class="hljs-string">": "</span> + classScore._2))  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/l908yujj8qpgpgboebcnlmx0/image_1ari0vb9ie2f1sb7ehc13tvl411t.png" alt="image_1ari0vb9ie2f1sb7ehc13tvl411t.png-34.3kB" title=""> <br>– join(otherDataset, [numTasks]) :在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering">    <span class="hljs-keyword">val</span> studentList = Array(
        Tuple2(<span class="hljs-number">1</span>, <span class="hljs-string">"leo"</span>),
        Tuple2(<span class="hljs-number">2</span>, <span class="hljs-string">"jack"</span>),
        Tuple2(<span class="hljs-number">3</span>, <span class="hljs-string">"tom"</span>));
   <span class="hljs-keyword">val</span> scoreList = Array(
        Tuple2(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>),
        Tuple2(<span class="hljs-number">2</span>, <span class="hljs-number">90</span>),
        Tuple2(<span class="hljs-number">3</span>, <span class="hljs-number">60</span>));
    <span class="hljs-keyword">val</span> students = sc.parallelize(studentList);
    <span class="hljs-keyword">val</span> scores = sc.parallelize(scoreList);
    <span class="hljs-keyword">val</span> studentScores = students.join(scores)  
    studentScores.foreach(studentScore =&gt; { 
      println(<span class="hljs-string">"student id: "</span> + studentScore._1);
      println(<span class="hljs-string">"student name: "</span> + studentScore._2._1)
      println(<span class="hljs-string">"student socre: "</span> + studentScore._2._2)  
      println(<span class="hljs-string">"======================================="</span>)  
    })  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/5v7wdvubqgxvsyd5d9w9qpvq/image_1ari12cb8122f11tv1ufv14ao1i4n2a.png" alt="image_1ari12cb8122f11tv1ufv14ao1i4n2a.png-76.8kB" title=""> <br>– groupWith(otherDataset, [numTasks]) : 在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup <br>– cartesian(otherDataset) : 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。 <br>– repartition（）：重新分区，当数据处理到最后剩下很少的数据集时，可以使用repartition（）进行重新分区</p><ol style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li><p>常用Action</p><p>– reduce(func) : 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行</p></li></ol><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering">    <span class="hljs-keyword">val</span> numberArray = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
    <span class="hljs-keyword">val</span> numbers = sc.parallelize(numberArray, <span class="hljs-number">1</span>)  
    <span class="hljs-keyword">val</span> sum = numbers.reduce(_ + _)  
    println(sum) </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">–collect() : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用。</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering">   <span class="hljs-keyword">val</span> numberArray = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
    <span class="hljs-keyword">val</span> numbers = sc.parallelize(numberArray, <span class="hljs-number">1</span>)  
    <span class="hljs-keyword">val</span> doubleNumbers = numbers.map { num =&gt; num * <span class="hljs-number">2</span> }  
    <span class="hljs-keyword">val</span> doubleNumberArray = doubleNumbers.collect()
    <span class="hljs-keyword">for</span>(num &lt;- doubleNumberArray) {
      println(num)  
    }</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">–count() : 返回数据集的元素个数</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering">    <span class="hljs-keyword">val</span> numberArray = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
    <span class="hljs-keyword">val</span> numbers = sc.parallelize(numberArray, <span class="hljs-number">1</span>)  
    <span class="hljs-keyword">val</span> count = numbers.count()
    println(count)  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">–take(n) : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering">    <span class="hljs-keyword">val</span> numberArray = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)
    <span class="hljs-keyword">val</span> numbers = sc.parallelize(numberArray, <span class="hljs-number">1</span>)  
    <span class="hljs-keyword">val</span> top3Numbers = numbers.take(<span class="hljs-number">3</span>)
    <span class="hljs-keyword">for</span>(num &lt;- top3Numbers) {
      println(num)  
    }</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">–first() : 返回数据集的第一个元素（类似于take(1)） <br>–saveAsTextFile(path) : 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本 <br>–saveAsSequenceFile(path) : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等） <br>–foreach(func) : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互</p><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark共享变量" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t8"></a>Spark共享变量</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>共享变量概念</li></ul><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">共享变量是Spark一个非常重要的特性，在默认情况下，如果一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中，此时每个task只能操作自己的那份变量副本。如果多个task想要共享某个变量，这种方式是做不到的。 <br>Spark为此提供了两种共享变量，一种是Broadcast Variable（广播变量），一种是Accumulator（累加变量）。广播变量会将使用到的变量，仅仅为每个节点拷贝一份，更大的用处是优化性能，减少网络传输以及内存的消耗。累加变量则可以然多个task共同操作一份变量，主要可以进行累加。</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>共享变量示例 <br>Spark提供的Broadcast Variable是只读的，可以通过调用SparkContext的broadcast()方法来针对某个变量创建广播变量。每个节点可以使用广播变量的value()方法来获取值。</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs ocaml has-numbering">   <span class="hljs-keyword">val</span> factor = <span class="hljs-number">3</span>
   <span class="hljs-keyword">val</span> factorBroadcast = sc.broadcast(factor)

   <span class="hljs-keyword">val</span> arr =Array(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)
   <span class="hljs-keyword">val</span> rdd = sc.parallelize(arr)
   <span class="hljs-keyword">val</span> mutiRdd = rdd.map(num =&gt; num*factorBroadcast.<span class="hljs-keyword">value</span>())
   mutiRdd.foreach(num =&gt; println(num))</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Spark提供的Accumulator，主要用于多个节点对一个变量进行共享的操作，task只能对Accumulator进行累加操作，不能读取它的值，只有Driver程序可以读取。</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs ocaml has-numbering">    <span class="hljs-keyword">val</span> sumAccumulator = sc.accumulator(<span class="hljs-number">0</span>)

    <span class="hljs-keyword">val</span> arr = Array(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)
    <span class="hljs-keyword">val</span> rdd = sc.parallelize(arr)
    rdd.foreach(num =&gt; sumAccumulator +=num)
    println(sumAccumulator.<span class="hljs-keyword">value</span>)</code></pre><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-内核" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t9"></a>Spark 内核</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h3 id="spark-依赖" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t10"></a>Spark 依赖</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Spark依赖分为宽依赖和窄依赖：</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>窄依赖</li><li>子RDD的每个分区依赖于常数个（即与数据规模无关）</li><li>输入输出一对一的算子，且结果RDD的分区结构不变，主要是map、flatMap</li><li>输入输出一对一，但结果RDD的分区结构发生了变化，如 union、coalesce</li><li><p>从输入中选择部分元素的算子，如filter、distinct、subtract、sample</p></li><li><p>宽依赖</p></li><li><p>子RDD的每个分区依赖于所有父RDD分区</p></li><li>对单个RDD基于key进行重组和reduce，如 groupByKey、reduceByKey‘</li><li>对两个RDD基于key进行join和重组，如join <br>区分宽依赖和窄依赖是根据父RDD的每个分区数据给多少个子RDD的每个分区数据： <br>1 -&gt; 1 ：窄依赖 <br>1 -&gt; N ：宽依赖，有宽依赖的地方必有shuffle <br><img src="http://static.zybuluo.com/vin123456/4irs9x3an1i288vnbfno8ed8/image_1arlj20jh5ul164g1t3dh9sqo79.png" alt="image_1arlj20jh5ul164g1t3dh9sqo79.png-315.5kB" title=""></li></ul><h3 id="spark-shuffle" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t11"></a>Spark Shuffle</h3><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>Shuffle过程的解析 <br>在Spark RDD的计算处理的过程中，每个宽依赖都伴随着Shuffle。 <br>首先看Shuffle过程 <br><img src="http://static.zybuluo.com/vin123456/1vo5e93k8w5x1q9zfzfowgb6/image_1arljq7cmo1663u54os0d1ea0m.png" alt="image_1arljq7cmo1663u54os0d1ea0m.png-185.5kB" title=""><br>依图所示： <br>假设有一个节点，上面运行了3个shuffleMapTask，每个shuffleMapTask，都会为每个ReduceTask创建一份bucket缓存以及对应的ShuffleBlockFile磁盘文件，shuffleMapTask的输出，会作为MapStatus，发送到DAGScheduler的MapOutputTrackerMaster中，每个MapStatus包含了每个ReduceTask要拉取的数据的大小。 <br>假设有另外一个节点，上面也运行了4个ReduceTask，现在等着去获取ShuffleMapTask的输出数据，来完成程序定义的算子，而ReduceTask会用BlockStoreShuffleFetcher去MapOutputTrackerMaster获取自己要拉取的文件的信息，然后底层通过BlockManager将数据拉取过来。每个ReduceTask拉取过来的数据，其实就会组成一个内部的RDD，叫shuffleRDD，优先放入内存，其次如果内存不够，那么写入磁盘，最后每个ReduceTask针对数据进行聚合，最后生成MapPartitionRDD，就是执行reduceByKey等操作希望获得的RDD。</li></ul><h3 id="spark-application添加jar包的三种方法" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t12"></a>Spark Application添加jar包的三种方法</h3><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>–jars <br>在bin/spark-submit 后面直接以–jars方式将jar包添加，须写绝对路径 <br>示例： <br><code style="font-size:14px;line-height:22px;">${SPARK_HOME}/bin/spark-submit --master /opt/jars/sparkexternale/xx1.jar, /opt/jars/sparkexternale/xx2.jar</code></li><li>–driver-class-path <br>示例：</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="language-shell hljs javascript has-numbering">bin/spark-shell \
--master local[<span class="hljs-number">3</span>] \
--driver-<span class="hljs-keyword">class</span>-path /opt/modules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/mysql-connector-java-<span class="hljs-number">5.1</span><span class="hljs-number">.27</span>-bin.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase-client-<span class="hljs-number">0.98</span><span class="hljs-number">.6</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase-common-<span class="hljs-number">0.98</span><span class="hljs-number">.6</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase-protocol-<span class="hljs-number">0.98</span><span class="hljs-number">.6</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase-server-<span class="hljs-number">0.98</span><span class="hljs-number">.6</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/htrace-core-<span class="hljs-number">2.04</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/protobuf-java-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/guava-<span class="hljs-number">12.0</span><span class="hljs-number">.1</span>.jar:<span class="hljs-regexp">/opt/m</span>odules/spark-<span class="hljs-number">1.6</span><span class="hljs-number">.1</span>-bin-<span class="hljs-number">2.5</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hive-hbase-handler-<span class="hljs-number">0.13</span><span class="hljs-number">.1</span>-cdh5<span class="hljs-number">.3</span><span class="hljs-number">.6</span>.jar</code></pre><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>SPARK_CLASSPATH <br>配置此环境变量：通常在企业中，提交Application使用脚本的方式，比如spark-app-submit.sh脚本，通常一个App设置一个脚本，即设置一个SPARK CLASSPATH <br>//spark-app-submit.sh:</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs ruby has-numbering"><span class="hljs-comment">#!/bin/sh</span>
<span class="hljs-comment">## SPARK_HOME</span>
<span class="hljs-constant">SPARK_HOME</span>=<span class="hljs-regexp">/opt/cdh</span>5.<span class="hljs-number">3.6</span>/spark-<span class="hljs-number">1.6</span>.<span class="hljs-number">1</span>-bin-<span class="hljs-number">2.5</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">3.6</span>
<span class="hljs-comment">## SPARK CLASSPATH</span>
<span class="hljs-constant">SPARK_CLASSPATH</span>=<span class="hljs-variable">$SPARK_CLASSPATH</span><span class="hljs-symbol">:/opt/jars/sparkexternale/xx</span>.<span class="hljs-symbol">jar:</span>/opt/jars/sparkexternale/yy.jar

<span class="hljs-variable">${</span><span class="hljs-constant">SPARK_HOME</span>}/bin/spark-submit --master <span class="hljs-symbol">spark:</span>/<span class="hljs-regexp">/vin01:7077 --deploy-mode cluster /opt</span><span class="hljs-regexp">/tools/scala</span>Project.jar</code></pre><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-sql" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t13"></a>Spark SQL</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h3 id="spark-sql的发展历程" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t14"></a>Spark SQL的发展历程</h3><blockquote style="border-left:8px solid rgb(221,223,228);background:rgb(238,240,244);color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;"><blockquote style="border-left:8px solid rgb(221,223,228);"><p style="font-size:14px;color:rgb(153,153,153);line-height:22px;">Spark 作为一个优秀的大数据计算框架，必然少不了支持SQL的框架，基于Hive的性能以及它与Spark的兼容，Shark项目由此诞生，Shark即Hive on Spark，它通过HQL解析，将HQL翻译成Spark上的RDD操作，然后通过Hive的metadata获取数据库里的表信息，实际的HDFS上的数据和文件会有Spark获取并放到Spark上计算。 <br>在Hive中，处理SQl的过程如下： <br>SQL –&gt; 语法解析–&gt;逻辑计划（优化）–&gt;物理计划–&gt;MapReduce <br>而Shark的诞生，是将某个Hive版本源码拿过来进行修改“物理计划”的部分，将其转化为Spark而不是MapReduce，这有很大的弊端，比如依赖于Hive。 <br>Shark的SQL处理过程如下： <br>SQL –&gt; 语法解析–&gt;逻辑计划（优化）–&gt;物理计划–&gt;Spark <br>在1.0之后SparkSQL诞生，它涵盖了Shark的所有特性，SparkSQL不再使用Hive的解析引擎，即不再与Hive共用语法解析和逻辑计划，它有了自己的解析引擎Catalyst。 <br><img src="http://static.zybuluo.com/vin123456/f60chgeno8dqoo9l7u6p5fvt/image_1as240e6u15hlqn3a6t6iu4t29.png" alt="image_1as240e6u15hlqn3a6t6iu4t29.png-345.5kB" title=""> <br>spark SQL的三大愿景就是：① Write less code ② Read less data ③ Let the optimizer do the hard work</p></blockquote></blockquote><h3 id="dataframe" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t15"></a>DataFrame</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二位表格，它与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二位数据集的每一列都带有名称和类型。使得SparkSQL得以洞察更多的结构信息，从而对其背后的额数据源以及作为用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标，反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单通用的流水线优化 <br><img src="http://static.zybuluo.com/vin123456/82h4k93f5cz2mdxksou9tzg2/image_1as2e570ag001vab1c23l3p14jrm.png" alt="image_1as2e570ag001vab1c23l3p14jrm.png-158.5kB" title=""></p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>DataFrame VS RDD 及DataFrame的创建 <br>在Spark安装目录下有示例数据，将其上传到HDFS上。 <br><img src="http://static.zybuluo.com/vin123456/qq1cszl1dliy0y6r8g1xu0tf/image_1as2enk54fn3tr85ja16k4a313.png" alt="image_1as2enk54fn3tr85ja16k4a313.png-55.1kB" title=""> <br>创建RDD，首选创建一个类来封装rdd <br><img src="http://static.zybuluo.com/vin123456/83ls70l8ifb299v7uk796293/image_1as2eons215h91tgup5111n4qd71g.png" alt="image_1as2eons215h91tgup5111n4qd71g.png-15.3kB" title=""> <br>再创建rdd读取该数据文件，并处理之后传递给People类，这样就得到了一个存储类的rdd <br><img src="http://static.zybuluo.com/vin123456/dnkb438lv8n0jxtenguapwtm/image_1as2epadgjnk1bh5jgc8hc671t.png" alt="image_1as2epadgjnk1bh5jgc8hc671t.png-112.6kB" title=""> <br>得到的结果是： <br><img src="http://static.zybuluo.com/vin123456/mo4vg7dqumcnn2qf4wf49bn5/image_1as2epn8c6qk1sfpn7f1kdh1o0c2a.png" alt="image_1as2epn8c6qk1sfpn7f1kdh1o0c2a.png-17.9kB" title=""> <br>从结果可以看出，创建出来的RDD只知道它存储的是一个类，具体类的参数名称的信息都不清楚。 <br>而如果创建DataFrame呢 <br>使用json格式数据创建DataFrame，（DataFrame中的read方法可以直接读取json格式的数据 <br><img src="http://static.zybuluo.com/vin123456/mx2xpef0uvr1qkcpf59gh6qc/image_1as2eugvu1ld2v4c1nec4832o2n.png" alt="image_1as2eugvu1ld2v4c1nec4832o2n.png-26.2kB" title=""> <br>创建DataFrame的入口是sqlContext或者HiveContext <br><img src="http://static.zybuluo.com/vin123456/l9pkh58nehuxzhutbfvx4kek/image_1as2evfpdki4higb811thj3lo34.png" alt="image_1as2evfpdki4higb811thj3lo34.png-27.8kB" title=""> <br>得到的结果是： <br><img src="http://static.zybuluo.com/vin123456/hqn3wud0qr6n7pvzrg2g071k/image_1as2evuvo1pu712d3f1srt5rf53h.png" alt="image_1as2evuvo1pu712d3f1srt5rf53h.png-21.2kB" title=""> <br>从创建的DataFrame可以得到数据的字段名，数据类型等。 <br>DataFrame的创建 <br><ol style="list-style:none;"><li>通过内置方法读取外部数据源，数据源可以是以下格式： <br>json hive jdbc parquet/orc text</li><li>通过scala的CASE CLASS转换 <br>首先创建一个case class，名为Employee，并且定义id和name两个参数 <br><code style="font-size:14px;line-height:22px;">case class Employee(id: Int, name: String)</code> <br>我们可以通过很多方式来初始化Employee类，比如从关系型数据库中获取数据以此来定义Employee类。但是在本文为了简单起见，我将直接定义一个Employee类的List，如下： <br><code style="font-size:14px;line-height:22px;">val listOfEmployees = List(Employee(1, "iteblog"), Employee(2, "Jason"), Employee(3, "Abhi"))</code> <br>我们将listOfEmployees列表传递给SQLContext类的createDataFrame 函数，这样我们就可以创建出DataFrame了！然后我们可以调用DataFrame的printuSchema函数，打印出该DataFrame的模式，我们可以看出这个DataFrame主要有两列：name和id，这正是我们定义Employee的两个参数，并且类型都一致。</li></ol></li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs vbnet has-numbering">val empFrame = sqlContext.createDataFrame(listOfEmployees)  
empFrame.printSchema
|-- id: <span class="hljs-built_in">integer</span> (nullable = <span class="hljs-literal">false</span>)  
|-- name: <span class="hljs-built_in">string</span> (nullable = <span class="hljs-literal">true</span>)  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">然后可以使用Spark支持的SQL功能来查询相关的数据。在使用这个功能之前，我们必须先对DataFrame注册成一张临时表，我们可以使用registerTempTable函数实现，如下： <br><code style="font-size:14px;line-height:22px;">empFrame.registerTempTable("employeeTable")</code> <br>注册为临时表就可以使用SQL语句来进行查询等操作了</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs smalltalk has-numbering">val sortedByNameEmployees = sqlContext.sql(<span class="hljs-comment">"select * from employeeTable order by name desc"</span>)  
sortedByNameEmployees.show()  
+-----+-------+  
<span class="hljs-localvars">|empId|   name|</span>  
+-----+-------+  
|    <span class="hljs-number">1</span><span class="hljs-localvars">|iteblog|</span>  
|    <span class="hljs-number">2</span>|  <span class="hljs-class">Jason</span>|  
|    <span class="hljs-number">3</span>|   <span class="hljs-class">Abhi</span>|  
+-----+-------+  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">转换原理：createDataFrame函数可以接收一切继承scala.Product类的集合对象 <br>查看Spark的DataFrame源码中createDataFrame函数 <br><code style="font-size:14px;line-height:22px;">def createDataFrame[A &lt;: Product : TypeTag](rdd: RDD[A]): DataFrame</code> <br>而case class类就是继承了Product。我们所熟悉的TupleN类型也是继承了scala.Product类的，所以我们也可以通过TupleN来创建DataFrame <br>- 通过RDD进行转换</p><blockquote style="border-left:8px solid rgb(221,223,228);background:rgb(238,240,244);color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;"><blockquote style="border-left:8px solid rgb(221,223,228);"><p style="font-size:14px;color:rgb(153,153,153);line-height:22px;">方式一： <br>The first method uses reflection to infer the schema of an RDD that contains specific types of objects. <br>这一种方式是通过自动推断，将RDD反射为DataFrame，但是这个Rdd必须是case class的类型。而且必须使用import sqlContext.implicits._来引包。 <br>演示： <br>定义一个case class 类People <br><code style="font-size:14px;line-height:22px;">case class People(name: String, age: Int)</code> <br>创建一个People类的RDD <br><code style="font-size:14px;line-height:22px;">val rdd = sc.textFile("/user/vin/people.txt").map(line =&gt; line.split(",")).map(x =&gt; People(x(0), x(1).trim.toInt))</code> <br><img src="http://static.zybuluo.com/vin123456/pc67rb635a2mx5bm01i5wlb0/image_1as2hrf7e7oiu3h1tc71kchf273u.png" alt="image_1as2hrf7e7oiu3h1tc71kchf273u.png-13.6kB" title=""><br>转换： <br><code style="font-size:14px;line-height:22px;">val people_df = rdd.toDF()</code> <br><img src="http://static.zybuluo.com/vin123456/fxtawtprnssi1lnxnaanlejj/image_1as2hs7sk53h1ifo17he2e2q714b.png" alt="image_1as2hs7sk53h1ifo17he2e2q714b.png-12.7kB" title=""> <br><span>在IDEA编程中需要使用import sqlContext.implicits._，否则jar包出错。</span> <br>方式二： <br>The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. <br>a DataFrame can be created programmatically with three steps: <br>第一步：将RDD（可以是任何类型）转换为RDD[Row] <br>需要引包：<code style="font-size:14px;line-height:22px;">import org.apache.spark.sql._</code> <br>演示： <br><code style="font-size:14px;line-height:22px;">val rdd = sc.textFile("/user/vin/people.txt") <br>import org.apache.spark.sql._ <br>val rowRdd = rdd.map(line =&gt; line.split(", ")).map(x =&gt; Row(x(0), x(1).toInt))</code> <br><img src="http://static.zybuluo.com/vin123456/gohiw9uhetbh358aaainv6lz/image_1as2hukd34cj1qdei067acsjo4o.png" alt="image_1as2hukd34cj1qdei067acsjo4o.png-27.3kB" title=""> <br>第二步： <br>创建schema <br>引包：<code style="font-size:14px;line-height:22px;">import org.apache.spark.sql.types._</code> <br><code style="font-size:14px;line-height:22px;">val schema = StructType(StructField("name",StringType,true) :: StructField("age",IntegerType,true) :: Nil)</code> <br><img src="http://static.zybuluo.com/vin123456/k75jfgbi979p9vzyl3oz31po/image_1as2i007kpivi5d193v1gnuqj555.png" alt="image_1as2i007kpivi5d193v1gnuqj555.png-40.8kB" title=""> <br>第三步：Apply the schema to the RDD of Rows <br><code style="font-size:14px;line-height:22px;">val people_df = sqlContext.createDataFrame(rowRdd, schema)</code> <br><img src="http://static.zybuluo.com/vin123456/yhcybdv8k9qtd7lhwt9xug51/image_1as2i1cso5lssktctnoh91e255i.png" alt="image_1as2i1cso5lssktctnoh91e255i.png-23.5kB" title=""><br>这种方式类似scala 的case class类创建DataFrame</p></blockquote></blockquote><h3 id="sparksql案例" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t16"></a>SparkSQL案例</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">需求：将Hive中的emp表与mysql中的dept表进行连接查询 <br>一、启动spark-shell <br><code style="font-size:14px;line-height:22px;">export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/modules/hive-0.13.1-cdh5.3.6/lib/mysql-connector-java-5.1 <br>bin/spark-shell local[2]</code> <br>二、引包并建立JDBC连接 <br><code style="font-size:14px;line-height:22px;">val url = "jdbc:mysql://vin01:3306/test?user=root&amp;password=123456" <br>import java.util.Properties <br>val props = new Properties()</code> <br><img src="http://static.zybuluo.com/vin123456/2u93oyfkxyajugj40yyq6eqq/image_1as2ielda19t5129p8b16jqp35v.png" alt="image_1as2ielda19t5129p8b16jqp35v.png-40.2kB" title=""> <br>三、创建DataFrame <br><img src="http://static.zybuluo.com/vin123456/fkyov6upuscjsm32moqblxjg/image_1as2if5phf8q1jdj1b161mqo1hcp6c.png" alt="image_1as2if5phf8q1jdj1b161mqo1hcp6c.png-73kB" title=""> <br>四、jion <br><code style="font-size:14px;line-height:22px;">val join_df = hive_emp_df.join(mysql_dept_df, "deptno")</code> <br><img src="http://static.zybuluo.com/vin123456/kze2u28s8pw4xb48htqhj1c2/image_1as2ifp51qfn13cl1tsq1rcc1p5j6p.png" alt="image_1as2ifp51qfn13cl1tsq1rcc1p5j6p.png-34kB" title=""> <br>五、 将jion出来的值注册为临时表，方便查询 <br><code style="font-size:14px;line-height:22px;">join_df.registerTempTable("join_emp_dept")</code> <br>查询： <br><code style="font-size:14px;line-height:22px;">sqlContext.sql("select empno, ename, deptno, deptname, sal from join_emp_dept order by empno").show</code> <br><img src="http://static.zybuluo.com/vin123456/7bzs6z55kh3hmeem7c38mwvd/image_1as2igv751qdb15fureqfqu34t76.png" alt="image_1as2igv751qdb15fureqfqu34t76.png-48.1kB" title=""></p><h3 id="spark-集成" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t17"></a>Spark 集成</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">一、Spark与Hive的集成 <br>Spark SQL通过sqlContext读取Hive中的数据，由于Spark需要读取Hive表中的元数据，所以需要将Hive conf下的hive-site.xml文件传递到Spark conf下，做软链接： <br><img src="http://static.zybuluo.com/vin123456/wdrex751oao4qoj4o663apvl/image_1as3occ2j151n2o85hn165aocm7j.png" alt="image_1as3occ2j151n2o85hn165aocm7j.png-27.6kB" title=""> <br>还需要指定mysql连接jar包： <br><img src="http://static.zybuluo.com/vin123456/zciuzpmpukqznq1tqou2yi9f/image_1as3ocvaf1plfuquf291vqenk780.png" alt="image_1as3ocvaf1plfuquf291vqenk780.png-29.6kB" title=""> <br>测试语句： <br><img src="http://static.zybuluo.com/vin123456/oyqrya669y8gjw1tg81ei5wl/image_1as3odgbd1sar1si4egbuuf1n7a8d.png" alt="image_1as3odgbd1sar1si4egbuuf1n7a8d.png-48.5kB" title=""> <br>Spark还提供直接使用SQL的命令行： <br><img src="http://static.zybuluo.com/vin123456/b8wg6urf3y3d2nar1f9jwgq2/image_1as3odu1419hk1ut21nu31f8313a68q.png" alt="image_1as3odu1419hk1ut21nu31f8313a68q.png-50.5kB" title=""> <br><img src="http://static.zybuluo.com/vin123456/z3dd0q0n00owuwqyopk4jxu3/image_1as3oe85e176436r14pqq73j0a97.png" alt="image_1as3oe85e176436r14pqq73j0a97.png-42.9kB" title=""> <br>二、Spark与Hbase集成 <br>读取hbase上的数据 <br>在pom文件中添加maven依赖 <br>版本：<code style="font-size:14px;line-height:22px;">&lt;hbase.version&gt;0.98.6-hadoop2&lt;/hbase.version&gt;</code> <br>依赖：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs xml has-numbering"><span class="hljs-tag">&lt;<span class="hljs-title">dependency</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-title">groupId</span>&gt;</span>org.apache.hbase<span class="hljs-tag">&lt;/<span class="hljs-title">groupId</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-title">artifactId</span>&gt;</span>hbase-server<span class="hljs-tag">&lt;/<span class="hljs-title">artifactId</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-title">version</span>&gt;</span>${hbase.version}<span class="hljs-tag">&lt;/<span class="hljs-title">version</span>&gt;</span>
        <span class="hljs-tag">&lt;/<span class="hljs-title">dependency</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-title">dependency</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-title">groupId</span>&gt;</span>org.apache.hbase<span class="hljs-tag">&lt;/<span class="hljs-title">groupId</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-title">artifactId</span>&gt;</span>hbase-client<span class="hljs-tag">&lt;/<span class="hljs-title">artifactId</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-title">version</span>&gt;</span>${hbase.version}<span class="hljs-tag">&lt;/<span class="hljs-title">version</span>&gt;</span>
        <span class="hljs-tag">&lt;/<span class="hljs-title">dependency</span>&gt;</span></code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">因为需要读取hbase的配置文件，所以需要将hbase-site.xml文件拷贝到resource中 <br><img src="http://static.zybuluo.com/vin123456/ogq2r0z0vgp9ozuhh8wrvsag/image_1as3ohv331ihi1n9r1br72sd1d6n9k.png" alt="image_1as3ohv331ihi1n9r1br72sd1d6n9k.png-18.9kB" title=""> <br>我们知道hbase的数据存储在hdfs上，spark读取hbase的数据与mapreduce读取hbase的数据方法是一样的，首先我们在spark core中sparkContext.scala类中找到两个读取hadoop文件的api <br><img src="http://static.zybuluo.com/vin123456/nsf1r4sx257pd13x9xlas5rj/image_1as3qd4ec1dtnn6pre9ev914iia1.png" alt="image_1as3qd4ec1dtnn6pre9ev914iia1.png-126.9kB" title=""> <br><img src="http://static.zybuluo.com/vin123456/t2i4wxx2xwim095lbu76llxe/image_1as3qdcvo42f1jbq1salrrq1ipuae.png" alt="image_1as3qdcvo42f1jbq1salrrq1ipuae.png-154.5kB" title=""> <br>上面两个类就是读取hadoop文件的函数，这里使用新API进行解析 <br>源码解析： <br>在newAPIHadoopRDD中，有四个参数conf 、fClass 、kClass 和 vClass，还有一个返回值RDD， <br>其中conf为设置配置文件，fClass是读取HDFS文件所使用的格式，在这里是TableInputFormat，vClass和KClass的具体含义与mapreduce和hbase的集成中一样。 <br>完整代码：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs scala has-numbering"><span class="hljs-keyword">package</span> org.bigdata.spark.app.hbase

<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.client.Result
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable
<span class="hljs-keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableInputFormat
<span class="hljs-keyword">import</span> org.apache.spark.{SparkContext , SparkConf}
<span class="hljs-javadoc">/**
 * Created by hp-pc on 2016/8/10.
 */</span>
<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SparkReadHbase</span> {</span>
  <span class="hljs-keyword">def</span> main(args: Array[String]) {
    <span class="hljs-comment">//step 0: SparkContext</span>
    <span class="hljs-keyword">val</span> sparkConf =<span class="hljs-keyword">new</span> SparkConf()
      .setAppName(<span class="hljs-string">"SparkReadHbase  Application"</span>)
      .setMaster(<span class="hljs-string">"local[2]"</span>)
    <span class="hljs-comment">//create SparkContext</span>
    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> SparkContext(sparkConf)
    <span class="hljs-javadoc">/**
     *def newAPIHadoopRDD[K, V, F &lt;: NewInputFormat[K, V]](
      conf: Configuration = hadoopConfiguration,
      fClass: Class[F],* <span class="hljs-javadoctag">@param</span> fClass Class of the InputFormat
      kClass: Class[K],* <span class="hljs-javadoctag">@param</span> kClass Class of the keys
      vClass: Class[V]): RDD[(K, V)] = withScope {
    assertNotStopped()        * <span class="hljs-javadoctag">@param</span> vClass Class of the values
     */</span>
<span class="hljs-comment">//创建一个rdd读取hbase ，读取hbase需要定义hbase配置，结合hbase与mapreduce集成时hbase配置定义方法，再设置读取哪张表</span>
    <span class="hljs-keyword">val</span> conf = HBaseConfiguration.create()
    conf.set(TableInputFormat.INPUT_TABLE,<span class="hljs-string">"user"</span>)
    <span class="hljs-keyword">val</span> hbaseRdd = sc.newAPIHadoopRDD(
      conf,
      classOf[TableInputFormat],
      classOf[ImmutableBytesWritable],
      classOf[Result]

    )
    println(hbaseRdd.count()+<span class="hljs-string">"============================================="</span>)
    sc.stop()
  }
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">这里只是统计hbase有多少行，如果需要输出hbase具体数据，就需要使用具体api，代码如下：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs avrasm has-numbering">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.hbase</span>.{CellUtil, HBaseConfiguration}
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.hbase</span><span class="hljs-preprocessor">.client</span><span class="hljs-preprocessor">.Result</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.hbase</span><span class="hljs-preprocessor">.io</span><span class="hljs-preprocessor">.ImmutableBytesWritable</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.hbase</span><span class="hljs-preprocessor">.mapreduce</span><span class="hljs-preprocessor">.TableInputFormat</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.hadoop</span><span class="hljs-preprocessor">.hbase</span><span class="hljs-preprocessor">.util</span><span class="hljs-preprocessor">.Bytes</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span>.{SparkContext, SparkConf}

object SparkReadHBase {

  def main(args: Array[String]) {
    // step <span class="hljs-number">0</span>: SparkContext
    val sparkConf = new SparkConf()
      <span class="hljs-preprocessor">.setAppName</span>(<span class="hljs-string">"SparkReadHBase Applicaiton"</span>) // name
      <span class="hljs-preprocessor">.setMaster</span>(<span class="hljs-string">"local[2]"</span>) // --master local[<span class="hljs-number">2</span>] | spark://xx:<span class="hljs-number">7077</span> | yarn

    // Create SparkContext
    val sc = new SparkContext(sparkConf)

    <span class="hljs-comment">/**
     *
    conf: Configuration  hadoopConfiguration,
      fClass: Class[F],
      kClass: Class[K],
      vClass: Class[V]

      RDD[(K, V)]
     */</span>
    val conf = HBaseConfiguration<span class="hljs-preprocessor">.create</span>()
    //   <span class="hljs-comment">/** Job parameter that specifies the input table. */</span>
    // val INPUT_TABLE: String = <span class="hljs-string">"hbase.mapreduce.inputtable"</span>
    conf<span class="hljs-preprocessor">.set</span>(TableInputFormat<span class="hljs-preprocessor">.INPUT</span>_TABLE, <span class="hljs-string">"stu"</span>)

    // RDD[(ImmutableBytesWritable, Result)]
    val hbaseRdd = sc<span class="hljs-preprocessor">.newAPIHadoopRDD</span>(
      conf,
      classOf[TableInputFormat],
      classOf[ImmutableBytesWritable],
      classOf[Result]
    )
    // 上面必须填写，具体含义，与我们讲解的MapReduc与HBase集成是一样的

    println(hbaseRdd<span class="hljs-preprocessor">.count</span>() + <span class="hljs-string">"============================"</span>)

    hbaseRdd<span class="hljs-preprocessor">.map</span>(tuple =&gt; {
      val rowkey = Bytes<span class="hljs-preprocessor">.toString</span>(tuple._1<span class="hljs-preprocessor">.get</span>())
      val result = tuple._2

      var rowStr = rowkey + <span class="hljs-string">", "</span>

      for(cell &lt;- result<span class="hljs-preprocessor">.rawCells</span>()){
        rowStr += Bytes<span class="hljs-preprocessor">.toString</span>(CellUtil<span class="hljs-preprocessor">.cloneFamily</span>(cell)) + <span class="hljs-string">":"</span> +
          Bytes<span class="hljs-preprocessor">.toString</span>(CellUtil<span class="hljs-preprocessor">.cloneQualifier</span>(cell)) + <span class="hljs-string">"-&gt;"</span> +
          Bytes<span class="hljs-preprocessor">.toString</span>(CellUtil<span class="hljs-preprocessor">.cloneValue</span>(cell)) + <span class="hljs-string">"----"</span>
      }
      // return
      rowStr
    })<span class="hljs-preprocessor">.foreach</span>(println)

    sc<span class="hljs-preprocessor">.stop</span>()

  }

}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">测试： <br>在命令行测试，需要导入包，前两种方法都不行，使用SPARK_CLASSPATH，代码如下：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs lasso has-numbering">export SPARK_CLASSPATH<span class="hljs-subst">=</span>/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/mysql<span class="hljs-attribute">-connector</span><span class="hljs-attribute">-java</span><span class="hljs-subst">-</span><span class="hljs-number">5.1</span><span class="hljs-number">.27</span><span class="hljs-attribute">-bin</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase<span class="hljs-attribute">-client</span><span class="hljs-subst">-</span><span class="hljs-number">0.98</span><span class="hljs-number">.6</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase<span class="hljs-attribute">-common</span><span class="hljs-subst">-</span><span class="hljs-number">0.98</span><span class="hljs-number">.6</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase<span class="hljs-attribute">-protocol</span><span class="hljs-subst">-</span><span class="hljs-number">0.98</span><span class="hljs-number">.6</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hbase<span class="hljs-attribute">-server</span><span class="hljs-subst">-</span><span class="hljs-number">0.98</span><span class="hljs-number">.6</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/htrace<span class="hljs-attribute">-core</span><span class="hljs-subst">-</span><span class="hljs-number">2.04</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/protobuf<span class="hljs-attribute">-java</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/guava<span class="hljs-subst">-</span><span class="hljs-number">12.0</span><span class="hljs-number">.1</span><span class="hljs-built_in">.</span>jar:/opt/modules/spark<span class="hljs-subst">-</span><span class="hljs-number">1.6</span><span class="hljs-number">.1</span><span class="hljs-attribute">-bin</span><span class="hljs-subst">-</span><span class="hljs-number">2.5</span><span class="hljs-number">.0</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span>/exlibs/hive<span class="hljs-attribute">-hbase</span><span class="hljs-attribute">-handler</span><span class="hljs-subst">-</span><span class="hljs-number">0.13</span><span class="hljs-number">.1</span><span class="hljs-attribute">-cdh5</span><span class="hljs-number">.3</span><span class="hljs-number">.6</span><span class="hljs-built_in">.</span>jar</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">在命令行中粘贴代码： <br>使用paste <br><img src="http://static.zybuluo.com/vin123456/hvqo9a7qeyh1gwifkkt495j5/image_1as3qkemp1pjq1h4f5a82o41mcjar.png" alt="image_1as3qkemp1pjq1h4f5a82o41mcjar.png-86.5kB" title=""> <br>将读取hbase的代码以粘贴模式输入： <br><img src="http://static.zybuluo.com/vin123456/ja1366kx7r3yzkmfphtw5rfe/image_1as3ql03a1093h591mh41r681s30b8.png" alt="image_1as3ql03a1093h591mh41r681s30b8.png-76.6kB" title=""> <br>输出： <br><img src="http://static.zybuluo.com/vin123456/fts6qrogosmcsuig5ok2n1er/image_1as3qlccm17hi1bauoh1jpo1giobl.png" alt="image_1as3qlccm17hi1bauoh1jpo1giobl.png-46.7kB" title=""> <br>打印hbase信息的代码： <br><img src="http://static.zybuluo.com/vin123456/n0kdhudpqsuxhh1y02erpl5b/image_1as3qlqoe8c11knk1b3s11dv1oc5c2.png" alt="image_1as3qlqoe8c11knk1b3s11dv1oc5c2.png-56.8kB" title=""> <br>输出： <br><img src="http://static.zybuluo.com/vin123456/57h1hx87nngg723fw1vpz0fr/image_1as3qm8r71ve31n5febg12mn10hfcf.png" alt="image_1as3qm8r71ve31n5febg12mn10hfcf.png-45.7kB" title=""></p><h3 id="spark中聚合函数" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t18"></a>SPARK中聚合函数</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">在DataFrame类中，有如下五组函数 <br>* @groupname basic <br>Basic DataFrame functions <br>* @groupname dfops <br>Language Integrated Queries <br>* @groupname rdd <br>RDD Operations <br>* @groupname output <br>Output Operations <br>* @groupname action <br>Actions <br>agg()支持SQL 语言and DSL(Domain)语言，有如下需求：在hbase中存储有emp表，使用agg函数进行统计： <br>首先创建dataframe： <br><code style="font-size:14px;line-height:22px;">val emp_df = sqlContext.sql(“select * from tmp”)</code> <br>或者：<code style="font-size:14px;line-height:22px;">val emp_df = sqlContext.read.table(“emp”)</code> <br>统计工资的平均值和comm的最大值的几种写法 <br>（1）<code style="font-size:14px;line-height:22px;">emp_df.agg("sal" -&gt; "avg", "comm" -&gt; "max").show</code> <br>（2）<code style="font-size:14px;line-height:22px;">emp_df.agg(Map("sal" -&gt; "avg", "comm" -&gt; "max")).show</code> <br>（3）<code style="font-size:14px;line-height:22px;">emp_df.agg(max($"comm"), avg($"sal")).show //DSL写法</code></p><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">按照分组进行统计，即统计各个部门的comm最大值和sal平均值 <br><code style="font-size:14px;line-height:22px;">emp_df.groupBy($"deptno").agg(max($"comm"), avg($"sal")).show</code></p><h3 id="spark中定义udfudaf" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t19"></a>SPARK中定义UDF、UDAF</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">UDF：在实际开发中，通常在创建sqlContext后，注册UDF、 UDAF，语法： <br><code style="font-size:14px;line-height:22px;">sqlContext.udf.register( <br>“”, //函数名称 <br>.... //函数体 <br>)</code> <br>需求：针对hbase中的emp表，如果comm（奖金）是null的话，返回0.0，编写udf：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs coffeescript has-numbering">sqlContext.udf.register(
      <span class="hljs-string">"trans_comm"</span>, 
      <span class="hljs-function"><span class="hljs-params">(comm: Double)</span> =&gt;</span> {
        <span class="hljs-keyword">if</span>(comm == <span class="hljs-literal">null</span>){
          <span class="hljs-number">0.0</span>
        }<span class="hljs-keyword">else</span>{
          comm
        }
      }
) </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">注册完成后可以在sql中使用该udf <br>UDAF：定义UDAF：UDAF是用户自定义聚合函数，它的特点是多对一，即输入多个值，输出一个值。定义UDAF需要继承UserDefinedAggregateFunction 这个基类，然后重写它的抽象方法。 <br>解析：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs scala has-numbering"><span class="hljs-keyword">import</span> org.apache.spark.sql.Row
<span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
<span class="hljs-keyword">import</span> org.apache.spark.sql.types.{StructType, DataType}

<span class="hljs-javadoc">/**
 * Created by hp-pc on 2016/8/17.
 */</span>
<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">AvgUDAF</span>  <span class="hljs-keyword">extends</span> <span class="hljs-title">UserDefinedAggregateFunction</span>{</span>
  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> inputSchema: StructType = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> update(buffer: MutableAggregationBuffer, input: Row): Unit = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> bufferSchema: StructType = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> initialize(buffer: MutableAggregationBuffer): Unit = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> deterministic: Boolean = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> evaluate(buffer: Row): Any = ???

  <span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> dataType: DataType = ???</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">** 其中inputSchema是传递给聚合函数的参数的类型，在sparkSql中都是DataFrame，而DataFrame都是StructType，在这里还要封装，使用StructField，具体实现（以工资sal字段为例、Double类型）：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs python has-numbering">  override <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inputSchema</span>:</span> StructType = StructType(
  StructField(<span class="hljs-string">"sal"</span>, DoubleType, true) :: Nil
)</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">true代表是否为空，Nil表示空List， :: 符号表示将前面的数组和Nil合并为一个List 。因为StructType里存放的是数组，所以这里把它转换为List，这一段代码就是指定输入字段的类型。 <br>**其中的dataType是指定输出数据的类型，以工资为例的话就是DoubleType类型，具体代码如下： <br><code style="font-size:14px;line-height:22px;">override def dataType: DataType = DoubleType</code> <br>**其中的bufferSchema就是依据需求定义的缓冲字段的类型和名称，以求平均工资为例，缓冲的是工资总和和个数两个字段，这里工资总和是Double类型，个数是Int类型，具体代码如下</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs scala has-numbering">   <span class="hljs-javadoc">/**
 * 依据需求定义缓冲数据字段的名称和类型
 */</span>
<span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> bufferSchema: StructType = StructType(
  StructField(<span class="hljs-string">"sal_total"</span>, DoubleType, <span class="hljs-keyword">true</span>) ::
    StructField(<span class="hljs-string">"sal_count"</span>, IntegerType, <span class="hljs-keyword">true</span>) :: Nil
    )</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">**其中的evaluate函数就是将结果计算并返回，它的Any最后就是DataType ，代码实现如下：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs python has-numbering">override <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(buffer: Row)</span>:</span> Any = {
  val salTotal = buffer.getDouble(<span class="hljs-number">0</span>)
  val salCount = buffer.getInt(<span class="hljs-number">1</span>)

  // <span class="hljs-keyword">return</span>
  salTotal / salCount
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">**其中的deterministic是确定唯一性，将其值设为true即可 <br><code style="font-size:14px;line-height:22px;">override def deterministic: Boolean = true</code> <br>**其中的initialize是初始化定义的字段，这里将Double初始化值为0.0，Int值初始化为0，分别为第一个数据和第二个数据，代码实现如下：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs python has-numbering">/**
 * 对缓冲数据的字段值进行初始化
 * <span class="hljs-decorator">@param buffer</span>
 */
override <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span><span class="hljs-params">(buffer: MutableAggregationBuffer)</span>:</span> Unit = {
  buffer.update(<span class="hljs-number">0</span>, <span class="hljs-number">0.0</span>)
  buffer.update(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">**其中的update函数是更新缓存数据的值，从这个函数的参数可以看出它是从bufferSchema函数中获取值，使用buffer.getDouble(0)来获取bufferSchema中的List中第一个值，及sal_total的值，使用buffer.getInt(1)来获取List中的第二个值，及sal_count的值,代码实现如下： <br><code style="font-size:14px;line-height:22px;">// 获取缓冲数据 <br>val salTotal1 = buffer1.getDouble(0) <br>val salCount1 = buffer1.getInt(1)</code> <br>上面只是获取缓冲数据，还需要获取新传递的数据，传递的数据葱花inputSchema函数中获取。这些数据需要更新到缓冲中，代码实现如下： <br><code style="font-size:14px;line-height:22px;">// 获取传递进来的数据 <br>val inputSal = input.getDouble(0)</code> <br>接收到数据之后，将会更新到缓冲数据，最后计算的其实是更新完的缓冲的数据，在这个例子中的更新方法是将sal_total进行累加，将sal_count进行加一计数。代码实现如下：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs sql has-numbering">// 更新缓冲数据
  buffer.<span class="hljs-operator"><span class="hljs-keyword">update</span>(<span class="hljs-number">0</span>, salTotal + inputSal)
  buffer.<span class="hljs-keyword">update</span>(<span class="hljs-number">1</span>, salCount + <span class="hljs-number">1</span>)
}</span></code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">**其中merge函数是在合并分区的时候用到的，我们读取的数据实在HDFS文件系统上，必然会被分为很多块，每个块都有自己的缓冲，有自己的task，当将这些缓冲合并在一起返回最终结果时就会用到merge，合并之后的缓冲会存储在buffer1中。代码实现如下：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs scala has-numbering"><span class="hljs-javadoc">/**
 * 从字面看，是合并
 * Merges two aggregation buffers
 *  and stores the updated buffer values back to `buffer1`.
 *  *  This is called when we merge two partially aggregated data together.
 *  * <span class="hljs-javadoctag">@param</span> buffer1
 * <span class="hljs-javadoctag">@param</span> buffer2
 */</span>
<span class="hljs-keyword">override</span> <span class="hljs-keyword">def</span> merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
  <span class="hljs-comment">// 获取缓冲数据</span>
  <span class="hljs-keyword">val</span> salTotal1 = buffer1.getDouble(<span class="hljs-number">0</span>)
  <span class="hljs-keyword">val</span> salCount1 = buffer1.getInt(<span class="hljs-number">1</span>)

  <span class="hljs-comment">// 获取缓冲数据</span>
  <span class="hljs-keyword">val</span> salTotal2 = buffer2.getDouble(<span class="hljs-number">0</span>)
  <span class="hljs-keyword">val</span> salCount2 = buffer2.getInt(<span class="hljs-number">1</span>)

  <span class="hljs-comment">// 更新缓冲数据</span>
  buffer1.update(<span class="hljs-number">0</span>, salTotal1 + salTotal2)
  buffer1.update(<span class="hljs-number">1</span>, salCount1 + salCount2)
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">将上述代码合并之后即是一个求平均工资的UDAF，同样，在使用的时候也要对其进行注册。 <br>注册方法：将上述代码在spark中使用paste模式进行粘贴执行，再执行下面方法进行注册，即可使用。</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs avrasm has-numbering">    sqlContext<span class="hljs-preprocessor">.udf</span><span class="hljs-preprocessor">.register</span>(
      <span class="hljs-string">"avg_sal"</span>,
      AvgSalUDAF
    )</code></pre><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><h2 id="spark-streaming" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t20"></a>Spark Streaming</h2><hr style="color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Streaming，是一种数据传送技术，它把客户机收到的数据变成一个稳定连续的流，源源不断的送出，使用户听到的声音或者看到的图像十分平稳，而且用户在整个文件传送完之前就可以开始在屏幕上浏览文件。 <br>三种流处理技术：</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>Apache Storm</li><li>Spark Streaming</li><li>Apache Samza <br>上述三种实时计算系统都是开源的分布式系统，具有低延时、可扩展和容错性诸多优点，他们的共同特色在于：允许在运行数据流代码时，将任务分配到一系列具有容错能力的计算机上并行运行。</li></ul><h3 id="spark-streaming工作原理" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t21"></a>Spark Streaming工作原理</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Spark Streaming是一个可扩展，高吞吐、具有容错率的流式计算框架，它从数据源（soket、flume 、kafka）得到数据，并将流式数据分成很多RDD，根据时间间隔以批次（batch）为单位进行处理，能实现实时统计，累加，和一段时间内的指标的统计。</p><blockquote style="border-left:8px solid rgb(221,223,228);background:rgb(238,240,244);color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;"><blockquote style="border-left:8px solid rgb(221,223,228);"><p style="font-size:14px;color:rgb(153,153,153);line-height:22px;">当运行Spark Streaming 框架时，Application会执行StreamingContext，并且在底层运行的是SparkContext，然后Driver在每个Executor上一直运行一个Receiver来接受数据 <br><img src="http://static.zybuluo.com/vin123456/2gxlc1ffqvjqedp8vxp7nqr9/image_1as49tnf31qq5vet2gu1bs41ivdcs.png" alt="image_1as49tnf31qq5vet2gu1bs41ivdcs.png-59.7kB" title=""> <br>Receiver通过input stream接收数据并将数据分成块（blocks），之后存储在Executor的内存中，blocks会在其他的Executor上进行备份 <br><img src="http://static.zybuluo.com/vin123456/3w52offyj0bajrvia7m4mwra/image_1as4a2m8a18obaub100k1koh1g7nd9.png" alt="image_1as4a2m8a18obaub100k1koh1g7nd9.png-63.1kB" title=""> <br>Executor将存储的blocks回馈给StreamingContext，当经过一定时间后，StreamingContext将在这一段时间内的blocks，也称为批次（batch）当作RDD来进行处理，并通过SparkContext运行Spark jobs，Spark jobs通过运行tasks在每个Executor上处理存储在内存中的blocks <br><img src="http://static.zybuluo.com/vin123456/zut3psn48e4oy0trxf1dd73k/image_1as4aa2dt14n25641b50e731ubldm.png" alt="image_1as4aa2dt14n25641b50e731ubldm.png-65.7kB" title=""> <br>这个循环每隔一个批次执行一次 <br><img src="http://static.zybuluo.com/vin123456/3pxiemu9zh482xxlha1u5z2n/image_1as4acn181ais15h412il7i2cjbe3.png" alt="image_1as4acn181ais15h412il7i2cjbe3.png-67.6kB" title=""></p></blockquote></blockquote><h3 id="dstream" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t22"></a>DStream</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">DStream（Discretized Stream）是Spark Streaming的一个基本抽象，它表示一个连续的数据流，可以是从数据源接受的输入数据流，也可以是通过转换输入数据流而生成的新的待处理的数据流，实际上，DStream代表的是一系列连续的RDDs，每个DStream的RDD都包含了一个批次的数据，对DStream的操作就是对它的一系列RDD进行操作，它有两种方式创建，一是接收数据源的流数据创建，二是通过转换，每一个时间间隔会生成一个RDD。 <br><img src="http://static.zybuluo.com/vin123456/v4bh7rnd74zvvxq5xj5dnowq/image_1as4ao1hu99e1i0kumhqa71i2beg.png" alt="image_1as4ao1hu99e1i0kumhqa71i2beg.png-60.9kB" title=""></p><h3 id="spark-streaming编程模型" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t23"></a>Spark Streaming编程模型</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">首先导入包：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs avrasm has-numbering">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span><span class="hljs-preprocessor">.StreamingContext</span>._ </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">创建StreamingContext：（这里local<a href="http://static.zybuluo.com/vin123456/g6v74xzge1crsgbleqvu8b6q/image_1arfndoeh1chn2mu143c16b51bp0m.png" rel="nofollow">2</a>表示启动了两个线程，必须两个以上</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs fsharp has-numbering"><span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> SparkConf().setMaster(<span class="hljs-string">"local[2]"</span>).setAppName(<span class="hljs-string">"NetworkWordCount"</span>)
<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(conf, Seconds(<span class="hljs-number">1</span>))  <span class="hljs-comment">//设置批次时间,测试可以用5秒</span>
<span class="hljs-comment">//在Spark-shell中可以通过传递sc方式创建StreamingContext：</span>
<span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">1</span>))</code></pre><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>第一步：接收数据 <br>ssc创建好之后就可以读取数据源了，根据StreamingContext源码可以看到读取数据源的方法： <br><img src="http://static.zybuluo.com/vin123456/5wyjp6tbpgdu6kepprujf0p2/image_1as4b3fe84ig1mbe1j5qlhb6det.png" alt="image_1as4b3fe84ig1mbe1j5qlhb6det.png-142.3kB" title=""> <br>这里举例使用套接字作为数据源，即使用socketTextStream方法 <br>查看其注释：</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs python has-numbering">/**
 - Create a input stream <span class="hljs-keyword">from</span> TCP source hostname:port. Data <span class="hljs-keyword">is</span> received using
 - a TCP socket <span class="hljs-keyword">and</span> the receive bytes <span class="hljs-keyword">is</span> interpreted <span class="hljs-keyword">as</span> UTF8 encoded `\n` delimited lines.
 - <span class="hljs-decorator">@param hostname      Hostname to connect to for receiving data</span>
 - <span class="hljs-decorator">@param port          Port to connect to for receiving data</span>
 - <span class="hljs-decorator">@param storageLevel  Storage level to use for storing the received objects</span>
 -                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)
 */
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">socketTextStream</span><span class="hljs-params">(
    hostname: String,
    port: Int,
    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
  )</span>:</span> ReceiverInputDStream[String] = withNamedScope(<span class="hljs-string">"socket text stream"</span>) {
  socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">从上面的源码中可以看出该方法有三个参数，最后一个有默认值，那么以最简单的方式创建一个Dstream： <br><code style="font-size:14px;line-height:22px;">val socketDStream= scc.socketTextStream(“vin01”,9999)</code> <br>这里的socketDStream是一行一行的数据 <br>- 第二步：基于DStream进行处理数据</p><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">// 将行数据分隔成单词 <br><code style="font-size:14px;line-height:22px;">val words = socketDStream.flatMap(_.split(" "))</code> <br>// 计算这一批次的词频，先将单词转换成元组，再reduce <br><code style="font-size:14px;line-height:22px;">val pairs = words.map(word =&gt; (word, 1)) <br>val wordCounts = pairs.reduceByKey(_ + _)</code> <br>第三步：输出 <br><code style="font-size:14px;line-height:22px;">wordCounts.print()</code> <br>最后启动该应用即可： <br><code style="font-size:14px;line-height:22px;">ssc.start() // Start the computation</code> <br><code style="font-size:14px;line-height:22px;">ssc.awaitTermination() // Wait for the computation to terminate</code> <br>运行测试： <br><img src="http://static.zybuluo.com/vin123456/kpf52uo95qmgkeyjeyqk1mu3/image_1as4bb2vufb3u2b12caf6i1d0ifa.png" alt="image_1as4bb2vufb3u2b12caf6i1d0ifa.png-22.6kB" title=""> <br>输出： <br><img src="http://static.zybuluo.com/vin123456/3bon7v7gqb2uskatu5l0n572/image_1as4bbj2h1lqu14g71p21i48169rfn.png" alt="image_1as4bbj2h1lqu14g71p21i48169rfn.png-12.5kB" title=""></p><h3 id="spark-streaming-读取hdfs数据" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t24"></a>Spark Streaming 读取HDFS数据</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">读取HDFS上的数据，需要用到Spark Streaming类中的一个方法： <br>即：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs python has-numbering">/**
 * Create a input stream that monitors a Hadoop-compatible filesystem
 * <span class="hljs-keyword">for</span> new files <span class="hljs-keyword">and</span> reads them <span class="hljs-keyword">as</span> text files (using key <span class="hljs-keyword">as</span> LongWritable, value
 * <span class="hljs-keyword">as</span> Text <span class="hljs-keyword">and</span> input format <span class="hljs-keyword">as</span> TextInputFormat). Files must be written to the
 * monitored directory by <span class="hljs-string">"moving"</span> them <span class="hljs-keyword">from</span> another location within the same
 * file system. File names starting <span class="hljs-keyword">with</span> . are ignored.
 * <span class="hljs-decorator">@param directory HDFS directory to monitor for new file</span>
 */
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">textFileStream</span><span class="hljs-params">(directory: String)</span>:</span> DStream[String] = withNamedScope(<span class="hljs-string">"text file stream"</span>) {
  fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString)
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">同样，使用wordcount模板，更改的是创建DStream的方式</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs avrasm has-numbering">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span><span class="hljs-preprocessor">.StreamingContext</span>._

val ssc = new StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))

val socketDStream = ssc<span class="hljs-preprocessor">.textFileStream</span>(<span class="hljs-string">"/user/vin/sparkstreaming/hdfsfiles"</span>)

val words = socketDStream<span class="hljs-preprocessor">.flatMap</span>(_<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">" "</span>))
val pairs = words<span class="hljs-preprocessor">.map</span>(word =&gt; (word, <span class="hljs-number">1</span>))
val wordCounts = pairs<span class="hljs-preprocessor">.reduceByKey</span>(_ + _)

wordCounts<span class="hljs-preprocessor">.print</span>()

ssc<span class="hljs-preprocessor">.start</span>()             
ssc<span class="hljs-preprocessor">.awaitTermination</span>()  </code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">将数据文件上传到HDFS上： <br><img src="http://static.zybuluo.com/vin123456/em1nd2srqiw27255cyp0k6zt/image_1as4cp4h4dm11hnl1k6310njdi6g4.png" alt="image_1as4cp4h4dm11hnl1k6310njdi6g4.png-20.6kB" title=""> <br>执行结果： <br><img src="http://static.zybuluo.com/vin123456/08z3g21sktsemzbb71v9gy1l/image_1as4cpvmg8dl6pu1cf9tg9199tgh.png" alt="image_1as4cpvmg8dl6pu1cf9tg9199tgh.png-31.2kB" title=""> <br>*通常，在开发测试中，通常把代码写入脚本中，然后再spark-shell中执行该脚本： <br>步骤：创建.scala文件，将代码拷贝至该文件中，在spark-shell中使用 :load +绝对路径执行* <br><img src="http://static.zybuluo.com/vin123456/g1bsz0xzot32hxhkrm9t4xqh/image_1as4cr5621kq5hmk85d2215l6gu.png" alt="image_1as4cr5621kq5hmk85d2215l6gu.png-48kB" title=""></p><h3 id="spark-streaming的集成" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t25"></a>Spark Streaming的集成</h3><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>与Flume进行集成 <br>spark与提供了与Flume集成时的通用类:flumeUtils，但是必须依赖flume的某些jar包，所以在开发时，maven工程中要添加依赖： <br>（参考官网），如果在命令行测试，则需要添加classpath，这里在spark主目录下创建exlibs来存放jar包。 <br>flumeUtils.scala源码 <br><img src="http://static.zybuluo.com/vin123456/y93vh61rndegwwex5jbuip2t/image_1as4dkre2182vi68q3t9ts15s7ho.png" alt="image_1as4dkre2182vi68q3t9ts15s7ho.png-117.1kB" title=""><br>启动spark-shell时，多个包之间用逗号隔开。</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs tex has-numbering">bin/spark-shell <span class="hljs-command">\
</span>--master local<span class="hljs-special">[</span>3<span class="hljs-special">]</span> <span class="hljs-command">\
</span>--jars exlibs/mysql-connector-java-5.1.27-bin.jar,<span class="hljs-command">\
</span>exlibs/spark-streaming-flume_2.10-1.6.1.jar,<span class="hljs-command">\
</span>exlibs/flume-avro-source-1.5.0-cdh5.3.6.jar,<span class="hljs-command">\
</span>exlibs/flume-ng-sdk-1.5.0-cdh5.3.6.jar</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">集成测试：在启动spark-shell前，首先配置flume，设置接收源，这里测试基于push的测试， <br>配置flume：创建flume-push-spark.properties文件，$FLUME_HOME/conf下， <br>通过notepad++进行配置 <br>主要是配置sinks <br><img src="http://static.zybuluo.com/vin123456/ocxi9f9x9i1zpm0m8ugu5g88/image_1as4d1q8f18981eha1r6f1l4m1p07hb.png" alt="image_1as4d1q8f18981eha1r6f1l4m1p07hb.png-83.5kB" title=""> <br><span>注：spark-push.txt文件已经存在</span> <br>flume配置完成后，编写与flume集成的第一个测试程序wordcount：flume.scala（存储在spark主目录下）</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs avrasm has-numbering">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span><span class="hljs-preprocessor">.StreamingContext</span>._
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.streaming</span><span class="hljs-preprocessor">.flume</span>._

val ssc = new StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))

// Step <span class="hljs-number">1</span>：Recevier Data From Where
// Flume: FlumeUtils, Kafka: KafkaUtils
val flumeDStream = FlumeUtils<span class="hljs-preprocessor">.createStream</span>(ssc, <span class="hljs-string">"vin01"</span>, <span class="hljs-number">9988</span>)<span class="hljs-preprocessor">.map</span>(event =&gt; new String(event<span class="hljs-preprocessor">.event</span><span class="hljs-preprocessor">.getBody</span><span class="hljs-preprocessor">.array</span>()))

// Step <span class="hljs-number">2</span>: Process Data Base DStream
// DStream[Long] 
val words = flumeDStream<span class="hljs-preprocessor">.flatMap</span>(_<span class="hljs-preprocessor">.split</span>(<span class="hljs-string">" "</span>))

// Count each word <span class="hljs-keyword">in</span> each batch
val pairs = words<span class="hljs-preprocessor">.map</span>(word =&gt; (word, <span class="hljs-number">1</span>))
val wordCounts = pairs<span class="hljs-preprocessor">.reduceByKey</span>(_ + _)

// Step <span class="hljs-number">3</span>: Output Result
// Print the first ten elements of each RDD generated <span class="hljs-keyword">in</span> this DStream to the console
wordCounts<span class="hljs-preprocessor">.print</span>()

ssc<span class="hljs-preprocessor">.start</span>()             // Start the computation
ssc<span class="hljs-preprocessor">.awaitTermination</span>()  // Wait for the computation to terminate

sc<span class="hljs-preprocessor">.stop</span></code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">启动Spark shell: <br><img src="http://static.zybuluo.com/vin123456/knrl8dgyyi928k0yq1au21ru/image_1as4dro78hjc1uqq1tjogp41ta6i5.png" alt="image_1as4dro78hjc1uqq1tjogp41ta6i5.png-41.8kB" title=""> <br>spark-shell application应用提交： <br><code style="font-size:14px;line-height:22px;">：load /opt/modules/spark-1.6.1-bin-2.5.0-cdh5.3.6/flume.scala</code> <br>启动flume来监控spark-shell.txt文件： <br>启动命令：(添加上将结果显示在控制台上的参数console)</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs lasso has-numbering">bin/flume<span class="hljs-attribute">-ng</span> agent <span class="hljs-subst">--</span>conf conf<span class="hljs-subst">/</span> <span class="hljs-subst">--</span>name a1 <span class="hljs-subst">--</span>conf<span class="hljs-attribute">-file</span> conf/flume<span class="hljs-attribute">-push</span><span class="hljs-attribute">-spark</span><span class="hljs-built_in">.</span>properties <span class="hljs-attribute">-Dflume</span><span class="hljs-built_in">.</span>root<span class="hljs-built_in">.</span>logger<span class="hljs-subst">=</span>DEBUG,console</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><span>在flume启动前需要先在spark-shell中启动Flume的wordcount程序，否则运行报错：（先有接收的才能推给它）</span> <br><img src="http://static.zybuluo.com/vin123456/xgk6okbfb2ejc2emwgxqwt9o/image_1as4duh1gioddai1c2f1c8bh9rii.png" alt="image_1as4duh1gioddai1c2f1c8bh9rii.png-25.2kB" title=""> <br>测试：使用echo命令往spark-push.txt 里面追加数据 <br><img src="http://static.zybuluo.com/vin123456/4b6oimq93ye3ox2h78tnnw5u/image_1as4duuic3qo1p2caqebii19cjiv.png" alt="image_1as4duuic3qo1p2caqebii19cjiv.png-61.1kB" title=""> <br>测试结果： <br><img src="http://static.zybuluo.com/vin123456/v5p4ftn69ut7j1ds49s1e9pa/image_1as4dvctq1o4cac61eaq14k51j4hjc.png" alt="image_1as4dvctq1o4cac61eaq14k51j4hjc.png-71.7kB" title=""></p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>Spark Streaming与kafka的集成（基于直接取direct） <br>SparkStreaming与kafka集成，基于Direct方式，没有Recevier，数据存储在kafka中，kafka中有Topic，Topic中有分区，所以当spark job运行时，会调用kafka消费者api，找到某个topic，从zookeeper上获取偏移量offset，从而创建rdd，然后再进行rdd数据处理，处理完后更新zookeeper上的偏移量，下一个job运行时同样的流程，只不过传递的参数不一样 <br>参考源码，创建最简单的DStream： <br><img src="http://static.zybuluo.com/vin123456/hbn23guqapmoebfrocztebjf/image_1as4ej47fkmq1ak91kt8ls81lbjp.png" alt="image_1as4ej47fkmq1ak91kt8ls81lbjp.png-200.4kB" title=""> <br>集成使用： <br>若使用idea编程，需在pom文件中添加依赖包 <br>（参考官网） <br>编程：(此程序使用了不仅进行wordcount，还对其进行了累加)</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs sql has-numbering">import kafka.serializer.StringDecoder
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.{SparkConf, SparkContext}

    val ssc = new StreamingContext(sc, Seconds(5))

    // <span class="hljs-operator"><span class="hljs-keyword">set</span> checkpoint
    ssc.checkpoint(<span class="hljs-string">"sparkstreaming/kafka/"</span>)

    // Step <span class="hljs-number">1</span>： Recevier Data <span class="hljs-keyword">From</span> <span class="hljs-keyword">Where</span>
    val kafkaParams = Map(<span class="hljs-string">"metadata.broker.list"</span> -&gt; <span class="hljs-string">"vin01:9092"</span>)
    val topics = <span class="hljs-keyword">Set</span>(<span class="hljs-string">"sparkPullTopic"</span>)

    // InputDStream[(K, V)]
    val socketDStream = KafkaUtils.createDirectStream[
      String, String,StringDecoder, StringDecoder](
      ssc,
      kafkaParams, // Map[String, String]
      topics  // <span class="hljs-keyword">Set</span>[String]
    ).map(_._2)

    // Step <span class="hljs-number">2</span>: Process Data Base DStream
    // Split <span class="hljs-keyword">each</span> line <span class="hljs-keyword">into</span> words
    val words = socketDStream.flatMap(_.split(<span class="hljs-string">" "</span>))

    // <span class="hljs-aggregate">Count</span> <span class="hljs-keyword">each</span> word <span class="hljs-keyword">in</span> <span class="hljs-keyword">each</span> batch
    val pairs = words.map(word =&gt; (word, <span class="hljs-number">1</span>))
    /**
     * updateFunc: (Seq[V], <span class="hljs-keyword">Option</span>[S]) =&gt; <span class="hljs-keyword">Option</span>[S]
     *
     * DStream[(<span class="hljs-keyword">Key</span>, <span class="hljs-keyword">Value</span>)]
     * Seq[V]
     *    V: 代表的是 DStream中<span class="hljs-keyword">Value</span>的类型,针对WordCount程序来说，V是<span class="hljs-keyword">Int</span>
     * <span class="hljs-keyword">Option</span>[S]
     *    <span class="hljs-keyword">Option</span>\<span class="hljs-keyword">Some</span>\None
     *    S: 代表的是状态State，存储的是以前分析的结果，针对WordCount程序来说，S是<span class="hljs-aggregate">Count</span>，<span class="hljs-keyword">Int</span>
     *      S可以是任意类型，依据实际需求而定
     *
     */
    val wordCounts = pairs.updateStateByKey(
      (<span class="hljs-keyword">values</span>: Seq[<span class="hljs-keyword">Int</span>], state: <span class="hljs-keyword">Option</span>[<span class="hljs-keyword">Int</span>]) =&gt; {
        //获取当前的要计算的值
        val currentCount = <span class="hljs-keyword">values</span>.<span class="hljs-aggregate">sum</span>
        // 获取以前状态中的值
        val previousCount = state.getOrElse(<span class="hljs-number">0</span>)

        // <span class="hljs-keyword">update</span> state <span class="hljs-keyword">and</span> return
        <span class="hljs-keyword">Some</span>(currentCount + previousCount)
      }
    )

    // Step <span class="hljs-number">3</span>: <span class="hljs-keyword">Output</span> Result
    // Print the <span class="hljs-keyword">first</span> ten elements <span class="hljs-keyword">of</span> <span class="hljs-keyword">each</span> RDD generated <span class="hljs-keyword">in</span> this DStream <span class="hljs-keyword">to</span> the console
    wordCounts.print()

    ssc.<span class="hljs-keyword">start</span>()             // <span class="hljs-keyword">Start</span> the computation
    ssc.awaitTermination()  // Wait <span class="hljs-keyword">for</span> the computation <span class="hljs-keyword">to</span> terminate

    sc.stop</span></code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">kafka配置： <br>创建topic： <br>查看当前有哪些topic： <br><code style="font-size:14px;line-height:22px;">bin/kafka-topics.sh --list --zookeeper vin01:2181</code> <br>创建sparkPull topic：（一个副本、一个分区） <br><code style="font-size:14px;line-height:22px;">bin/kafka-topics.sh --create --zookeeper vin01:2181 --replication-factor 1 --partitions 1 --topic sparkPull</code> <br><img src="http://static.zybuluo.com/vin123456/3ampe04cfq2fh5hiwufem7p6/image_1as4f7bje1njs35410teb1u6mtk6.png" alt="image_1as4f7bje1njs35410teb1u6mtk6.png-53kB" title=""> <br>打开producer： <br><code style="font-size:14px;line-height:22px;">bin/kafka-console-producer.sh --broker-list vin01:9092 --topic sparkPull</code> <br><img src="http://static.zybuluo.com/vin123456/hx6fq5980eylhdewq8s85of2/image_1as4f8etkdum13au10dvpr41q59kj.png" alt="image_1as4f8etkdum13au10dvpr41q59kj.png-30kB" title=""> <br>打开consumer以便于监控： <br><code style="font-size:14px;line-height:22px;">bin/kafka-console-consumer.sh --zookeeper vin01:2181 --topic sparkPull --from-beginning</code> <br><img src="http://static.zybuluo.com/vin123456/n5fptim9dhrdc1aqzl60plhx/image_1as4f8ujc6t9g3t1qqh16no1rdql0.png" alt="image_1as4f8ujc6t9g3t1qqh16no1rdql0.png-14.8kB" title=""> <br>启动spark-shell： <br>kafka依赖于几个包，在启动时必须指定（参考spark添加jar包的三种方式)：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs tex has-numbering">bin/spark-shell  <span class="hljs-command">\
</span>--master local<span class="hljs-special">[</span>3<span class="hljs-special">]</span>  <span class="hljs-command">\
</span>--jars exlibs/mysql-connector-java-5.1.27-bin.jar, <span class="hljs-command">\
</span>exlibs/spark-streaming-kafka_2.10-1.6.1.jar, <span class="hljs-command">\
</span>exlibs/kafka_2.10-0.8.2.1.jar, <span class="hljs-command">\
</span>exlibs/kafka-clients-0.8.2.1.jar, <span class="hljs-command">\ </span>
exlibs/zkclient-0.3.jar, <span class="hljs-command">\
</span>exlibs/metrics-core-2.2.0.jar</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><img src="http://static.zybuluo.com/vin123456/ilgvjybipnqntl1se3yg6opg/image_1as4fb09l80q76bc3t2t2m7jld.png" alt="image_1as4fb09l80q76bc3t2t2m7jld.png-62.9kB" title=""> <br>此时jar包就导入了 <br><img src="http://static.zybuluo.com/vin123456/npx7ks79wwta9n4vr3rb1ml3/image_1as4fburf1f2rd96rh51hni35nlq.png" alt="image_1as4fburf1f2rd96rh51hni35nlq.png-80.2kB" title=""> <br>运行之前的kafka.scala应用程序： <br><code style="font-size:14px;line-height:22px;">：load /opt/modules/spark-1.6.1-bin-2.5.0-cdh5.3.6/kafka.scala</code> <br>运行成功 <br><img src="http://static.zybuluo.com/vin123456/nqd4v4qg6pbki7v2inqmspx6/image_1as4fd05f1adqdvn1h6j16jt1hesm7.png" alt="image_1as4fd05f1adqdvn1h6j16jt1hesm7.png-85.1kB" title=""> <br>测试：在produce界面输入单词 <br>第一次输入第一行回车 <br>第二次输入第二行回车 <br><img src="http://static.zybuluo.com/vin123456/bhx5k6zmsv0vsshchmpjex1d/image_1as4fdjae1d0983q10fk15u71ha1mk.png" alt="image_1as4fdjae1d0983q10fk15u71ha1mk.png-34.3kB" title=""> <br>在consumer界面看到输出： <br><img src="http://static.zybuluo.com/vin123456/zkgb5vxx5uwqozeyctac7uj6/image_1as4fe4b6qb1r2h1u12sgc1kuqn1.png" alt="image_1as4fe4b6qb1r2h1u12sgc1kuqn1.png-37.5kB" title=""> <br>查看统计结果： <br>第一次输出： <br><img src="http://static.zybuluo.com/vin123456/8shwj5gbg4ztu72gln1cg2y5/image_1as4fek2vgmf1jo0imf1tv4dnqne.png" alt="image_1as4fek2vgmf1jo0imf1tv4dnqne.png-24.3kB" title=""> <br>第二次输出：（实现了累加） <br><img src="http://static.zybuluo.com/vin123456/c6dgzqlya9181jkuc8u2wzwh/image_1as4ff3f012g41k56ttp11qi1u4pnr.png" alt="image_1as4ff3f012g41k56ttp11qi1u4pnr.png-30.2kB" title=""></p><h3 id="spark-streaming常用api解析" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t26"></a>Spark Streaming常用API解析</h3><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>UpdateStateByKey <br>UpdateStateByKey通常用作更新记录使用，能将Spark Streaming之前处理的数据记录起来，进而实现累加功能 <br>updateStateBykey方法存在于PairDStreamFunctions.scala中，可以从DStream.scala中的 object DStream中的toPairDStreamFunctions方法中链接到PairDStreamFunctions.scala中。 <br><img src="http://static.zybuluo.com/vin123456/9vc3qwv8w3oxl3fcriry7sty/image_1as4v2daq127n11dj12knggo73ho8.png" alt="image_1as4v2daq127n11dj12knggo73ho8.png-92.6kB" title=""> <br><img src="http://static.zybuluo.com/vin123456/ppct2js0dzm7tol3p76c41ir/image_1as4v2lpg1k2p6691ldg1u3g1oamol.png" alt="image_1as4v2lpg1k2p6691ldg1u3g1oamol.png-176.6kB" title=""> <br>解析updateStateByKey：</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs applescript has-numbering">/**
 * Return a new <span class="hljs-string">"state"</span> DStream <span class="hljs-keyword">where</span> <span class="hljs-keyword">the</span> state <span class="hljs-keyword">for</span> each key <span class="hljs-keyword">is</span> updated <span class="hljs-keyword">by</span> applying
 * <span class="hljs-keyword">the</span> <span class="hljs-keyword">given</span> function <span class="hljs-function_start"><span class="hljs-keyword">on</span></span> <span class="hljs-keyword">the</span> previous state <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> key <span class="hljs-keyword">and</span> <span class="hljs-keyword">the</span> new values <span class="hljs-keyword">of</span> each key.
 * Hash partitioning <span class="hljs-keyword">is</span> used <span class="hljs-keyword">to</span> generate <span class="hljs-keyword">the</span> RDDs <span class="hljs-keyword">with</span> Spark's default <span class="hljs-type">number</span> <span class="hljs-keyword">of</span> partitions.
 * @param updateFunc State update function. If `this` function returns None, <span class="hljs-keyword">then</span>
 *                   corresponding state key-value pair will be eliminated.
 * @tparam S State type
 */
def updateStateByKey[S: ClassTag](
    updateFunc: (Seq[V], Option[S]) =&gt; Option[S]
  ): DStream[(K, S)] = ssc.withScope {
  updateStateByKey(updateFunc, defaultPartitioner())
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">其中： <br><code style="font-size:14px;line-height:22px;">updateFunc: (Seq[V], Option[S]) =&gt; Option[S]</code> <br>代表一个匿名函数，Options[S]是该函数粉返回类型， <br>Seq[V]表示泛型，因为DStream是[(key , value)]格式的，Seq[V]中V代表的是DStream中value的类型，针对wordcount程序来说，V为整型Int，Seq[V]表示一个集合，存储了value的类型。 <br>Option[S]代表一个状态，Option有两个子类：Some、None， <br>这里的S代表的代表的是一个状态，存储的是以前分析的结果，由于不同的应用分析结果不同，针对wordcount程序来说，分析结果是Count，其类型也是Int。但在不同的应用中，S可以是任意的类型，依据实际需求而定。 <br>根据分析可以得出如下代码： <br>…… <br><code style="font-size:14px;line-height:22px;">val WordCounts = Pairs.updataStateByKey((values:Seq[Int], state:Option[Int]) =&gt; { <br>//获取当前要计算的值 <br>val currentCount = values.sum <br>//获取以前状态中的值 <br>val previousCount = state.getOrElse(0) <br>//更新状态，返回 <br>Some(currentCount + prevousCount) <br>})</code> <br>完整代码：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs scala has-numbering"><span class="hljs-keyword">import</span> org.apache.spark.streaming._
<span class="hljs-keyword">import</span> org.apache.spark.{SparkConf, SparkContext}

<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">UpdateStateWordCount</span> {</span>
  <span class="hljs-keyword">def</span> main(args: Array[String]) {
    <span class="hljs-comment">// step 0: SparkContext</span>
    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> SparkConf()
      .setAppName(<span class="hljs-string">"LogAnalyzer Applicaiton"</span>) <span class="hljs-comment">// name</span>
      .setMaster(<span class="hljs-string">"local[2]"</span>) <span class="hljs-comment">// --master local[2] | spark://xx:7077 | yarn</span>

    <span class="hljs-comment">// Create SparkContext</span>
    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> SparkContext(sparkConf)

    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(sc, Seconds(<span class="hljs-number">5</span>))

    <span class="hljs-comment">// set checkpoint</span>
    ssc.checkpoint(<span class="hljs-string">"sparkstreaming/socketwc/"</span>)

    <span class="hljs-comment">// Step 1： Recevier Data From Where</span>
    <span class="hljs-keyword">val</span> socketDStream = ssc.socketTextStream(<span class="hljs-string">"hadoop-senior01.ibeifeng.com"</span>, <span class="hljs-number">9999</span>)


    <span class="hljs-comment">// transformFunc: RDD[T] =&gt; RDD[U]</span>
    <span class="hljs-comment">// 仅仅针对DStream中的RDD来操作，在实际开发中，RDD的操作更加方便</span>
    <span class="hljs-javadoc">/**
     * WordCount
     *    . ? @ # $ !
     *    如果单词是上述标点符号，统计毫无意义，可以进行过滤
     */</span>
    <span class="hljs-keyword">val</span> filterRdd = sc.parallelize(List(<span class="hljs-string">"."</span>, <span class="hljs-string">"?"</span>, <span class="hljs-string">"@"</span>, <span class="hljs-string">"#"</span>, <span class="hljs-string">"!"</span>)).map((_, <span class="hljs-keyword">true</span>))
    socketDStream.transform(rdd =&gt; {
      <span class="hljs-keyword">val</span> tupleRdd = rdd.map((_, <span class="hljs-number">1</span>))
      <span class="hljs-comment">// join,filter</span>
      tupleRdd.leftOuterJoin(filterRdd)
    })

    <span class="hljs-comment">// Step 2: Process Data Base DStream</span>
    <span class="hljs-comment">// Split each line into words</span>
    <span class="hljs-keyword">val</span> words = socketDStream.flatMap(_.split(<span class="hljs-string">" "</span>))

    <span class="hljs-comment">// Count each word in each batch</span>
    <span class="hljs-keyword">val</span> pairs = words.map(word =&gt; (word, <span class="hljs-number">1</span>))
    <span class="hljs-javadoc">/**
     * updateFunc: (Seq[V], Option[S]) =&gt; Option[S]
     *
     * DStream[(Key, Value)]
     * Seq[V]
     *    V: 代表的是 DStream中Value的类型,针对WordCount程序来说，V是Int
     * Option[S]
     *    Option\Some\None
     *    S: 代表的是状态State，存储的是以前分析的结果，针对WordCount程序来说，S是Count，Int
     *      S可以是任意类型，依据实际需求而定
     *
     *  回顾一下：
     *    reduceByKey()
     *    =
     *    reduce(Key, Values)
     *
     */</span>
    <span class="hljs-keyword">val</span> wordCounts = pairs.updateStateByKey(
      (values: Seq[Int], state: Option[Int]) =&gt; {
        <span class="hljs-comment">//获取当前的要计算的值</span>
        <span class="hljs-keyword">val</span> currentCount = values.sum
        <span class="hljs-comment">// 获取以前状态中的值</span>
        <span class="hljs-keyword">val</span> previousCount = state.getOrElse(<span class="hljs-number">0</span>)

        <span class="hljs-comment">// update state and return</span>
        Some(currentCount + previousCount)
      }
    )

    <span class="hljs-keyword">val</span> wcDStream = pairs.updateStateByKey(
      (values: Seq[Int], state: Option[Int]) =&gt; Some(values.sum + state.getOrElse(<span class="hljs-number">0</span>)))

    <span class="hljs-comment">// Step 3: Output Result</span>
    <span class="hljs-comment">// Print the first ten elements of each RDD generated in this DStream to the console</span>
    wordCounts.print()

    ssc.start()             <span class="hljs-comment">// Start the computation</span>
    ssc.awaitTermination()  <span class="hljs-comment">// Wait for the computation to terminate</span>

    sc.stop
  }

}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">测试： <br>打开nc 9999端口，运行程序： <br>两次在端口输入： <br><img src="http://static.zybuluo.com/vin123456/gsrycyx2uqdpz63pegr2z2yf/image_1as4vbbgdk3svg416i91ajm1ssjp2.png" alt="image_1as4vbbgdk3svg416i91ajm1ssjp2.png-28.7kB" title=""> <br>第一次输出： <br><img src="http://static.zybuluo.com/vin123456/4mjsc56dxhzpr8l5xdhvkz2a/image_1as4vbpql1jlnk0q1g0b17621okupf.png" alt="image_1as4vbpql1jlnk0q1g0b17621okupf.png-15.2kB" title=""> <br>第二次输出： <br><img src="http://static.zybuluo.com/vin123456/pk546joltgfntq1rp128kjfa/image_1as4vchs514a91ht9ro3l619eups.png" alt="image_1as4vchs514a91ht9ro3l619eups.png-23.8kB" title=""> <br>实现了累加统计，测试成功 <br>注：在代码中，使用了transform方法，对其进行解析：</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>DStream的transform方法</li></ul><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">transform方法在DStream中，其源码为：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs python has-numbering">/**
 * Return a new DStream <span class="hljs-keyword">in</span> which each RDD <span class="hljs-keyword">is</span> generated by applying a function
 * on each RDD of <span class="hljs-string">'this'</span> DStream.
 */
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transform</span>[<span class="hljs-title">U</span>:</span> ClassTag](transformFunc: RDD[T] =&gt; RDD[U]): DStream[U] = ssc.withScope {
  // because the DStream <span class="hljs-keyword">is</span> reachable <span class="hljs-keyword">from</span> the outer object here, <span class="hljs-keyword">and</span> because
  // DStreams can<span class="hljs-string">'t be serialized with closures, we can'</span>t proactively check
  // it <span class="hljs-keyword">for</span> serializability <span class="hljs-keyword">and</span> so we <span class="hljs-keyword">pass</span> the optional false to SparkContext.clean
  val cleanedF = context.sparkContext.clean(transformFunc, false)
  transform((r: RDD[T], t: Time) =&gt; cleanedF(r))
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">可以看出，它将DStream中的RDD进行单独单独操作，最终返回的还是DStream，所以如果进行wordcount过滤的话，将需要过滤的符号存储在RDD中，对RDD 进行join操作。匹配过滤掉这些字符。 <br>具体代码：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs scala has-numbering"> <span class="hljs-javadoc">/**
     * WordCount
     *    . ? @ # $ !
     *    如果单词是上述标点符号，统计毫无意义，可以进行过滤
     */</span>
    <span class="hljs-keyword">val</span> filterRdd = sc.parallelize(List(<span class="hljs-string">"."</span>, <span class="hljs-string">"?"</span>, <span class="hljs-string">"@"</span>, <span class="hljs-string">"#"</span>, <span class="hljs-string">"!"</span>)).map((_, <span class="hljs-keyword">true</span>))
    socketDStream.transform(rdd =&gt; {
      <span class="hljs-keyword">val</span> tupleRdd = rdd.map((_, <span class="hljs-number">1</span>))
      <span class="hljs-comment">// join,filter</span>
      tupleRdd.leftOuterJoin(filterRdd).filter(tuple =&gt; {
<span class="hljs-keyword">val</span> x1 = tuple_1
<span class="hljs-keyword">val</span> x2 =tuple_2 <span class="hljs-comment">//(i, option[boolean])</span>
<span class="hljs-keyword">if</span> (!x2._2.isEmpty){
 <span class="hljs-keyword">true</span>
}<span class="hljs-keyword">else</span>
{<span class="hljs-keyword">false</span>
}

})
    })</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">解释： <br>filterRdd存储了这些符号，进行RDD之间的join需要将其映射成元组对，所以对其进行映射， <br>在transform中，对RDD进行操作，首先将其映射为tupleRdd（元组对），以此tupleRdd为准与filterRdd进行join。join完成之后还是元组对，所以对该元组对进行操作,首先判断tupleRdd中的值是否是这些符号之中的一个，join之后是两个元组对进行join，所以x2在这里也是一个元组对，它的类型是( 1 , option(boolean))，如果是这个符号，那么返回的是(1,true)，否则返回的是空(1,none)，所以x2._2如果是空，则为这些符号，需要过滤掉。其中filter(func) : 返回一个新的数据集，由经过func函数后返回值为true的原元素组成。 <br>join(otherDataset, [numTasks]) :在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集</p><ul style="list-style:none;color:rgb(51,51,51);font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;font-size:14px;background-color:rgb(255,255,255);"><li>DStream的foreachRDD方法 <br>DStream中还有一个方法foreachRDD，它与transform一样是对RDD进行操作，但是它没有返回值，比如遇到需求将分析出来的结果存储在mysql中：</li></ul><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs php has-numbering"><span class="hljs-comment">// foreachFunc: (RDD[T], Time) =&gt; Unit</span>
dstream.foreachRDD(rdd =&gt; {
  <span class="hljs-comment">// 将分析的数据存储到JDBC中，MySQL数据中</span>
  val connection = createJDBCConnection()  <span class="hljs-comment">// executed at the driver</span>
  rdd.<span class="hljs-keyword">foreach</span> { record =&gt;
    connection.putStateResult(record) <span class="hljs-comment">// executed at the worker</span>
  }
})</code></pre><h3 id="spark-streaming的窗口函数" style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);"><a name="t27"></a>Spark Streaming的窗口函数</h3><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">spark streaming提供了窗口操作，允许在某个大小的窗口中进行操作，常用于统计某个时间段内指标： <br>比如需求：对词频的统计，要求每次统计的数据是最近10s的数据 <br>分析windows源码： <br><img src="http://static.zybuluo.com/vin123456/kpf2fdb1i5iaf5x6xw1exjbq/image_1as50a3ff17hb2ak9cs1kso1j0mq9.png" alt="image_1as50a3ff17hb2ak9cs1kso1j0mq9.png-231.7kB" title=""> <br>分析reduceByKeyAndWindow：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs applescript has-numbering">/**
 * Return a new DStream <span class="hljs-keyword">by</span> applying `reduceByKey` <span class="hljs-keyword">over</span> a sliding window. This <span class="hljs-keyword">is</span> similar <span class="hljs-keyword">to</span>
 * `DStream.reduceByKey()` <span class="hljs-keyword">but</span> applies <span class="hljs-keyword">it</span> <span class="hljs-keyword">over</span> a sliding window. Hash partitioning <span class="hljs-keyword">is</span> used <span class="hljs-keyword">to</span>
 * generate <span class="hljs-keyword">the</span> RDDs <span class="hljs-keyword">with</span> Spark's default <span class="hljs-type">number</span> <span class="hljs-keyword">of</span> partitions.
 * @param reduceFunc associative reduce function
 * @param windowDuration width <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> window; must be a multiple <span class="hljs-keyword">of</span> this DStream's
 *                       batching interval
 * @param slideDuration  sliding interval <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> window (i.e., <span class="hljs-keyword">the</span> interval <span class="hljs-keyword">after</span> which
 *                       <span class="hljs-keyword">the</span> new DStream will generate RDDs); must be a multiple <span class="hljs-keyword">of</span> this
 *                       DStream's batching interval
 */
def reduceByKeyAndWindow(
    reduceFunc: (V, V) =&gt; V,
    windowDuration: Duration,
    slideDuration: Duration
  ): DStream[(K, V)] = ssc.withScope {
  reduceByKeyAndWindow(reduceFunc, windowDuration, slideDuration, defaultPartitioner())
}</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">在源码中，可以看到需要传递三个参数，reduceFunc：RDD的操作，windowDuration：窗口的大小，即每次处理几个批次的数据，必须是接收数据时间间隔的整数倍（即 Seconds(5)的整数倍），slideDuration表示窗口的时间间隔，即每隔多少秒窗口执行一次。 <br>在wordcount程序中只需修改：</p><pre class="prettyprint" style="font-size:14px;line-height:22px;"><code class="hljs vbscript has-numbering">val wordCounts = pairs.reduceByKey(_ + _)  这一行代码，修改为：
val wordCounts = pairs.reduceByKeyAndWindow((x:<span class="hljs-built_in">Int</span>,y:<span class="hljs-built_in">Int</span>)=x+y,Seconds(<span class="hljs-number">10</span>),<span class="hljs-built_in">Second</span>(<span class="hljs-number">4</span>))</code></pre><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">Second(10)表示每次处理2 X5秒的数据,在这里是统计两个批次的数据，Second(4)表示每隔4s执行一次窗口。</p><p style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">转载自<a href="https://blog.csdn.net/vinfly_li/article/details/79396821" rel="nofollow">https://blog.csdn.net/vinfly_li/article/details/79396821</a></p>            </div>
                </div>