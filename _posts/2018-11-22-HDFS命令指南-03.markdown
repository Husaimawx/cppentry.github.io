---
layout:     post
title:      HDFS命令指南-03
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="1前言">1、前言</h1>

<p>本文写于2018年02月份，以当前HDFS版本2.9.0为主，主要参考为官方文档，其中加入了一些自己的理解，如有不对之处，还请多多指教，感谢！</p>

<p>所有HDFS命令都可以用bin/hdfs脚本调用。不带任何参数运行hdfs脚本将打印所有命令的描述信息。</p>

<pre><code>用法：hdfs [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]
</code></pre>

<p>Hadoop有一个选项解析框架，它会去解析选项，并运行其对应的类方法。</p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>–config –loglevel</td>
  <td align="center">常用的一组shell选项。这些记录在“命令手册”页面上。</td>
</tr>
<tr>
  <td>GENERIC_OPTIONS</td>
  <td align="center">多个命令支持的一组常用选项。有关更多信息，请参阅Hadoop命令手册</td>
</tr>
<tr>
  <td>COMMAND COMMAND_OPTIONS</td>
  <td align="center">以下各节介绍了各种命令及其选项。这些命令分为两组，用户命令和管理命令。</td>
</tr>
</tbody></table>




<h1 id="2用户命令">2、用户命令</h1>

<p>对hadoop集群用户有帮助的命令。</p>



<h2 id="classpath">classpath</h2>

<p>用法: <code>hdfs classpath [--glob |--jar &lt;path&gt; |-h |--help]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>–glob</td>
  <td align="center">扩展通配符</td>
</tr>
<tr>
  <td>–jar path</td>
  <td align="center">将类路径写入mainfest中，并生成一个jar包</td>
</tr>
</tbody></table>


<p>该命令主要为打印Hadoop jar中所加载的类路径。如果不带参数调用，则打印由命令脚本中所有配置的类路径。路径中包含分隔符。另外一个选项是，将classpath写入到mainfest中，生成一个jar包，这个命令是很有用的，尤其是当类太多时，控制台无法完整打印时。</p>



<h2 id="dfs">dfs</h2>

<p>用法: <code>hdfs dfs [COMMAND [COMMAND_OPTIONS]]</code></p>

<p>在HDFS上运行文件操作相关命令。COMMAND_OPTIONS选项可以在页面<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html" rel="nofollow">File System Shell Guide</a>上找到。</p>



<h2 id="fetchdt">fetchdt</h2>

<p>用法: <code>hdfs fetchdt &lt;opts&gt; &lt;token_file_path&gt;</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>–webservice NN_Url</td>
  <td align="center">与NameNode连接的地址（以http或https开头）</td>
</tr>
<tr>
  <td>–renewer name</td>
  <td align="center">令牌更新者的名称</td>
</tr>
<tr>
  <td>–cancel</td>
  <td align="center">取消委派令牌</td>
</tr>
<tr>
  <td>–renew</td>
  <td align="center">续订授权令牌。委托令牌必须使用<code>-renewer name</code>选项获取。</td>
</tr>
<tr>
  <td>–print</td>
  <td align="center">打印授权令牌信息</td>
</tr>
<tr>
  <td>token_file_path</td>
  <td align="center">指定令牌存储的文件路径</td>
</tr>
</tbody></table>


<p>该命令主要从从NameNode获取授权令牌。有关更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#fetchdt" rel="nofollow">fetchdt</a>。</p>



<h2 id="fsck">fsck</h2>

<p>用法：</p>

<pre><code> hdfs fsck &lt;path&gt;
      [-list-corruptfileblocks |
      [-move | -delete | -openforwrite]
      [-files [-blocks [-locations | -racks | -replicaDetails | -upgradedomains]]]
      [-includeSnapshots]
      [-storagepolicies] [-maintenance] [-blockId &lt;blk_Id&gt;]
</code></pre>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>path</td>
  <td align="center">指定路径下进行操作</td>
</tr>
<tr>
  <td>-delete</td>
  <td align="center">删除损坏的文件</td>
</tr>
<tr>
  <td>-files</td>
  <td align="center">打印出正在检查的文件</td>
</tr>
<tr>
  <td>-files -blocks</td>
  <td align="center">打印块报告</td>
</tr>
<tr>
  <td>-files -blocks -locations</td>
  <td align="center">打印每个块的位置</td>
</tr>
<tr>
  <td>-files -blocks -racks</td>
  <td align="center">打印出数据节点位置的网络拓扑</td>
</tr>
<tr>
  <td>-files -blocks -replicaDetails</td>
  <td align="center">打印出每个副本的详细信息</td>
</tr>
<tr>
  <td>-files -blocks -upgradedomains</td>
  <td align="center">为每个块打印升级域</td>
</tr>
<tr>
  <td>-includeSnapshots</td>
  <td align="center">如果指定路径下已经包含快照数据,则包含快照数据。</td>
</tr>
<tr>
  <td>-list-corruptfileblocks</td>
  <td align="center">打印缺失块和文件列表信息</td>
</tr>
<tr>
  <td>-move</td>
  <td align="center">将损坏的文件移至/lost+found目录</td>
</tr>
<tr>
  <td>-openforwrite</td>
  <td align="center">打印正在打开写入的文件</td>
</tr>
<tr>
  <td>-storagepolicies</td>
  <td align="center">打印块的存储策略摘要</td>
</tr>
<tr>
  <td>-maintenance</td>
  <td align="center">打印维护状态节点的详细信息。</td>
</tr>
<tr>
  <td>-blockId</td>
  <td align="center">打印出块的信息</td>
</tr>
</tbody></table>


<p>运行HDFS文件系统检查工具。更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#fsck" rel="nofollow">fsck</a>。</p>



<h2 id="getconf">getconf</h2>

<pre><code>hdfs getconf -namenodes
hdfs getconf -secondaryNameNodes
hdfs getconf -backupNodes
hdfs getconf -includeFile
hdfs getconf -excludeFile
hdfs getconf -nnRpcAddresses
hdfs getconf -confKey [key]   
</code></pre>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-namenodes</td>
  <td align="center">获取集群中namenode节点列表</td>
</tr>
<tr>
  <td>-secondaryNameNodes</td>
  <td align="center">获取集群中secondaryNameNode节点列表</td>
</tr>
<tr>
  <td>-backupNodes</td>
  <td align="center">获取群集中备份节点的列表</td>
</tr>
<tr>
  <td>-includeFile</td>
  <td align="center">获取dfs.hosts文件路径，该文件中定义了可以加入集群的数据节点列表</td>
</tr>
<tr>
  <td>-excludeFile</td>
  <td align="center">获取dfs.exclude文件路径，该文件中定义了不加入集群的数据节点列表</td>
</tr>
<tr>
  <td>-nnRpcAddresses</td>
  <td align="center">获取namenode rpc地址</td>
</tr>
<tr>
  <td>-confKey [key]</td>
  <td align="center">从配置获取特定的密钥</td>
</tr>
</tbody></table>


<p>该命令主要从配置目录获取配置信息，进行后续处理。</p>



<h2 id="groups">groups</h2>

<p>用法： <code>hdfs groups [username ...]</code></p>

<p>返回一个或多个用户的组信息。</p>



<h2 id="lssnapshottabledir">lsSnapshottableDir</h2>

<p>用法: <code>hdfs lsSnapshottableDir [-help]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-help</td>
  <td align="center">打印帮助信息</td>
</tr>
</tbody></table>


<p>获取快照目录的列表。如果以超级用户的身份运行时，它会返回所有可快照的目录。否则，它只会返回当前用户所拥有的快照目录。 </p>



<h2 id="jmxget">jmxget</h2>

<p>用法: <code>hdfs jmxget [-localVM ConnectorURL | -port port | -server mbeanserver | -service service]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-help</td>
  <td align="center">打印帮助信息</td>
</tr>
<tr>
  <td>-localVM ConnectorURL</td>
  <td align="center">连接到同一台机器上的虚拟机</td>
</tr>
<tr>
  <td>-port mbean server port</td>
  <td align="center">指定mbean服务器端口，如果没有它将尝试连接到同一个VM中的MBean Server</td>
</tr>
<tr>
  <td>-server</td>
  <td align="center">指定mbean服务器（默认为localhost）</td>
</tr>
<tr>
  <td>-service NameNode|DataNode</td>
  <td align="center">指定jmx服务。 NameNode默认情况下。</td>
</tr>
</tbody></table>


<p>该命令主要从服务转储JMX信息。</p>



<h2 id="oev">oev</h2>

<p>用法: <code>hdfs oev [OPTIONS] -i INPUT_FILE -o OUTPUT_FILE</code></p>

<p>执行命令必须的参数：</p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>–i,–inputFile arg</td>
  <td align="center">输入edits文件，如果是xml后缀，表示XML格式，其他表示二进制。</td>
</tr>
<tr>
  <td>-o,–outputFile arg</td>
  <td align="center">输出文件的名称。如果指定的文件存在，会被覆盖，文件的格式由-p选项确定</td>
</tr>
</tbody></table>


<p>执行命令可选的参数：</p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-f,–fix-txids</td>
  <td align="center">重置输入edits文件中的transaction IDs</td>
</tr>
<tr>
  <td>-h,–help</td>
  <td align="center">显示帮助信息</td>
</tr>
<tr>
  <td>-r,–recover</td>
  <td align="center">使用recovery模式，跳过eidts中的错误记录。</td>
</tr>
<tr>
  <td>-p,–processor arg</td>
  <td align="center">选择对文件应用哪种格式进行处理，当前支持的格式是：二进制（Hadoop使用的本机二进制格式），xml（默认，XML格式），统计信息（打印关于edits文件的统计信息）</td>
</tr>
<tr>
  <td>-v,–verbose</td>
  <td align="center">打印更多详细信息，默认为false）。</td>
</tr>
</tbody></table>


<p>该命令主要用于离线查看edits文件。更多信息，请查看<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html" rel="nofollow">Offline Edits Viewer Guide</a> </p>



<h2 id="oiv">oiv</h2>

<p>命令hdfs oiv用于将fsimage文件转换成其他格式的进行存储和查看，如文本文件、XML文件。</p>

<p>用法: hdfs oiv [OPTIONS] -i INPUT_FILE</p>

<p>执行命令必须的参数：</p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-i|–inputFile input file</td>
  <td align="center">如果使用ReverseXML处理器）进行处理，需要指定输入fsimage文件（或XML文件）</td>
</tr>
</tbody></table>


<p>执行命令可选的参数：</p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-o,–outputFile output file</td>
  <td align="center">指定输出文件名，如果指定了输出处理器，则会自己生成一个。如果指定的文件已经存在，它将被无提示地覆盖。 （默认情况下输出到stdout）如果输入文件是XML文件，它还会创建 .md5。</td>
</tr>
<tr>
  <td>-p,–processor processor</td>
  <td align="center">指定要应用于映象文件（image file）处理器。目前可选项是Web（默认），XML，分隔符，FileDistribution和ReverseXML。</td>
</tr>
<tr>
  <td>-addr address</td>
  <td align="center">指定要监听的地址（主机：端口）。 （默认为localhost：5978）。该选项与Web处理器一起使用。</td>
</tr>
<tr>
  <td>-maxSize size</td>
  <td align="center">指定要分析的文件大小的范围[0，maxSize]（以字节为单位）（默认为128GB）。该选项与FileDistribution处理器一起使用。</td>
</tr>
<tr>
  <td>-step size</td>
  <td align="center">以字节为单位指定分布的粒度（默认为2MB）。该选项与FileDistribution处理器一起使用。</td>
</tr>
<tr>
  <td>-format</td>
  <td align="center">以可读的方式格式化输出结果而不是多个字节。 （默认为false）。该选项与FileDistribution处理器一起使用。</td>
</tr>
<tr>
  <td>-delimiter arg</td>
  <td align="center">分隔字符串以与分隔处理器一起使用。</td>
</tr>
<tr>
  <td>-t,–temp temporary dir</td>
  <td align="center">使用临时目录来缓存中间结果以生成分隔输出。如果未设置，分隔处理器在输出文本之前在内存中构造namespace。</td>
</tr>
<tr>
  <td>-h,–help</td>
  <td align="center">显示工具使用和帮助和退出信息</td>
</tr>
</tbody></table>




<h2 id="oivlegacy">oiv_legacy</h2>

<p>用法:<code>hdfs oiv_legacy [OPTIONS] -i INPUT_FILE -o OUTPUT_FILE</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-i,–inputFile input file</td>
  <td align="center">指定要处理的输入fsimage文件。</td>
</tr>
<tr>
  <td>-o,–outputFile output file</td>
  <td align="center">指定输出文件名，如果指定了输出处理器，则会自动生成一个。如果指定的文件已经存在，它将被无提示地覆盖。</td>
</tr>
</tbody></table>


<p>执行命令可选的参数：</p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-p|–processor processor</td>
  <td align="center">指定要应用于（image file）映像文件的图像处理器。有效的选项有Ls（默认），XML，分隔符，缩进，FileDistribution和NameDistribution。</td>
</tr>
<tr>
  <td>-maxSize size</td>
  <td align="center">指定要分析的文件大小的范围[0，maxSize]（以字节为单位）（默认为128GB）。该选项与FileDistribution处理器一起使用。</td>
</tr>
<tr>
  <td>-step size</td>
  <td align="center">以字节为单位指定分布的粒度（默认为2MB）。该选项与FileDistribution处理器一起使用。</td>
</tr>
<tr>
  <td>-format</td>
  <td align="center">以可读的方式格式化输出结果而不是多个字节。 （默认为false）。该选项与FileDistribution处理器一起使用。</td>
</tr>
<tr>
  <td>-skipBlocks</td>
  <td align="center">不枚举文件中的各个块。当namespaces文件特别大时，这可以节省非常多的时间和空间当。 使用Ls处理器读取块时，将会准备的确认文件大小并忽略此选项。</td>
</tr>
<tr>
  <td>-printToScreen</td>
  <td align="center">处理器输出到控制台以及指定的文件。在非常大的namespaces上，这可能会将处理时间增加一个数量级。</td>
</tr>
<tr>
  <td>-delimiter arg</td>
  <td align="center">与分隔处理器一起使用时，将默认制表符分隔符替换为由arg指定的字符串。</td>
</tr>
<tr>
  <td>-h|–help</td>
  <td align="center">显示工具使用和帮助和退出信息</td>
</tr>
</tbody></table>


<p>用于旧版本的Hadoop离线image文件查看器，请查看 <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html#oiv_legacy_Command" rel="nofollow">oiv_legacy Command</a> 更多信息。</p>



<h2 id="snapshotdiff">snapshotDiff</h2>

<p>用法:<code>hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;</code></p>

<p>用于确认HDFS快照之间的差异。有关更多信息，请参阅 <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html#Get_Snapshots_Difference_Report" rel="nofollow">HDFS Snapshot Documentation </a>文档。</p>



<h2 id="version">version</h2>

<p>用法: hdfs version <br>
打印版本信息。</p>



<h1 id="3管理命令">3、管理命令</h1>

<p>对hadoop集群管理员有用的命令。</p>



<h2 id="balancer">balancer</h2>

<p>用法：</p>

<pre><code>hdfs balancer
          [-policy &lt;policy&gt;]
          [-threshold &lt;threshold&gt;]
          [-exclude [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]
          [-include [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]
          [-source [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]
          [-blockpools &lt;comma-separated list of blockpool ids&gt;]
          [-idleiterations &lt;idleiterations&gt;]
          [-runDuringUpgrade]
</code></pre>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-policy </td>
  <td align="center">datanode（默认）：如果每个datanode是平衡的，则集群是平衡的。   blockpool：如果每个数据节点中的每个块池均衡，则集群将保持平衡</td>
</tr>
<tr>
  <td>-threshold </td>
  <td align="center">磁盘容量的百分比。这会覆盖默认的阈值。</td>
</tr>
<tr>
  <td>-exclude -f |</td>
  <td align="center">排除指定的数据节点不被平衡器平衡。</td>
</tr>
<tr>
  <td>-include -f |</td>
  <td align="center">仅平衡指定datanode。</td>
</tr>
<tr>
  <td>-source -f |</td>
  <td align="center">只选取指定的datanode作为源节点。</td>
</tr>
<tr>
  <td>-blockpools </td>
  <td align="center">只在列表中包含的blockpools上运行。</td>
</tr>
<tr>
  <td>-idleiterations </td>
  <td align="center">退出前的最大空闲迭代次数。这会覆盖默认值（默认值：5）。</td>
</tr>
<tr>
  <td>-runDuringUpgrade</td>
  <td align="center">集群升级时，是否进行平衡。</td>
</tr>
<tr>
  <td>-h</td>
  <td align="center">–help</td>
</tr>
</tbody></table>


<p>运行集群平衡程序时。管理员可以简单地按下Ctrl-C来停止重新平衡过程。有关更多详情，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Balancer" rel="nofollow">Balancer</a>。</p>

<p>请注意，数据块控制策略比数据节点的控制策略更严格。</p>

<p>除了上述命令选项外，还从2.7.0开始引入了一个固定功能，以防止某些副本被（balancer/mover）平衡器/移动器移动。此功能在默认情况下处于禁用状态，可通过配置属性<code>“dfs.datanode.block-pinning.enabled”</code>进行启用。启用后，此功能仅影响写入create（）方法调用中指定的有利节点的块。当我们想要维护数据局部性时，这个特性非常有用，适用于HBase regionserver等应用程序。</p>



<h2 id="cacheadmin">cacheadmin</h2>

<p>用法：</p>

<pre><code>hdfs cacheadmin [-addDirective -path &lt;path&gt; -pool &lt;pool-name&gt; [-force] [-replication &lt;replication&gt;] [-ttl &lt;time-to-live&gt;]]
hdfs cacheadmin [-modifyDirective -id &lt;id&gt; [-path &lt;path&gt;] [-force] [-replication &lt;replication&gt;] [-pool &lt;pool-name&gt;] [-ttl &lt;time-to-live&gt;]]
hdfs cacheadmin [-listDirectives [-stats] [-path &lt;path&gt;] [-pool &lt;pool&gt;] [-id &lt;id&gt;]]
hdfs cacheadmin [-removeDirective &lt;id&gt;]
hdfs cacheadmin [-removeDirectives -path &lt;path&gt;]
hdfs cacheadmin [-addPool &lt;name&gt; [-owner &lt;owner&gt;] [-group &lt;group&gt;] [-mode &lt;mode&gt;] [-limit &lt;limit&gt;] [-maxTtl &lt;maxTtl&gt;]]
hdfs cacheadmin [-modifyPool &lt;name&gt; [-owner &lt;owner&gt;] [-group &lt;group&gt;] [-mode &lt;mode&gt;] [-limit &lt;limit&gt;] [-maxTtl &lt;maxTtl&gt;]]
hdfs cacheadmin [-removePool &lt;name&gt;]
hdfs cacheadmin [-listPools [-stats] [&lt;name&gt;]]
hdfs cacheadmin [-help &lt;command-name&gt;]
</code></pre>

<p>有关更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html#cacheadmin_command-line_interface" rel="nofollow">HDFS Cache Administration Documentation</a>文档。</p>



<h2 id="crypto">crypto</h2>

<p>用法：</p>

<pre><code>hdfs crypto -createZone -keyName &lt;keyName&gt; -path &lt;path&gt;
hdfs crypto -listZones
hdfs crypto -provisionTrash -path &lt;path&gt;
hdfs crypto -help &lt;command-name&gt;
</code></pre>

<p>有关更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#crypto_command-line_interface" rel="nofollow">HDFS Transparent Encryption Documentation</a>文档。</p>



<h2 id="datanode">datanode</h2>

<p>用法: <code>hdfs datanode [-regular | -rollback | -rollingupgrade rollback]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-regular</td>
  <td align="center">正常的datanode启动（默认）</td>
</tr>
<tr>
  <td>-rollback</td>
  <td align="center">将datanode回滚到以前的版本。使用该命令前，应该停止Datanode，并分发旧的hadoop版本后使用。</td>
</tr>
<tr>
  <td>-rollingupgrade rollback</td>
  <td align="center">回滚滚动升级操作</td>
</tr>
</tbody></table>


<p>运行HDFS数据节点。</p>



<h2 id="dfsadmin">dfsadmin</h2>

<p>用法：</p>

<pre><code>hdfs dfsadmin [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]]
hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]
hdfs dfsadmin [-saveNamespace]
hdfs dfsadmin [-rollEdits]
hdfs dfsadmin [-restoreFailedStorage true |false |check]
hdfs dfsadmin [-refreshNodes]
hdfs dfsadmin [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]
hdfs dfsadmin [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]
hdfs dfsadmin [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]
hdfs dfsadmin [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]
hdfs dfsadmin [-finalizeUpgrade]
hdfs dfsadmin [-rollingUpgrade [&lt;query&gt; |&lt;prepare&gt; |&lt;finalize&gt;]]
hdfs dfsadmin [-refreshServiceAcl]
hdfs dfsadmin [-refreshUserToGroupsMappings]
hdfs dfsadmin [-refreshSuperUserGroupsConfiguration]
hdfs dfsadmin [-refreshCallQueue]
hdfs dfsadmin [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]]
hdfs dfsadmin [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start |status |properties&gt;]
hdfs dfsadmin [-printTopology]
hdfs dfsadmin [-refreshNamenodes datanodehost:port]
hdfs dfsadmin [-getVolumeReport datanodehost:port]
hdfs dfsadmin [-deleteBlockPool datanode-host:port blockpoolId [force]]
hdfs dfsadmin [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]
hdfs dfsadmin [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]
hdfs dfsadmin [-fetchImage &lt;local directory&gt;]
hdfs dfsadmin [-allowSnapshot &lt;snapshotDir&gt;]
hdfs dfsadmin [-disallowSnapshot &lt;snapshotDir&gt;]
hdfs dfsadmin [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]
hdfs dfsadmin [-evictWriters &lt;datanode_host:ipc_port&gt;]
hdfs dfsadmin [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]
hdfs dfsadmin [-metasave filename]
hdfs dfsadmin [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]
hdfs dfsadmin [-listOpenFiles]
hdfs dfsadmin [-help [cmd]]
</code></pre>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]</td>
  <td align="center">报告基本的文件系统信息和统计信息.dfs与“du”这两个命令统计的结果会不同，因为它可以测量复制，校验和，快照等使用的原始空间。</td>
</tr>
<tr>
  <td>-safemode enter|leave|get|wait|forceExit</td>
  <td align="center">安全模式维护命令。安全模式是Namenode中的其中一个状态：1.不接受对命名空间的更改（只读）2、不会复制或删除块。3、Namenode启动时会自动进入安全模式，当配置的块的最小百分比满足最小复制条件时，会自动离开安全模式。如果Namenode检测到任何异常，那么它将在安全模式下逗留直到问题解决。 如果该异常是特意操作导致的，则管理员可以使用-safemode forceExit退出安全模式。可能需要forceExit的情况是：a、Namenode元数据不一致。如果Namenode检测到元数据已被修改，则Namenode将进入forceExit状态。此时，用户可以去修复这个问题，使用正确的元数据文件或forceExit重新启动Namenode（如果接受数据丢失的情况话）。b、回滚会导致元数据被替换，很少会在Namenode中触发安全模式forceExit状态。在这种情况下，您可以继续执行-safemode forceExit。 安全模式也可以手动执行命令进入，同时也只能手动关闭。</td>
</tr>
<tr>
  <td>-saveNamespace</td>
  <td align="center">将当前命名空间保存到存储目录并重置edits log。该命令需要安全模式。</td>
</tr>
<tr>
  <td>-rollEdits</td>
  <td align="center">在活动的NameNode上滚动编辑日志。</td>
</tr>
<tr>
  <td>-restoreFailedStorage true|false|check</td>
  <td align="center">该选项将打开/关闭自动尝试恢复失败的存储副本。如果失败的存储器变为可用状态，则系统将在检查点期间尝试恢复编辑fsimage文件。’check’选项将返回当前设置。</td>
</tr>
<tr>
  <td>-refreshNodes</td>
  <td align="center">重新读取主机列列表、排除列表以更新允许连接到Namenode的Datanode以及那些应该停用或重试的Datanode</td>
</tr>
<tr>
  <td>-setQuota  …</td>
  <td align="center">有关详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html#Administrative_Commands" rel="nofollow">HDFS Quotas Guide</a></td>
</tr>
<tr>
  <td>-clrQuota …</td>
  <td align="center">有关详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html#Administrative_Commands" rel="nofollow">HDFS Quotas Guide</a></td>
</tr>
<tr>
  <td>-setSpaceQuota  [-storageType ] …</td>
  <td align="center">有关详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html#Administrative_Commands" rel="nofollow">HDFS Quotas Guide</a></td>
</tr>
<tr>
  <td>-clrSpaceQuota [-storageType ] …</td>
  <td align="center">有关详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html#Administrative_Commands" rel="nofollow">HDFS Quotas Guide</a></td>
</tr>
<tr>
  <td>-finalizeUpgrade</td>
  <td align="center">完成HDFS的升级。 Datanodes删除他们以前的版本工作目录，然后Namenode做相同的工作。致辞完成了升级的过程。</td>
</tr>
<tr>
  <td>-rollingUpgrade [||]</td>
  <td align="center">有关详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html#dfsadmin_-rollingUpgrade" rel="nofollow">Rolling Upgrade document</a></td>
</tr>
<tr>
  <td>-refreshServiceAcl</td>
  <td align="center">重新加载服务级别授权策略文件。</td>
</tr>
<tr>
  <td>-refreshUserToGroupsMappings</td>
  <td align="center">刷新用户到组映射。</td>
</tr>
<tr>
  <td>-refreshSuperUserGroupsConfiguration</td>
  <td align="center">刷新超级用户代理组映射</td>
</tr>
<tr>
  <td>-refreshCallQueue</td>
  <td align="center">从配置重新加载调用队列。</td>
</tr>
<tr>
  <td>-refresh   [arg1..argn]</td>
  <td align="center">运行时刷新触发上由指定的资源。所有其他参数都发送给指定主机。</td>
</tr>
<tr>
  <td>-reconfig   </td></tr></tbody></table>

<h2 id="haadmin">haadmin</h2>

<p>用法：</p>

<pre><code>hdfs haadmin -transitionToActive &lt;serviceId&gt; [--forceactive]
hdfs haadmin -transitionToStandby &lt;serviceId&gt;
hdfs haadmin -failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;
hdfs haadmin -getServiceState &lt;serviceId&gt;
hdfs haadmin -getAllServiceState
hdfs haadmin -checkHealth &lt;serviceId&gt;
hdfs haadmin -help &lt;command&gt;
</code></pre>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-checkHealth</td>
  <td align="center">检查给定NameNode的健康状况</td>
</tr>
<tr>
  <td>-failover</td>
  <td align="center">在两个NameNode之间启动故障转移</td>
</tr>
<tr>
  <td>-getServiceState</td>
  <td align="center">确定给定的NameNode是Active还是Standby</td>
</tr>
<tr>
  <td>-getAllServiceState</td>
  <td align="center">返回所有NameNode的状态</td>
</tr>
<tr>
  <td>-transitionToActive</td>
  <td align="center">将给定NameNode的状态转换为Active（警告：fencing还未完成）</td>
</tr>
<tr>
  <td>-transitionToStandby</td>
  <td align="center">将给定NameNode的状态转换为Standby（警告：fencing还未完成）</td>
</tr>
<tr>
  <td>-help [cmd]</td>
  <td align="center">如果没有指定，则显示给定命令或所有命令的帮助信息。</td>
</tr>
</tbody></table>


<p>有关此命令的更多信息，请参见使用NFS的<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html#Administrative_commands" rel="nofollow">HDFS HA</a>和 <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#Administrative_commands" rel="nofollow">HDFS HA with QJM </a>。</p>



<h2 id="journalnode">journalnode</h2>

<p>用法：<code>hdfs journalnode</code></p>

<p>这个命令会启动<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#Administrative_commands" rel="nofollow">HDFS HA with QJM</a>。</p>



<h2 id="mover">mover</h2>

<p>用法：<code>hdfs mover [-p &lt;files/dirs&gt; | -f &lt;local file name&gt;]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-f </td>
  <td align="center">指定一个包含要迁移的HDFS文件/目录列表的本地文件。</td>
</tr>
<tr>
  <td>-p </td>
  <td align="center">指定要迁移的HDFS文件/目录列表，使用空格作为分隔符。</td>
</tr>
</tbody></table>


<p>运行数据迁移实用程序。有关更多详情，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html#Mover_-_A_New_Data_Migration_Tool" rel="nofollow">Mover</a>。</p>

<p>请注意，如果省略-p和-f选项，则默认路径是根目录。</p>

<p>此外，从2.7.0开始引入了一个固定功能，以防止某些副本被平衡器/移动器移动。此引脚功能在默认情况下处于禁用状态，可通过配置属性“dfs.datanode.block-pinning.enabled”启用。启用后，此功能只影响<code>create（）</code>方法调用中指定的节点的块。当我们想要维护数据局部性时，这个特性非常有用，适用于HBase regionserver等应用程序。</p>

<h2 id="namenode">namenode</h2>

<pre><code>hdfs namenode [-backup] |
          [-checkpoint] |
          [-format [-clusterid cid ] [-force] [-nonInteractive] ] |
          [-upgrade [-clusterid cid] [-renameReserved&lt;k-v pairs&gt;] ] |
          [-upgradeOnly [-clusterid cid] [-renameReserved&lt;k-v pairs&gt;] ] |
          [-rollback] |
          [-rollingUpgrade &lt;rollback|downgrade |started&gt; ] |
          [-finalize] |
          [-importCheckpoint] |
          [-initializeSharedEdits] |
          [-bootstrapStandby [-force] [-nonInteractive] [-skipSharedEditsCheck] ] |
          [-recover [-force] ] |
          [-metadataVersion ]
</code></pre>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-backup</td>
  <td align="center">启动备份节点</td>
</tr>
<tr>
  <td>-checkpoint</td>
  <td align="center">启动检查点节点</td>
</tr>
<tr>
  <td>-format [-clusterid cid]</td>
  <td align="center">格式化指定的NameNode。它会启动NameNode，对其进行格式化，最后关闭它。如果目录dir已经存在，并且群集禁用了重新格式化，将抛出NameNodeFormatException。</td>
</tr>
<tr>
  <td>-upgrade [-clusterid cid] [-renameReserved ]</td>
  <td align="center">升级指定的NameNode，然后关闭它。</td>
</tr>
<tr>
  <td>-upgradeOnly [-clusterid cid] [-renameReserved ]</td>
  <td align="center">在发布新的Hadoop版本后，Namenode应该以升级选项启动。</td>
</tr>
<tr>
  <td>-rollback</td>
  <td align="center">将NameNode回滚到以前的版本。这应该在停止集群并分发旧的Hadoop版本后再使用。</td>
</tr>
<tr>
  <td>-rollingUpgrade </td>
  <td align="center">有关详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html#NameNode_Startup_Options" rel="nofollow">Rolling Upgrade document</a></td>
</tr>
<tr>
  <td>-finalize</td>
  <td align="center">不再支持该命令。改为使用<code>dfsadmin -finalizeUpgrade</code>。</td>
</tr>
<tr>
  <td>-importCheckpoint</td>
  <td align="center">从检查点目录加载image文件并将其保存到当前目录中。检查点目录是从属性dfs.namenode.checkpoint.dir中读取的</td>
</tr>
<tr>
  <td>-initializeSharedEdits</td>
  <td align="center">格式化新的共享编辑目录并复制足够的编辑日志段，以便备用NameNode可以启动。</td>
</tr>
<tr>
  <td>-bootstrapStandby [-force] [-nonInteractive] [-skipSharedEditsCheck]</td>
  <td align="center">允许通过复制活动NameNode中的最新命名空间快照来引导备用NameNode的存储目录。这在首次配置HA群集时使用。选项-force或-nonInteractive的含义与namenode -format命令中所述的含义相同。-skipSharedEditsCheck选项跳过编辑检查，确保在共享目录中已有足够的编辑可以从活动上的最后一个检查点启动。</td>
</tr>
<tr>
  <td>-recover [-force]</td>
  <td align="center">在损坏的文件系统上恢复丢失的元数据。详情请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Recovery_Mode" rel="nofollow">HDFS用户指南</a>。</td>
</tr>
<tr>
  <td>-metadataVersion</td>
  <td align="center">验证配置的目录是否存在，然后打印软件和image的元数据版本信息。</td>
</tr>
</tbody></table>


<p>运行namenode。有关升级，回滚和完成的更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback" rel="nofollow">Upgrade Rollback</a>。</p>



<h2 id="nfs3">nfs3</h2>

<p>用法:<code>hdfs nfs3</code></p>

<p>这个命令会启动NFS3点网关以便与<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html#Start_and_stop_NFS_gateway_service" rel="nofollow">HDFS NFS3服务</a>一起使用。</p>



<h2 id="portmap">portmap</h2>

<p>用法:<code>hdfs portmap</code></p>

<p>这命令会启动RPC portmap用于<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html#Start_and_stop_NFS_gateway_service" rel="nofollow">HDFS NFS3服务</a></p>



<h2 id="secondarynamenode">secondarynamenode</h2>

<p>用法: <code>hdfs secondarynamenode [-checkpoint [force]] | [-format] | [-geteditsize]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-checkpoint [force]</td>
  <td align="center">如果EditLog大小&gt; = fs.checkpoint.size，则Checkpoint SecondaryNameNode。如果使用强制，则不考虑EditLog大小。</td>
</tr>
<tr>
  <td>-format</td>
  <td align="center">在启动期间格式化本地存储。</td>
</tr>
<tr>
  <td>-geteditsize</td>
  <td align="center">在NameNode上打印未勾选事务的数量。</td>
</tr>
</tbody></table>


<p>运行HDFS secondary namenode. 更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Secondary_NameNode" rel="nofollow">Secondary Namenode</a>.</p>

<h2 id="storagepolicies">storagepolicies</h2>

<p>用法：</p>

<pre><code>hdfs storagepolicies
      [-listPolicies]
      [-setStoragePolicy -path &lt;path&gt; -policy &lt;policy&gt;]
      [-getStoragePolicy -path &lt;path&gt;]
      [-unsetStoragePolicy -path &lt;path&gt;]
      [-help &lt;command-name&gt;]
</code></pre>

<p>列出 all/Gets/sets/unsets 存储策略。有关更多信息，请参阅<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html" rel="nofollow">HDFS存储策略文档</a>。</p>



<h2 id="zkfc">zkfc</h2>

<p>用法: <code>hdfs zkfc [-formatZK [-force] [-nonInteractive]]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-formatZK</td>
  <td align="center">格式化Zookeeper实例。 <code>-force</code>：如果znode存在，则格式化znode。 <code>-nonInteractive</code>：如果znode存在，则格式化znode中止，除非指定了<code>-force</code>选项。</td>
</tr>
<tr>
  <td>-h</td>
  <td align="center">显示帮助信息</td>
</tr>
</tbody></table>


<p>该命令会启动一个Zookeeper Failover Controller进程，以便与<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#Administrative_commands" rel="nofollow">HDFS HA with QJM</a>一起使用。</p>



<h1 id="4调试命令">4、调试命令</h1>

<p>该组命令终于帮助管理员调试HDFS中出现的问题。这些命令仅适用于高级用户。</p>



<h2 id="verifymeta">verifyMeta</h2>

<p>用法: <code>hdfs debug verifyMeta -meta &lt;metadata-file&gt; [-block &lt;block-file&gt;]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-block block-file</td>
  <td align="center">可选参数，用于指定数据节点本地文件系统上块文件的绝对路径。</td>
</tr>
<tr>
  <td>-meta metadata-file</td>
  <td align="center">数据节点的本地文件系统上元数据文件的绝对路径。</td>
</tr>
</tbody></table>


<p>验证HDFS元数据和块文件。如果指定了块文件，将验证元数据文件中的校验和是否与块文件匹配。</p>



<h2 id="computemeta">computeMeta</h2>

<p>用法: <code>hdfs debug computeMeta -block &lt;block-file&gt; -out &lt;output-metadata-file&gt;</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>-block block-file</td>
  <td align="center">数据节点本地文件系统上块文件的绝对路径。</td>
</tr>
<tr>
  <td>-out output-metadata-file</td>
  <td align="center">输出元数据文件的绝对路径，用于存储块文件的校验和计算结果。</td>
</tr>
</tbody></table>


<p>从块文件中计算HDFS元数据。如果指定了块文件，将计算块文件中的校验和，并将其保存到指定的输出元数据文件中。</p>

<p>意：使用风险该命令存在风险！如果块文件损坏并覆盖它的元文件，它将在HDFS中显示为“良好”，但无法读取数据。只能用作最后一项措施时进行使用，并且当您100％确定该块文件良好时。</p>



<h2 id="recoverlease">recoverLease</h2>

<p>用法: <code>hdfs debug recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;]</code></p>

<table>
<thead>
<tr>
  <th>命名选项</th>
  <th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
  <td>[-path path]</td>
  <td align="center">要为其恢复租约的HDFS路径</td>
</tr>
<tr>
  <td>[-retries num-retries]</td>
  <td align="center">客户端将重试调用recoverLease的次数。默认的重试次数是1</td>
</tr>
</tbody></table>


<p>在指定的路径上恢复租约。路径必须驻留在HDFS文件系统上。默认的重试次数是1。</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>