---
layout:     post
title:      Spark Streaming初探
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>Spark Streaming初探</h1>
<p>Spark Streaming是一个基于Spark核心的流式计算的扩展。</p>
<p>主要有以下两个特点：</p>
<p>1. 高吞吐量</p>
<p>2. 容错能力强</p>
<h2>1.原理：</h2>
<p>Spark Streaming支持多种数据源的输入，向Flume,Kafka,HDFS,ZeroMQ,Twitter以及原始的TCP <span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:22.399999618530273px;">sockets。</span></p>
<p>数据可以使用Spark的RDD的Transformation，也可以应用很多Spark内置的机器学习算法，还有图计算。</p>
<p>以下是Spark官方截图：</p>
<p><img src="https://img-blog.csdn.net/20140414172003796?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb29wc29vbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></p>
<p>1.1主要实现原理</p>
<p>Spark Streaming会接收线上的数据流，然后将数据流分成独立的批次。</p>
<p>然后Spark Streaming Engine就会分别处理这些独立的批数据，最后生成分批的结果。</p>
<p><img src="https://img-blog.csdn.net/20140414172008609?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb29wc29vbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></p>
<p>Spark Streaming 提出了一种高度的抽象叫 <span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:22.399999618530273px;"><strong>DStream（</strong></span><span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:22.399999618530273px;">discretized
 stream）离散流，</span>代表了一段持续的数据流。</p>
<p>创建DStream可以从文件创建，也可以从Kafka或者Flume，代表了一个RDD的序列。</p>
<p><br></p>
<h2>2.WorkCount流式计算实例</h2>
<div>摘自官方例子的代码：</div>
<div><pre><code class="language-java">package org.apache.spark.streaming.examples

import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.storage.StorageLevel

object NetworkWordCount {
  def main(args: Array[String]) {
    if (args.length &lt; 3) {
      System.err.println("Usage: NetworkWordCount &lt;master&gt; &lt;hostname&gt; &lt;port&gt;\n" +
        "In local mode, &lt;master&gt; should be 'local[n]' with n &gt; 1")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(args(0), "NetworkWordCount", Seconds(1),
      System.getenv("SPARK_HOME"), StreamingContext.jarOfClass(this.getClass))

    // Create a NetworkInputDStream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val lines = ssc.socketTextStream(args(1), args(2).toInt, StorageLevel.MEMORY_ONLY_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}</code></pre><br>
首先我们需要建立一个发送数据的端口连接：</div>
<div><pre><code class="language-java">nc -lk 9999</code></pre>进程会阻塞，当我们输入数据的时候，会向9999这个端口发送我们的数据。</div>
<div><br></div>
<div>首先我们先创建一个Spark Streaming engine</div>
<div>现在我们需要Spark Streaming Engine来监听这个端口。</div>
<div><pre><code class="language-java"> val ssc = new StreamingContext(args(0), "NetworkWordCount", Seconds(1),
      System.getenv("SPARK_HOME"), StreamingContext.jarOfClass(this.getClass))</code></pre><br><br></div>
<div>这里args(1)就是ip, args(2)就是port</div>
<div>我们看到这里实践是创建了一个socketTextStream，其实还有很多选择。</div>
<div>StorageLevel是Memory_only，很明显，创建的是一个RDD</div>
<div><pre><code class="language-java">val lines = ssc.socketTextStream(args(1), args(2).toInt, StorageLevel.MEMORY_ONLY_SER)</code></pre><br>
WordCount的逻辑很简单，这里不赘述。</div>
<div>最后要启动Spark Streaming要使用这句：</div>
<div><pre><code class="language-java">ssc.start()
ssc.awaitTermination()</code></pre>
<div><br></div>
<div>运行结果：</div>
<div>1.在终端输入</div>
<div><pre><code class="language-java"># nc -lk 9999
what is your name ? 
my name is sheng li ~ haha</code></pre>
<pre></pre>
<pre></pre>
<pre></pre>
</div>
<div><pre><code class="language-java">-------------------------------------------
Time: 1397470893000 ms
-------------------------------------------
(is,1)
(what,1)
(your,1)
(?,1)
(name,1)

-------------------------------------------
Time: 1397470894000 ms</code></pre><br><pre><code class="language-java">-------------------------------------------
Time: 1397470962000 ms
-------------------------------------------
(haha,1)
(my,1)
(is,1)
(~,1)
(li,1)
(sheng,1)
(name,1)</code></pre></div>
<div><br></div>
<div>大致流程：</div>
1.Spark Streaming Engine 监听9999这个端口发送的信息，当作数据源，其实是一个RDD。<br>
2.这里会有一个lines是一个DStream，是一个不可变的分布式弹性数据集。</div>
<div>lines实际上是带一个带版本的RDD，每一个时刻它代表的是不同的数据，比如第一个输入的是what is your name,那么这个时刻Lines代表的是这句话。</div>
<div>第二次输入的是my name is shengli ~ haha 这是另一个时刻的Lines。</div>
<div>3.DStream可以应用和RDD一样的API，其中Transformation变形成其它的RDD。</div>
<div>这里会变成Words DStream。</div>
<p>4.最后计算完成，输出wordCounts.print()。</p>
<p><br></p>
<p>总结：</p>
<p>实际上每次的input都是一个RDD@time N</p>
<p>每个RDD@time N 都可以被transform成其它的RDD进行处理。</p>
<p>看起来相当简单。</p>
<p><br></p>
<p>再来看下官方的解释图，会很好的理解：</p>
<p><img src="https://img-blog.csdn.net/20140414182347328?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb29wc29vbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></p>
<p><br></p>
<p><img src="https://img-blog.csdn.net/20140414182352437?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvb29wc29vbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></p>
<p>参考文献：<a href="http://spark.apache.org/docs/0.9.1/streaming-programming-guide.html" rel="nofollow">http://spark.apache.org/docs/0.9.1/streaming-programming-guide.html</a></p>
<p><br></p>
<p>原创文章，转载注明出处<a href="http://blog.csdn.net/oopsoom/article/details/23692079" rel="nofollow">http://blog.csdn.net/oopsoom/article/details/23692079</a></p>
            </div>
                </div>