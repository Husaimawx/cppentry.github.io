---
layout:     post
title:      Linux--hadoop--jupyter-----27
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>hadoop fs -mkdir -p /user/hadoop  创建一个目录</p><p>hadoop fs -rm-r /user/hadoop  删除目录</p><p>hadoop fs -put data.txt  上传文件</p><p>hadoop fs -cat  data.txt   查看文件</p><p>exit（） 退出<br>source 更新</p><p>start-dfs.sh 启动hadoop   三个Node</p><p>start-yarn.sh   两个Manager</p><p>jupyter notebook --ip python5  虚拟机里退出jupyter   Ctrl+c</p><p>在丘比特网址写代码，先运行环境代码</p><p>import os<br>import sys<br>spark_home = os.environ.get('SPARK_HOME',None)<br>if not spark_home:<br>    raise ValueError('SPARK_HOME enviroment variable is not set')<br>sys.path.insert(0,os.path.join(spark_home,'python'))<br>sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.10.4-src.zip'))<br>exec(open(os.path.join(spark_home,'python/pyspark/shell.py')).read())<br></p><p>等出现welcome to 就OK了，就可以在下面的框里面写代码了</p><div><br></div><br><p>map 将数据打散成你想要的格式</p><p>reduce 根据键值对进行统计</p><p>hadoop 存贮数据  MapReduce分析数据</p><p>hadoop是建立在yarn（集群）上</p><p>hadoop做Wordcount</p><p><span style="background-color:rgb(51,204,255);"><span style="color:#cc0000;">用hadoop自带的MapReduce做数据分析</span></span></p><p><span style="background-color:rgb(255,255,204);">在hadoop根路径进文件</span></p><p><span style="background-color:rgb(255,255,204);">/hadoop-2.7.6/share/hadoop/mapreduce/有一个jar包  hadoop-mapreduce-examples-2.7.6.jar<br></span></p><p><span style="background-color:rgb(255,255,204);">**上传文件  hadoop fs -put 文件名   如果不在根目录下，需要在文件名前加~/<br></span></p><p><span style="background-color:rgb(255,255,204);">hadoop jar hadoop-mapreduce-examples-2.7.6.jar wordcount /user/hadoop/data.txt /user/output<br>（通过调用上面的jar包 单词统计这个文件  将结果放在这个文件里）<br></span></p><p><span style="background-color:rgb(255,255,204);">通过命令查看hadoop fs -cat /user/output/part-r-00000</span></p><p>在hadoop上输spark-shell命令 出来Scala界面，(scala和hadoop都是java为基础)<br>输出一句话是println（）<br>区分大小写:类名第一个字母大写；方法名称第一个字母小写 <br>注释 /**/   //<br>面向行的语言，以；结束或换行。<br>常量是 val    变量var  声明时要给出数据类型<br>var myVar ：String=“foo”<br></p><p>变量可以修改值 ，常量不能修改值</p><p>循环<br>if（布尔表达式）<br>while循环<br>do……while循环<br>for循环<br>&lt;-  相当于Python循环中的in<br>1 to 10 是全输出<br>1 until 10  不输出10<br></p><p><br>Scala写法<br>var lines=sc.textFile("hdfs://python5:9000/user/hadoop/data.txt")这是文件的路径<br>定义变量<br></p><p>浏览器 http：//python5：8088/cluster</p><p>拆分：<br>map本身带循环<br>lines.map（x=&gt;x.split("  ").collect()）<br><br>lines.map（x=&gt;x.split("  ")）.map(x=&gt;for(i &lt;- x)println(i)).collect()<br>将文件每个单词以空格分开输出<br><br>lines.map（x=&gt;x.split("  ")）.flatMap(x=&gt;for(i &lt;- x)yield（i，1）).collect()<br></p><p>将上面的三个数组 整合成一个数组</p><p>lines.map（x=&gt;x.split("  ")）.flatMap(x=&gt;for(i &lt;- x)yield（i，1）).groupByKey（）.collect()</p>只是键分组，并没统计<br><br>lines.map(x=&gt;x.split(' ')）.flatMap(x=&gt;for(i &lt;- x)yield（i，1）).groupByKey（）.map（x=&gt;println(x._1,x._2.sum)).collect()<br>x._1第一个元素    在第二个时调用sum，可以统计第一个元素出现次数<br><br>lines.map（x=&gt;x.split("  ")）.flatMap(x=&gt;for(i &lt;- x)yield（i，1）).reduceByKey((x,y）=&gt;x+y).collect()<br><p>通过 reduceByKey wordcount</p>            </div>
                </div>