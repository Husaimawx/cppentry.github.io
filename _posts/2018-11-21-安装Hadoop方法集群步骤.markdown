---
layout:     post
title:      安装Hadoop方法集群步骤
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h2>       在Linux安装hadoop。。。。。 。</h2>

<h3>   我解压的hadoop2.7.3（更名为hadoop）文件路径是在 /home/Mcwang/soft/hadoop</h3>

<h3>    我们要修改的配置文件是在：/home/Mcwang /hadoop/etc/hadoop下的文件</h3>

<h2>上传并解压缩</h2>

<p>        上传到/home/Mcwang/soft<br>
    tar -zvxf  hadoop-2.7.3tar.gz <br>
    mv hadoop-2.7.3 hadoop  &gt;&gt;将解压的文件更名为hadoop<br>
    ln -s hadoop hadoop.soft   //建立解压文件的软链接<br>
    rm hadoop-2.7.3.tar.gz         //可以删除这个压缩包<br>
    su root<br>
    vi /etc/profile    //需要在root用户下配置文件<br>
配置如下：</p>

<p>    <img alt="" class="has" src="https://img-blog.csdn.net/20180514135344718?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>    然后执行：source /etc/profile              //使之生效，</p>

<p>    <img alt="" class="has" src="https://img-blog.csdn.net/20180514135714546?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h3>    进行测试一下： hdfs / hadoop 之一</h3>

<p>    <img alt="" class="has" src="https://img-blog.csdn.net/20180514140154356?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>    cd hadoop           //hadoop下新建三个文件夹</p>

<p>             mkdir dfs dfs/name dfs/data<br>
             mkdur tmp<br>
    <img alt="" class="has" src="https://img-blog.csdn.net/20180514154744953?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>    接下来修改配置文件：七个 （第七个文件我们需要copy一个文件 并且修改）</p>

<p>    我们要修改的配置文件是在：/soft /hadoop/etc/hadoop下的文件</p>

<p>     <span style="color:#ff0000;">   注意：上传文件的时候，一定要在普通用户下（如Mcwang）上传，否则修改文件的时候要求在root用户下<br>
修改，以后操作起来也是不方便，所以最好不要在root下安装程序。（安装软件的命令有些只能root使用）</span></p>

<p>        <img alt="" class="has" src="https://img-blog.csdn.net/20180514144226510?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h3>    修改是在[Mcwang@master hadoop]下执行：   </h3>

<p>     前提：上传Hadoop的时候一定要在普通用户Mcwang下上传，不然修改这七个文件需要root权限!</p>

<p>        <span style="color:#ff0000;">      1.修改vi core-site.xml    --tmp路径，master</span></p>

<p>     <span style="color:#000000;">   </span><span style="color:#000000;">  &lt;!-- Put site-specific property overrides in this file. --&gt;<br>
&lt;configuration&gt;<br>
         &lt;property&gt;<br>
             &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>
             &lt;value&gt;file:/usr/hadoop/tmp&lt;/value&gt;<br>
     &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;<br>
     &lt;/property&gt;<br>
&lt;property&gt;<br>
             &lt;name&gt;fs.defaultFS&lt;/name&gt;<br>
                 &lt;value&gt;hdfs://master:9000&lt;/value&gt;<br>
     &lt;/property&gt;<br>
    &lt;/configuration&gt;</span><br><img alt="" class="has" src="https://img-blog.csdn.net/20180515174221196?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><span style="color:#ff0000;">2.修改hadoop-env.sh   --安装jdk的路径</span></p>

<p>       <span style="color:#000000;">#export JAVA_HOME=${JAVA_HOME}<br>
    export JAVA_HOME=/home/Mcwang/soft/jdk.soft<br><img alt="" class="has" src="https://img-blog.csdn.net/201805141555384?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#ff0000;"> 3.修改hdfs-site.xml    --master,  /dfs/name和/dfs/data路径</span></p>

<p><span style="color:#000000;">   <span style="color:#000000;">&lt;configuration&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;<br>
&lt;value&gt;Master:50090&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
             &lt;name&gt;dfs.replication&lt;/name&gt;<br>
&lt;value&gt;1&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
         &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br>
         &lt;value&gt;file:/usr/hadoop/tmp/dfs/name&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br>
&lt;value&gt;file:/usr/hadoop/tmp/dfs/data&lt;/value&gt;<br>
&lt;/property&gt;<br>
  &lt;/configuration&gt;</span><br><img alt="" class="has" src="https://img-blog.csdn.net/2018051610255481?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#ff0000;">4.修改mapred-site.xml     --先复制，master（2个）</span></p>

<p>    <img alt="" class="has" src="https://img-blog.csdn.net/20180514162448266?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><span style="color:#000000;">改一下master<br>
&lt;configuration&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>
&lt;value&gt;yarn&lt;/value&gt;<br>
    &lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br>
&lt;value&gt;master:10020&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;<br>
&lt;value&gt;master:19888&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;/configuration&gt;<br><img alt="" class="has" src="https://img-blog.csdn.net/20180514162959626?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#000000;">  <span style="color:#ff0000;"> 5.修改vi yarn-site.xml --只需要修改Master-master 1</span></span></p>

<h2><span style="color:#000000;">    <span style="color:#000000;">&lt;configuration&gt; </span></span></h2>

<p><span style="color:#000000;"><span style="color:#000000;">    &lt;!-- Site specific YARN configuration properties --&gt;<br>
     &lt;property&gt;<br>
         &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>
     &lt;value&gt;master&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
     &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>
     &lt;/property&gt;<br>
&lt;/configuration&gt;</span><br>
    <img alt="" class="has" src="https://img-blog.csdn.net/20180514163529726?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<h3><span style="color:#000000;">  <span style="color:#ff0000;">  6.修改 vi slaves --先删除，后添加slave1,slave2</span></span></h3>

<p><span style="color:#000000;">    <img alt="" class="has" src="https://img-blog.csdn.net/2018051416375299?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p>        <img alt="" class="has" src="https://img-blog.csdn.net/20180514163859520?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>    <span style="color:#ff0000;"> 7.修改 vi yarn-env.sh   --jdk.soft的路径</span></p>

<p>        <img alt="" class="has" src="https://img-blog.csdn.net/20180514192416535?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"> </p>

<p>      <span style="color:#ff0000;">  添加：export JAVA_HOME=/home/Mcwang/soft/jdk.soft</span></p>

<h3><span style="color:#ff0000;">启动hadoop</span></h3>

<p>        因为master是namenode,slave1和slave2都是datanode,所以只需对master进行初始化，也就是<br>
对hdfs进行初始化</p>

<p><span style="color:#ff0000;">1 .打包文件夹 /soft/hadoop ,复制到 datanode 节点机(指的是slave1，slave2)，保证节点机环境配置<br>
与master保持一致格式化文件系统.（将hadoop.tar.gz打包scp到其他机器）<br><br><img alt="" class="has" src="https://img-blog.csdn.net/20180515142913448?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"><br>
    在最下面会出现：格式化成功，</span><br><img alt="" class="has" src="https://img-blog.csdn.net/20180515181838874?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>2.启动文件服务 -全部<br>
    start-all.sh  //无论哪个文件下</p>

<p>   <img alt="" class="has" src="https://img-blog.csdn.net/20180516141641886?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"><br>
slave1：<br>
    satrt-all.sh 需要输入多次密码 由于没有免密，所以需要多次输入密码<br>
slave2：<br>
    start-all.sh<br>
 </p>

<p>3分别在各个主机上执行 jps 查看服务情况</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180516141930927?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180518124029704?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180518124104471?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>4.如果要在Windows下的浏览器上访问hadoop <br>
需要进行以下更改：<br>
修改文件：C:/Windows/System32/drivers/etc/hosts<br>
在文件中添加键值对：<br>
192.168.29.136 master<br>
192.168.29.137 slave1</p>

<p>192.138.29.138 slave2</p>

<p>http://master:50070</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180518123947696?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW96ZWx1bHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p> </p>

<p> </p>

<p> </p>

<p> </p>

<p> </p>            </div>
                </div>