---
layout:     post
title:      HDFS设计思路，HDFS使用，查看集群状态，HDFS，HDFS上传文件，HDFS下载文件，yarn web管理界面信息查看，运行一个mapreduce程序，mapreduce的demo
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/toto1297488504/article/details/72802439				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1>26 集群使用初步</h1><div><p>HDFS的设计思路</p><p>l 设计思想   </p><p>分而治之：将大文件、大批量文件，分布式存放在大量服务器上，<strong>以便于采取分而治之的方式对海量数据进行运算分析；</strong></p><p> </p><p>l 在大数据系统中作用：</p><p>为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务</p><p> </p><p>l 重点概念：文件切块，副本存放，元数据</p><br></div><h2>26.1 HDFS使用</h2><p>1、查看集群状态</p><p>命令：   hdfs  dfsadmin –report </p><p><img src="https://img-blog.csdn.net/20170529154718971?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p>可以看出，集群共有3个datanode可用</p><p>也可打开web控制台查看HDFS集群信息，在浏览器打开<a href="http://hadoop:50070/" rel="nofollow">http://hadoop:50070/</a></p><p><img src="https://img-blog.csdn.net/20170529154741164?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p>2、上传文件到HDFS</p><p>查看HDFS中的目录信息</p><p>命令：hadoop  fs  –ls  /</p><p><img src="https://img-blog.csdn.net/20170529154808129?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p> </p><p>上传文件</p><p>命令:hadoop fs -put ./<span style="color:#FF0000;">findbugs-1.3.9</span> /</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p>[toto@hadoop software]$ <span style="color:#FF0000;">hadoop fs -put ./findbugs-1.3.9 /</span></p>  <p>put: `/findbugs-1.3.9/LICENSE-ASM.txt':  File exists</p>  <p>put:  `/findbugs-1.3.9/LICENSE-AppleJavaExtensions.txt': File exists</p>  <p>put: `/findbugs-1.3.9/LICENSE-bcel.txt':  File exists</p>  <p>put:  `/findbugs-1.3.9/LICENSE-commons-lang.txt': File exists</p>  <p>put:  `/findbugs-1.3.9/LICENSE-docbook.txt': File exists</p>  <p>put: `/findbugs-1.3.9/LICENSE-dom4j.txt':  File exists</p>  <p>put:  `/findbugs-1.3.9/LICENSE-jFormatString.txt': File exists</p></td> </tr></tbody></table><p>查看上传后的信息列表（<span style="color:#FF0000;">hadoop fs –ls /</span> 或 <span style="color:#FF0000;">hadoop fs -ls /findbugs-1.3.9</span>）</p><p><img src="https://img-blog.csdn.net/20170529154832786?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p> </p><p>从HDFS下载文件</p><p>命令：<span style="color:#FF0000;">hadoop fs -get /findbugs-1.3.9/LICENSE-ASM.txt</span></p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p>[toto@hadoop learn]$ <span style="color:#FF0000;">cd /home/toto/learn</span></p>  <p>/home/toto/learn</p>  <p>[toto@hadoop learn]$ <span style="color:#FF0000;">pwd</span></p>  <p>/home/toto/learn</p>  <p>[toto@hadoop learn]$ <span style="color:#FF0000;">hadoop fs -get /findbugs-1.3.9/LICENSE-ASM.txt</span></p>  <p>[toto@hadoop learn]$<span style="color:#FF0000;"> ls</span></p>  <p><span style="color:#FF0000;">LICENSE-ASM.txt</span></p></td> </tr></tbody></table><p> </p><p>yarn的管理界面是：http://hadoop:8088/cluster</p><p><img src="https://img-blog.csdn.net/20170529154851239?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p> </p><h2>26.2模拟运行一个mapreduce程序</h2><p>模拟运行一个mapreduce程序的时候，需要先启动hdfs,启动命令是：</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p><span style="color:#555555;">[toto@hadoop1 hadoop-2.8.0]$</span><span style="color:#FF0000;">cd /home/toto/software/hadoop-2.8.0</span></p>  <p><span style="color:#555555;">[toto@hadoop1 hadoop-2.8.0]$</span><span style="color:#FF0000;">sbin/start-dfs.sh</span></p></td> </tr></tbody></table><p>在/home/toto/software/hadoop-2.8.0/share/hadoop/mapreduce下有一个mapreduce的运行例子：</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p align="left">[toto@hadoop  mapreduce]$ <span style="color:#FF0000;">cd /home/toto/software/hadoop-2.8.0/share/hadoop/mapreduce</span></p>  <p>[toto@hadoop mapreduce]$ <span style="color:#FF0000;">pwd</span></p>  <p>/home/toto/software/hadoop-2.8.0/share/hadoop/mapreduce</p>  <p>[toto@hadoop mapreduce]$ <span style="color:#FF0000;">ll</span></p>  <p>总用量 5088</p>  <p>-rw-r--r--. 1 toto hadoop  562900 3月  17 13:31  hadoop-mapreduce-client-app-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop  782739 3月  17 13:31 hadoop-mapreduce-client-common-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop 1571179 3月  17 13:31  hadoop-mapreduce-client-core-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop  195000 3月  17 13:31  hadoop-mapreduce-client-hs-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop   31533 3月  17 13:31 hadoop-mapreduce-client-hs-plugins-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop   66999 3月  17 13:31  hadoop-mapreduce-client-jobclient-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop 1587158 3月  17 13:31  hadoop-mapreduce-client-jobclient-2.8.0-tests.jar</p>  <p>-rw-r--r--. 1 toto hadoop   75495 3月  17 13:31  hadoop-mapreduce-client-shuffle-2.8.0.jar</p>  <p>-rw-r--r--. 1 toto hadoop  301934 3月  17 13:31<span style="color:#FF0000;">  hadoop-mapreduce-examples-2.8.0.jar</span></p>  <p>drwxr-xr-x. 2 toto hadoop    4096 3月  17 13:31 jdiff</p>  <p>drwxr-xr-x. 2 toto hadoop    4096 3月  17 13:31 lib</p>  <p>drwxr-xr-x. 2 toto hadoop    4096 3月  17 13:31 lib-examples</p>  <p>drwxr-xr-x. 2 toto hadoop    4096 3月  17 13:31 sources</p>  <p>[toto@hadoop mapreduce]$</p>  <p> </p>  <p>使用命令运行mapreduce命令：</p>  <p>[toto@hadoop mapreduce]$ <span style="color:#FF0000;">hadoop jar hadoop-mapreduce-examples-2.8.0.jar pi 5 5</span></p>  <p>Number of Maps  = 5</p>  <p>Samples per Map = 5</p>  <p>Wrote input for Map #0</p>  <p>Wrote input for Map #1</p>  <p>Wrote input for Map #2</p>  <p>Wrote input for Map #3</p>  <p>Wrote input for Map #4</p>  <p>Starting Job</p>  <p>17/05/29 14:47:36 INFO client.RMProxy:  Connecting to ResourceManager at hadoop/192.168.106.80:8032</p>  <p>17/05/29 14:47:37 INFO  input.FileInputFormat: Total input files to process : 5</p>  <p>17/05/29 14:47:37 INFO  mapreduce.JobSubmitter: number of splits:5</p>  <p>17/05/29 14:47:38 INFO  mapreduce.JobSubmitter: Submitting tokens for job: job_1495998405307_0001</p>  <p>17/05/29 14:47:39 INFO  impl.YarnClientImpl: Submitted application application_1495998405307_0001</p>  <p>17/05/29 14:47:39 INFO mapreduce.Job: The  url to track the job: http://hadoop:8088/proxy/application_1495998405307_0001/</p>  <p>17/05/29 14:47:39 INFO mapreduce.Job:  Running job: job_1495998405307_0001</p>  <p>17/05/29 14:48:00 INFO mapreduce.Job: Job  job_1495998405307_0001 running in uber mode : false</p>  <p>17/05/29 14:48:00 INFO  mapreduce.Job:  map 0% reduce 0%</p></td> </tr></tbody></table><p>进入hdfs的管理界面（http://hadoop:8088/cluster/apps），查看程序运行情况：</p><p><img src="https://img-blog.csdn.net/20170529155015132?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p> </p><p> <img src="https://img-blog.csdn.net/20170529155032994?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p><br></p><p><br></p><h2>26.2 MAPREDUCE使用</h2><p>mapreduce是hadoop中的分布式运算编程框架，只要按照其编程规范，只需要编写少量的业务逻辑代码即可实现一个强大的海量数据并发处理程序</p><p> </p><h3>26.2.1 Demo开发——wordcount</h3><p>1、需求</p><p>从大量（比如T级别）文本文件中，统计出每一个单词出现的总次数</p><p> </p><p>2、mapreduce实现思路</p><p>Map阶段：</p><p>a)        从HDFS的源数据文件中逐行读取数据</p><p>b)        将每一行数据切分出单词</p><p>c)        为每一个单词构造一个键值对(单词，1)</p><p>d)        将键值对发送给reduce</p><p> </p><p>Reduce阶段：</p><p>a)        接收map阶段输出的单词键值对</p><p>b)        将相同单词的键值对汇聚成一组</p><p>c)        对每一组，遍历组中的所有“值”，累加求和，即得到每一个单词的总次数</p><p>d)        将(单词，总次数)输出到HDFS的文件中</p><p> </p><p> </p><p>1、  具体编码实现</p><p>(1)定义一个mapper类</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p>//首先要定义四个泛型的类型</p>  <p>//keyin:  LongWritable    valuein: Text</p>  <p>//keyout: Text             valueout:IntWritable</p>  <p> </p>  <p>public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text,  IntWritable&gt;{</p>  <p>         //map方法的生命周期：  框架每传一行数据就被调用一次</p>  <p>         //key :  这一行的起始点在文件中的偏移量</p>  <p>         //value: 这一行的内容</p>  <p>         @Override</p>  <p>         protected void  map(LongWritable key, Text value, Context context) throws IOException,  InterruptedException {</p>  <p>                   //拿到一行数据转换为string</p>  <p>                   String line =  value.toString();</p>  <p>                   //将这一行切分出各个单词</p>  <p>                   String[] words =  line.split(" ");</p>  <p>                   //遍历数组，输出&lt;单词，1&gt;</p>  <p>                   for(String  word:words){</p>  <p>                            context.write(new Text(word), new  IntWritable(1));</p>  <p>                   }</p>  <p>         }</p>  <p>}</p></td> </tr></tbody></table><p> </p><p>(2)定义一个reducer类</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p>         //生命周期：框架每传递进来一个kv 组，reduce方法被调用一次</p>  <p>         @Override</p>  <p>         protected  void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)  throws IOException, InterruptedException {</p>  <p>                   //定义一个计数器</p>  <p>                   int  count = 0;</p>  <p>                   //遍历这一组kv的所有v，累加到count中</p>  <p>                   for(IntWritable  value:values){</p>  <p>                            count  += value.get();</p>  <p>                   }</p>  <p>                   context.write(key,  new IntWritable(count));</p>  <p>         }</p>  <p>}</p></td> </tr></tbody></table><p> </p><p>(3)定义一个主类，用来描述job并提交job</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p>public class WordCountRunner {</p>  <p>         //把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里。。。。。。）描述成一个job对象</p>  <p>         //把这个描述好的job提交给集群去运行</p>  <p>         public  static void main(String[] args) throws Exception {</p>  <p>                   Configuration  conf = new Configuration();</p>  <p>                   Job  wcjob = Job.getInstance(conf);</p>  <p>                   //指定我这个job所在的jar包</p>  <p>//               wcjob.setJar("/home/hadoop/wordcount.jar");</p>  <p>                   wcjob.setJarByClass(WordCountRunner.class);</p>  <p>                   </p>  <p>                   wcjob.setMapperClass(WordCountMapper.class);</p>  <p>                   wcjob.setReducerClass(WordCountReducer.class);</p>  <p>                   //设置我们的业务逻辑Mapper类的输出key和value的数据类型</p>  <p>                   wcjob.setMapOutputKeyClass(Text.class);</p>  <p>                   wcjob.setMapOutputValueClass(IntWritable.class);</p>  <p>                   //设置我们的业务逻辑Reducer类的输出key和value的数据类型</p>  <p>                   wcjob.setOutputKeyClass(Text.class);</p>  <p>                   wcjob.setOutputValueClass(IntWritable.class);</p>  <p>                   </p>  <p>                   //指定要处理的数据所在的位置</p>  <p>                   FileInputFormat.setInputPaths(wcjob,  "hdfs://hdp-server01:9000/wordcount/data/big.txt");</p>  <p>                   //指定处理完成之后的结果所保存的位置</p>  <p>                   FileOutputFormat.setOutputPath(wcjob,  new Path("hdfs://hdp-server01:9000/wordcount/output/"));</p>  <p>                   </p>  <p>                   //向yarn集群提交这个job</p>  <p>                   boolean  res = wcjob.waitForCompletion(true);</p>  <p>                   System.exit(res?0:1);</p>  <p>         }</p></td> </tr></tbody></table><p> </p><h3>26.2.2 程序打包运行</h3><p>1.        将程序打包</p><p>2.        准备输入数据</p><p>vi  /home/hadoop/test.txt</p><table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td valign="top"><p>Hello tom</p>  <p>Hello jim</p>  <p>Hello ketty</p>  <p>Hello world</p>  <p>Ketty tom</p></td> </tr></tbody></table><p>在hdfs上创建输入数据文件夹：</p><p>hadoop   fs mkdir  -p  /wordcount/input</p><p>将words.txt上传到hdfs上</p><p>         hadoop  fs  –put  /home/hadoop/words.txt  /wordcount/input</p><p><img src="https://img-blog.csdn.net/20170529155121008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p> </p><p>3.        将程序jar包上传到集群的任意一台服务器上</p><p> </p><p>4.        使用命令启动执行wordcount程序jar包</p><p>$ hadoop jar wordcount.jar cn.toto.bigdata.mrsimple.WordCountDriver/wordcount/input /wordcount/out</p><p><img src="https://img-blog.csdn.net/20170529155137870?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p>5.        查看执行结果</p><p>$ hadoop fs –cat /wordcount/out/part-r-00000</p><p><img src="https://img-blog.csdn.net/20170529155159557?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG90b3R1enVvcXVhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p> </p>            </div>
                </div>