---
layout:     post
title:      第51课： Spark大型项目下的Spark Streaming本质剖析
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p><strong>1. Spark Streaming本质</strong> <br>
官网URL： <br>
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="nofollow">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p>

<p><img src="https://img-blog.csdn.net/20160921205550937" alt="这里写图片描述" title=""></p>

<p>流处理作为一个处理引擎，一定有输入数据和输出数据，也就是：Input和Output，在Spark2.0是Input Table、Output Table。</p>

<p><strong>1.1 输入数据源：</strong> <br>
流处理的数据来源可以是：</p>

<ul>
<li>Kafka（90%把它作为输入数据来源）</li>
<li>Flume</li>
<li>HDFS/S3</li>
<li>Kinesis( 是Amazon 的一种云服务,可以通过Kinesis实时收集并处理数据)</li>
<li>Twitter</li>
</ul>

<p><strong>1.2 流处理：</strong> <br>
Spark Streaming作为流处理处理引擎，会根据输入数据来源发生Computation。 <br>
计算是针对具体的代码业务逻辑，Spark Streaming的流处理是<strong>基于时间间隔(Batch Interval)进行的批处理</strong>。时间在不断流逝，按指定间隔的时间单位进行数据处理，期间数据不断流进来，但是在指定间隔的时间内，处理的数据是不变的，跟RDD数据不变性原则是一致的。 <br>
Spark Streaming的流处理就是不断进行的批处理，加上了时间维度，一个批处理结束之后，继续下一个批处理。在一个时间间隔内接收了多少数据，就处理多少数据。</p>

<p><strong>1.3 输出数据</strong> <br>
流出输出结果可以保持于：</p>

<ul>
<li>HDFS</li>
<li>Databases</li>
<li>Dashboards（仪表盘）</li>
</ul>

<hr>

<p><strong>2. NetworkWordCount示例</strong> <br>
网络单词统计，该示例为官方自带示例： <br>
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval" rel="nofollow">http://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval</a></p>

<blockquote>
  <p>Before we go into the details of how to write your own Spark Streaming program, let’s take a quick look at what a simple Spark Streaming program looks like. Let’s say we want to count the number of words in text data received from a data server listening on a TCP socket. All you need to do is as follows.</p>
</blockquote>



<pre class="prettyprint"><code class=" hljs scala"><span class="hljs-keyword">package</span> org.apache.spark.examples.streaming

<span class="hljs-keyword">import</span> org.apache.spark.SparkConf
<span class="hljs-keyword">import</span> org.apache.spark.storage.StorageLevel
<span class="hljs-keyword">import</span> org.apache.spark.streaming.{Seconds, StreamingContext}

<span class="hljs-javadoc">/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;
 * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999`
 */</span>
<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">NetworkWordCount</span> {</span>
  <span class="hljs-keyword">def</span> main(args: Array[String]) {
    <span class="hljs-keyword">if</span> (args.length &lt; <span class="hljs-number">2</span>) {
      System.err.println(<span class="hljs-string">"Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;"</span>)
      System.exit(<span class="hljs-number">1</span>)
    }

    StreamingExamples.setStreamingLogLevels()

    <span class="hljs-comment">// Create the context with a 1 second batch size</span>
    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> SparkConf().setMaster(<span class="hljs-string">"local[2]"</span>).setAppName(<span class="hljs-string">"NetworkWordCount"</span>)
    <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> StreamingContext(sparkConf, Seconds(<span class="hljs-number">1</span>))

    <span class="hljs-comment">// Create a socket stream on target ip:port and count the</span>
    <span class="hljs-comment">// words in input stream of \n delimited text (eg. generated by 'nc')</span>
    <span class="hljs-comment">// Note that no duplication in storage level only for running locally.</span>
    <span class="hljs-comment">// Replication necessary in distributed scenario for fault tolerance.</span>
    <span class="hljs-keyword">val</span> lines = ssc.socketTextStream(args(<span class="hljs-number">0</span>), args(<span class="hljs-number">1</span>).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    <span class="hljs-keyword">val</span> words = lines.flatMap(_.split(<span class="hljs-string">" "</span>))
    <span class="hljs-keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="hljs-number">1</span>)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}</code></pre>

<p><strong>运行示例：</strong> <br>
1）在TERMINAL 1运行NetCat： <br>
 $ nc -lk 9999 <br>
… <br>
hello world <br>
…</p>

<p>2）在TERMINAL 2执行示例Job <br>
$ ./bin/run-example streaming.NetworkWordCount localhost 9999</p>

<p>… <br>
 =================  <br>
 Time: 1357008430000 ms <br>
 =================  <br>
(hello,1) <br>
(world,1) <br>
…</p>

<p>3）代码解读 <br>
根据应用程序和可用的群集资源的延迟要求，必须设置批处理间隔。本示例中batch interval设置为1秒。</p>

<p>Advanced Sources：</p>

<ul>
<li>Kafka </li>
<li>Flume </li>
<li>Kinesis </li>
</ul>

<hr>

<p><strong>3. Discretized Streams (DStreams)</strong></p>

<p>Spark Streaming提供的基本抽象，内部由一系列连续的RDD表示，每个RDD包含了一个时间间隔的数据，如下图所示： <br>
<img src="https://img-blog.csdn.net/20160926201637845" alt="这里写图片描述" title=""></p>

<p>任何应用于DSream上的操作会转换成在底层的RDD上的操作。flatMap作用于每个lines DStream下的RDD上，然后生成words  DStream下的RDD，例如下图所示： <br>
<img src="https://img-blog.csdn.net/20160926201650954" alt="这里写图片描述" title=""> <br>
底层的RDD transformations操作被Spark engine操作，DStream操作隐藏了这些细节，提供了开发者更高级别的API，使开发更加便捷。</p>

<hr>

<p><strong>4. Input DStreams and Receivers</strong></p>

<p>每一个input DStream（除了file stream）会和Receiver相关联，Receiver接收数据并把它存储在Spark的内存中以供计算。</p>

<p>Spark Streaming provides two categories of built-in streaming sources.</p>

<ul>
<li>Basic sources:  <br>
Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.</li>
<li>Advanced sources:  <br>
Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.</li>
</ul>

<p>在一个streaming application中，可以创建多个input DStreams，来并行的接收多个流数据。这样会创建多个receivers同时接收数据流。需要注意的是一个Worker/executor是一个长期运行的task，于是它占有一个core，因此要记得分配足够的cores（如果是local模式，就是线程）</p>

<p>注意： <br>
     local模式下，不要使用”local”或者”local[1]”作为master URL，否则会只有一个线程被用来running task locally。如果使用了一个基于receiver（e.g. sockets,kafka,flume,etc.）的input DStream，那么这个单线程会用来运行receiver，就没有线程用来运行处理received data的程序，所以一般使用”local[n]” as the master URL，<strong>n大于需要运行的receivers的数量</strong>！ <br>
     同样的，运行在集群上时cores的数量必须大于receivers的数量，否则系统只会接受数据，不会处理数据。</p>

<p><strong>4.1 Basic Sources</strong> <br>
We have already taken a look at the ssc.<strong>socketTextStream</strong>(…) in the quick example which creates a DStream from text data received over a TCP socket connection. Besides sockets, the <strong>StreamingContext</strong> API provides methods for creating DStreams from files as input sources.</p>

<p><strong>1) File Streams</strong></p>



<pre class="prettyprint"><code class=" hljs markdown"> streamingContext.fileStream[<span class="hljs-link_label">KeyClass, ValueClass, InputFormatClass</span>](<span class="hljs-link_url">dataDirectory</span>)</code></pre>

<p>Spark Streaming will monitor the directory dataDirectory and process any files created in that directory (files written in nested directories not supported). Note that</p>

<ul>
<li>The files must have the same data format.</li>
<li>The files must be created in the dataDirectory by atomically moving or renaming them into the data directory.</li>
<li>Once moved, the files must not be changed. So if the files are being continuously appended, the new data will not be read. <br>
For simple text files, there is an easier method <strong>streamingContext.textFileStream(dataDirectory).</strong> And file streams do not require running a receiver, hence does not require allocating cores.</li>
</ul>

<p><strong>2) Streams based on Custom Receivers:</strong>  <br>
DStreams can be created with data streams received through custom receivers. See the <a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" rel="nofollow">Custom Receiver Guide</a> and <a href="https://github.com/spark-packages/dstream-akka" rel="nofollow" title="DStream Akka">DStream Akka</a>  for more details.</p>

<p><strong>3) Queue of RDDs as a Stream: </strong> <br>
 For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using <strong>streamingContext.queueStream(queueOfRDDs)</strong>. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream.</p>

<p><strong>4.2 Advanced Sources</strong> <br>
Some of these advanced sources are as follows.</p>

<ul>
<li>Kafka、</li>
<li>Flume、</li>
<li>Kinesis <br>
不能在Spark shell中测试，除非下载相关的依赖包，并且加到classpath中。</li>
</ul>

<p><strong>4.3 Custom Sources</strong> <br>
Input DStreams can also be created out of custom data sources. All you have to do is implement a user-defined receiver (see next section to understand what that is) that can receive data from the custom sources and push it into Spark. See the <a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" rel="nofollow" title="Custom Receiver Guide">Custom Receiver Guide</a> for details.</p>

<hr>

<p><strong>4.4 Transformations on DStreams</strong> <br>
Similar to that of RDDs, transformations allow the data from the input DStream to be modified. DStreams support many of the transformations available on normal Spark RDD’s. Some of the common ones are as follows.</p>

<ul>
<li>map</li>
<li>flatMap</li>
<li>repartition</li>
<li>……</li>
<li>updateStateByKey</li>
<li>transform <br>
其中，updateStateByKey 和 transform更值得详细谈论。</li>
</ul>

<hr>

<p><strong>4.5 Transformations on DStreams</strong></p>

<p>Spark Streaming还提供了窗口的计算，它允许你请求变换一个滑动窗口的数据。下图说明了这个滑动窗口。 <br>
<img src="https://img-blog.csdn.net/20160927144343693" alt="这里写图片描述" title=""></p>

<p>如图所示，每次窗口在源DStream上滑动，合并和操作落入窗内的源RDDs，产生窗口化的DStream的RDDs。 <br>
在这个具体示例中，操作被应用在最近3个时间单位的数据上，并且每2个时间单位滑动一次。这表明，任何窗口操作都需要指定两个参数。</p>

<ul>
<li>窗口长度：窗口的持续时间(图上是3).</li>
<li>滑动的时间间隔：窗口操作执行的时间间隔 (图上是2).</li>
</ul>

<hr>

<p><strong>5. Output Operations on DStreams</strong> <br>
<img src="https://img-blog.csdn.net/20160927104738674" alt="这里写图片描述" title=""></p>

<hr>

<p>8、Structured Streaming本质</p>

<p>Input Table 和 Output Table</p>

<p>Output Table <br>
Structured Streaming预计在Spark 2.3的时候成熟</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>