---
layout:     post
title:      Hive源码解析与开发实战笔记--讲师贾杰
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/china_demon/article/details/51821428				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
Hive实战<br>
目录<br>
Hadoop生态系统<br>
日志分析系统<br>
Hive介绍<br>
Hive shell常用操作<br>
    hive -e<br>
    hive -f<br>
    hive -v<br>
    hive -i<br>
    hive -S<br><br><br>
Hive环境搭建<br>
Hive基本使用<br>
-----------------------------------------------------------------------------<br>
日志分析系统-流程<br>
数据收集=》数据清洗=》数据存储与管理=》数据分析=》数据显示<br><br><br>
Hadoop 日志分析系统<br><br><br>
Hive介绍<br>
什么是Hive？<br>
hive是基于Hadoop的一个数据仓库工具<br>
可以将结构化的数据文件映射为一张数据库表，并提供类sql（HQL）的查询功能<br>
可以将sql语句转换为MapReduce任务进行运行<br>
可以用来进行数据提取转化加载（ETL）<br><br><br>
优点与缺点<br>
成本低，入手较快<br>
可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用<br>
不支持实时查询<br><br><br>
Hive安装<br>
安装前准备<br>
JDK1.6+<br>
Hadoop<br>
1.x,2.x<br>
Hive安装包<br>
0.13,...,0.9<br>
Mysql<br>
mysql-connection-java<br><br><br>
下载压缩包<br>
tar -xzf 解压文件到目录，比如：/home/hive-0.9/*<br>
配置环境变量<br>
vi /etc/profile<br>
export HIVE_HOME= /home/hive-0.9<br>
export PATH=$PATH:$HIVE_HOME/bin<br>
source /etc/profile<br><br><br>
修改hive配置文件<br>
$HIVE_HOME/conf/hive-default.xml.template<br>
修改为hive-site.xml<br><br><br>
修改配置hive-site.xml内容（可选）<br>
hive.metastore.warehouse.dir<br>
hive.querylog.location<br><br><br>
终端：输入hive回车<br>
show tables；(命令后面加分号，回车)<br>
显示：OK<br><br><br>
Hive元数据存储<br>
Derby<br>
单session<br>
在启动终端目录创建元数据文件<br>
不能多用户共享<br><br><br>
MySql<br>
安装MySql，配置账户，权限<br>
mysql-connection-java-5.1.22-bin.jar,拷贝至hive安装目录lib目录下<br>
修改hive-site.xml<br><br><br>
MySql配置<br>
javax.jdo.option.ConnectionURL<br>
jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<br><br><br>
javax.jdo.option.ConnectionDriverName<br>
com.mysql.jdbc.Driver<br>
javax.jdo.option.ConnectionUserName<br>
root<br>
javax.jdo.option.ConnectionPassword<br>
12345<br><br><br>
&gt;&gt;show tables;<br>
&gt;&gt;create table test1(name String)<br><br><br>
&gt;&gt;mysql -uroot -p<br>
&gt;&gt;show databases;<br><br><br>
Hive Client<br>
访问hive的方式<br>
Ci<br>
Hwi<br>
    hive --service hwi<br>
    http://localhost:9999/hwi<br>
HiveServer<br>
    hive --service hiveserver<br><br><br>
Hive-JDBC<br>
private<span> </span>static String<br>
HiveDriver="org.apache.hadoop.hive.jdbc.HiveDriver";<br>
private static String<br>
url="jdbc:hive://192.168.1.198:1000/default";<br>
private static String name = "";<br>
private static String password="";<br><br><br>
Class.forName(HiveDriver);<br>
Connection conn = <br>
DriverManager.getConnection(url,name,password);<br>
Statemetn stat = conn.createStatement();<br>
String sql = "show tables";<br><br><br>
启动hive<br>
&gt;&gt;hive --service hiveserver<br>
http://192.168.1.198:10000<br><br><br>
netstat -ano | grep 10000<br><br><br><br><br>
HiveJDBC.java<br><br><br>
package com.hive;<br><br><br>
import java.sql.Connection;<br>
import java.sql.DriverManager;<br>
import java.sql.ResultSet;<br>
import java.sql.SQLException;<br>
import java.sql.Statement;<br><br><br>
public class HiveJdbc{<br>
    private static String HiveDriver = "org.apache.hadoop.hive.jdbc.HiveDriver";<br>
    private static String url = "jdbc:hive://127.0.0.1:10000/default";<br>
    private static String name = "";<br>
    private static String password = "";<br><br><br>
    public static void main(String[] args) throws SQLException{<br>
        try {<br><span></span>Class.forName(HiveDriver);<br><span></span>} catch (ClassNotFoundException e) {<br><span></span>// TODO Auto-generated catch block<br><span></span>e.printStackTrace();<br><span></span>}<br><span></span>Connection conn = DriverManager.getConnection(url,name,password);<br><span></span>Statement stat = conn.createStatement();<br><span></span>String sql1 = "show tables";<br><span></span>String sql2 = "select * from import_stock_d limit 1000";<br><span></span>ResultSet rs = stat.executeQuery(sql1);<br><span></span>while(rs.next()){<br><span></span>   System.out.println(rs.getString(1));<br><span></span>}<br><span></span>   }<br><br><br>
}<br>
---------------------------------------------------------------------------------------------<br>
对于 hive 1.2 及以上的版本，hive不再使用，而直接使用 hiveserver2 命令；<br>
在Linux shell：<br>
[root@hadoop0 ~]# hiveserver2 &amp;<br><br><br>
hadoop dfsadmin -safemode leave<br>
set hive.cli.print.current.db=true 显示打印库名称<br>
set hive.cli.print.header=true     显示表头名称<br><br><br>
-----------------------------------Hive表操作----------------------------------------------<br>
Hive数据类型<br>
Hive文件格式<br>
Hive表的创建<br>
Hive操作表<br>
Hive表分区<br>
Hive查询表<br><br><br>
Hive基本使用-数据类型<br>
基本数据类型<br>
tinyint，smallint，int，bigint，boolean，float，double，string，binary，timestamp，decimal，char，varchar，date<br>
-------------------------------------------------------------------------<br>
RCFile：把一列数据转换成一行数据，提高查询速度。<br>
----------------------------------------------------------------------------------------------------<br>
Hive基本使用-表<br>
CREATE[EXTERNAL] TABLE[IF NOT EXISTS][db_name.]table_name<br>
[(col_name data_type[COMMENT col_comment],...)]<br>
[PARTITIONED BY (col_name,col_name,...)[SORTED BY(col_name[ASC|DESC],...)] INTO num_buckets BUCKETS]<br>
[<br>
[ROW FORMAT row_format][STORED AS file_format]|STORED BY 'storage.handler.class.name'[WITH SERDEPROPERTIES(...)]<br>
]<br>
[LOCATION hdfs_path]<br>
[TBLPROPERTIES(property_name=property_value,...)]<br>
[AS select_statement]<br><br><br>
create table [external] employees(<br>
name string,<br>
salary float,<br>
subordinates array&lt;string&gt;,<br>
deductions map&lt;string,float&gt;,<br>
address struct&lt;street:string,city:string,state:string,zip:int&gt;<br>
)<br>
row format delimited fields terminated by '\t'  --字段分隔符 默认/001<br>
lines terminated by '\n' stored as textfile;     ---行分隔符 默认/002<br><br><br>
--内部表实例<br>
create table testtable(<br>
name string comment 'name value',<br>
address string comment 'address value'<br>
)<br>
row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;<br>
load data local inpath '/home/data/data' overwrite into table testtable; <br><br><br>
show tables;<br>
desc testtable;<br>
desc extended testtable;<br>
desc formatted testtable;<br>
--删除表<br>
drop table tablename;<br>
--显示建表语句<br>
show create table tablename;<br><br><br>
--外部表实例<br>
create external table if not exists employees(<br>
name string,<br>
salary float,<br>
subordinates array&lt;string&gt;,<br>
deductions map&lt;string,float&gt;,<br>
address struct&lt;street:string,city:string,state:string,zip:int&gt;<br>
)<br>
row format delimited <br>
fields terminated by '\t'   --每个字段值的分隔符<br>
collection item terminated by ','  ---集合类型字段中单个record的分隔符<br>
map keys terminated by ':'       --map类型字段单个record中key和value的分隔符<br>
lines terminated by '\n' <br>
stored as textfile<br>
location '/warehouse/employee';<br><br><br>
数据格式：<br>
wang<span> </span>123<span> </span>
a1,a2,a3<span> </span>k1:1,k2:2,k3:3<span></span>s1,s2,s3,4<br>
liu<span> </span>123<span> </span>
a4,a5,a6<span> </span>k1:1,k2:2,k3:3<span></span>s1,s2,s3,4<br>
zhang<span> </span>123<span> </span>
a7,a8,a9<span> </span>k1:1,k2:2,k3:3<span></span>s1,s2,s3,4<br><br><br>
selecct  subordinates[1] from employees;查找索引为1的数据<br>
selecct  deductions["k2"] from employees;<br>
selecct  address.city from employees;<br><br><br>
Hive建表的其他方式<br>
由一个表创建另外一张表<br>
Create table test3 like test2;<br><br><br>
从其他表查询创建表<br>
Create table test4 as select name,addr from test5;<br><br><br>
Hive不同文件读取对比<br>
stored as textfile<br>
    直接查看hdfs<br>
    hadoop fs -text<br>
stored as sequencefile<br>
    hadoop fs -text<br>
stored as rcfile<br>
    hive-service rcfilecat path<br>
stored as inputformat'class'<br>
    outformat'class'<br><br><br>
create table test_text(name string,val string) stored as textfile;<br>
create table test_seq(name string,val string) stored as sequencefile;<br><br><br>
？？？自动以输入流 输出流没有实现！<br>
自定义outputformat和inputformat<br>
--------------inputformat-----------------------------------------------------------------<br>
UDInputFormat.java<br>
package com.zyf.hive.inputformat;<br><br><br>
import java.io.IOException;<br>
import org.apache.hadoop.fs.FileStatus;<br>
import org.apache.hadoop.fs.Path;<br>
import org.apache.hadoop.io.MapFile;<br>
import org.apache.hadoop.mapred.FileInputFormat;<br>
import org.apache.hadoop.mapred.JobConf;<br>
import org.apache.hadoop.mapred.Reporter;<br>
import org.apache.hadoop.mapreduce.lib.input.FileSplit;<br><br><br>
public class UDInputFormat&lt;K,V&gt; extends  FileInputFormat&lt;K, V&gt;{<br><span></span>@Override<br><span></span>public org.apache.hadoop.mapred.RecordReader&lt;K, V&gt; getRecordReader(<br><span></span>org.apache.hadoop.mapred.InputSplit split, JobConf job,<br><span></span>Reporter reporter) throws IOException {<br><span></span>reporter.setStatus(split.toString());<br>
    <span></span>return (org.apache.hadoop.mapred.RecordReader&lt;K, V&gt;) new UDRecordReader&lt;K,V&gt;(job,(FileSplit) split);<br><span></span>}<br><span></span><br><span></span>@Override<br>
    protected FileStatus[] listStatus(JobConf job) throws IOException {<br>
    <span></span>FileStatus[] files = super.listStatus(job);<br>
    <span></span>for(int i = 0;i &lt; files.length;i++){<br>
    <span></span>FileStatus file = files[i];<br>
    <span></span>if(file.isDir()){<br>
    <span></span>Path dataFile = new Path(file.getPath(),MapFile.DATA_FILE_NAME);<br>
    <span></span>org.apache.hadoop.fs.FileSystem fs = file.getPath().getFileSystem(job);<br>
    <span></span>files[i] = fs.getFileStatus(dataFile);<br>
    <span></span>}<br>
    <span></span>}<br>
    <span></span>return files;<br>
    }<br>
}<br><br><br>
---------------------------------------------------------------------------------<br>
UDRecordReader.java<br>
package com.zyf.hive.inputformat;<br><br><br>
import java.io.IOException;<br>
import org.apache.hadoop.conf.Configuration;<br>
import org.apache.hadoop.mapreduce.lib.input.FileSplit;<br>
import org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader;<br><br><br>
public class UDRecordReader&lt;K,V&gt; extends SequenceFileRecordReader&lt;K, V&gt; {<br>
    public UDRecordReader(Configuration conf,FileSplit split) throws IOException{<br>
    <span></span>super();<br>
    }<br>
}<br>
-----------------------------------------------------<br>
add jar /root/dev_store/UDInputFormat.jar<br><br><br>
drop table testinputformat;<br>
create table if not exists testinputformat(<br>
name string comment 'name value',<br>
addr string comment 'addr value'<br>
)<br>
row format delimited fields terminated by '\t' lines terminated by '\n'<br>
stored as inputformat 'com.zyf.hive.inputformat.UDInputFormat'<br>
outputformat 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'<br>
location ''<br><br><br>
load data local inpath '/home/data/data' into table testtable;<br><br><br>
add jar path;<br>
add jar /home/data/UDInputFormat.jar;<br>
-----------------------------------------------------------------------------------<br>
Hive使用SerDe<br>
SerDe是"Serializer和Deserializer的简写"<br>
Hive使用SerDe(和FileFormat)来读、写表的行<br>
读写数据的顺序如下：<br>
HDFS文件--&gt; InputFileFormat--&gt; &lt;key,value&gt; --&gt; Deserializer --&gt; Row对象<br>
Row对象--&gt; Serializer --&gt; &lt;key,value&gt; --&gt; OutputFileFormat --&gt; HDFS文件<br><br><br>
create table apachelog(<br>
t_host STRING,<br>
t_identity STRING,<br>
t_user STRING,<br>
t_time STRING,<br>
t_request STRING,<br>
t_status STRING,<br>
t_size STRING,<br>
t_referer STRING,<br>
t_agent STRING)<br>
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'<br>
WITH SERDEPROPERTIES(<br>
"input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([0-9]*) ([0-9]*) ([^ ]*) ([^ ]*)"<br>
)STORED AS TEXTFILE;<br><br><br>
load data local inpath '/root/dev_store/apache.access.log' overwrite into table apachelog;<br><br><br>
load data local inpath '/root/dev_store/apache.access.2.log' overwrite into table apachelog;<br>
select host from apachelog;<br><br><br><br><br>
add jar /usr/hadoop/apache-hive-1.2.1-bin/lib/hive-contrib-1.2.1.jar;<br><br><br>
drop table apachelog;<br>
create table if not exists apachelog(<br>
t_host STRING,<br>
t_identity STRING,<br>
t_user STRING,<br>
t_time STRING,<br>
t_request STRING,<br>
t_status STRING,<br>
t_size STRING,<br>
t_referer STRING,<br>
t_agent STRING)<br>
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'<br>
WITH SERDEPROPERTIES ("input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*) ([^ ]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?",<br>
"output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s %10$s")STORED AS TEXTFILE;<br><br><br><br><br>
数据：<br>
127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326<br><br><br>
---------------------------------------------------------------------------------------------------<br><br><br>
Hive 分区表<br>
分区<br>
在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作<br>
分区表指的是在创建表时指定partition的分区空间<span> </span><br><br><br>
分区语法<br>
Create table tablename(<br>
name string<br>
)<br>
partitioned by (key type,...)<br><br><br>
Hive 分区表<br>
create table employee(<br>
name string,<br>
salary float,<br>
subordinates array&lt;string&gt;,<br>
deductions map&lt;string,float&gt;,<br>
address struct&lt;street:string, city:string, state:string, zip:int&gt;<br>
) <br>
partitioned by (dt string,type string)<br>
row format delimited<br>
fields terminated by '\t'<br>
collection items terminated by ','<br>
map keys terminated by ':'<br>
lines terminated by '\n' stored as textfile;<br><br><br><br><br>
create table if not exists employees(<br>
name string,<br>
salary float,<br>
subordinates array&lt;string&gt;,<br>
deductions map&lt;string,float&gt;,<br>
address struct&lt;street:string,city:string,state:string,zip:int&gt;<br>
)<br>
partitioned by (dt string,type string)<br>
row format delimited <br>
fields terminated by '\t'   --每个字段值的分隔符<br>
collection item terminated by ','  ---集合类型字段中单个record的分隔符<br>
map keys terminated by ':'       --map类型字段单个record中key和value的分隔符<br>
lines terminated by '\n' <br>
stored as textfile<br><br><br>
&gt;&gt;desc formatted employees;<br><br><br>
增加分区<br>
Alter table employees add if not exists partition(country='xxx'[,state='yyyy'])<br><br><br>
删除分区<br>
Alter table employees drop if exists partition(country='xxx'[,state='yyyy'])<br><br><br>
--显示分区<br>
show partitions employees;<br>
---------------------------------------------------------------------------------------------------------<br>
Hive 分桶<br>
分桶<br>
对于每一个表或者分区，Hive可以进一步组成桶，也就是说桶是更为细粒度的数据范围划分<br>
Hive是针对某一列进行分桶<br>
Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中<br><br><br>
好处<br>
获得更高的查询处理效率。<br>
使取样(sampling)更高效<br><br><br>
create table bucketed_user<br>
(<br>
id int,<br>
name string<br>
)<br>
clustered by(id) sorted by(name) into 4 buckets<br>
row format delimited fields terminated by '\t' stored as textfile;<br>
Set hive.enforce.bucketing = true;<br><br><br>
Hive基本使用-查询<br>
基本语法<br>
SELECT[ALL|DISTINCT]select_expr,select_expr,... FROM table_name<br>
[WHERE where_condition]<br><br><br>
-----------------------------------------------------------------------------------------<br>
                               *Hive 数据操作*<br><br><br>
Hive执行命令方式<br>
Hive数据操作<br>
Hive动态分区<br>
Hive高级查询<br>
Hive桶<br>
Hive索引<br>
Hive视图<br><br><br>
前面讲的几种<br>
cli,jdbc,hwi,<br>
beeline 从客户端执行，走jdbc<br>
详细讲cli shell<br>
hive -help<br>
hive --help<br>
list,source<br>
注：命令脚本必须在集群的节点或hiveclient执行<br><br><br>
hive -e "select * from test3" &gt; /home/cloudera/data<br>
hive -S -e "select * from test3" &gt; /home/cloudera/data<br>
hive -v -e "select * from test3" &gt; /home/cloudera/data<br>
hive -f "/home/data/hql/select_hql"<br><br><br>
test.sh<br>
#!/bin/base<br>
time=''<br>
hive -e "select * from testtext where name=${time}"<br>
Hive操作-变量<br>
配置变量<br>
set val='';<br>
${hiveconf:val}<br><br><br>
环境变量<br>
${env:HOME}，注env查看所有环境变量<br><br><br>
set val=wer;<br>
set val<br>
select * from testtext where name = '${hiveconf:val}'<br>
select '${env:HOME}' from testtext;<br><br><br>
--\N hive底层默认的格式<br><br><br>
Hive数据加载注意问题<br>
分隔符问题，且分隔符默认只有单个字符<br>
数据类型对应问题<br>
Load数据，字段类型不能互相转化时，查询返回NULL<br>
select查询插入，字段类型不能互相转化时，插入数据为NULL<br><br><br>
select查询插入数据，字段值顺序要与表中字段顺序一致，名称可不一致<br>
Hive在数据加载时不做检查，查询时检查<br>
外部分区表需要添加分区才能看到数据<br><br><br>
Hive数据加载<br>
外表数据加载<br>
    创建表时指定数据位置<br>
        create external table tablename() location''<br>
    查询插入，同内表<br>
    使用Hadoop命令拷贝数据到指定位置（hive的shell中执行和Linux的shell执行）<br>
   <br>
分区表数据加载<br>
    内部分区表和外部分区表数据加载<br>
        内部分区表数据加载方式类似于内表<br><span></span>外部表分区数据加载方式类似于外表<br><span></span>   注意：数据存放的路径层次要和表分区一致；<br><span></span>   如果分区表没有新增分区，即使目标路径下已经有数据了，<br><span></span>   但依然查不到数据<br>
不同之处<br>
    加载数据指定目标表的同时，需要指定分区<br><br><br>
Hive分区表数据加载<br>
本地数据加载<br>
Load data local inpath 'localpath'[overwrite] into table tablename partition(pn='')<br><br><br>
加载hdfs数据<br>
Load data inpath 'hdfspath'[overwrite] into table tablename partition(pn='')<br><br><br>
由查询语句加载数据<br>
insert[overwrite] into table tablename partition(pn='')<br>
select col1,col2 from table where ...<br>
----------------------------------------------------------------<br>
--数据导出及动态分区<br><br><br>
Hive 数据导出<br>
导出的方式<br>
Hadoop命令的方式<br>
    get<br>
    text<br>
通过INSERT... DIRECTORY方式<br>
    insert overwrite[local] directory '/tmp/ca_employees'<br>
    [row format delimited fields terminated by '\t']<br>
    select name,salary,address from employees<br><br><br>
example1:<br>
insert overwrite local directory '/home/data3'<br>
row format delimited fields terminated by '\t'<br>
select name,addr from testtext;<br><br><br><br><br>
Shell命令加管道：hive-f/e | sed/grep/awk &gt;file<br>
第三方工具<br>
----------------------------------------------------------------------<br>
hadoop fs -text /warehouse/testtext/*<br><br><br><br><br>
Hive 动态分区<br>
动态分区<br>
不需要为不同的分区添加不同的插入语句<br>
分区不确定，需要从数据中获取<br><br><br>
几个参数<br>
set hive.exec.dynamic.partition=true //使用动态分区<br>
set hive.exec.dynamic.partition.mode=nonstrict; //无限制模式<br>
如果模式是strict，则必须有一个静态分区，且放在最前面(例如有三个字段，一个静态分区，两个动态分区，那么静态分区要放在最前面)<br><br><br>
set hive.exec.max.dynamic.partitions.pernode=10000;//每个节点生成动态分区的最大个数<br>
set hive.exec.max.dynamic.partitions=100000;//生成动态分区的最大个数<br>
set hive.exec.max.created.files=1500000;//一个任务最多可以创建的文件数目<br>
set dfs.datanode.max.xcievers=8192;//限定一次最多打开的文件数<br><br><br>
create table d_part(<br>
name string<br>
)<br>
partitioned by (value string)<br>
row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;<br><br><br>
show partitions d_part;<br><br><br>
select * from d_part;<br><br><br>
set hive.exec.dynamic.partition.mode=nonstrick;<br>
set hive.exec.dynamic.partition=true<br><br><br>
insert overwrite table d_part partition(value)<br>
select name,addr as value from testtext;<br>
-------------------------------------------------------------------------------------------------------------<br>
表属性操作<br>
修改表明<br>
    alter table table_name rename to new_table_name;<br>
修改列名<br>
    alter table tablename change column c1 c2 int comment 'xxxxxxxxxx'<br><br><br>
after severity;//可以把该列放到指定列的后面，或者使用'first'放到第一位<br>
增加列<br>
    alter table tablename add columns(c1 string comment 'xxxxxxxxxx',c2 long comment 'yyyyyyyyyy')<br><br><br>
alter table test change column type type string after name;<br>
alter table test change column type type string first;<br>
alter table test change column type col2 int;<br><br><br>
修改tblproperties<br>
alter table table_name set tblproperties(property_name=property_value,property_name=property_value,...)<br>
针对无分区与有分区表不同<br>
alter table table_name<br>
set serdeproperties('field.delim'='\t');<br>
有分区表<br>
alter table test1 partition(dt='xxxxxx') set serdeproperties('field.delim'='\t');<br>
alter table test set tblproperties('comment'='xxxxxx');<br>
--无分区表<br>
create table city(<br>
time string,<br>
country string,<br>
province string,<br>
city string<br>
)<br>
row format delimited fields terminated by '#' lines terminated by '\n'<br>
stored as textfile<br>
load data local inpath '/home/data/city' into table city;<br>
alter table city set serdeproperties('field.delim'='\t');<br><br><br>
---分区表<br>
create table city(<br>
time string,<br>
country string,<br>
province string,<br>
city string<br>
)<br>
partitioned by (dt string)<br>
row format delimited fields terminated by '#' lines terminated by '\n'<br>
stored as textfile<br><br><br>
表属性操作<br>
修改location<br>
alter table table_name[partition()] set location 'path'<br>
alter table table_name set TBLPROPERTIES<br>
('EXTERNAL'='TRUE');//内部表转外部表<br>
alter table table_name set TBLPROPERTIES<br>
('EXTERNAL'='FALSE');//外部表转内部表<br><br><br>
alter table city set location 'hdfs://master:9000/location';<br>
alter table city set tblproperties('EXTERNAL'='TRUE');<br><br><br>
1、alter table properties<br>
2、alter serde properties<br>
3、alter table/partition file format<br>
4、alter table storage properties<br>
5、alter table rename partition<br>
6、alter table set location<br><br><br>
wiki LanguageManual DDL<br><br><br>
show partitions test_part;<br><br><br>
Hive高级查询<br>
查询操作<br>
group by、Order by、Join、distribute by、Sort by、cluster by、Union all<br><br><br>
底层的实现<br>
Mapreduce<br><br><br>
几个简单的聚合操作<br>
count 计数<br>
    count(*) count(1) count(col)<br>
sum求和<br>
    sum(可转成数字的值)返回bigint<br>
avg求平均值<br>
    avg（可转成数字的值）返回double<br>
distinct 不同值个数<br>
    count（distinct col）<br><br><br>
Order by<br>
按照某些字段顺序<br>
样例<br>
    select col1,other...<br>
    from table<br>
    where condition<br>
    order by col1,col2[asc|desc]<br>
注意<br>
    order by 后面可以有多列进行排序，默认按字典排序 <br>
    order by 为全局排序<br>
    order by 需要reduce操作，且只有一个reduce，与配置无关<br><br><br>
group by<br>
按照某些字段的值进行分组，有相同值放到一起<br>
样例<br>
    select col1[,col2],count(1),sel_expr（聚合操作）<br>
    from table<br>
    where condition<br>
    group by col1[,col2]<br>
    [having]<br>
注意<br>
    select 后面非聚合列必须出现在group by中<br>
    除了普通列就是一些聚合操作<br>
    group by 后面也可以跟表达式，比如substr（col）<br><br><br>
特性<br>
    使用了reduce操作，受限于reduce数量，设置reduce参数mapred.reduce.tasks<br>
    输出文件个数与reduce数相同，文件大小与reduce处理的数据有关<br>
问题<br>
    网络负载过重<br>
    数据倾斜，优化参数hive.groupby.skewindata<br><br><br>
set mapred.reduce.tasks=5;<br>
set hive.groupby.skewindata=true;<br><br><br>
select * from m order by col desc,col2 asc;<br>
----------------------------------------------------------<br>
--------------------join操作----------------------------<br>
表连接<br>
两个表m,n之间按照on条件连接，m中的一条记录和n中的一条记录组成一条新记录<br>
join等值连接，只有某个值在m和n中同时存在时<br>
left outer join左外连接，左边表中的值无论是否在b中存在时，都输出，右边表中的值只有在左边表中存在时才输出<br>
right outer join 和left outer join 相反<br>
left semi join 类似exists<br>
mapjoin 在map端完成join操作，不需要用reduce，基于内存做join，属于优化操作<br><br><br>
col col2 a<br>
1   w <br>
3   r<br>
5   e<br><br><br>
col3 col4 b<br>
1    f<br>
1    g<br>
5    j<br>
2    p<br><br><br>
(select col from a) s join<br>
(select col from b) t on s.col = t.col3<br><br><br>
结果：<br>
1 w f<br>
1 w g<br>
5 e j <br>
-------------------------------------------<br>
(select col from a) s left outer join<br>
(select col from b) t on s.col = t.col3<br>
结果：<br>
1 w f<br>
1 w g<br>
5 e j <br>
3 r null<br>
------------------------------------------<br>
(select col from a) s right outer join<br>
(select col from b) t on s.col = t.col3<br>
结果：<br>
1 w f<br>
1 w g<br>
5 e j <br>
2 p null<br>
-------------------------------------------<br>
select s.col,s.col2,t.col4<br>
(select col,col2 from a) s left semi join<br>
(select col3,col4 from b) t on s.col = t.col3<br><br><br>
1 w f<br>
5 e j<br><br><br>
--------------------------------------------------------------------------<br>
样例<br>
select m.col as col,m.col2 as col2,n.col3 as col3<br>
from <br>
(select col,col2 from test where ...(map端执行)) m(左表)<br>
[leftouter|right outer|left semi] join<br>
n(右表)<br>
on m.col = n.col<br>
where condition(reduce端执行)<br>
set hive.optimize.skewjoin = true;<br><br><br>
数据输出对比<br>
|------------|----------------|------------------|------------------|<br>
|数据        |   join         |   leftouterjoin  |  rightouterjoin  |<br>
|------------|----------------|------------------|------------------|<br>
|左表M       |  col col2 col3 |  col col2 col3   |   col col2   col3|<br>
|col col2    |  A   1    6    |  A   1     6     |    A    1      6 |<br>
|A   1       |  C   5    4    |  C   5     4     |    C    5      4 |<br>
|C   5       |  C   3    4    |  B   2     NULL  |    C    3      4 |<br>
|B   2       |                |  C   3     4     |    D    NULL   5 |<br>
|C   3       |                |                  |                  |<br>
|------------|                |                  |                  |<br>
|右表        |                |                  |                  |<br>
|col col3    |                |                  |                  |<br>
|C   4       |                |                  |                  |<br>
|D   5       |                |                  |                  |<br>
|A   6       |                |                  |                  |<br>
|------------|----------------|------------------|------------------|<br><br><br>
Mapjoin<br>
mapjoin(map side join)<br>
在map端把小表加载到内存中，然后读取大表，和内存中的小表完成连接操作<br>
其中使用了分布式缓存技术<br><br><br>
优缺点<br>
不消耗集群的reduce资源（reduce相对紧缺）<br>
减少了reduce操作，加快程序执行<br>
降低网络负载<br><br><br>
占用部分内存，所以加载到内存中的表不能过大，因为每个计算节点都会加载一次<br>
生成较多的小文件<br><br><br>
Mapjoin<br>
配置以下参数，是hive自动根据sql，选择使用common join或者map join<br>
set hive.auto.convert.join = true;<br>
hive.mapjoin.smalltable.filesize默认值是25mb<br>
第二种方式，手动指定<br>
select /*+mapjoin(n)*/m.col,m.col2,n.col3 from m<br>
join n<br>
on m.col = n.col<br>
简单总结一下，mapjoin的使用场景：<br>
1、关联操作中有一张表非常小<br>
2、不等值的链接操作<br><br><br>
select /*+mapjoin(n)*/ m.city,n.province from<br>
(select province,city from city) m<br>
join<br>
(select province from province) n<br>
on m.province = n.province<br><br><br>
load data local inpath '/home/data/city' into table city;<br>
set hive.auto.convert.join;<br>
set hive.auto.convert.join=true<br><br><br>
Hive 分桶<br>
分桶<br>
    对于每一个表（table）或者分区，Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分<br>
    Hive是针对某一列进行分桶<br>
    Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中<br><br><br>
好处<br>
    获得更高的查询处理效率。<br>
    是取样（smpling）更高效<br><br><br>
create table bucketed_user(<br>
id int,<br>
name string<br>
)<br>
clustered by(id) sorted by(name) into 4 buckets<br>
row format delimited fields terminated by '\t' stored as textfile;<br>
set hive.enforce.bucketing=true;<br><br><br>
分桶的使用<br>
select * from bucketed_user<br>
tablesample(bucket 1 out of 2 on id)<br>
bucket join<br>
set hive.optimize.bucketmapjoin = true;<br>
set hive.optimize.bucketmapjoin.sortedmerge = true;<br>
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;<br><br><br>
Hive 分桶<br>
连接两个在（包含连接列）相同列上划分了桶的表，可以使用Map端连接（Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这<br>
两个表都进行了桶操作。那么将保持相同列值的桶进行JOIN操作就可以，可以大大减少JOIN的数据量。<br>
对于map端连接的情况，两个表以相同方式划分桶。处理左边表内某个桶的mapper知道右边表内相匹配的行在对应的桶内。因此，mapper只需要获取那个桶（这只是右边表内存数据的一小部分）<br>
即可进行连接。这一优化方法并不一定要求两个表必须桶的个数相同，两个表的桶个数是倍数关系也可以。<br><br><br>
------------Distribute by 和 Sort by----------------------------------------------------------------<br>
Distribute分散数据<br>
distribute by col<br>
按照col列把数据分散到不同的reduce<br>
Sort排序<br>
sort by col2<br>
按照col列把数据排序<br>
select col1,col2 from M<br>
distribute by col1<br>
sort by col1 asc,col2 desc;<br>
两者结合出现，确保每个reduce的输出都是有序的<br><br><br>
对比<br>
distribute by 与 group by<br>
都是按key值划分数据<br>
都使用reduce操作<br>
唯一不同，distribute by只是单纯的分散数据，而group by把相同key的数据聚集到一起，后续必须是聚合操作<br><br><br>
--------------------------------------------<br>
order by 与 sort by<br>
order by 是全局排序<br>
sort by 只是确保每个reduce上面输出的数据有序，如果只有一个reduce时，和order by作用一样<br><br><br><br><br>
应用场景<br>
map输出的文件大小不均<br>
reduce输出文件大小不均<br>
小文件过多<br>
文件超大<br>
----------------------------------------<br>
cluster by<br>
把有相同值得数据聚集到一起，并排序<br>
效果<br>
cluster by col<br>
distribute by col order by col<br><br><br>
set mapred.reduce.tasks=5;<br>
insert overwrite table city<br>
select time,<br>
country,<br>
province,<br>
city<br>
from info <br>
distribute by province<br><br><br>
set mapred.reduce.tasks=1;<br>
insert overwrite table province partition(dt='20140901')<br>
select time,<br>
country,<br>
province,<br>
city<br>
from city<br>
distribute by country<br>
--------------------------------------<br>
Union all<br>
多个表的数据合并成一个表，hive不支持union<br>
样例<br>
select col<br>
from(<br>
select a as col from t1<br>
union all<br>
select b as col from t2<br>
)tmp<br>
------------------------------------<br>
Hive 函数<br>
目录<br>
函数分类<br>
|-----|-----------------------------------------|<br>
|     |               简单函数---map阶段        |<br>
|     |  <span> </span>     聚合函数---reduce阶段     |<br>
|     |<span> </span>     集合函数---map阶段        |<br>
|     |<span> </span>     特殊函数                  |<br>
|     |    内置函数                             |<br>
|函数-|-----------------------------------------|<br>
|     |    自定义函数   UDF           map阶段   |<br>
|     |<span> </span>               UDAF          reduce阶段|<br>
|-----|-----------------------------------------|<br><br><br><br><br>
内置函数<br>
正则表达式<br>
自定义函数<br><br><br>
cli命令<br>
1、显示当前会话有多少函数可用<br>
SHOW FUNCTIONS；<br>
2、显示函数的描述信息<br>
DESC FUNCTION concat；<br>
3、显示函数的扩展描述信息<br>
DESC FUNCTION EXTENDED concat;<br><br><br>
------------------简单函数----------------------------<br>
函数的计算粒度-单条记录<br>
关系运算<br>
数学运算<br>
逻辑运算<br>
数值运算<br>
类型转型<br>
日期函数<br>
条件函数<br>
字符串函数<br>
统计函数<br><br><br>
-----------------聚合函数-------------------------------<br>
函数处理的数据粒度-多条记录<br>
sum()-求和<br>
count()-求数据量<br>
avg()-求平均值<br>
distinct-求不同值数<br>
min-求最小值<br>
max-求最大值<br>
-----------------集合函数--------------------------<br>
复合类型构建<br>
复杂类型访问<br>
复杂类型长度<br>
-------------------特殊函数-----------------------<br>
窗口函数<span> </span><br>
分析函数<br>
混合函数<br>
UDTF<br>
----------------窗口函数--------------------------------<br>
应用场景<br>
    用于分区排序<br>
    动态Group By<br>
    Top N<br>
    累计计算<br>
    层次查询<br>
Windowing functions<br>
    lead<br>
    lag<br>
    FIRST_VALUE<br>
    LAST_VALUE<br>
------------------分析函数-------------------------------------<br>
The OVER clause<br>
  COUNT<br>
  SUM<br>
  MIN<br>
  MAX<br>
  AVG<br>
Analytics functions<br>
  RANK<br>
  ROW_NUMBER<br>
  DENSE_RANK<br>
  CUME_DIST<br>
  PERCENT_RANK<br>
  NTILE<br>
-------------------混合函数----------------------------<br>
java_method(class,method[,arg1[,arg2..]])<br>
reflect(class,method[,arg1[,arg2...]])<br>
hash(a1[,a2...])<br>
----------------UDTF----------------------<br>
表函数<br>
lateraView:LATERAL VIEW udtf(expression) tableAlias AS columnAlias(',',columnAlias) * fromClause:FROM baseTable(lateralView)*<br>
例<br>
explode函数<br>
------------------------------------------<br>
select id,money from winfunc<br>
where id='1001' or id='1002' and money='100'<br><br><br>
case(money as bigint)<br><br><br>
if(con,'','') case when con then when then else end<br><br><br>
get_json_object<br><br><br>
select get_json_object('{"name":"jack","age":"20"}','$.name') from winfunc limit 1<br>
(注意数据格式)<br><br><br>
select parse_url('http://baidu.com/path/p.php?k1=v1&amp;k2=v2#Ref1','HOST') from winfunc limit 1;<br><br><br>
concat<br>
concat_ws(string SEP,array&lt;string&gt;) 参数<br><br><br>
select concat(type,'123') from winfunc;<br><br><br>
collect_set()<br>
collect_list()<br><br><br>
sum(money)<br>
count(*)<br><br><br>
first_value(money) over(partition by id order by money rows between 1 preceding and 1 following)<br><br><br>
select id,name, first_value(money) over(partition by id order by money rows between 1 preceding and 3 following) from winfunc;<br><br><br>
select id,name,rank() over(partition by id order by name) from winfunc;<br><br><br>
select id,name,dense_rank() over(partition by id order by name) from winfunc;<br><br><br>
select id,name,money,cume_dist() over(partition by id order by money) from winfunc;<br><br><br>
select id,name,money,ntile(2) over (partition by id order by money desc) from winfunc;<br><br><br>
select id,name,money,java_method("java.lang.Math","sqrt",cast(id as double)) from winfunc;<br><br><br>
select id,adid from winfunc lateral view explode(split(type,'B')) tt as adid<br><br><br>
select userid,pageid,visitdate,rank() over(partition by userid order by pageid) from (select distinct userid,pageid,visitdate from test) a;<br><br><br>
select id,name,row_number() over(partition by id order by name) from winfunc;<br><br><br>
hive -e "select distinct userid,pageid, first_value(pageid) over(partition by userid order by pageid rows between 1 preceding and 5 following) first_value from test where visitdate='20150501'" &gt;test.txt<br><br><br>
hive -e "select distinct userid,pageid, last_value(pageid) over(partition by userid order by pageid rows between 1 preceding and 5 following) last_value from test where visitdate='20150501'" &gt;test.txt<br><br><br><br><br>
hive -e "select distinct userid,pageid, first_value(pageid) over(partition by userid order by pageid ) first_value from test where visitdate='20150501'" &gt;test.txt<br><br><br>
hive -e "select userid,pageid,visitdate,rank() over(partition by userid order by pageid) from test" &gt;test.txt<br><br><br><br><br>
lead(money,2) over(order by money)<br><br><br>
rank() over(partition by id order by money)<br>
dense_rank() over(partition by id order by money)<br><br><br>
cume_dist() over(partition by id order by money) ((相同值最大行号)/(行数))*每个值的个数，与前面的累加<br><br><br>
percetn_rank()over(partition by id order by money) ((相同值最小行号-1)/(行数-1))<br><br><br>
第一个总是从0开始的<br>
select id,money,cume_dist() over(partition by id order by money),percent_rank() over(partition by id order by money) from winfunc;<br><br><br>
ntile(2) over (order by money desc nulls last) 分片<br>
select id,money,ntile(2) over (order by money desc ) from winfunc;<br><br><br>
select java_method("java.lang.Math","sqrt",cast(id as double)) from winfunc;<br><br><br>
select id,adid from winfunc lateral view explode(split(type,'B')) tt as adid;<br><br><br><br><br>
select id,money,first_value(money) over(partition by id order by money) from winfunc;<br>
select id,money,first_value(money) over (partition by id order by money) rows between 1 preceding and 1 following) from winfunc;<br>
select id,money,lead(money,2) over(order by money) from winfunc;<br>
select id,money,rank() over(partition by id order by money) from winfunc;<br><br><br>
-------------------------------------------<br>
winfunc<br>
1001<span> </span>100.0<span> </span>
ABC<br>
1001<span> </span>150.0<span> </span>
BCD<br>
1001<span> </span>200.0<span> </span>
CDE<br>
1001<span> </span>150.0<span> </span>
DEF<br>
1002<span> </span>200.0<span> </span>
ABC<br>
1002<span> </span>200.0<span> </span>
ABC<br>
1002<span> </span>100.0<span> </span>
BCD<br>
1002<span> </span>300.0<span> </span>
CDE<br>
1002<span> </span>50.0<span> </span>
DEF<br>
1002<span> </span>400.0<span> </span>
EFG<br>
1003<span> </span>100.0<span> </span>
ABC<br>
1003<span> </span>50.0<span> </span>
BCD<br>
1004<span> </span>100.0<span> </span>
ABC<br>
1004<span> </span>90.0<span> </span>
ABC<br>
1004<span> </span>30.0<span> </span>
ABC<br>
1004<span> </span>80.0<span> </span>
ABC<br>
1004<span> </span>40.0<span> </span>
ABC<br>
1004<span> </span>70.0<span> </span>
ABC<br>
1004<span> </span>50.0<span> </span>
ABC<br>
1004<span> </span>60.0<span> </span>
ABC<br><br><br><br><br>
create table winfunc(id int,money float,name string)<br>
row format delimited fields terminated by '\t' lines terminated by '\n'<br>
stored as textfile<br><br><br>
------------------------------------------------<br>
select if(2&gt;1,v1,v2) from winfunc<br><br><br>
select case when id='1001' then 'v1' when id ='1002' then 'v2' else 'v3' end from winfunc<br><br><br><br><br>
正则表达式<br>
使用正则表达式的函数<br><br><br>
A LIKE B,字符‘_’表示任意单个字符，而字符“%”表示任意数量的字符<br>
A RLIKE B<br>
regexp_replace(string A,string B,string C)<br>
regexp_extract(string subject,string pattern,int index)<br><br><br>
select 1 from dual where 'footbar' rlike ^f.*r$;<br>
select regexp_replace('foobar','oo|ar',") from dual;<br>
select regexp_extract('foothebar','foo(.*?)(bar)',1) from winfunc;<br>
select regexp_extract('979|7.10.80|8684','.*\\|(.*)',1) from winfunc limit 1;<br>
select regexp_extract('979|7.10.80|8684','(.*?)\\|(,*)',1) from winfunc limit 1;<br>
-------------------自定义函数--------------------------------------------------------------<br>
UDF&lt;-- 自定义函数 --&gt;UDAF<br><br><br>
UDF<br>
UDF-用户自定义函数(user defined function)<br>
    针对单条纪录<br>
创建函数<br>
    自定义一个Java类<br>
    继承UDF类<br>
    重写evaluate方法<br>
    打jar包<br>
    hive执行add jar<br>
    hive执行创建模板函数<br>
    hql中使用<br><br><br>
---------------------------------------------------------------------------------------<br>
udf<br>
BigThan.java<br><br><br>
import org.apache.hadoop.hive.ql.exec.UDF;<br>
import org.apache.hadoop.io.Text;<br><br><br>
public class BigThan extends UDF{<br>
    public boolean evalute(final Text t1,final Text t2){<br>
        if(t1==null || t2==null){<br><span></span>   return false;<br><span></span>}<br>
        double num = Double.parseDouble(t1.toString());<br><span></span>double tmp = Double.parseDouble(t2.toString());<br><span></span>if(num&gt;tmp){<br><span></span>   return true;<br><span></span>}else{<br><span></span>   return false;<br><span></span>}<br>
    }<br>
}<br><br><br>
udftest.java<br><br><br>
import org.apache.hadoop.hive.ql.exec.UDF;<br>
public class udftest extends UDF{<br>
    public boolean evaluate(Text t1,Text t2){<br>
        if(t1==null || t2==null){<br><span></span>   return false;<br><span></span>}<br><span></span>double d1 = Double.parseDouble(t1.toString());<br><span></span>double d2 = Double.parseDouble(t2.toString());<br><span></span>if(d1&gt;d2){<br><span></span>   return true;<br><span></span>}else{<br><span></span>   return false;<br><span></span>}<br>
    }<br>
}<br><br><br>
add jar /home/jar/function.jar;<br>
create temporary function bigthan as 'com.peixun.udf.udftest';<br>
select name,addr.bigthan(addr.80) from<span> </span>testtext;<br><br><br>
UDAF<br>
UDAF用户自定义聚合函数<br>
user defined aggregation function<br>
针对纪录集合<br><br><br>
开发通过UDAF有两个步骤<br>
第一个是编写resolver类，resolver负责类型检查，操作符重载。<br>
第二个是编写evaluator类，evaluator真正实现UDAF的逻辑<br><br><br>
通常来说，顶层UDAF类继承<br>
org.apache.hadoop.hive.ql.udf.GenericUDAFResolver2,里面编写嵌套类evaluator实现UDAF的逻辑<br><br><br>
一、实现resolver<br>
resolver通常继承<br>
org.apache.hadoop.hive.ql.udf.GenericUDAFResolver2,但是更建议继承AbstractGenericUDAFResolver,隔离将来hive接口的变化。GenericUDAFResolver和GenericUDAFResolver2接口的区别是，<br>
后面的允许evaluator实现可以访问更多的信息，例如DISTINCT限定符，通配符FUNCTION(*).<br>
二、实现evaluator<br>
所有evaluators必须继承抽象类<br>
org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.子类必须实现它的一些抽象方法，实现UDAF的逻辑。<br>
Mode<br>
这个类比较重要，它表示了udaf在MapReduce的各个阶段，理解Mode的含义，就可以理解了hive的UDAF的运行流程。<br>
public static enum Mode{<br>
    PARTIAL1,<br>
    PARTIAL2,<br>
    FINAL,<br>
    COMPLETE<br>
};<br>
------------------------------------------------------------------------------------------------------------<br>
udaftest.java<br>
public class udaftest extends AbstractGenericUDAFResolver{<br>
    <br>
}<br>
CountBigThan.java<br>
public class CountBigThan extends AbstractGenericUDAFResolver{<br><br><br>
}<br>
----------------------------------------------------------------<br>
PARTIAL1:这个是MapReduce的map阶段：从原始数据到部分数据聚合，将会调用iterate()和terminatePartial()<br>
PARTIAL2:这个是MapReduce的map端的Combiner阶段，负责在map端合并map的数据；从部分数据聚合到部分数据聚合，将会调用merge（）和terminatePartial()<br>
FINAL:mapreduce的reduce阶段：从部分数据的聚合到完全聚合，将会调用merge（）和terminatePartiao()<br>
COMPLETE:如果出现了这个阶段，表示MapReduce只有map，没有reduce，所以map端就直接出结果了；从原始数据直接到完全聚合，将会调用iterate()和terminate()<br><br><br>
跟着源码学<br>
src\ql\src\java\org\apache\hadoop\hive\ql\udf\genreic<br><br><br>
-------------永久函数------------------------<br>
如果希望在hive中自定义一个函数，且能永久使用，则修改源码添加相应的函数类，然后在修改ql/src/java/org/apache/hadoop/hive/ql/exec/Function Registry.java类，添加相应的注册函数代码。<br>
registerUDF("parse_url",UDFParseUrl.class,false);<br>
新建hiverc文件<br>
jar包放到安装目录下或者指定目录下<br>
$HOME/.hiverc<br>
把初始化语句加载到文件中<br>
----------------------------------------------<br>
select bitthan(addr,80) from testtext;<br><br><br>
mapreduce阶段调用函数<br>
MAP<br>
    init()<br>
    iterate()<br>
    terminatePartial()<br>
REDUCE<br>
    init()<br>
    merge()<br>
    terminate()<br>
Combiner<br>
    merge()<br>
    terminatePartial()<br>
---------------------Hive HQL优化---------------<br>
Hive 执行<br>
Hive 表优化<br>
HiveSQL 优化<br>
Hive job优化<br>
Hive Map优化<br>
Hive Shuffle优化<br>
Hive Reduce优化<br>
Hive 权限管理<br>
---------------------------------------------------------------------------<br>
hive查询操作优化<br>
join优化<br>
    hive.optimize.skewjoin=true;如果是join过程出现倾斜 应该设置为true<br>
    set hive.skewjoin.key=1000000;--这个是join的键对应的记录条数超过这个值则会进行优化<br>
mapjoin<br>
    set hive.auto.convert.join=true;<br>
    hive.mapjoin.smalltable.filesize默认值是25mb<br>
    select /*+mapjoin(A*/ f.a,f.b from A t join B f on(f.a = t.a)<br>
简单总结一下,mapjoin的使用场景：<br>
    1、关联操作中有一张表非常小<br>
    2、不等值的链接操作<br>
----------------------------------------------------------------------------<br>
bucket join<br>
两个表以相同方式划分桶<br>
两个表的桶个数是倍数关系<br><br><br>
create table order(cid int,price float) clustered by(cid) into 32 buckets;<br>
create table customer(id int,first string) clustered by(id) into 32 buckets;<br>
select price from order t join customer s on t.cid=s.id<br><br><br>
join优化前 <br>
select m.cid,u.id from order m join customer u on m.cid = u.cid where m.dt = '2013-12-12';<br>
优化后<br>
select m.cid,u.id from (select cid from order where dt='2013-12-12') m join customer u on m.cid = u.id;<br>
--------------------------------------------------------------------------<br>
count distinct 优化<br>
优化前<br>
    Select count(distinct id) from tablename<br>
优化后<br>
    Select count(1) from (select distinct id from tablename) tmp;<br>
    Select count(1) from (select id from tablename group by id) tmp;<br>
----------------------------------------------------------------------------------<br>
select count(distinct city) from info;<br>
select count(distinct city) from (select distinct city from info) tmp;<br>
set mapred.reduce.tasks=3;<br>
-----------------------------------<br>
select a,sum(b),count(distinct c),count(distinct d) from test<br>
优化后<br>
select a,sum(b) as b,count(c) as c,count(d) as d from(<br>
select a,o as b,c,null as d from test group by a,c <br>
union all<br>
select a,o as b,null as c,d from test group by a,d<br>
union all<br>
select a,b,null as c,null as d from test<br>
) tmp1 group by a;<br>
-------Hive优化和权限管理------------------<br>
Hive优化目标<br>
    在有限的资源下，执行效率高<br>
常见问题<br>
    数据倾斜<br>
    Map数设置<br>
    Reduce数设置<br>
    其他<br>
---------------------------------<br>
Hive执行<br><br><br>
||<br>
VV<br>
HQL<br><br><br>
||<br>
VV<br>
Job<br><br><br>
||<br>
VV<br>
Map/Reduce<span> </span><br>
----------------------------------<br>
执行计划<br>
查看执行计划<br>
explain[extended]hql<br><br><br>
explain extended hql 查看更详细的执行计划<br><br><br>
样例<br>
select col,count(1) from test2 group by col;<br>
explain select col,count(1) from test2 group by col;<br>
------语法树-----------<br>
ABSTRACTSYNTAX TREE:<br>
(TOK_QUERY(TOK_FROM(TOK_TABREF<br>
(TOK_TABNAME test2)))(TOK_INSERT<br>
(TOK_DESTINATION(TOK_DIR TOK_TMP_FILE))<br>
(TOK_SELECT(TOK_SELEXPR<br>
(TOK_TABLE_OR_COL col))(TOK_SELEXPR<br>
(TOK_FUNCTION count1)))(TOK_GROUPBY<br>
(TOK_TABLE_OR_COL col))))<br>
------执行阶段--------------------------<br>
STAGE DEPENDENCIES:<br>
Stage-1 is a root stage<br>
Stage-0 is a root stage<br>
----------Hive执行过程（需要截图）----------<br><br><br>
----------Hive 表优化--------------<br>
分区<br>
    静态分区<br>
    动态分区<br>
    set hive.exec.dynamic.partition=true;<br>
    set hive.exec.dynamic.partition.mode=nonstrict;<br>
分桶<br>
   set hive.enforce.bucketing = true;<br>
   set hive.enforce.sorting = true;<br>
数据<br>
    相同数据尽量聚集在一起<br>
----------Hive job优化-----------------------------<br>
并行化执行<br>
每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间<br><br><br>
set hive.exec.parallel = true;<br>
set hive.exec.parallel.thread.numbe=8;<br>
本地化执行<br>
    set hive.exec.model.local.auto = true;<br>
当一个job满足如下条件才能真正使用本地模式:<br>
    1、job的输入数据大小必须小于参数：<br>
hive.exec.mode.local.auto.inputbytes.max（默认128MB）<br>
    2、job的map数必须小于参数:<br>
hive.exec.mode.local.auto.tasks.max(默认4)<br>
    3、job的reduce数必须为o或者1<br>
------------------------------------------------<br>
job合并输入小文件<br>
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat<br>
合并文件数由mapred.max.split.size限制的大小决定<br>
job合并输出小文件<br>
set hive.merge.smallfiles.avgsize=256000000;当输出文件平均大小小于该值，启动新job合并文件<br>
set hive.merge.size.per.task=64000000;合并之后的文件大小<br>
-------------------------------------------<br>
JVM重利用<br>
set mapred.job.reuse.jvm.num.tasks=20;<br>
JVM重利用可以是JOB长时间保留slot，直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为<br>
有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。<br>
---------------------------------------------<br>
压缩数据<br>
中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式<br>
set hive.exec.compress.intermediate = true;<br>
set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;<br>
set hive.intermediate.compression.type=BLOCK;<br>
hive查询最终的输出也可以压缩<br>
set hive.exec.compress.output=true;<br>
set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;<br>
set mapred.output.compression.type=BLOCK;<br>
-------------------------------------------------<br>
set mapred.map.tasks=10;<br>
(1)默认map个数<br>
default_num = total_size/block_size;<br>
(2)期望大小<br>
goal_num=mapred.map.tasks;<br>
(3)设置处理的文件大小<br>
split_size=max(mapred.min.split.size,block_size);<br>
split_num=total_size/split_size;<br>
(4)计算的map个数<br>
compute_map_num = min(split_num,max(default_num,goal_num))<br>
--------------------------------------------<br>
Hive Map 优化<br>
经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点：<br>
(1)如果想增加map个数，则设置mapred.map.tasks为一个较大的值。<br>
(2)如果想减少map个数，则设置mapred.min.split.size为一个较大的值。<br>
情况1：输入文件size巨大，但不是小文件<br>
增大mapred.min.split.size的值<br>
情况2:输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用CombineFileInputFormat将多个input path合并成一个<br>
InputSplit送给mapper处理，从而减少mapper的数量。<br>
--------------------------------------<br>
map端聚合<br>
set hive.map.aggr=true;<br>
推测执行<br>
mapred.map.tasks.speculative.execution<br>
--------Hive Shuffle优化-------------------------------<br><br><br>
Map端<br>
io.sort.mb<br>
io.sort.spill.percent<br>
min.num.spill.for.combine<br>
io.sort.factor<br>
io.sort.record.percent<br><br><br>
Reduce端<br>
mapred.reduce.parallel.copies<br>
mapred.reduce.copy.backoff<br>
io.sort.factor<br>
mapred.job.shuffle.input.buffer.percent<br>
mapred.job.shuffle.input.buffer.percent<br>
mapred.job.reduce.input.buffer.percent<br>
--------------Hive Reduce优化------------<br>
需要reduce操作的查询<br>
聚合函数<br>
    sum,count,distinct...<br>
高级查询<br>
    group by,join,distribute by,cluster by ...<br>
    order by 比较特殊，只需要一个reduce<br>
推测执行<br>
    mapred.reduce.tasks.speculative.execution<br>
    hive.mapred.reduce.tasks.speculative.execution<br><br><br>
Reduce 优化<br>
set mapred.reduce.tasks=10;直接设置<br>
hive.exec.reducers.max<br>
hive.exec.reducers.bytes.per.reducer 默认:1G<br>
计算公式<br>
numRTasks = min[maxReducers,input.size/perReducer]<br>
maxReducers = hive.exec.reducers.max<br>
perReducer = hive.exec.reducers.bytes.per.reducer<br>
----------------HIVE 案例实战---------------------------------<br>
日志处理流程<br>
Flume-ng<br>
Kafka<br>
Flume-ng+Kafka+Hdfs<br>
Hive仓库<br>
日志处理<br><br><br>
日志处理流程<br>
数据收集 ==》数据清洗 ==》数据存储与管理 ==》数据分析 ==》数据显示<br><br><br>
--------------Flume-ng介绍-------------------------------------<br>
Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统<br>
Flume支持在日志系统中定制各类数据发送方，用于收集数据<br>
Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。<br>
当前Flume有两个版本Flume0.9X版本的统称Flume-og,Flume1.X版本的统称Flume-ng。<br>
主要元素<br>
agent source channel sink<br><br><br>
Agent使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks<br>
Client生产数据，运行在一个独立的线程<br>
Source从Client收集数据，传递给Channel<br>
Sink从Channel收集数据，运行在一个独立线程<br>
Channel连接sources和sinks<br>
Events可以是日志记录、avro对象等<br><br><br>
https://cwiki.apache.org/confluence/display/FlUME/Getting+Started<br>
-----------------------------------------------------------<br>
Flume安装<br>
修改配置文件<br>
agent.sources=r1<br>
agent.sinks=s1<br>
agent.channels=c1<br>
agent.source.r1.channels=c1<br>
agent.sinks.s1.channel=c1<br>
#Describe/configure the source<br>
agent.sources.r1.type=exec<br>
agent.sources.r1.command=tail-F /home/flume/loginfo<br>
#Use a channel which buffers events in memory<br>
agent.channels.c1.type=memory<br>
agent.channels.c1.capacity=1000<br>
agent.channels.c1.transcationCapacity=100<br>
agent.sinks.s1.type=logger<br><br><br>
启动Flume<br>
bin/flume-ng agent --conf ./conf/ -f conf/flume-conf.properties-Dflume.root.logger=DEBUG,console-n agent<br>
------------------------------------------------------------------------------<br>
Kafka安装<br>
Kafka是高吞吐量日志处理的分布式消息队列<br>
Kafka几个概念<br>
    broker<br>
    producer<br>
    consumer<br>
    topic<br>
    partition<br><br><br>
 下载安装包<br>
 解压安装包<br>
 tar -xzf kafka-&lt;VERSION&gt;.tgz<br>
 编译代码<br>
 ./sbt update<br>
 ./sbt package<br>
 配置环境变量，生效配置文件<br>
 export KAFKA_HOME=/home/kafka/kafka-0.7.2-incubating-src<br>
 export KAFKA_CONF_DIR=/home/kaflka-0.7.2-incubating-src/config<br>
 export PATH=$PATH:$KAFKA_HOME/bin<br><br><br>
 使用kafka几个操作<br>
 bin/zookeeper-server-start.sh config/zookeeper.properties<br>
 bin/kafka-server-start.sh config/server.properties<br>
 bin/kafka-console-producer.sh --zookeeper localhost:2181 --topic test<br>
 bin/kafka-console-consumer.sh --zookeeper localhost:2181 -topic test --from-beginning<br><br><br>
 学习地址<br>
 http://kafka.apache.org/07/quickstart.html<br>
----------------Flume----------------------------------------------------------<br>
主要配置Flume<br>
同一个source多个sink<br>
HDFS<br>
Kafka（特殊性需要自定义）<br>
注意几个问题<span> </span><br>
JDK版本<br>
Hadoop jar 包版本<br>
hdfs 端口问题<br><br><br>
-----------hive数据仓库------------------------------------------<br>
内容<br>
    不同格式数据源处理<br>
    不同数据格式统一格式<br>
    不同来源数据统一字段<br>
    非统一字段使用集合<br>
    来自不同来源使用分区<br><br><br>
-------------日志处理------------------------------------------<br>
Flume收集日志<br>
日志分发<br>
    hdfs<br>
    kafka<br>
日志处理<br>
    MR处理HDFS数据<br>
    Spark处理kafka数据<br>
Hive管理HDFS数据<br>
    编写hql统计数据<br>
----------------------------------------------------------------<br>
create table testflume (a string,b string) row format delimited fields terminated<br>
by '\t' lines terminated by '\n' stored as textfile location '/root/sou'<br>
-----------------------------------------------------------------<br>
desc fromated testflume;<br>
alter table testflume set tblproperties('EXTERNAL'='TRUE');<br>
select * from testflume;<br>
---------------------------------------------<br>
LogHandler.java<br>
import java.io.IOException;<br>
public class LogHandler extends Configuraed implements Tool{<br>
    public static void main(String[] args) throws Exception{<br>
        int exit = ToolRunner.run(new LogHandler(),args);<br><span></span>System.exit(exit);<br>
    }<br>
    @Override<br>
    public int run(String[] args) throws Exception{<br>
       Configuration conf = new Configuration();<br>
       Path dst = new Path(args[1]);<br>
       FileSystem fs = FileSystem.get(new Path("hdfs://192.168.1.198:8020")).toUri(),conf);<br>
       if(fs.exists(dst)){<br>
           fs.delete(dst,true);<br>
       }<br>
       Job job = new Job(conf,"LongHandler");<br>
       job.setMapperClass(LogMapper.class);<br>
       Job.setOutputKeyClass(Text.class);<br>
       Job.setOutputValue(Text.class);<br>
       FileInputFormat.addInputPath(job,new Path(args[0]));<br>
       FileOutputFormat.setOutputPath(job,new Path(args[1]));<br><br><br>
       boolean success = job.waitForCompletion(true );<br>
       return success ? 0 : 1;<br>
    }<br><br><br>
}<br>
public static class LogMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt;{<br>
    @Override<br>
    protected void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{<br>
        String str = value.toString().replace(",","\t");<br><span></span>context.write(new Text(str),new Text(""));<br>
    }<br>
}<br><br><br>
public static class LogReducer extends Reducer&lt;Text,Text,Text,Text&gt;{<br>
    @Override<br>
    protected void reduce(Text key,Iterable&lt;Text&gt; it,Context context) throws IOException,InterruptedException{<br>
        context.write(key,new Text("")); <br>
    }<br>
}<br><br><br>
hdfs://192.168.1.198:8020/root/source hdfs://192.168.1.198:8020/root/result<br>
-------电信案例实践------------------------------------------------------------------------------------------------------------------<br>
订单商品模块<br>
--订单主要信息表ods_b2c_orders<br>
drop table if exists itqsc.ods_b2c_orders;<br>
create external table itqsc.ods_b2c_orders(<br>
order_id bigint     , --订单ID<br>
order_no string     , --订单号<br>
order_date timestamp, --订单日期<br>
user_id bigint      , --用户ID<br>
user_name string    , --登录名<br>
order_money double  , --订单金额<br>
order_type string   , --订单类型<br>
order_status string , --订单状态<br>
pay_type string     , --支付类型<br>
pay_status string   , --支付状态<br>
order_source string , --订单来源<br>
last_update_time timestamp, --订单的最后修改时间<br>
dw_date timestamp<br>
)<br>
partitioned by<br>
(dt string)<br>
LOCATION 'hdfs://hadoop0:9000/user/hadoop/dev/itqsc/ods_b2c_orders';<br>
------------------------------------------------------------------------------<br>
--订单商品信息表ods_b2c_orders_goods<br>
--订单的详细信息表ods_b2c_ordrs_desc<br>
--订单与商品宽表dm_b2c_orders_goods<br>
--订单宽表dm_b2c_orders<br>
--购物车表ods_b2c_cart<br>
--订单指标表dm_user_order_tag<br>
--商品信息表ods_b2c_goods<br>
--商品信息汇总表dm_user_goods_amt<br>
--shell脚本调用--------------------------------<br>
#!/bin/bash<br>
#============================<br>
#dm_b2c_orders.sh<br>
#==============================<br>
DT='date -d '-1 day' "+%Y-%m-%d"'<br>
sysdate = 'date "+%Y-%m-%d"'<br>
if [$1];then<br>
    DT=$1<br>
fi<br><br><br>
SQL="<br>
insert overwrite table itqsc.dm_b2c_orders partition(dt='"${DT}"')<br>
select a.order_id,<br>
       a.order_no,<br>
       a.order_date,<br>
       a.user_id,<br>
       a.user_name,<br>
       a.order_money,<br>
       a.order_type,<br>
       a.order_status,<br>
       a.pay_type,<br>
       a.pay_status,<br>
       a.order_source,<br>
       b.consignee,<br>
       b.area_id,<br>
       b.area_name,<br>
       b.address,<br>
       b.mobilephone,<br>
       b.telphone,<br>
       b.coupon_id,<br>
       b.coupon_money,<br>
       b.carriage_money,<br>
       b.create_time,<br>
       a.last_update_time,<br>
       '"${sysdate()}"' dw_date<br>
       from (select * from itqsc.ods_b2c_orders where dt = '"${DT}"') a<br>
       join (select * from itqsc.ods_b2c_orders_desc where dt = '"${DT}"') b<br>
       on (a.order_id = b.order_id);<br>
"<br>
 echo "${SQL}"<br>
hive -e "$SQL"<br>
-------------------------------------------------------------------------------------------<br>
集合类型<br>
ARRAY：ARRAY类型是由一系列相同数据类型的元素组成，这些元素可以通过下标来访问，例array[1]<br>
MAP:MAP包含key-》value键值对，可以通过key来访问元素，例map['key']<br>
STRUCT:可以包含不同数据类型的元素，这些元素可以通过“点语法”的方式获得，例struct.key1<br><br><br><br><br>
Hive基本使用-文件<br>
文件格式<br>
textfile<br>
Sequencefile<br>
Rcfile<br><br><br>
扩展接口<br>
默认的文件读取方式<br>
自定义inputformat<br>
自定义serde<br><br><br>
load inpath '/home/data/data' overwrite into table testtable;<br>            </div>
                </div>