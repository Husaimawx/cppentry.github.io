---
layout:     post
title:      搭建Spark服务器环境
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>一、spark服务器环境：<br>
1.配置java环境；<br>
2.下载spark包和对应版本的hadoop包；<br>
3.进入spark安装目录，在conf目录下spark-env.sh文件的最后加入环境变量：</p>

<pre class="has">
<code>LD_LIBRARY_PATH=$HADOOP_HOME/lib/native</code></pre>

<p><br>
4.执行sbin/start-master.sh和sbin/start-slave.sh spark://hostname:7077</p>

<p>启动spark并检查是否有错误和警告日志；</p>

<p>访问master：<a href="http://i/" rel="nofollow">http://i</a>p:8080</p>

<p>5.提交python作业：</p>

<pre class="has">
<code>bin/spark-submit wordcount.py</code></pre>

<p>6.提交jar作业：</p>

<pre class="has">
<code>bin/spark-submit --class com.test.spark.SparkTest --num-executors 3 --driver-memory 512M --executor-memory 512M --executor-cores 1 --master spark://hostname:7077 /home/guest/bigdata/spark-2.3.0-bin-hadoop2.7/test.jar</code></pre>

<p> </p>            </div>
                </div>