---
layout:     post
title:      hive 2.0 安装配置
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><br></p>
<p>1. Hive的安装<br>
系统环境<br>
装好hadoop的环境后，我们可以把Hive装在namenode机器上(c1)。<br>
hadoop的环境，请参考Hadoop环境搭建<br><br><br>
下载: apache-hive-2.0.0-bin.tar.gz <br>
解压到： /home/hadoop/hadoop/hive<br><br><br>
hive配置<br><br><br>
~ cd /home/hadoop/hadoop/hive/conf<br>
~ cp hive-default.xml.template hive-site.xml<br>
~ cp hive-log4j2.properties.template hive-log4j2.properties<br><br><br>
修改hive-site.xml配置文件<br>
把Hive的元数据存储到MySQL中<br><br><br>
~ vi conf/hive-site.xml<br><br><br>
&lt;configuration&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;<br>
&lt;value&gt;hdfs://v001:9000/usr/hive/warehouse&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;<br>
&lt;value&gt;hdfs://v001:9000/usr/hive/warehouse&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.querylog.location&lt;/name&gt;<br>
&lt;value&gt;/usr/hive/log&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt; <br>
&lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; <br>
&lt;value&gt;true&lt;/value&gt; <br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;datanucleus.autoCreateTables&lt;/name&gt;<br>
&lt;value&gt;true&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;datanucleus.autoCreateColumns&lt;/name&gt;<br>
&lt;value&gt;true&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>
&lt;value&gt;jdbc:mysql://v002:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;<br>
&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt; <br>
&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; <br>
 &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; <br>
&lt;description&gt;驱动名&lt;/description&gt; <br>
 &lt;/property&gt; <br>
&lt;property&gt; <br>
 &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; <br>
&lt;value&gt;hive&lt;/value&gt; <br>
 &lt;description&gt;用户名&lt;/description&gt; <br>
 &lt;/property&gt; <br>
 &lt;property&gt; <br>
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; <br>
 &lt;value&gt;hive&lt;/value&gt; <br>
&lt;description&gt;密码&lt;/description&gt; <br>
&lt;/property&gt; <br>
&lt;property&gt; <br>
 &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; <br>
&lt;value&gt;/usr/hive/warehouse&lt;/value&gt; <br>
 &lt;description&gt;数据路径（相对hdfs）&lt;/description&gt; <br>
 &lt;/property&gt;<br>
 &lt;property&gt; <br>
 &lt;property&gt;<br>
 &lt;name&gt;hive.exec.scratdir&lt;/name&gt;<br>
 &lt;value&gt;/usr/hive/tmp&lt;/value&gt;<br>
 &lt;/property&gt;<br>
 &lt;property&gt;<br>
 &lt;name&gt;hive.querylog.location&lt;/name&gt;<br>
 &lt;value&gt;/usr/hive/log&lt;/value&gt;<br>
 &lt;/property&gt;<br>
&lt;name&gt;hive.metastore.uris&lt;/name&gt; <br>
 &lt;value&gt;thrift://v001:9083&lt;/value&gt; <br>
&lt;description&gt;运行hive的主机地址及端口&lt;/description&gt; <br>
&lt;/property&gt; <br>
&lt;/configuration&gt;<br><br><br><br><br>
设置环境变量<br><br><br>
~ sudo vi /etc/environment<br><br>
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/hadoop/hive/toolkit/ant184/bin:/home/hadoop/hive/toolkit/jdk16/bin:/home/hadoop/hive/toolkit/maven3/bin:/home/hadoop/hive/toolkit/hadoop-1.0.3/bin:/home/hadoop/hive/toolkit/hive-0.9.0/bin"<br><br><br>
export JAVA_HOME=/opt/jdk1.7.0_55<br>
export HADOOP_HOME=/home/hadoop/hadoop<br>
export HIVE_HOME=/home/hadoop/hive<br>
export CLASS_PATH=$CALSSPATH:$HIVE_HOME/lib<br>
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH:$HIVE_HOME/bin<br><br><br>
在hdfs上面，创建目录<br><br>
$HADOOP_HOME/bin/hadoop fs -mkidr /tmp<br>
$HADOOP_HOME/bin/hadoop fs -mkidr /usr/hive/warehouse<br>
$HADOOP_HOME/bin/hadoop fs -mkidr /usr/hive/log<br>
$HADOOP_HOME/bin/hadoop fs -mkidr /usr/hive/tmp<br>
$HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp<br>
$HADOOP_HOME/bin/hadoop fs -chmod g+w /usr/hive/warehouse<br>
$HADOOP_HOME/bin/hadoop fs -chmod g+w /usr/hive/log<br>
$HADOOP_HOME/bin/hadoop fs -chmod g+w /usr/hive/tmp<br><br><br>
在MySQL中创建数据库<br><br>
create database hive;<br>
grant all on hive.* to hive@'%' identified by 'hive';<br>
grant all on hive.* to hive@localhost identified by 'hive';<br>
ALTER DATABASE hive CHARACTER SET latin1;<br><br><br>
手动上传mysql的jdbc库到hive/lib<br><br><br>
~ ls /home/hadoop/hive/lib<br>
mysql-connector-java-5.1.17.jar<br><br><br>
启动hive<br><br>
#启动metastore服务<br>
~ bin/hive --service metastore &amp;<br>
Starting Hive Metastore Server<br><br><br>
ps ax|grep metastore<br><br>
#启动hiveserver服务<br>
~ bin/hive --service hiveserver2 &amp;<br>
Starting Hive Thrift Server<br><br>
ps ax|grep HiveServer2<br><br><br>
#启动hive客户端<br>
~ bin/hive shell<br>
Logging initialized using configuration in file:/root/hive-0.9.0/conf/hive-log4j.properties<br>
Hive history file=/tmp/root/hive_job_log_root_201211141845_1864939641.txt<br><br><br>
hive&gt; show tables<br>
OK<br>
查询MySQL数据库中的元数据<br><br><br>
~ mysql -h v002 -u hive -p<br>
mysql&gt; use hive;<br>
Database changed<br><br><br>
mysql&gt; show tables;<br>
+---------------------------+<br>
| Tables_in_hive            |<br>
+---------------------------+<br>
| bucketing_cols            |<br>
| cds                       |<br>
| columns_v2                |<br>
| database_params           |<br>
| dbs                       |<br>
| func_ru                   |<br>
| funcs                     |<br>
| global_privs              |<br>
| part_col_stats            |<br>
| partition_key_vals        |<br>
| partition_keys            |<br>
| partition_params          |<br>
| partitions                |<br>
| roles                     |<br>
| sd_params                 |<br>
| sds                       |<br>
| sequence_table            |<br>
| serde_params              |<br>
| serdes                    |<br>
| skewed_col_names          |<br>
| skewed_col_value_loc_map  |<br>
| skewed_string_list        |<br>
| skewed_string_list_values |<br>
| skewed_values             |<br>
| sort_cols                 |<br>
| tab_col_stats             |<br>
| table_params              |<br>
| tbl_privs                 |<br>
| tbls                      |<br>
| version                   |<br>
+---------------------------+<br>
30 rows in set (0.00 sec)<br><br><br>
Hive已经成功安装，下面是hive的使用攻略。<br><br><br>
2. Hive的基本使用<br>
1. 进入hive控制台<br><br><br><br><br>
~ cd /home/hadoop/hive<br><br><br>
~ bin/hive shell<br>
Logging initialized using configuration in file:/home/hadoop/hive/toolkit/hive-0.9.0/conf/hive-log4j.properties<br>
Hive history file=/tmp/cos/hive_job_log_cos_201307160003_95040367.txt<br>
hive&gt;<br>
新建表<br><br><br>
#创建数据(文本以tab分隔)<br>
~ vi /home/hadoop/hive/demo/t_hive.txt<br><br>
16      2       3<br>
61      12      13<br>
41      2       31<br>
17      21      3<br>
71      2       31<br>
1       12      34<br>
11      2       34<br><br><br>
#创建新表<br>
hive&gt; CREATE TABLE t_hive (a int, b int, c int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';<br>
OK<br>
Time taken: 0.489 seconds<br><br><br>
#导入数据t_hive.txt到t_hive表<br>
hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/hive/demo/t_hive.txt' OVERWRITE INTO TABLE t_hive ;<br>
Copying data from file:/home/hadoop/hive/demo/t_hive.txt<br>
Copying file: file:/home/hadoop/hive/demo/t_hive.txt<br>
Loading data to table default.t_hive<br>
Deleted hdfs://c1.wtmart.com:9000/user/hive/warehouse/t_hive<br>
OK<br>
Time taken: 0.397 seconds<br>
查看表和数据<br><br><br>
#查看表 <br>
hive&gt; show tables;<br>
OK<br>
t_hive<br>
Time taken: 0.099 seconds<br><br><br>
#正则匹配表名<br>
hive&gt;show tables '*t*';<br>
OK<br>
t_hive<br>
Time taken: 0.065 seconds<br><br><br>
#查看表数据<br>
hive&gt; select * from t_hive;<br>
OK<br>
16      2       3<br>
61      12      13<br>
41      2       31<br>
17      21      3<br>
71      2       31<br>
1       12      34<br>
11      2       34<br>
Time taken: 0.264 seconds<br><br><br>
#查看表结构<br>
hive&gt; desc t_hive;<br>
OK<br>
a       int<br>
b       int<br>
c       int<br>
Time taken: 0.1 seconds<br>
修改表<br><br><br>
#增加一个字段<br>
hive&gt; ALTER TABLE t_hive ADD COLUMNS (new_col String);<br>
OK<br>
Time taken: 0.186 seconds<br>
hive&gt; desc t_hive;<br>
OK<br>
a       int<br>
b       int<br>
c       int<br>
new_col string<br>
Time taken: 0.086 seconds<br><br><br>
#重命令表名<br>
~ ALTER TABLE t_hive RENAME TO t_hadoop;<br>
OK<br>
Time taken: 0.45 seconds<br>
hive&gt; show tables;<br>
OK<br>
t_hadoop<br>
Time taken: 0.07 seconds<br>
删除表<br><br><br>
hive&gt; DROP TABLE t_hadoop;<br>
OK<br>
Time taken: 0.767 seconds<br><br><br>
hive&gt; show tables;<br>
OK<br>
Time taken: 0.064 seconds<br><br><br>
3. Hive交互式模式<br>
quit,exit:  退出交互式shell<br>
reset: 重置配置为默认值<br>
set &lt;key&gt;=&lt;value&gt; : 修改特定变量的值(如果变量名拼写错误，不会报错)<br>
set :  输出用户覆盖的hive配置变量<br>
set -v : 输出所有Hadoop和Hive的配置变量<br>
add FILE[S] *, add JAR[S] *, add ARCHIVE[S] * : 添加 一个或多个 file, jar, archives到分布式缓存<br>
list FILE[S], list JAR[S], list ARCHIVE[S] : 输出已经添加到分布式缓存的资源。<br>
list FILE[S] *, list JAR[S] *,list ARCHIVE[S] * : 检查给定的资源是否添加到分布式缓存<br>
delete FILE[S] *,delete JAR[S] *,delete ARCHIVE[S] * : 从分布式缓存删除指定的资源<br>
! &lt;command&gt; :  从Hive shell执行一个shell命令<br>
dfs &lt;dfs command&gt; :  从Hive shell执行一个dfs命令<br>
&lt;query string&gt; : 执行一个Hive 查询，然后输出结果到标准输出<br>
source FILE &lt;filepath&gt;:  在CLI里执行一个hive脚本文件<br><br><br>
4. 数据导入<br>
还以刚才的t_hive为例。<br><br><br>
#创建表结构<br>
hive&gt; CREATE TABLE t_hive (a int, b int, c int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';<br>
从操作本地文件系统加载数据(LOCAL)<br><br><br>
hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/hive/demo/t_hive.txt' OVERWRITE INTO TABLE t_hive ;<br>
Copying data from file:/home/hadoop/hive/demo/t_hive.txt<br>
Copying file: file:/home/hadoop/hive/demo/t_hive.txt<br>
Loading data to table default.t_hive<br>
Deleted hdfs://c1.wtmart.com:9000/user/hive/warehouse/t_hive<br>
OK<br>
Time taken: 0.612 seconds<br><br><br>
#在HDFS中查找刚刚导入的数据<br>
~ hadoop fs -cat /user/hive/warehouse/t_hive/t_hive.txt<br><br><br>
16      2       3<br>
61      12      13<br>
41      2       31<br>
17      21      3<br>
71      2       31<br>
1       12      34<br>
11      2       34<br>
从HDFS加载数据<br><br><br>
创建表t_hive2<br>
hive&gt; CREATE TABLE t_hive2 (a int, b int, c int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';<br><br><br>
#从HDFS加载数据<br>
hive&gt; LOAD DATA INPATH '/user/hive/warehouse/t_hive/t_hive.txt' OVERWRITE INTO TABLE t_hive2;<br>
Loading data to table default.t_hive2<br>
Deleted hdfs://c1.wtmart.com:9000/user/hive/warehouse/t_hive2<br>
OK<br>
Time taken: 0.325 seconds<br><br><br>
#查看数据<br>
hive&gt; select * from t_hive2;<br>
OK<br>
16      2       3<br>
61      12      13<br>
41      2       31<br>
17      21      3<br>
71      2       31<br>
1       12      34<br>
11      2       34<br>
Time taken: 0.287 seconds<br>
从其他表导入数据<br><br><br><br><br>
hive&gt; INSERT OVERWRITE TABLE t_hive2 SELECT * FROM t_hive ;<br><br><br>
Total MapReduce jobs = 2<br>
Launching Job 1 out of 2<br>
Number of reduce tasks is set to 0 since there's no reduce operator<br>
Starting Job = job_201307131407_0002, Tracking URL = http://c1.wtmart.com:50030/jobdetails.jsp?jobid=job_201307131407_0002<br>
Kill Command = /home/hadoop/hive/toolkit/hadoop-1.0.3/libexec/../bin/hadoop job  -Dmapred.job.tracker=hdfs://c1.wtmart.com:9001 -kill job_201307131407_0002<br>
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0<br>
2013-07-16 10:32:41,979 Stage-1 map = 0%,  reduce = 0%<br>
2013-07-16 10:32:48,034 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec<br>
2013-07-16 10:32:49,050 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec<br>
2013-07-16 10:32:50,068 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec<br>
2013-07-16 10:32:51,082 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec<br>
2013-07-16 10:32:52,093 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec<br>
2013-07-16 10:32:53,102 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec<br>
2013-07-16 10:32:54,112 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.03 sec<br>
MapReduce Total cumulative CPU time: 1 seconds 30 msec<br>
Ended Job = job_201307131407_0002<br>
Ended Job = -314818888, job is filtered out (removed at runtime).<br>
Moving data to: hdfs://c1.wtmart.com:9000/tmp/hive-cos/hive_2013-07-16_10-32-31_323_5732404975764014154/-ext-10000<br>
Loading data to table default.t_hive2<br>
Deleted hdfs://c1.wtmart.com:9000/user/hive/warehouse/t_hive2<br>
Table default.t_hive2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 56, raw_data_size: 0]<br>
7 Rows loaded to t_hive2<br>
MapReduce Jobs Launched:<br>
Job 0: Map: 1   Cumulative CPU: 1.03 sec   HDFS Read: 273 HDFS Write: 56 SUCCESS<br>
Total MapReduce CPU Time Spent: 1 seconds 30 msec<br>
OK<br>
Time taken: 23.227 seconds<br><br><br>
hive&gt; select * from t_hive2;<br>
OK<br>
16      2       3<br>
61      12      13<br>
41      2       31<br>
17      21      3<br>
71      2       31<br>
1       12      34<br>
11      2       34<br>
Time taken: 0.134 seconds<br>
创建表并从其他表导入数据<br><br><br>
#删除表<br>
hive&gt; DROP TABLE t_hive;<br><br><br>
#创建表并从其他表导入数据<br>
hive&gt; CREATE TABLE t_hive AS SELECT * FROM t_hive2 ;<br><br><br>
Total MapReduce jobs = 2<br>
Launching Job 1 out of 2<br>
Number of reduce tasks is set to 0 since there's no reduce operator<br>
Starting Job = job_201307131407_0003, Tracking URL = http://c1.wtmart.com:50030/jobdetails.jsp?jobid=job_201307131407_0003<br>
Kill Command = /home/hadoop/hive/toolkit/hadoop-1.0.3/libexec/../bin/hadoop job  -Dmapred.job.tracker=hdfs://c1.wtmart.com:9001 -kill job_201307131407_0003<br>
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0<br>
2013-07-16 10:36:48,612 Stage-1 map = 0%,  reduce = 0%<br>
2013-07-16 10:36:54,648 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec<br>
2013-07-16 10:36:55,657 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec<br>
2013-07-16 10:36:56,666 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec<br>
2013-07-16 10:36:57,673 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec<br>
2013-07-16 10:36:58,683 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec<br>
2013-07-16 10:36:59,691 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.13 sec<br>
MapReduce Total cumulative CPU time: 1 seconds 130 msec<br>
Ended Job = job_201307131407_0003<br>
Ended Job = -670956236, job is filtered out (removed at runtime).<br>
Moving data to: hdfs://c1.wtmart.com:9000/tmp/hive-cos/hive_2013-07-16_10-36-39_986_1343249562812540343/-ext-10001<br>
Moving data to: hdfs://c1.wtmart.com:9000/user/hive/warehouse/t_hive<br>
Table default.t_hive stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 56, raw_data_size: 0]<br>
7 Rows loaded to hdfs://c1.wtmart.com:9000/tmp/hive-cos/hive_2013-07-16_10-36-39_986_1343249562812540343/-ext-10000<br>
MapReduce Jobs Launched:<br>
Job 0: Map: 1   Cumulative CPU: 1.13 sec   HDFS Read: 272 HDFS Write: 56 SUCCESS<br>
Total MapReduce CPU Time Spent: 1 seconds 130 msec<br>
OK<br>
Time taken: 20.13 seconds<br><br><br>
hive&gt; select * from t_hive;<br>
OK<br>
16      2       3<br>
61      12      13<br>
41      2       31<br>
17      21      3<br>
71      2       31<br>
1       12      34<br>
11      2       34<br>
Time taken: 0.109 seconds<br>
仅复制表结构不导数据<br><br><br>
hive&gt; CREATE TABLE t_hive3 LIKE t_hive;<br>
hive&gt; select * from t_hive3;<br>
OK<br>
Time taken: 0.077 seconds<br>
从MySQL数据库导入数据<br>
我们将在介绍Sqoop时讲。<br><br><br>
5. 数据导出<br>
从HDFS复制到HDFS其他位置<br><br><br>
~ hadoop fs -cp /user/hive/warehouse/t_hive /<br><br><br>
~ hadoop fs -ls /t_hive<br>
Found 1 items<br>
-rw-r--r--   1 cos supergroup         56 2013-07-16 10:41 /t_hive/000000_0<br><br><br>
~ hadoop fs -cat /t_hive/000000_0<br>
1623<br>
611213<br>
41231<br>
17213<br>
71231<br>
11234<br>
11234<br><br><br>
通过Hive导出到本地文件系统<br><br><br>
hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/t_hive' SELECT * FROM t_hive;<br>
Total MapReduce jobs = 1<br>
Launching Job 1 out of 1<br>
Number of reduce tasks is set to 0 since there's no reduce operator<br>
Starting Job = job_201307131407_0005, Tracking URL = http://c1.wtmart.com:50030/jobdetails.jsp?jobid=job_201307131407_0005<br>
Kill Command = /home/hadoop/hive/toolkit/hadoop-1.0.3/libexec/../bin/hadoop job  -Dmapred.job.tracker=hdfs://c1.wtmart.com:9001 -kill job_201307131407_0005<br>
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0<br>
2013-07-16 10:46:24,774 Stage-1 map = 0%,  reduce = 0%<br>
2013-07-16 10:46:30,823 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec<br>
2013-07-16 10:46:31,833 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec<br>
2013-07-16 10:46:32,844 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec<br>
2013-07-16 10:46:33,856 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec<br>
2013-07-16 10:46:34,865 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec<br>
2013-07-16 10:46:35,873 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec<br>
2013-07-16 10:46:36,884 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.87 sec<br>
MapReduce Total cumulative CPU time: 870 msec<br>
Ended Job = job_201307131407_0005<br>
Copying data to local directory /tmp/t_hive<br>
Copying data to local directory /tmp/t_hive<br>
7 Rows loaded to /tmp/t_hive<br>
MapReduce Jobs Launched:<br>
Job 0: Map: 1   Cumulative CPU: 0.87 sec   HDFS Read: 271 HDFS Write: 56 SUCCESS<br>
Total MapReduce CPU Time Spent: 870 msec<br>
OK<br>
Time taken: 23.369 seconds<br><br><br>
#查看本地操作系统<br>
hive&gt; ! cat /tmp/t_hive/000000_0;<br>
hive&gt; 1623<br>
611213<br>
41231<br>
17213<br>
71231<br>
11234<br>
11234<br><br><br>
6. Hive查询HiveQL<br>
注：以下代码将去掉map,reduce的日志输出部分。<br><br><br>
普通查询：排序，列别名，嵌套子查询<br><br><br>
hive&gt; FROM (<br>
    &gt;   SELECT b,c as c2 FROM t_hive<br>
    &gt; ) t<br>
    &gt; SELECT t.b, t.c2<br>
    &gt; WHERE b&gt;2<br>
    &gt; LIMIT 2;<br>
12      13<br>
21      3<br>
连接查询：JOIN<br><br><br>
hive&gt; SELECT t1.a,t1.b,t2.a,t2.b<br>
    &gt; FROM t_hive t1 JOIN t_hive2 t2 on t1.a=t2.a<br>
    &gt; WHERE t1.c&gt;10;<br><br><br>
1       12      1       12<br>
11      2       11      2<br>
41      2       41      2<br>
61      12      61      12<br>
71      2       71      2<br>
聚合查询1：count, avg<br><br><br>
hive&gt; SELECT count(*), avg(a) FROM t_hive;<br>
7       31.142857142857142<br>
聚合查询2：count, distinct<br><br><br><br><br>
hive&gt; SELECT count(DISTINCT b) FROM t_hive;<br>
3<br>
聚合查询3：GROUP BY, HAVING<br><br><br>
#GROUP BY<br>
hive&gt; SELECT avg(a),b,sum(c) FROM t_hive GROUP BY b,c<br>
16.0    2       3<br>
56.0    2       62<br>
11.0    2       34<br>
61.0    12      13<br>
1.0     12      34<br>
17.0    21      3<br><br><br>
#HAVING<br>
hive&gt; SELECT avg(a),b,sum(c) FROM t_hive GROUP BY b,c HAVING sum(c)&gt;30<br>
56.0    2       62<br>
11.0    2       34<br>
1.0     12      34<br><br><br>
7. Hive视图<br>
Hive视图和数据库视图的概念是一样的，我们还以t_hive为例。<br><br><br>
hive&gt; CREATE VIEW v_hive AS SELECT a,b FROM t_hive where c&gt;30;<br>
hive&gt; select * from v_hive;<br>
41      2<br>
71      2<br>
1       12<br>
11      2<br><br><br>
删除视图<br><br><br>
hive&gt; DROP VIEW IF EXISTS v_hive;<br>
OK<br>
Time taken: 0.495 seconds<br>
8. Hive分区表<br>
分区表是数据库的基本概念，但很多时候数据量不大，我们完全用不到分区表。Hive是一种OLAP数据仓库软件，涉及的数据量是非常大的，所以分区表在这个场景就显得非常重要！！<br><br><br>
下面我们重新定义一个数据表结构：t_hft<br><br><br>
创建数据<br><br><br>
~ vi /home/hadoop/hive/demo/t_hft_20130627.csv<br>
000001,092023,9.76<br>
000002,091947,8.99<br>
000004,092002,9.79<br>
000005,091514,2.2<br>
000001,092008,9.70<br>
000001,092059,9.45<br><br><br>
~ vi /home/hadoop/hive/demo/t_hft_20130628.csv<br>
000001,092023,9.76<br>
000002,091947,8.99<br>
000004,092002,9.79<br>
000005,091514,2.2<br>
000001,092008,9.70<br>
000001,092059,9.45<br>
创建数据表<br><br><br><br><br>
DROP TABLE IF EXISTS t_hft;<br>
CREATE TABLE t_hft(<br>
SecurityID STRING,<br>
tradeTime STRING,<br>
PreClosePx DOUBLE<br>
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';<br>
创建分区数据表<br>
根据业务：按天和股票ID进行分区设计<br><br><br>
DROP TABLE IF EXISTS t_hft;<br>
CREATE TABLE t_hft(<br>
SecurityID STRING,<br>
tradeTime STRING,<br>
PreClosePx DOUBLE<br>
) PARTITIONED BY (tradeDate INT)<br>
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';<br>
导入数据<br><br><br>
#20130627<br>
hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/hive/demo/t_hft_20130627.csv' OVERWRITE INTO TABLE t_hft PARTITION (tradeDate=20130627);<br>
Copying data from file:/home/hadoop/hive/demo/t_hft_20130627.csv<br>
Copying file: file:/home/hadoop/hive/demo/t_hft_20130627.csv<br>
Loading data to table default.t_hft partition (tradedate=20130627)<br><br><br>
#20130628<br>
hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/hive/demo/t_hft_20130628.csv' OVERWRITE INTO TABLE t_hft PARTITION (tradeDate=20130628);<br>
Copying data from file:/home/hadoop/hive/demo/t_hft_20130628.csv<br>
Copying file: file:/home/hadoop/hive/demo/t_hft_20130628.csv<br>
Loading data to table default.t_hft partition (tradedate=20130628)<br><br><br>
查看分区表<br><br><br>
hive&gt; SHOW PARTITIONS t_hft;<br>
tradedate=20130627<br>
tradedate=20130628<br>
Time taken: 0.082 seconds<br>
查询数据<br><br><br>
hive&gt; select * from t_hft where securityid='000001';<br>
000001  092023  9.76    20130627<br>
000001  092008  9.7     20130627<br>
000001  092059  9.45    20130627<br>
000001  092023  9.76    20130628<br>
000001  092008  9.7     20130628<br>
000001  092059  9.45    20130628<br><br><br>
hive&gt; select * from t_hft where tradedate=20130627 and PreClosePx&lt;9;<br>
000002  091947  8.99    20130627<br>
000005  091514  2.2     20130627<br>
Hive基于使用完成.。<br><br><br>
转载请注明出处：<br>
http://write.blog.csdn.net/postedit<br></p>
            </div>
                </div>