---
layout:     post
title:      【课程】Spark从零开始
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/suixinlun/article/details/81630567				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="一spark介绍">一、Spark介绍</h1>



<h2 id="11-spark简介">1.1 Spark简介</h2>

<p><strong>Spark是什么：</strong> <br>
   Spark是一个快速且通用的集群计算平台。</p>

<p><strong>Spark的特点：</strong> <br>
1.Spark是快速的</p>

<blockquote>
  <p>Spark扩充了流行的MapReduce计算模型 <br>
  Spark是基于内存的计算 </p>
</blockquote>

<p>2.Spark是通用的</p>

<blockquote>
  <p>Spark的设计容纳了其他分布式系统拥有的功能，批处理，迭代式计算，交互查询和流处理等。 <br>
  优点：降低了维护成本</p>
</blockquote>

<p>3.Spark是高度开放的</p>

<blockquote>
  <p>Spark提供了Python, Java, Scala, SQL的API和丰富的内置库。 <br>
  Spark和其他的大数据工具整合的很好，包括Hadoop， Kafka等</p>
</blockquote>



<h2 id="12-spark生态介绍">1.2 Spark生态介绍</h2>

<p><strong>Spark历史</strong></p>

<ul>
<li>诞生于2009年，加州大学伯克利分校RAD实验室的一个研究项目，最初是基于Hadoop MapReduce</li>
<li>发现MapReduce在迭代式计算和交互式上抵消，引入内存存储</li>
<li>2010年3月份Spark开源</li>
<li>2011年AMP实验室在Spark上开发高级组件，像Spark Streaming</li>
<li>2013年转移到了Apache下，不久便成为顶级项目了。</li>
</ul>

<p><strong>Spark组件：</strong> <br>
Spark包含多个紧密集成的组件 <br>
<img src="https://img-blog.csdn.net/20180815200827354?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="Spark组件" title=""> <br>
<em>1) Spark Core:</em></p>

<blockquote>
  <p>包含Spark的基本功能，包含任务调度，内存管理，容错机制等。 <br>
  内部定义了RDDs（弹性分布式数据集） <br>
  提供了很多API来创建和操作这些RDDs。 <br>
  应用场景，为其他组件提供底层的服务。</p>
</blockquote>

<p><em>2) Spark SQL:</em></p>

<blockquote>
  <p>是Spark处理结构化数据的库，就像Hive SQL，MySQL一样。 <br>
  应用场景，企业中用来做报表统计。</p>
</blockquote>

<p><em>3) Spark Streaming:</em></p>

<blockquote>
  <p>是实施数据流处理的组件，类似Storm。 <br>
  Spark Streaming提供了API来操作实施流数据。 <br>
  应用场景，企业中用来从Kafka接受数据做实时统计。</p>
</blockquote>

<p><em>4) Mlib：</em></p>

<blockquote>
  <p>一个包含通用机器学习功能的包，Machine learning lib. <br>
  包含分类，聚类，回归等，还包括模型评估，和数据导入。 <br>
  Milb提供的上面这些方法，都支持集群上的横向扩展。 <br>
  应用场景，机器学习。</p>
</blockquote>

<p><em>5) Graphx：</em></p>

<blockquote>
  <p>是处理图的库（例如，社交网络图），并进行图的并行计算。 <br>
  像Spark Streaming， Spark SQL一样， 它也集成了RDD API。 <br>
  它提供了各种图的操作，和常用的图算法，例如PangeRank算法。 <br>
  应用场景，图计算</p>
</blockquote>

<p><em>6) Cluster Managers：</em></p>

<blockquote>
  <p>就是集群管理，Spark自带一个集群管理是单独调度器。 <br>
  常见集群管理包括Hadoop YARN， Apache Mesos。</p>
</blockquote>

<p><strong>紧密集成的优点：</strong></p>

<ol>
<li>Spark底层优化了，基于Spark底层的组件，也得到了相应的优化。</li>
<li>紧密集成，节省了各个组件组合使用时的部署，测试等时间。</li>
<li>向Spark增加新的组件时，其他组件，可立刻享用新组件的功能。</li>
</ol>



<h2 id="13-spark与hadoop的比较">1.3 Spark与Hadoop的比较</h2>

<p><strong>Hadoop应用场景</strong></p>

<ol>
<li>离线处理</li>
<li>对时效性要求不高</li>
</ol>

<p><strong>Spark应用场景</strong></p>

<ol>
<li>时效性要求高的场景</li>
<li>机器学习等领域</li>
</ol>

<p><strong>比较</strong> <br>
<em>Doug Cutting（Hadoop之父）的观点：</em></p>

<ol>
<li>这是生态系统，每个组件都有其作用，各善其职即可。</li>
<li>Spark不具有HDFS的存储能力，要借助HDFS等持久化数据。</li>
<li>大数据将会孕育出更多的新技术。</li>
</ol>



<h1 id="二spark的下载和安装">二、Spark的下载和安装</h1>

<p><strong>Spark目录</strong></p>

<blockquote>
  <p>bin：包含用来和Spark交互的可执行文件，如Spark shell。 <br>
  core：streaming, python, …包含主要组件的源代码。 <br>
  examples：包含一些单机的Spark jobs，可以单机运行的例子。</p>
</blockquote>

<p><strong>Spark的Shell</strong></p>

<blockquote>
  <p>Spark的shell使你能够处理分布在集群上的数据。 <br>
  Spark把数据加载到节点的内存中，因此分布式处理可在秒级完成。 <br>
  快速使迭代式计算，实时查询，分析一般能够在shell中完成。</p>
</blockquote>



<h1 id="三rdds">三、RDDs</h1>



<h2 id="31-rdds介绍">3.1 RDDs介绍</h2>

<p><strong>Driver program:</strong></p>

<blockquote>
  <p>包含程序的main()方法，RDDs的定义和操作。 <br>
  它管理很多节点，我们称作executors。</p>
</blockquote>

<p><img src="https://img-blog.csdn.net/20180815203035928?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="Driver Program" title=""></p>

<p><strong>SparkContext：</strong></p>

<blockquote>
  <p>Driver programs通过SparkContext对象访问Spark。 <br>
  SparkContext对象代表和一个集群的连接。 <br>
  在Shell中SparkContext自动创建好了，就是sc。</p>
</blockquote>

<p><strong>RDDs:</strong></p>

<blockquote>
  <p>Resilient distributed datasets（弹性分布式数据集，简称RDDs） <br>
  这些RDDs，并行的分布在整个集群中。 <br>
  RDDs是Spark分发数据和计算的基础抽象类。 <br>
  一个RDD是一个不可改变的分布式集合对象。 <br>
  Spark中，所有的计算都是通过RDDs的创建，转换，操作完成的。 <br>
  一个RDD内部有很多partition（分片）组成的。</p>
</blockquote>

<p><strong>分片partition：</strong></p>

<blockquote>
  <p>每个分片包括一部分数据，partitions可在集群不同节点上计算。 <br>
  分片是Spark并行处理的单元，Spark顺序的，并行的处理分片。</p>
</blockquote>

<p><strong>RDDs的创建方法：</strong> <br>
1) 把一个存在的集合传给SparkContext的parallelize()方法，测试用:</p>



<pre class="prettyprint"><code class=" hljs fix"><span class="hljs-attribute">val rdd </span>=<span class="hljs-string"> sc.parallelize(Array(1, 2, 2, 4), 4)</span></code></pre>

<blockquote>
  <p>第1个参数：待并行化处理的集合  <br>
  第2个参数：分区个数</p>
</blockquote>

<p>2) 加载外部数据集</p>



<pre class="prettyprint"><code class=" hljs fsharp"><span class="hljs-keyword">val</span> rddText = sc.textFile(<span class="hljs-string">"helloSpark.txt"</span>)</code></pre>

<p><strong>Scala的基础知识</strong> <br>
Scala的变量声明: 创建变量是val/var</p>

<blockquote>
  <p>val：变量值是不可修改的，类似java final。 <br>
  var：变量值定义完是可以修改的。</p>
</blockquote>

<p><strong>Scala的匿名函数和类型推断</strong> <br>
<code>lines.filter(line =&gt; line.contains("world"))</code>，定义一个匿名函数，接受line.</p>

<blockquote>
  <p>使用line这个Strig类型的变量上的contains方法，并且返回结果 <br>
  line不需要制定类型，会自动推断</p>
</blockquote>



<h2 id="32-rdd基本操作-transformation">3.2 RDD基本操作 Transformation</h2>

<p><strong>Transformations介绍：</strong> <br>
转换：从之前的RDD构建一个新的RDD，像map() 和 filter()。</p>

<p><strong>逐元素Transformation：</strong></p>

<blockquote>
  <p>map(): map()接收函数，把函数应用到RDD的每一个元素，返回新的RDD。 <br>
  filter(): filter()接受函数，返回只包含满足filter()函数的元素的新RDD。 <br>
  flatMap(): flatMap()对每个输入元素，输出多个输出元素。flat压扁的意思，将RDD中的元素压扁后返回一个新的RDD。</p>
</blockquote>

<p><strong>集合运算：</strong> <br>
RDDs支持数学集合的计算，例如并集union，交集intersection，差集subtract计算。</p>



<h2 id="33-rdd基本操作-action">3.3 RDD基本操作 Action</h2>

<p><strong>Action介绍：</strong> <br>
在RDD上计算出来一个结果。 <br>
把结果返回给driver program或保存在文件系统，count(), save() <br>
<img src="https://img-blog.csdn.net/20180815204718185?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="action1" title=""> <br>
<img src="https://img-blog.csdn.net/20180815204831143?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="action2" title=""> <br>
<em>reduce():</em></p>

<blockquote>
  <p>接收一个函数，作用在RDD两个类型相同的元素上，返回新元素； <br>
  可以实现，RDD中元素的累加，计数，和其它类型的聚集操作；</p>
</blockquote>

<p><em>collect():</em></p>

<blockquote>
  <p>遍历整个RDD，向driver program返回RDD的内容 <br>
  需要单机内存能够容纳下（因为数据要拷贝给driver，测试使用） <br>
  大数据的时候，使用saveAsTextFile() action等。</p>
</blockquote>

<p><em>take(n):</em></p>

<blockquote>
  <p>返回RDD的n个元素（同时尝试访问最少的partitions）。 <br>
  返回结果是无序的，测试使用。</p>
</blockquote>

<p><em>top():</em></p>

<blockquote>
  <p>排序（根据RDD中数据的比较器）</p>
</blockquote>



<h2 id="34-rdds的特性">3.4 RDDs的特性</h2>

<p><strong>RDDs的血统关系图：</strong> <br>
Spark维护着RDDs之间的依赖关系和创建关系，叫做血统关系图 <br>
Spark使用血统关系图来计算每个RDD的需求和恢复丢失的数据 <br>
<img src="https://img-blog.csdn.net/20180815210228253?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="Spark血统关系图" title=""></p>

<p><strong>延迟计算（Lazy Evaluation）：</strong> <br>
Spark对RDDs的计算是，他们第一次使用action操作的时候。 <br>
这种方式在处理大数据的时候特别有用，可以减少数据的传输。 <br>
Spark内部记录metadata表明transformations操作已经被响应了。 <br>
加载数据也是延迟计算，数据只有在必要的时候，才会被加载进去。</p>

<p><strong>RDD.persist(): 持久化</strong> <br>
默认每次在RDDs上面进行action操作时，Spark都重新计算RDDs，如果想重复利用一个RDD，可以使用RDD.persist()。 <br>
unpersist()方法从缓存中移除。 <br>
<img src="https://img-blog.csdn.net/20180815210732912?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="缓存级别" title=""> <br>
<img src="https://img-blog.csdn.net/20180815210748376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="缓存级别" title=""></p>



<h2 id="35-keyvalue对rdds">3.5 KeyValue对RDDs</h2>

<p><strong>创建KeyValue对RDDs</strong> <br>
使用map()函数，返回key/value对 <br>
例如，包含数行数据的RDD，把每行数据的第一个单词作为keys <br>
<img src="https://img-blog.csdn.net/20180815211152214?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="key value" title=""> <br>
<strong>KeyValue对RDDs的Transformations{(1,2),(3,4),(3,6)}</strong> <br>
<img src="https://img-blog.csdn.net/20180815211332146?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="key value" title=""> <br>
<img src="https://img-blog.csdn.net/20180816090505294?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="KeyValue" title=""></p>



<pre class="prettyprint"><code class=" hljs erlang"><span class="hljs-function"><span class="hljs-title">combineByKey</span><span class="hljs-params">(create<span class="hljs-variable">Combiner</span>, merge<span class="hljs-variable">Value</span>, merge<span class="hljs-variable">Combiners</span>, partitioner)</span></span></code></pre>

<blockquote>
  <p>最常用的基于key的聚合函数，返回的类型可以与输入类型不一样，许多基于key的聚合函数都用到了它，像groupByKey() <br>
  原理，遍历partition中的元素，元素的key，要么之前见过的，要么不是。如果是新元素，使用我们提供的createCombiner()函数，如果是这个partition中已经存在的key，就会使用mergeValue()函数，合计每个partition的结果的时候，使用mergeCombiners()函数 <br>
  <img src="https://img-blog.csdn.net/20180816091055505?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="举例" title=""> <br>
  <img src="https://img-blog.csdn.net/20180816091202385?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""> <br>
  <img src="https://img-blog.csdn.net/20180816091332339?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1aXhpbmx1bg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" title=""></p>
</blockquote>

<p><strong>课程视频链接：</strong><a href="https://www.imooc.com/video/14388" rel="nofollow">https://www.imooc.com/video/14388</a></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>