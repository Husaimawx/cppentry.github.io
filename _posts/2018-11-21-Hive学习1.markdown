---
layout:     post
title:      Hive学习1
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
目录：<br>
1.什么是hive？<br>
2.为什么使用 Hive<br>
3. 框架性能分析<br>
4.Hive 架构<br>
5.Hive 和 RDBMS 的对比<br>
6.Hive 的数据存储<br>
8、Hive 基本使用<br><br><br>
1. Hive<br>
什么是hive？<br>
 hive是由Facedoop实现并开源、<br>
 是基于hadoop的一个数据仓库工具，这句话怎么理解呢<br>
 hadoop由三部分组成的hdfs分布式文件系统，mapreduce分布式计算引擎，yarn<br>
 资源调度系统，数据存储在hdfs，计算使用mr，资源分配使用yarn、<br>
 可以将结构化的数据映射为一张数据库表，并提供HQL（hive sql）查询功能、<br>
 使不熟悉MR的用户很方便地利用HQL处理和计算HDFS上面的结构化数据、 <br>
 适用于离线的批量数据计算<br>
 所以说 Hive 是基于 hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计<br>
 算框架<br><br><br>
2.<br>
为什么使用 Hive<br>
直接使用 MapReduce 所面临的问题：<br>
人员学习成本太高<br>
项目周期要求太短<br>
MapReduce 实现复杂查询逻辑开发难度太大<br>
为什么要使用 Hive：<br>
更友好的接口：操作接口采用类 SQL 的语法，提供快速开发的能力<br>
更低的学习成本：避免了写 MapReduce，减少开发人员的学习成本<br>
更好的扩展性：可自由扩展集群规模而无需重启服务，还支持用户自定义函数<br>
 <br>
 <br>
3. 框架性能分析<br>
优点：<br>
1、可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务<br>
横向扩展：通过分担压力的方式扩展集群的规模<br>
纵向扩展：一台服务器 cpu i7-6700k 4 核心 8 线程，8 核心 16 线程，内存 64G =&gt; 128G<br>
2、延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数<br>
3、良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行<br>
缺点：<br>
1、hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结<br>
果导入到文件中（当前选择的 hive-1.2.1 的版本支持记录级别的插入操作）<br>
2、Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能<br>
用在交互查询系统中。<br>
3、hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而<br>
不是 OLTP（联机事务处理），这就是数据处理的两大级别）。<br><br><br><br><br>
4.Hive 架构<br>
基本组成<br>
一、用户接口<br>
CLI，Shell 终端命令行（Command Line Interface），最常用（学习，调试，生产）<br>
JDBC/ODBC，是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过<br>
这连接至 Hive server 服务<br>
Web UI，通过浏览器访问 Hive<br>
二、Thrift Server<br>
Thrift 是 Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发，<br>
Hive 集成了该服务，能让不同的编程语言调用 hive 的接口<br>
三、元数据存储<br>
元数据，通俗的讲，就是存储在 Hive 中的数据的描述信息。<br>
Hive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和<br>
外部表），表的数据所在目录<br>
Metastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存<br>
储目录不固定。数据库跟着 Hive 走，极度不方便管理<br>
解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程）<br>
四、Driver：编译器（Compiler），优化器（Optimizer），执行器（Executor）<br>
Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成查询计划<br>
的生成。生成的查询计划存储在 HDFS 中，并随后由 MapReduce 调用执行<br>
五、执行流程<br>
HiveQL 通过命令行或者客户端提交，经过 Compiler 编译器，运用 MetaStore 中的元数<br>
据进行类型检测和语法分析，生成一个逻辑方案(logical plan)，然后通过的优化处理，产生<br>
一个 MapReduce 任务。<br><br><br>
5.Hive 和 RDBMS 的对比<br>
对比项           Hive                       RDBMS<br>
查询语言         HQL                        SQL<br>
数据存储         HDFS                       Raw Device or Local FS<br>
执行器           MapReduce                  Executor<br>
数据插入         支持批量导入/单条插入      支持单条或者批量导入<br>
数据操作         覆盖追加                   行级更新删除<br>
处理数据规模     大                         小<br>
执行延迟         高                         低<br>
分区             支持                       支持<br>
索引             0.8 版本之后加入简单索引   支持复杂的索引<br>
扩展性           高（好）                   有限（差）<br>
数据加载模式     读时模式（快）             写时模式（慢）<br>
应用场景         海量数据查询               实时查询<br>
总结：Hive 具有 SQL 数据库的外表，但应用场景完全不同，Hive 只适合用来做海量离线数<br>
据统计分析，也就是数据仓库。<br>
mysql能够按照行修改而hive不行呀<br><br><br><br><br>
6.Hive 的数据存储<br><br><br>
1、Hive 的存储结构包括数据库、表、视图、分区和表数据等。数据库，表，分区等等都对<br>
应 HDFS 上的一个目录。表数据对应 HDFS 对应目录下的文件。<br>
2、Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式，因为 Hive 是读模式<br>
（Schema On Read），可支持 TextFile，SequenceFile，RCFILE 或者自定义格式等<br>
3、 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据<br>
Hive 的默认列分隔符：控制符 ctrl + A，\x01<br>
Hive 的默认行分隔符：换行符 \n<br>
4、Hive 中包含以下数据模型：<br>
database：在 HDFS 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹<br>
table：在 HDFS 中表现所属 database 目录下一个文件夹<br>
external table：与 table 类似，不过其数据存放位置可以指定任意 HDFS 目录路径<br>
partition：在 HDFS 中表现为 table 目录下的子目录<br>
bucket：在 HDFS 中表现为同一个表目录或者分区目录下根据 hash 散列之后的多个文件<br>
view：与传统数据库类似，只读，基于基本表创建<br>
5、Hive 的元数据存储在 RDBMS 中，除元数据外的其它所有数据都基于 HDFS 存储。默认情<br>
况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的<br>
测试。实际生产环境中不适用，为了支持多用户会话，则需要一个独立的元数据库，使用<br>
MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。<br>
6、Hive 中的表分为内部表、外部表、分区表和 Bucket 表<br>
内部表和外部表的区别：<br>
删除内部表，删除表元数据和数据<br>
删除外部表，删除元数据，不删除数据<br>
内部表和外部表的使用选择：<br>
大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于<br>
选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。<br>
使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中<br>
使用外部表的场景是针对一个数据集有多个不同的 Schema<br>
分区表和分桶表的区别：<br>
Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同<br>
时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MR 程序的 HashPartitioner 的<br>
原理类似<br>
分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所<br>
以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列<br>
形成的多个文件，所以数据的准确性也高很多<br><br><br><br><br>
8、Hive 基本使用<br>
1、 创建库：create database mydb;<br>
2、 查看库：show databases;<br>
3、 切换数据库：use mydb;<br>
4、 创建表：create table t_user(id string, name string)<br>
或 create table t_user2 (id string, name string) row format delimited fields terminated by ',';<br>
5、 插入数据：insert into tables t_user values (‘001’,’mazhonghua’)<br>
6、 查询数据：select * from t_user;<br>
7、 导入数据：<br>
a) 导入 HDFS 数据： load data inpath '/mingxing.txt' into table t_user1;<br>
b) 导入本地数据：load data local inpath '/root/hivedata/mingxing.txt' into table t_user1;<br>
小技能补充：<br>
1、 进入到用户的主目录，使用命令 cat /home/hadoop/.hivehistory 可以查看到 hive 执行的<br>
历史命令<br>
2、 执行查询时若想显示表头信息时，请执行命令：<br>
Hive&gt; set hive.cli.print.header=true;<br>
3、 hive的执行日志的存储目录在${java.io.tmpdir}/${user.name}/hive.log中，假如使用hadoop<br>
用户操作的 hive，那么日志文件的存储路径为：/temp/hadoop/hive.log<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>            </div>
                </div>