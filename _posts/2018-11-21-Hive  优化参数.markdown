---
layout:     post
title:      Hive  优化参数
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/xfg0218/article/details/61930858				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><span style="font-size:24px;">HIVE优化参数</span></p>
<p><br></p>
<p>hive&gt; set;</p>
_hive.hdfs.session.path=/tmp/hive/root/bd4f450f-2f3f-460d-8539-5ee573701e59<br>
_hive.local.session.path=/tmp/root/bd4f450f-2f3f-460d-8539-5ee573701e59<br>
_hive.tmp_table_space=/tmp/hive/root/bd4f450f-2f3f-460d-8539-5ee573701e59/_tmp_space.db<br>
datanucleus.autoCreateSchema=true<br>
datanucleus.autoStartMechanismMode=checked<br>
datanucleus.cache.level2=false<br>
datanucleus.cache.level2.type=none<br>
datanucleus.connectionPoolingType=BONECP<br>
datanucleus.fixedDatastore=false<br>
datanucleus.identifierFactory=datanucleus1<br>
datanucleus.plugin.pluginRegistryBundleCheck=LOG<br>
datanucleus.rdbms.useLegacyNativeValueStrategy=true<br>
datanucleus.storeManagerType=rdbms<br>
datanucleus.transactionIsolation=read-committed<br>
datanucleus.validateColumns=false<br>
datanucleus.validateConstraints=false<br>
datanucleus.validateTables=false<br>
dfs.block.access.key.update.interval=600<br>
dfs.block.access.token.enable=false<br>
dfs.block.access.token.lifetime=600<br>
dfs.blockreport.initialDelay=0<br>
dfs.blockreport.intervalMsec=21600000<br>
dfs.blockreport.split.threshold=1000000<br>
dfs.blocksize=134217728<br>
dfs.bytes-per-checksum=512<br>
dfs.cachereport.intervalMsec=10000<br>
dfs.client-write-packet-size=65536<br>
dfs.client.block.write.replace-datanode-on-failure.best-effort=false<br>
dfs.client.block.write.replace-datanode-on-failure.enable=true<br>
dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT<br>
dfs.client.block.write.retries=3<br>
dfs.client.cached.conn.retry=3<br>
dfs.client.context=default<br>
dfs.client.datanode-restart.timeout=30<br>
dfs.client.domain.socket.data.traffic=false<br>
dfs.client.failover.connection.retries=0<br>
dfs.client.failover.connection.retries.on.timeouts=0<br>
dfs.client.failover.max.attempts=15<br>
dfs.client.failover.sleep.base.millis=500<br>
dfs.client.failover.sleep.max.millis=15000<br>
dfs.client.file-block-storage-locations.num-threads=10<br>
dfs.client.file-block-storage-locations.timeout.millis=1000<br>
dfs.client.https.keystore.resource=ssl-client.xml<br>
dfs.client.https.need-auth=false<br>
dfs.client.mmap.cache.size=512<br>
dfs.client.mmap.cache.timeout.ms=3600000<br>
dfs.client.mmap.enabled=true<br>
dfs.client.mmap.retry.timeout.ms=300000<br>
dfs.client.read.shortcircuit=false<br>
dfs.client.read.shortcircuit.skip.checksum=false<br>
dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000<br>
dfs.client.read.shortcircuit.streams.cache.size=256<br>
dfs.client.short.circuit.replica.stale.threshold.ms=1800000<br>
dfs.client.slow.io.warning.threshold.ms=30000<br>
dfs.client.use.datanode.hostname=false<br>
dfs.client.use.legacy.blockreader.local=false<br>
dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000<br>
dfs.datanode.address=0.0.0.0:50010<br>
dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f<br>
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240<br>
dfs.datanode.balance.bandwidthPerSec=1048576<br>
dfs.datanode.block.id.layout.upgrade.threads=12<br>
dfs.datanode.bp-ready.timeout=20<br>
dfs.datanode.cache.revocation.polling.ms=500<br>
dfs.datanode.cache.revocation.timeout.ms=900000<br>
dfs.datanode.data.dir=/home/hadoop/data/data<br>
dfs.datanode.data.dir.perm=700<br>
dfs.datanode.directoryscan.interval=21600<br>
dfs.datanode.directoryscan.threads=1<br>
dfs.datanode.dns.interface=default<br>
dfs.datanode.dns.nameserver=default<br>
dfs.datanode.drop.cache.behind.reads=false<br>
dfs.datanode.drop.cache.behind.writes=false<br>
dfs.datanode.du.reserved=0<br>
dfs.datanode.failed.volumes.tolerated=0<br>
dfs.datanode.fsdatasetcache.max.threads.per.volume=4<br>
dfs.datanode.handler.count=10<br>
dfs.datanode.hdfs-blocks-metadata.enabled=false<br>
dfs.datanode.http.address=0.0.0.0:50075<br>
dfs.datanode.https.address=0.0.0.0:50475<br>
dfs.datanode.ipc.address=0.0.0.0:50020<br>
dfs.datanode.max.locked.memory=0<br>
dfs.datanode.max.transfer.threads=4096<br>
dfs.datanode.readahead.bytes=4193404<br>
dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp<br>
dfs.datanode.slow.io.warning.threshold.ms=300<br>
dfs.datanode.sync.behind.writes=false<br>
dfs.datanode.use.datanode.hostname=false<br>
dfs.default.chunk.view.size=32768<br>
dfs.encrypt.data.transfer=false<br>
dfs.encrypt.data.transfer.cipher.key.bitlength=128<br>
dfs.ha.automatic-failover.enabled=false<br>
dfs.ha.log-roll.period=120<br>
dfs.ha.tail-edits.period=60<br>
dfs.heartbeat.interval=3<br>
dfs.http.policy=HTTP_ONLY<br>
dfs.https.enable=false<br>
dfs.https.server.keystore.resource=ssl-server.xml<br>
dfs.image.compress=false<br>
dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec<br>
dfs.image.transfer.bandwidthPerSec=0<br>
dfs.image.transfer.chunksize=65536<br>
dfs.image.transfer.timeout=60000<br>
dfs.journalnode.http-address=0.0.0.0:8480<br>
dfs.journalnode.https-address=0.0.0.0:8481<br>
dfs.journalnode.rpc-address=0.0.0.0:8485<br>
dfs.namenode.accesstime.precision=3600000<br>
dfs.namenode.acls.enabled=false<br>
dfs.namenode.audit.loggers=default<br>
dfs.namenode.avoid.read.stale.datanode=false<br>
dfs.namenode.avoid.write.stale.datanode=false<br>
dfs.namenode.backup.address=0.0.0.0:50100<br>
dfs.namenode.backup.http-address=0.0.0.0:50105<br>
dfs.namenode.checkpoint.check.period=60<br>
dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary<br>
dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}<br>
dfs.namenode.checkpoint.max-retries=3<br>
dfs.namenode.checkpoint.period=3600<br>
dfs.namenode.checkpoint.txns=1000000<br>
dfs.namenode.datanode.registration.ip-hostname-check=true<br>
dfs.namenode.decommission.interval=30<br>
dfs.namenode.decommission.nodes.per.interval=5<br>
dfs.namenode.delegation.key.update-interval=86400000<br>
dfs.namenode.delegation.token.max-lifetime=604800000<br>
dfs.namenode.delegation.token.renew-interval=86400000<br>
dfs.namenode.edit.log.autoroll.check.interval.ms=300000<br>
dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0<br>
dfs.namenode.edits.dir=${dfs.namenode.name.dir}<br>
dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager<br>
dfs.namenode.edits.noeditlogchannelflush=false<br>
dfs.namenode.enable.retrycache=true<br>
dfs.namenode.fs-limits.max-blocks-per-file=1048576<br>
dfs.namenode.fs-limits.max-component-length=255<br>
dfs.namenode.fs-limits.max-directory-items=1048576<br>
dfs.namenode.fs-limits.max-xattr-size=16384<br>
dfs.namenode.fs-limits.max-xattrs-per-inode=32<br>
dfs.namenode.fs-limits.min-block-size=1048576<br>
dfs.namenode.handler.count=10<br>
dfs.namenode.http-address=0.0.0.0:50070<br>
dfs.namenode.https-address=0.0.0.0:50470<br>
dfs.namenode.inotify.max.events.per.rpc=1000<br>
dfs.namenode.invalidate.work.pct.per.iteration=0.32f<br>
dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}<br>
dfs.namenode.lazypersist.file.scrub.interval.sec=300<br>
dfs.namenode.list.cache.directives.num.responses=100<br>
dfs.namenode.list.cache.pools.num.responses=100<br>
dfs.namenode.list.encryption.zones.num.responses=100<br>
dfs.namenode.logging.level=info<br>
dfs.namenode.max.extra.edits.segments.retained=10000<br>
dfs.namenode.max.objects=0<br>
dfs.namenode.name.dir=/home/hadoop/data/name<br>
dfs.namenode.name.dir.restore=false<br>
dfs.namenode.num.checkpoints.retained=2<br>
dfs.namenode.num.extra.edits.retained=1000000<br>
dfs.namenode.path.based.cache.block.map.allocation.percent=0.25<br>
dfs.namenode.path.based.cache.refresh.interval.ms=30000<br>
dfs.namenode.path.based.cache.retry.interval.ms=30000<br>
dfs.namenode.reject-unresolved-dn-topology-mapping=false<br>
dfs.namenode.replication.considerLoad=true<br>
dfs.namenode.replication.interval=3<br>
dfs.namenode.replication.min=1<br>
dfs.namenode.replication.work.multiplier.per.iteration=2<br>
dfs.namenode.resource.check.interval=5000<br>
dfs.namenode.resource.checked.volumes.minimum=1<br>
dfs.namenode.resource.du.reserved=104857600<br>
dfs.namenode.retrycache.expirytime.millis=600000<br>
dfs.namenode.retrycache.heap.percent=0.03f<br>
dfs.namenode.safemode.extension=30000<br>
dfs.namenode.safemode.min.datanodes=0<br>
dfs.namenode.safemode.threshold-pct=0.999f<br>
dfs.namenode.secondary.http-address=hadoop1:50090<br>
dfs.namenode.secondary.https-address=0.0.0.0:50091<br>
dfs.namenode.stale.datanode.interval=30000<br>
dfs.namenode.startup.delay.block.deletion.sec=0<br>
dfs.namenode.support.allow.format=true<br>
dfs.namenode.write.stale.datanode.ratio=0.5f<br>
dfs.namenode.xattrs.enabled=true<br>
dfs.permissions.enabled=false<br>
dfs.permissions.superusergroup=supergroup<br>
dfs.replication=3<br>
dfs.replication.max=512<br>
dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}<br>
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000<br>
dfs.storage.policy.enabled=true<br>
dfs.stream-buffer-size=4096<br>
dfs.support.append=true<br>
dfs.user.home.dir.prefix=/user<br>
dfs.webhdfs.enabled=true<br>
dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$<br>
fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem<br>
hadoop.bin.path=/usr/local/hadoop-2.6.4/bin/hadoop<br>
hadoop.fuse.connection.timeout=300<br>
hadoop.fuse.timer.period=5<br>
hadoop.hdfs.configuration.version=1<br>
hive.analyze.stmt.collect.partlevel.stats=true<br>
hive.archive.enabled=false<br>
hive.auto.convert.join=true<br>
hive.auto.convert.join.noconditionaltask=true<br>
hive.auto.convert.join.noconditionaltask.size=10000000<br>
hive.auto.convert.join.use.nonstaged=false<br>
hive.auto.convert.sortmerge.join=false<br>
hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ<br>
hive.auto.convert.sortmerge.join.to.mapjoin=false<br>
hive.auto.progress.timeout=0s<br>
hive.autogen.columnalias.prefix.includefuncname=false<br>
hive.autogen.columnalias.prefix.label=_c<br>
hive.binary.record.max.length=1000<br>
hive.cache.expr.evaluation=true<br>
hive.cbo.costmodel.cpu=0.000001<br>
hive.cbo.costmodel.extended=false<br>
hive.cbo.costmodel.hdfs.read=1.5<br>
hive.cbo.costmodel.hdfs.write=10.0<br>
hive.cbo.costmodel.local.fs.read=4.0<br>
hive.cbo.costmodel.local.fs.write=4.0<br>
hive.cbo.costmodel.network=150.0<br>
hive.cbo.enable=true<br>
hive.cbo.returnpath.hiveop=false<br>
hive.cli.errors.ignore=false<br>
hive.cli.pretty.output.num.cols=-1<br>
hive.cli.print.current.db=false<br>
hive.cli.print.header=false<br>
hive.cli.prompt=hive<br>
hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore<br>
hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation<br>
hive.compactor.abortedtxn.threshold=1000<br>
hive.compactor.check.interval=300s<br>
hive.compactor.cleaner.run.interval=5000ms<br>
hive.compactor.delta.num.threshold=10<br>
hive.compactor.delta.pct.threshold=0.1<br>
hive.compactor.initiator.on=false<br>
hive.compactor.worker.threads=0<br>
hive.compactor.worker.timeout=86400s<br>
hive.compat=0.12<br>
hive.compute.query.using.stats=false<br>
hive.compute.splits.in.am=true<br>
hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role<br>
hive.conf.validation=true<br>
hive.convert.join.bucket.mapjoin.tez=false<br>
hive.counters.group.name=HIVE<br>
hive.debug.localtask=false<br>
hive.decode.partition.name=false<br>
hive.default.fileformat=TextFile<br>
hive.default.fileformat.managed=none<br>
hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe<br>
hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>
hive.display.partition.cols.separately=true<br>
hive.downloaded.resources.dir=/tmp/${hive.session.id}_resources<br>
hive.enforce.bucketing=false<br>
hive.enforce.bucketmapjoin=false<br>
hive.enforce.sorting=false<br>
hive.enforce.sortmergebucketmapjoin=false<br>
hive.entity.capture.transform=false<br>
hive.entity.separator=@<br>
hive.error.on.empty.partition=false<br>
hive.exec.check.crossproducts=true<br>
hive.exec.compress.intermediate=false<br>
hive.exec.compress.output=false<br>
hive.exec.concatenate.check.index=true<br>
hive.exec.copyfile.maxsize=33554432<br>
hive.exec.counters.pull.interval=1000<br>
hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__<br>
hive.exec.drop.ignorenonexistent=true<br>
hive.exec.dynamic.partition=true<br>
hive.exec.dynamic.partition.mode=strict<br>
hive.exec.infer.bucket.sort=false<br>
hive.exec.infer.bucket.sort.num.buckets.power.two=false<br>
hive.exec.job.debug.capture.stacktraces=true<br>
hive.exec.job.debug.timeout=30000<br>
hive.exec.local.scratchdir=/tmp/root<br>
hive.exec.max.created.files=100000<br>
hive.exec.max.dynamic.partitions=1000<br>
hive.exec.max.dynamic.partitions.pernode=100<br>
hive.exec.mode.local.auto=false<br>
hive.exec.mode.local.auto.input.files.max=4<br>
hive.exec.mode.local.auto.inputbytes.max=134217728<br>
hive.exec.orc.block.padding.tolerance=0.05<br>
hive.exec.orc.compression.strategy=SPEED<br>
hive.exec.orc.default.block.padding=true<br>
hive.exec.orc.default.block.size=268435456<br>
hive.exec.orc.default.buffer.size=262144<br>
hive.exec.orc.default.compress=ZLIB<br>
hive.exec.orc.default.row.index.stride=10000<br>
hive.exec.orc.default.stripe.size=67108864<br>
hive.exec.orc.dictionary.key.size.threshold=0.8<br>
hive.exec.orc.encoding.strategy=SPEED<br>
hive.exec.orc.memory.pool=0.5<br>
hive.exec.orc.skip.corrupt.data=false<br>
hive.exec.orc.split.strategy=HYBRID<br>
hive.exec.orc.zerocopy=false<br>
hive.exec.parallel=false<br>
hive.exec.parallel.thread.number=8<br>
hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger<br>
hive.exec.rcfile.use.explicit.header=true<br>
hive.exec.rcfile.use.sync.cache=true<br>
hive.exec.reducers.bytes.per.reducer=256000000<br>
hive.exec.reducers.max=1009<br>
hive.exec.rowoffset=false<br>
hive.exec.scratchdir=/tmp/hive<br>
hive.exec.script.allow.partial.consumption=false<br>
hive.exec.script.maxerrsize=100000<br>
hive.exec.script.trust=false<br>
hive.exec.show.job.failure.debug.info=true<br>
hive.exec.stagingdir=.hive-staging<br>
hive.exec.submit.local.task.via.child=true<br>
hive.exec.submitviachild=false<br>
hive.exec.tasklog.debug.timeout=20000<br>
hive.exec.temporary.table.storage=default<br>
hive.execution.engine=mr<br>
hive.exim.strict.repl.tables=true<br>
hive.exim.uri.scheme.whitelist=hdfs,pfile<br>
hive.explain.dependency.append.tasktype=false<br>
hive.explain.user=false<br>
hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe<br>
hive.fetch.task.aggr=false<br>
hive.fetch.task.conversion=more<br>
hive.fetch.task.conversion.threshold=1073741824<br>
hive.file.max.footer=100<br>
hive.fileformat.check=true<br>
hive.groupby.mapaggr.checkinterval=100000<br>
hive.groupby.orderby.position.alias=false<br>
hive.groupby.skewindata=false<br>
hive.hadoop.supports.splittable.combineinputformat=false<br>
hive.hashtable.initialCapacity=100000<br>
hive.hashtable.key.count.adjustment=1.0<br>
hive.hashtable.loadfactor=0.75<br>
hive.hbase.generatehfiles=false<br>
hive.hbase.snapshot.restoredir=/tmp<br>
hive.hbase.wal.enabled=true<br>
hive.heartbeat.interval=1000<br>
hive.hmshandler.force.reload.conf=false<br>
hive.hmshandler.retry.attempts=10<br>
hive.hmshandler.retry.interval=2000ms<br>
hive.hwi.listen.host=0.0.0.0<br>
hive.hwi.listen.port=9999<br>
hive.hwi.war.file=${env:HWI_WAR_FILE}<br>
hive.ignore.mapjoin.hint=true<br>
hive.in.test=false<br>
hive.in.tez.test=false<br>
hive.index.compact.binary.search=true<br>
hive.index.compact.file.ignore.hdfs=false<br>
hive.index.compact.query.max.entries=10000000<br>
hive.index.compact.query.max.size=10737418240<br>
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat<br>
hive.insert.into.external.tables=true<br>
hive.insert.into.multilevel.dirs=false<br>
hive.int.timestamp.conversion.in.seconds=false<br>
hive.io.rcfile.column.number.conf=0<br>
hive.io.rcfile.record.buffer.size=4194304<br>
hive.io.rcfile.record.interval=2147483647<br>
hive.io.rcfile.tolerate.corruptions=false<br>
hive.jobname.length=50<br>
hive.join.cache.size=25000<br>
hive.join.emit.interval=1000<br>
hive.lazysimple.extended_boolean_literal=false<br>
hive.limit.optimize.enable=false<br>
hive.limit.optimize.fetch.max=50000<br>
hive.limit.optimize.limit.file=10<br>
hive.limit.pushdown.memory.usage=-1.0<br>
hive.limit.query.max.table.partition=-1<br>
hive.limit.row.max.size=100000<br>
hive.localize.resource.num.wait.attempts=5<br>
hive.localize.resource.wait.interval=5000ms<br>
hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager<br>
hive.lock.mapred.only.operation=false<br>
hive.lock.numretries=100<br>
hive.lock.sleep.between.retries=60s<br>
hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__<br>
hive.log.every.n.records=0<br>
hive.log.explain.output=false<br>
hive.map.aggr=true<br>
hive.map.aggr.hash.force.flush.memory.threshold=0.9<br>
hive.map.aggr.hash.min.reduction=0.5<br>
hive.map.aggr.hash.percentmemory=0.5<br>
hive.map.groupby.sorted=false<br>
hive.map.groupby.sorted.testmode=false<br>
hive.mapjoin.bucket.cache.size=100<br>
hive.mapjoin.check.memory.rows=100000<br>
hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55<br>
hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3<br>
hive.mapjoin.hybridgrace.hashtable=true<br>
hive.mapjoin.hybridgrace.memcheckfrequency=1024<br>
hive.mapjoin.hybridgrace.minnumpartitions=16<br>
hive.mapjoin.hybridgrace.minwbsize=524288<br>
hive.mapjoin.localtask.max.memory.usage=0.9<br>
hive.mapjoin.optimized.hashtable=true<br>
hive.mapjoin.optimized.hashtable.wbsize=10485760<br>
hive.mapjoin.smalltable.filesize=25000000<br>
hive.mapper.cannot.span.multiple.partitions=false<br>
hive.mapred.local.mem=0<br>
hive.mapred.mode=nonstrict<br>
hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner<br>
hive.mapred.reduce.tasks.speculative.execution=true<br>
hive.mapred.supports.subdirectories=false<br>
hive.merge.mapfiles=true<br>
hive.merge.mapredfiles=false<br>
hive.merge.orcfile.stripe.level=true<br>
hive.merge.rcfile.block.level=true<br>
hive.merge.size.per.task=256000000<br>
hive.merge.smallfiles.avgsize=16000000<br>
hive.merge.sparkfiles=false<br>
hive.merge.tezfiles=false<br>
hive.metadata.move.exported.metadata.to.trash=true<br>
hive.metastore.aggregate.stats.cache.clean.until=0.8<br>
hive.metastore.aggregate.stats.cache.enabled=true<br>
hive.metastore.aggregate.stats.cache.fpp=0.01<br>
hive.metastore.aggregate.stats.cache.max.full=0.9<br>
hive.metastore.aggregate.stats.cache.max.partitions=10000<br>
hive.metastore.aggregate.stats.cache.max.reader.wait=1000ms<br>
hive.metastore.aggregate.stats.cache.max.variance=0.01<br>
hive.metastore.aggregate.stats.cache.max.writer.wait=5000ms<br>
hive.metastore.aggregate.stats.cache.size=10000<br>
hive.metastore.aggregate.stats.cache.ttl=600s<br>
hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED<br>
hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED<br>
hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL<br>
hive.metastore.authorization.storage.checks=false<br>
hive.metastore.batch.retrieve.max=300<br>
hive.metastore.batch.retrieve.table.partition.max=1000<br>
hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order<br>
hive.metastore.client.connect.retry.delay=1s<br>
hive.metastore.client.drop.partitions.using.expressions=true<br>
hive.metastore.client.socket.lifetime=0s<br>
hive.metastore.client.socket.timeout=600s<br>
hive.metastore.connect.retries=3<br>
hive.metastore.direct.sql.batch.size=0<br>
hive.metastore.disallow.incompatible.col.type.changes=false<br>
hive.metastore.dml.events=false<br>
hive.metastore.event.clean.freq=0s<br>
hive.metastore.event.db.listener.timetolive=86400s<br>
hive.metastore.event.expiry.duration=0s<br>
hive.metastore.execute.setugi=true<br>
hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore<br>
hive.metastore.failure.retries=1<br>
hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl<br>
hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl<br>
hive.metastore.integral.jdo.pushdown=false<br>
hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM<br>
hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false<br>
hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore<br>
hive.metastore.sasl.enabled=false<br>
hive.metastore.schema.verification=false<br>
hive.metastore.schema.verification.record.version=true<br>
hive.metastore.server.max.message.size=104857600<br>
hive.metastore.server.max.threads=1000<br>
hive.metastore.server.min.threads=200<br>
hive.metastore.server.tcp.keepalive=true<br>
hive.metastore.stats.ndv.densityfunction=false<br>
hive.metastore.thrift.compact.protocol.enabled=false<br>
hive.metastore.thrift.framed.transport.enabled=false<br>
hive.metastore.try.direct.sql=true<br>
hive.metastore.try.direct.sql.ddl=true<br>
hive.metastore.warehouse.dir=/user/hive/warehouse<br>
hive.multi.insert.move.tasks.share.dependencies=false<br>
hive.multigroupby.singlereducer=true<br>
hive.new.job.grouping.set.cardinality=30<br>
hive.optimize.bucketingsorting=true<br>
hive.optimize.bucketmapjoin=false<br>
hive.optimize.bucketmapjoin.sortedmerge=false<br>
hive.optimize.constant.propagation=true<br>
hive.optimize.correlation=false<br>
hive.optimize.distinct.rewrite=true<br>
hive.optimize.groupby=true<br>
hive.optimize.index.autoupdate=false<br>
hive.optimize.index.filter=false<br>
hive.optimize.index.filter.compact.maxsize=-1<br>
hive.optimize.index.filter.compact.minsize=5368709120<br>
hive.optimize.index.groupby=false<br>
hive.optimize.listbucketing=false<br>
hive.optimize.metadataonly=true<br>
hive.optimize.null.scan=true<br>
hive.optimize.ppd=true<br>
hive.optimize.ppd.storage=true<br>
hive.optimize.reducededuplication=true<br>
hive.optimize.reducededuplication.min.reducer=4<br>
hive.optimize.remove.identity.project=true<br>
hive.optimize.sampling.orderby=false<br>
hive.optimize.sampling.orderby.number=1000<br>
hive.optimize.sampling.orderby.percent=0.1<br>
hive.optimize.skewjoin=false<br>
hive.optimize.skewjoin.compiletime=false<br>
hive.optimize.sort.dynamic.partition=false<br>
hive.optimize.union.remove=false<br>
hive.orc.cache.stripe.details.size=10000<br>
hive.orc.compute.splits.num.threads=10<br>
hive.orc.row.index.stride.dictionary.check=true<br>
hive.orc.splits.include.file.footer=false<br>
hive.outerjoin.supports.filters=true<br>
hive.parquet.timestamp.skip.conversion=true<br>
hive.plan.serialization.format=kryo<br>
hive.ppd.recognizetransivity=true<br>
hive.ppd.remove.duplicatefilters=true<br>
hive.prewarm.enabled=false<br>
hive.prewarm.numcontainers=10<br>
hive.query.id=root_20170313061836_b4acb04a-23e4-41ff-8c0b-d13acdaae651<br>
hive.query.result.fileformat=TextFile<br>
hive.query.string=select * from default.hiveVar<br>
hive.querylog.enable.plan.progress=true<br>
hive.querylog.location=/tmp/root<br>
hive.querylog.plan.progress.interval=60000ms<br>
hive.reorder.nway.joins=true<br>
hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory<br>
hive.resultset.use.unique.column.names=true<br>
hive.rework.mapredwork=false<br>
hive.rpc.query.plan=false<br>
hive.sample.seednumber=0<br>
hive.scratch.dir.permission=700<br>
hive.script.auto.progress=false<br>
hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist<br>
hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID<br>
hive.script.operator.truncate.env=false<br>
hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader<br>
hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter<br>
hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe<br>
hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator<br>
hive.security.authorization.enabled=false<br>
hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider<br>
hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\..*\.dynamic\.partitions\..*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez.queue.name|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketing|hive\.enforce\.bucketmapjoin|hive\.enforce\.sorting|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.mapred\.supports\.subdirectories|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.server2\.logging\.operation\.level|hive\.support\.sql11\.reserved\.keywords|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout<br>
hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl<br>
hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile<br>
hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator<br>
hive.security.metastore.authorization.auth.reads=true<br>
hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider<br>
hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe<br>
hive.server.read.socket.timeout=10s<br>
hive.server.tcp.keepalive=true<br>
hive.server2.allow.user.substitution=true<br>
hive.server2.async.exec.keepalive.time=10s<br>
hive.server2.async.exec.shutdown.timeout=10s<br>
hive.server2.async.exec.threads=100<br>
hive.server2.async.exec.wait.queue.size=100<br>
hive.server2.authentication=NONE<br>
hive.server2.enable.doAs=true<br>
hive.server2.global.init.file.location=/usr/local/hive/conf<br>
hive.server2.idle.operation.timeout=5d<br>
hive.server2.idle.session.check.operation=true<br>
hive.server2.idle.session.timeout=7d<br>
hive.server2.logging.operation.enabled=true<br>
hive.server2.logging.operation.level=EXECUTION<br>
hive.server2.logging.operation.log.location=/tmp/root/operation_logs<br>
hive.server2.long.polling.timeout=5000ms<br>
hive.server2.map.fair.scheduler.queue=true<br>
hive.server2.max.start.attempts=30<br>
hive.server2.session.check.interval=6h<br>
hive.server2.support.dynamic.service.discovery=false<br>
hive.server2.table.type.mapping=CLASSIC<br>
hive.server2.tez.initialize.default.sessions=false<br>
hive.server2.tez.sessions.per.default.queue=1<br>
hive.server2.thrift.exponential.backoff.slot.length=100ms<br>
hive.server2.thrift.http.cookie.auth.enabled=true<br>
hive.server2.thrift.http.cookie.is.httponly=true<br>
hive.server2.thrift.http.cookie.is.secure=true<br>
hive.server2.thrift.http.cookie.max.age=86400s<br>
hive.server2.thrift.http.max.idle.time=1800s<br>
hive.server2.thrift.http.path=cliservice<br>
hive.server2.thrift.http.port=10001<br>
hive.server2.thrift.http.worker.keepalive.time=60s<br>
hive.server2.thrift.login.timeout=20s<br>
hive.server2.thrift.max.message.size=104857600<br>
hive.server2.thrift.max.worker.threads=500<br>
hive.server2.thrift.min.worker.threads=5<br>
hive.server2.thrift.port=10000<br>
hive.server2.thrift.sasl.qop=auth<br>
hive.server2.thrift.worker.keepalive.time=60s<br>
hive.server2.transport.mode=binary<br>
hive.server2.use.SSL=false<br>
hive.server2.zookeeper.namespace=hiveserver2<br>
hive.session.history.enabled=false<br>
hive.session.id=bd4f450f-2f3f-460d-8539-5ee573701e59<br>
hive.session.silent=false<br>
hive.skewjoin.key=100000<br>
hive.skewjoin.mapjoin.map.tasks=10000<br>
hive.skewjoin.mapjoin.min.split=33554432<br>
hive.smbjoin.cache.rows=10000<br>
hive.spark.client.connect.timeout=1000ms<br>
hive.spark.client.future.timeout=60s<br>
hive.spark.client.rpc.max.size=52428800<br>
hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5<br>
hive.spark.client.rpc.threads=8<br>
hive.spark.client.secret.bits=256<br>
hive.spark.client.server.connect.timeout=90000ms<br>
hive.spark.job.monitor.timeout=60s<br>
hive.ssl.protocol.blacklist=SSLv2,SSLv3<br>
hive.stageid.rearrange=none<br>
hive.start.cleanup.scratchdir=false<br>
hive.stats.atomic=false<br>
hive.stats.autogather=true<br>
hive.stats.collect.rawdatasize=true<br>
hive.stats.collect.scancols=false<br>
hive.stats.collect.tablekeys=false<br>
hive.stats.dbclass=fs<br>
hive.stats.dbconnectionstring=jdbc:derby:;databaseName=TempStatsStore;create=true<br>
hive.stats.deserialization.factor=1.0<br>
hive.stats.fetch.column.stats=false<br>
hive.stats.fetch.partition.stats=true<br>
hive.stats.gather.num.threads=10<br>
hive.stats.jdbc.timeout=30s<br>
hive.stats.jdbcdriver=org.apache.derby.jdbc.EmbeddedDriver<br>
hive.stats.join.factor=1.1<br>
hive.stats.key.prefix.max.length=150<br>
hive.stats.key.prefix.reserve.length=24<br>
hive.stats.list.num.entries=10<br>
hive.stats.map.num.entries=10<br>
hive.stats.max.variable.length=100<br>
hive.stats.ndv.error=20.0<br>
hive.stats.reliable=false<br>
hive.stats.retries.max=0<br>
hive.stats.retries.wait=3000ms<br>
hive.stats.tmp.loc=file:/tmp/root/bd4f450f-2f3f-460d-8539-5ee573701e59/hive_2017-03-13_06-18-36_096_6436639812842773897-1/-mr-10000/.hive-staging_hive_2017-03-13_06-18-36_096_6436639812842773897-1/-ext-10002<br>
hive.support.concurrency=false<br>
hive.support.quoted.identifiers=column<br>
hive.support.sql11.reserved.keywords=true<br>
hive.test.authz.sstd.hs2.mode=false<br>
hive.test.mode=false<br>
hive.test.mode.prefix=test_<br>
hive.test.mode.samplefreq=32<br>
hive.tez.auto.reducer.parallelism=false<br>
hive.tez.container.size=-1<br>
hive.tez.cpu.vcores=-1<br>
hive.tez.dynamic.partition.pruning=true<br>
hive.tez.dynamic.partition.pruning.max.data.size=104857600<br>
hive.tez.dynamic.partition.pruning.max.event.size=1048576<br>
hive.tez.exec.inplace.progress=true<br>
hive.tez.exec.print.summary=false<br>
hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat<br>
hive.tez.log.level=INFO<br>
hive.tez.max.partition.factor=2.0<br>
hive.tez.min.partition.factor=0.25<br>
hive.tez.smb.number.waves=0.5<br>
hive.transform.escape.input=false<br>
hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager<br>
hive.txn.max.open.batch=1000<br>
hive.txn.timeout=300s<br>
hive.txn.valid.txns=9223372036854775807:<br>
hive.typecheck.on.insert=true<br>
hive.udtf.auto.progress=false<br>
hive.unlock.numretries=10<br>
hive.user.install.directory=hdfs:///user/<br>
hive.variable.substitute=true<br>
hive.variable.substitute.depth=40<br>
hive.vectorized.execution.enabled=false<br>
hive.vectorized.execution.mapjoin.minmax.enabled=false<br>
hive.vectorized.execution.mapjoin.native.enabled=true<br>
hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false<br>
hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false<br>
hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1<br>
hive.vectorized.execution.reduce.enabled=true<br>
hive.vectorized.execution.reduce.groupby.enabled=true<br>
hive.vectorized.groupby.checkinterval=100000<br>
hive.vectorized.groupby.flush.percent=0.1<br>
hive.vectorized.groupby.maxentries=1000000<br>
hive.warehouse.subdir.inherit.perms=true<br>
hive.zookeeper.clean.extra.nodes=false<br>
hive.zookeeper.client.port=2181<br>
hive.zookeeper.connection.basesleeptime=1000ms<br>
hive.zookeeper.connection.max.retries=3<br>
hive.zookeeper.namespace=hive_zookeeper_namespace<br>
hive.zookeeper.session.timeout=1200000ms<br>
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory<br>
javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver<br>
javax.jdo.option.ConnectionPassword=123456<br>
javax.jdo.option.ConnectionURL=jdbc:mysql://hadoop2:3306/hive?createDatabaseIfNotExist=true<br>
javax.jdo.option.ConnectionUserName=root<br>
javax.jdo.option.DetachAllOnCommit=true<br>
javax.jdo.option.Multithreaded=true<br>
javax.jdo.option.NonTransactionalRead=true<br>
mapreduce.input.fileinputformat.input.dir.recursive=false<br>
mapreduce.input.fileinputformat.split.maxsize=256000000<br>
mapreduce.input.fileinputformat.split.minsize=1<br>
mapreduce.input.fileinputformat.split.minsize.per.node=1<br>
mapreduce.input.fileinputformat.split.minsize.per.rack=1<br>
mapreduce.job.committer.setup.cleanup.needed=false<br>
mapreduce.job.committer.task.cleanup.needed=false<br>
mapreduce.job.name=<br>
mapreduce.job.reduces=-1<br>
mapreduce.workflow.adjacency.Stage-0=Stage-2<br>
mapreduce.workflow.adjacency.Stage-1=Stage-7<br>
mapreduce.workflow.adjacency.Stage-3=Stage-0<br>
mapreduce.workflow.adjacency.Stage-4=Stage-0<br>
mapreduce.workflow.adjacency.Stage-5=Stage-6<br>
mapreduce.workflow.adjacency.Stage-6=Stage-0<br>
mapreduce.workflow.adjacency.Stage-7=Stage-4,Stage-3,Stage-5<br>
mapreduce.workflow.id=hive_root_20170313061836_b4acb04a-23e4-41ff-8c0b-d13acdaae651<br>
mapreduce.workflow.name=select * from default.hiveVar<br>
mapreduce.workflow.node.name=Stage-1<br>
nfs.allow.insecure.ports=true<br>
nfs.dump.dir=/tmp/.hdfs-nfs<br>
nfs.mountd.port=4242<br>
nfs.rtmax=1048576<br>
nfs.server.port=2049<br>
nfs.wtmax=1048576<br>
parquet.memory.pool.ratio=0.5<br>
rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine<br>
silent=off<br>
stream.stderr.reporter.enabled=true<br>
stream.stderr.reporter.prefix=reporter:<br>
env:CLASSPATH=/usr/local/hadoop-2.6.4/etc/hadoop:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/*:/usr/local/hadoop-2.6.4/share/hadoop/common/*:/usr/local/hadoop-2.6.4/share/hadoop/hdfs:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/*:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/*:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/*:/usr/local/hadoop-2.6.4/share/hadoop/yarn/*:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/*:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/*:/usr/local/hive/conf:/usr/local/hive/lib/accumulo-core-1.6.0.jar:/usr/local/hive/lib/accumulo-fate-1.6.0.jar:/usr/local/hive/lib/accumulo-start-1.6.0.jar:/usr/local/hive/lib/accumulo-trace-1.6.0.jar:/usr/local/hive/lib/activation-1.1.jar:/usr/local/hive/lib/ant-1.9.1.jar:/usr/local/hive/lib/ant-launcher-1.9.1.jar:/usr/local/hive/lib/antlr-2.7.7.jar:/usr/local/hive/lib/antlr-runtime-3.4.jar:/usr/local/hive/lib/apache-log4j-extras-1.2.17.jar:/usr/local/hive/lib/asm-commons-3.1.jar:/usr/local/hive/lib/asm-tree-3.1.jar:/usr/local/hive/lib/avro-1.7.5.jar:/usr/local/hive/lib/bonecp-0.8.0.RELEASE.jar:/usr/local/hive/lib/calcite-avatica-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-core-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-linq4j-1.2.0-incubating.jar:/usr/local/hive/lib/commons-beanutils-1.7.0.jar:/usr/local/hive/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hive/lib/commons-cli-1.2.jar:/usr/local/hive/lib/commons-codec-1.4.jar:/usr/local/hive/lib/commons-collections-3.2.1.jar:/usr/local/hive/lib/commons-compiler-2.7.6.jar:/usr/local/hive/lib/commons-compress-1.4.1.jar:/usr/local/hive/lib/commons-configuration-1.6.jar:/usr/local/hive/lib/commons-dbcp-1.4.jar:/usr/local/hive/lib/commons-digester-1.8.jar:/usr/local/hive/lib/commons-httpclient-3.0.1.jar:/usr/local/hive/lib/commons-io-2.4.jar:/usr/local/hive/lib/commons-lang-2.6.jar:/usr/local/hive/lib/commons-logging-1.1.3.jar:/usr/local/hive/lib/commons-math-2.1.jar:/usr/local/hive/lib/commons-pool-1.5.4.jar:/usr/local/hive/lib/commons-vfs2-2.0.jar:/usr/local/hive/lib/curator-client-2.6.0.jar:/usr/local/hive/lib/curator-framework-2.6.0.jar:/usr/local/hive/lib/curator-recipes-2.6.0.jar:/usr/local/hive/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/hive/lib/datanucleus-core-3.2.10.jar:/usr/local/hive/lib/datanucleus-rdbms-3.2.9.jar:/usr/local/hive/lib/derby-10.10.2.0.jar:/usr/local/hive/lib/eigenbase-properties-1.1.5.jar:/usr/local/hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:/usr/local/hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:/usr/local/hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:/usr/local/hive/lib/groovy-all-2.1.6.jar:/usr/local/hive/lib/guava-14.0.1.jar:/usr/local/hive/lib/hamcrest-core-1.1.jar:/usr/local/hive/lib/hive-accumulo-handler-1.2.1.jar:/usr/local/hive/lib/hive-ant-1.2.1.jar:/usr/local/hive/lib/hive-beeline-1.2.1.jar:/usr/local/hive/lib/hive-cli-1.2.1.jar:/usr/local/hive/lib/hive-common-1.2.1.jar:/usr/local/hive/lib/hive-contrib-1.2.1.jar:/usr/local/hive/lib/hive-exec-1.2.1.jar:/usr/local/hive/lib/hive-hbase-handler-1.2.1.jar:/usr/local/hive/lib/hive-hwi-1.2.1.jar:/usr/local/hive/lib/hive-jdbc-1.2.1.jar:/usr/local/hive/lib/hive-jdbc-1.2.1-standalone.jar:/usr/local/hive/lib/hive-metastore-1.2.1.jar:/usr/local/hive/lib/hive-serde-1.2.1.jar:/usr/local/hive/lib/hive-service-1.2.1.jar:/usr/local/hive/lib/hive-shims-0.20S-1.2.1.jar:/usr/local/hive/lib/hive-shims-0.23-1.2.1.jar:/usr/local/hive/lib/hive-shims-1.2.1.jar:/usr/local/hive/lib/hive-shims-common-1.2.1.jar:/usr/local/hive/lib/hive-shims-scheduler-1.2.1.jar:/usr/local/hive/lib/hive-testutils-1.2.1.jar:/usr/local/hive/lib/httpclient-4.4.jar:/usr/local/hive/lib/httpcore-4.4.jar:/usr/local/hive/lib/ivy-2.4.0.jar:/usr/local/hive/lib/janino-2.7.6.jar:/usr/local/hive/lib/jcommander-1.32.jar:/usr/local/hive/lib/jdo-api-3.0.1.jar:/usr/local/hive/lib/jetty-all-7.6.0.v20120127.jar:/usr/local/hive/lib/jetty-all-server-7.6.0.v20120127.jar:/usr/local/hive/lib/jline-2.12.jar:/usr/local/hive/lib/joda-time-2.5.jar:/usr/local/hive/lib/jpam-1.1.jar:/usr/local/hive/lib/json-20090211.jar:/usr/local/hive/lib/jsr305-3.0.0.jar:/usr/local/hive/lib/jta-1.1.jar:/usr/local/hive/lib/junit-4.11.jar:/usr/local/hive/lib/libfb303-0.9.2.jar:/usr/local/hive/lib/libthrift-0.9.2.jar:/usr/local/hive/lib/log4j-1.2.16.jar:/usr/local/hive/lib/mail-1.4.1.jar:/usr/local/hive/lib/maven-scm-api-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svn-commons-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svnexe-1.4.jar:/usr/local/hive/lib/mysql-connector-java-5.1.35-bin.jar:/usr/local/hive/lib/netty-3.7.0.Final.jar:/usr/local/hive/lib/opencsv-2.3.jar:/usr/local/hive/lib/oro-2.0.8.jar:/usr/local/hive/lib/paranamer-2.3.jar:/usr/local/hive/lib/parquet-hadoop-bundle-1.6.0.jar:/usr/local/hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/usr/local/hive/lib/plexus-utils-1.5.6.jar:/usr/local/hive/lib/regexp-1.3.jar:/usr/local/hive/lib/servlet-api-2.5.jar:/usr/local/hive/lib/snappy-java-1.0.5.jar:/usr/local/hive/lib/ST4-4.0.4.jar:/usr/local/hive/lib/stax-api-1.0.1.jar:/usr/local/hive/lib/stringtemplate-3.2.1.jar:/usr/local/hive/lib/super-csv-2.2.0.jar:/usr/local/hive/lib/tempus-fugit-1.1.jar:/usr/local/hive/lib/velocity-1.5.jar:/usr/local/hive/lib/xz-1.0.jar:/usr/local/hive/lib/zookeeper-3.4.6.jar:/usr/local/spark/lib/spark-assembly-1.6.2-hadoop2.6.0.jar::/usr/local/hbase-1.2.1/conf:/usr/local/hbase-1.2.1/lib/hbase-protocol-1.2.1.jar:/usr/local/hbase-1.2.1/lib/hbase-client-1.2.1.jar:/usr/local/hbase-1.2.1/lib/metrics-core-2.2.0.jar:/usr/local/hbase-1.2.1/lib/hbase-common-1.2.1.jar:/usr/local/hbase-1.2.1/lib/netty-all-4.0.23.Final.jar:/usr/local/hbase-1.2.1/lib/hbase-hadoop-compat-1.2.1.jar:/usr/local/hbase-1.2.1/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hbase-1.2.1/lib/hbase-server-1.2.1.jar:/usr/local/hadoop-2.6.4/contrib/capacity-scheduler/*.jar<br>
env:DISPLAY=localhost:10.0<br>
env:FLUME_HOME=/usr/local/flume<br>
env:G_BROKEN_FILENAMES=1<br>
env:HADOOP_CLASSPATH=/usr/local/hive/conf:/usr/local/hive/lib/accumulo-core-1.6.0.jar:/usr/local/hive/lib/accumulo-fate-1.6.0.jar:/usr/local/hive/lib/accumulo-start-1.6.0.jar:/usr/local/hive/lib/accumulo-trace-1.6.0.jar:/usr/local/hive/lib/activation-1.1.jar:/usr/local/hive/lib/ant-1.9.1.jar:/usr/local/hive/lib/ant-launcher-1.9.1.jar:/usr/local/hive/lib/antlr-2.7.7.jar:/usr/local/hive/lib/antlr-runtime-3.4.jar:/usr/local/hive/lib/apache-log4j-extras-1.2.17.jar:/usr/local/hive/lib/asm-commons-3.1.jar:/usr/local/hive/lib/asm-tree-3.1.jar:/usr/local/hive/lib/avro-1.7.5.jar:/usr/local/hive/lib/bonecp-0.8.0.RELEASE.jar:/usr/local/hive/lib/calcite-avatica-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-core-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-linq4j-1.2.0-incubating.jar:/usr/local/hive/lib/commons-beanutils-1.7.0.jar:/usr/local/hive/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hive/lib/commons-cli-1.2.jar:/usr/local/hive/lib/commons-codec-1.4.jar:/usr/local/hive/lib/commons-collections-3.2.1.jar:/usr/local/hive/lib/commons-compiler-2.7.6.jar:/usr/local/hive/lib/commons-compress-1.4.1.jar:/usr/local/hive/lib/commons-configuration-1.6.jar:/usr/local/hive/lib/commons-dbcp-1.4.jar:/usr/local/hive/lib/commons-digester-1.8.jar:/usr/local/hive/lib/commons-httpclient-3.0.1.jar:/usr/local/hive/lib/commons-io-2.4.jar:/usr/local/hive/lib/commons-lang-2.6.jar:/usr/local/hive/lib/commons-logging-1.1.3.jar:/usr/local/hive/lib/commons-math-2.1.jar:/usr/local/hive/lib/commons-pool-1.5.4.jar:/usr/local/hive/lib/commons-vfs2-2.0.jar:/usr/local/hive/lib/curator-client-2.6.0.jar:/usr/local/hive/lib/curator-framework-2.6.0.jar:/usr/local/hive/lib/curator-recipes-2.6.0.jar:/usr/local/hive/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/hive/lib/datanucleus-core-3.2.10.jar:/usr/local/hive/lib/datanucleus-rdbms-3.2.9.jar:/usr/local/hive/lib/derby-10.10.2.0.jar:/usr/local/hive/lib/eigenbase-properties-1.1.5.jar:/usr/local/hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:/usr/local/hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:/usr/local/hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:/usr/local/hive/lib/groovy-all-2.1.6.jar:/usr/local/hive/lib/guava-14.0.1.jar:/usr/local/hive/lib/hamcrest-core-1.1.jar:/usr/local/hive/lib/hive-accumulo-handler-1.2.1.jar:/usr/local/hive/lib/hive-ant-1.2.1.jar:/usr/local/hive/lib/hive-beeline-1.2.1.jar:/usr/local/hive/lib/hive-cli-1.2.1.jar:/usr/local/hive/lib/hive-common-1.2.1.jar:/usr/local/hive/lib/hive-contrib-1.2.1.jar:/usr/local/hive/lib/hive-exec-1.2.1.jar:/usr/local/hive/lib/hive-hbase-handler-1.2.1.jar:/usr/local/hive/lib/hive-hwi-1.2.1.jar:/usr/local/hive/lib/hive-jdbc-1.2.1.jar:/usr/local/hive/lib/hive-jdbc-1.2.1-standalone.jar:/usr/local/hive/lib/hive-metastore-1.2.1.jar:/usr/local/hive/lib/hive-serde-1.2.1.jar:/usr/local/hive/lib/hive-service-1.2.1.jar:/usr/local/hive/lib/hive-shims-0.20S-1.2.1.jar:/usr/local/hive/lib/hive-shims-0.23-1.2.1.jar:/usr/local/hive/lib/hive-shims-1.2.1.jar:/usr/local/hive/lib/hive-shims-common-1.2.1.jar:/usr/local/hive/lib/hive-shims-scheduler-1.2.1.jar:/usr/local/hive/lib/hive-testutils-1.2.1.jar:/usr/local/hive/lib/httpclient-4.4.jar:/usr/local/hive/lib/httpcore-4.4.jar:/usr/local/hive/lib/ivy-2.4.0.jar:/usr/local/hive/lib/janino-2.7.6.jar:/usr/local/hive/lib/jcommander-1.32.jar:/usr/local/hive/lib/jdo-api-3.0.1.jar:/usr/local/hive/lib/jetty-all-7.6.0.v20120127.jar:/usr/local/hive/lib/jetty-all-server-7.6.0.v20120127.jar:/usr/local/hive/lib/jline-2.12.jar:/usr/local/hive/lib/joda-time-2.5.jar:/usr/local/hive/lib/jpam-1.1.jar:/usr/local/hive/lib/json-20090211.jar:/usr/local/hive/lib/jsr305-3.0.0.jar:/usr/local/hive/lib/jta-1.1.jar:/usr/local/hive/lib/junit-4.11.jar:/usr/local/hive/lib/libfb303-0.9.2.jar:/usr/local/hive/lib/libthrift-0.9.2.jar:/usr/local/hive/lib/log4j-1.2.16.jar:/usr/local/hive/lib/mail-1.4.1.jar:/usr/local/hive/lib/maven-scm-api-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svn-commons-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svnexe-1.4.jar:/usr/local/hive/lib/mysql-connector-java-5.1.35-bin.jar:/usr/local/hive/lib/netty-3.7.0.Final.jar:/usr/local/hive/lib/opencsv-2.3.jar:/usr/local/hive/lib/oro-2.0.8.jar:/usr/local/hive/lib/paranamer-2.3.jar:/usr/local/hive/lib/parquet-hadoop-bundle-1.6.0.jar:/usr/local/hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/usr/local/hive/lib/plexus-utils-1.5.6.jar:/usr/local/hive/lib/regexp-1.3.jar:/usr/local/hive/lib/servlet-api-2.5.jar:/usr/local/hive/lib/snappy-java-1.0.5.jar:/usr/local/hive/lib/ST4-4.0.4.jar:/usr/local/hive/lib/stax-api-1.0.1.jar:/usr/local/hive/lib/stringtemplate-3.2.1.jar:/usr/local/hive/lib/super-csv-2.2.0.jar:/usr/local/hive/lib/tempus-fugit-1.1.jar:/usr/local/hive/lib/velocity-1.5.jar:/usr/local/hive/lib/xz-1.0.jar:/usr/local/hive/lib/zookeeper-3.4.6.jar:/usr/local/spark/lib/spark-assembly-1.6.2-hadoop2.6.0.jar::/usr/local/hbase-1.2.1/conf:/usr/local/hbase-1.2.1/lib/hbase-protocol-1.2.1.jar:/usr/local/hbase-1.2.1/lib/hbase-client-1.2.1.jar:/usr/local/hbase-1.2.1/lib/metrics-core-2.2.0.jar:/usr/local/hbase-1.2.1/lib/hbase-common-1.2.1.jar:/usr/local/hbase-1.2.1/lib/netty-all-4.0.23.Final.jar:/usr/local/hbase-1.2.1/lib/hbase-hadoop-compat-1.2.1.jar:/usr/local/hbase-1.2.1/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hbase-1.2.1/lib/hbase-server-1.2.1.jar:/usr/local/hadoop-2.6.4/contrib/capacity-scheduler/*.jar<br>
env:HADOOP_CLIENT_OPTS=-Xmx512m <br>
env:HADOOP_COMMON_HOME=/usr/local/hadoop-2.6.4<br>
env:HADOOP_CONF_DIR=/usr/local/hadoop-2.6.4/etc/hadoop<br>
env:HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS <br>
env:HADOOP_HDFS_HOME=/usr/local/hadoop-2.6.4<br>
env:HADOOP_HEAPSIZE=256<br>
env:HADOOP_HOME=/usr/local/hadoop-2.6.4<br>
env:HADOOP_HOME_WARN_SUPPRESS=true<br>
env:HADOOP_IDENT_STRING=root<br>
env:HADOOP_MAPRED_HOME=/usr/local/hadoop-2.6.4<br>
env:HADOOP_NAMENODE_OPTS=-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender <br>
env:HADOOP_NFS3_OPTS=<br>
env:HADOOP_OPTS= -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/usr/local/hadoop-2.6.4/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/local/hadoop-2.6.4 -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=/usr/local/hadoop-2.6.4/lib/native
 -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx512m  -Dhadoop.security.logger=INFO,NullAppender<br>
env:HADOOP_PID_DIR=<br>
env:HADOOP_PORTMAP_OPTS=-Xmx512m <br>
env:HADOOP_PREFIX=/usr/local/hadoop-2.6.4<br>
env:HADOOP_SECONDARYNAMENODE_OPTS=-Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender <br>
env:HADOOP_SECURE_DN_LOG_DIR=/<br>
env:HADOOP_SECURE_DN_PID_DIR=<br>
env:HADOOP_SECURE_DN_USER=<br>
env:HADOOP_YARN_HOME=/usr/local/hadoop-2.6.4<br>
env:HBASE_HOME=/usr/local/hbase-1.2.1<br>
env:HISTCONTROL=ignoredups<br>
env:HISTSIZE=1000<br>
env:HIVE_AUX_JARS_PATH=<br>
env:HIVE_CONF_DIR=/usr/local/hive/conf<br>
env:HIVE_HOME=/usr/local/hive<br>
env:HOME=/root<br>
env:HOSTNAME=hadoop1<br>
env:JAVA_HOME=/usr/local/jdk1.7<br>
env:KAFKA_HOME=/usr/local/kafka<br>
env:LANG=en_US.UTF-8<br>
env:LD_LIBRARY_PATH=:/usr/local/hadoop-2.6.4/lib/native<br>
env:LESSOPEN=||/usr/bin/lesspipe.sh %s<br>
env:LOGNAME=root<br>
env:LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:<br>
env:MAIL=/var/spool/mail/root<br>
env:MALLOC_ARENA_MAX=4<br>
env:NLSPATH=/usr/dt/lib/nls/msg/%L/%N.cat<br>
env:PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/jdk1.7/bin:/usr/local/hadoop-2.6.4/bin:/usr/local/hadoop-2.6.4/sbin:/usr/local/zookeeper-3.4.5/bin:usr/local/storm/bin:/usr/local/spark/bin:/usr/local/scala-2.10.6/bin:/usr/local/kafka/bin:/usr/local/flume/bin:/usr/local/redis-3.0.7/bin:/usr/local/hive/bin:/usr/local/hbase-1.2.1/bin:/usr/local/storm/bin:/root/bin<br>
env:PWD=/usr/local/start_sh<br>
env:REDIS_HOME=/usr/local/redis-3.0.7<br>
env:SCALA_HOME=/usr/local/scala-2.10.6<br>
env:SELINUX_LEVEL_REQUESTED=<br>
env:SELINUX_ROLE_REQUESTED=<br>
env:SELINUX_USE_CURRENT_RANGE=<br>
env:SERVICE_LIST=beeline cli help hiveburninclient hiveserver2 hiveserver hwi jar lineage metastore metatool orcfiledump rcfilecat schemaTool version <br>
env:SHELL=/bin/bash<br>
env:SHLVL=1<br>
env:SPARK_HOME=/usr/local/spark<br>
env:SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass<br>
env:SSH_CLIENT=192.168.215.1 49416 22<br>
env:SSH_CONNECTION=192.168.215.1 49416 192.168.215.133 22<br>
env:SSH_TTY=/dev/pts/0<br>
env:STORM_HOME=/usr/local/storm<br>
env:STROM_HOME=usr/local/storm<br>
env:TERM=xterm<br>
env:USER=root<br>
env:XFILESEARCHPATH=/usr/dt/app-defaults/%L/Dt<br>
env:ZOOKEEPER_HOME=/usr/local/zookeeper-3.4.5<br>
system:awt.toolkit=sun.awt.X11.XToolkit<br>
system:file.encoding=UTF-8<br>
system:file.encoding.pkg=sun.io<br>
system:file.separator=/<br>
system:hadoop.home.dir=/usr/local/hadoop-2.6.4<br>
system:hadoop.id.str=root<br>
system:hadoop.log.dir=/usr/local/hadoop-2.6.4/logs<br>
system:hadoop.log.file=hadoop.log<br>
system:hadoop.policy.file=hadoop-policy.xml<br>
system:hadoop.root.logger=INFO,console<br>
system:hadoop.security.logger=INFO,NullAppender<br>
system:java.awt.graphicsenv=sun.awt.X11GraphicsEnvironment<br>
system:java.awt.printerjob=sun.print.PSPrinterJob<br>
system:java.class.path=/usr/local/hadoop-2.6.4/etc/hadoop:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/hadoop-auth-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/hadoop-annotations-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/hadoop-common-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/hadoop-nfs-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/common/hadoop-common-2.6.4-tests.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/hadoop-hdfs-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/hadoop-hdfs-2.6.4-tests.jar:/usr/local/hadoop-2.6.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jline-2.12.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-server-common-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-client-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-api-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-registry-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-common-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.4-tests.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.4.jar:/usr/local/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.4.jar:/usr/local/hive/conf:/usr/local/hive/lib/accumulo-core-1.6.0.jar:/usr/local/hive/lib/accumulo-fate-1.6.0.jar:/usr/local/hive/lib/accumulo-start-1.6.0.jar:/usr/local/hive/lib/accumulo-trace-1.6.0.jar:/usr/local/hive/lib/activation-1.1.jar:/usr/local/hive/lib/ant-1.9.1.jar:/usr/local/hive/lib/ant-launcher-1.9.1.jar:/usr/local/hive/lib/antlr-2.7.7.jar:/usr/local/hive/lib/antlr-runtime-3.4.jar:/usr/local/hive/lib/apache-log4j-extras-1.2.17.jar:/usr/local/hive/lib/asm-commons-3.1.jar:/usr/local/hive/lib/asm-tree-3.1.jar:/usr/local/hive/lib/avro-1.7.5.jar:/usr/local/hive/lib/bonecp-0.8.0.RELEASE.jar:/usr/local/hive/lib/calcite-avatica-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-core-1.2.0-incubating.jar:/usr/local/hive/lib/calcite-linq4j-1.2.0-incubating.jar:/usr/local/hive/lib/commons-beanutils-1.7.0.jar:/usr/local/hive/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hive/lib/commons-cli-1.2.jar:/usr/local/hive/lib/commons-codec-1.4.jar:/usr/local/hive/lib/commons-collections-3.2.1.jar:/usr/local/hive/lib/commons-compiler-2.7.6.jar:/usr/local/hive/lib/commons-compress-1.4.1.jar:/usr/local/hive/lib/commons-configuration-1.6.jar:/usr/local/hive/lib/commons-dbcp-1.4.jar:/usr/local/hive/lib/commons-digester-1.8.jar:/usr/local/hive/lib/commons-httpclient-3.0.1.jar:/usr/local/hive/lib/commons-io-2.4.jar:/usr/local/hive/lib/commons-lang-2.6.jar:/usr/local/hive/lib/commons-logging-1.1.3.jar:/usr/local/hive/lib/commons-math-2.1.jar:/usr/local/hive/lib/commons-pool-1.5.4.jar:/usr/local/hive/lib/commons-vfs2-2.0.jar:/usr/local/hive/lib/curator-client-2.6.0.jar:/usr/local/hive/lib/curator-framework-2.6.0.jar:/usr/local/hive/lib/curator-recipes-2.6.0.jar:/usr/local/hive/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/hive/lib/datanucleus-core-3.2.10.jar:/usr/local/hive/lib/datanucleus-rdbms-3.2.9.jar:/usr/local/hive/lib/derby-10.10.2.0.jar:/usr/local/hive/lib/eigenbase-properties-1.1.5.jar:/usr/local/hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:/usr/local/hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:/usr/local/hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:/usr/local/hive/lib/groovy-all-2.1.6.jar:/usr/local/hive/lib/guava-14.0.1.jar:/usr/local/hive/lib/hamcrest-core-1.1.jar:/usr/local/hive/lib/hive-accumulo-handler-1.2.1.jar:/usr/local/hive/lib/hive-ant-1.2.1.jar:/usr/local/hive/lib/hive-beeline-1.2.1.jar:/usr/local/hive/lib/hive-cli-1.2.1.jar:/usr/local/hive/lib/hive-common-1.2.1.jar:/usr/local/hive/lib/hive-contrib-1.2.1.jar:/usr/local/hive/lib/hive-exec-1.2.1.jar:/usr/local/hive/lib/hive-hbase-handler-1.2.1.jar:/usr/local/hive/lib/hive-hwi-1.2.1.jar:/usr/local/hive/lib/hive-jdbc-1.2.1.jar:/usr/local/hive/lib/hive-jdbc-1.2.1-standalone.jar:/usr/local/hive/lib/hive-metastore-1.2.1.jar:/usr/local/hive/lib/hive-serde-1.2.1.jar:/usr/local/hive/lib/hive-service-1.2.1.jar:/usr/local/hive/lib/hive-shims-0.20S-1.2.1.jar:/usr/local/hive/lib/hive-shims-0.23-1.2.1.jar:/usr/local/hive/lib/hive-shims-1.2.1.jar:/usr/local/hive/lib/hive-shims-common-1.2.1.jar:/usr/local/hive/lib/hive-shims-scheduler-1.2.1.jar:/usr/local/hive/lib/hive-testutils-1.2.1.jar:/usr/local/hive/lib/httpclient-4.4.jar:/usr/local/hive/lib/httpcore-4.4.jar:/usr/local/hive/lib/ivy-2.4.0.jar:/usr/local/hive/lib/janino-2.7.6.jar:/usr/local/hive/lib/jcommander-1.32.jar:/usr/local/hive/lib/jdo-api-3.0.1.jar:/usr/local/hive/lib/jetty-all-7.6.0.v20120127.jar:/usr/local/hive/lib/jetty-all-server-7.6.0.v20120127.jar:/usr/local/hive/lib/jline-2.12.jar:/usr/local/hive/lib/joda-time-2.5.jar:/usr/local/hive/lib/jpam-1.1.jar:/usr/local/hive/lib/json-20090211.jar:/usr/local/hive/lib/jsr305-3.0.0.jar:/usr/local/hive/lib/jta-1.1.jar:/usr/local/hive/lib/junit-4.11.jar:/usr/local/hive/lib/libfb303-0.9.2.jar:/usr/local/hive/lib/libthrift-0.9.2.jar:/usr/local/hive/lib/log4j-1.2.16.jar:/usr/local/hive/lib/mail-1.4.1.jar:/usr/local/hive/lib/maven-scm-api-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svn-commons-1.4.jar:/usr/local/hive/lib/maven-scm-provider-svnexe-1.4.jar:/usr/local/hive/lib/mysql-connector-java-5.1.35-bin.jar:/usr/local/hive/lib/netty-3.7.0.Final.jar:/usr/local/hive/lib/opencsv-2.3.jar:/usr/local/hive/lib/oro-2.0.8.jar:/usr/local/hive/lib/paranamer-2.3.jar:/usr/local/hive/lib/parquet-hadoop-bundle-1.6.0.jar:/usr/local/hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/usr/local/hive/lib/plexus-utils-1.5.6.jar:/usr/local/hive/lib/regexp-1.3.jar:/usr/local/hive/lib/servlet-api-2.5.jar:/usr/local/hive/lib/snappy-java-1.0.5.jar:/usr/local/hive/lib/ST4-4.0.4.jar:/usr/local/hive/lib/stax-api-1.0.1.jar:/usr/local/hive/lib/stringtemplate-3.2.1.jar:/usr/local/hive/lib/super-csv-2.2.0.jar:/usr/local/hive/lib/tempus-fugit-1.1.jar:/usr/local/hive/lib/velocity-1.5.jar:/usr/local/hive/lib/xz-1.0.jar:/usr/local/hive/lib/zookeeper-3.4.6.jar:/usr/local/spark/lib/spark-assembly-1.6.2-hadoop2.6.0.jar::/usr/local/hbase-1.2.1/conf:/usr/local/hbase-1.2.1/lib/hbase-protocol-1.2.1.jar:/usr/local/hbase-1.2.1/lib/hbase-client-1.2.1.jar:/usr/local/hbase-1.2.1/lib/metrics-core-2.2.0.jar:/usr/local/hbase-1.2.1/lib/hbase-common-1.2.1.jar:/usr/local/hbase-1.2.1/lib/netty-all-4.0.23.Final.jar:/usr/local/hbase-1.2.1/lib/hbase-hadoop-compat-1.2.1.jar:/usr/local/hbase-1.2.1/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hbase-1.2.1/lib/hbase-server-1.2.1.jar:/usr/local/hadoop-2.6.4/contrib/capacity-scheduler/*.jar<br>
system:java.class.version=51.0<br>
system:java.endorsed.dirs=/usr/local/jdk1.7/jre/lib/endorsed<br>
system:java.ext.dirs=/usr/local/jdk1.7/jre/lib/ext:/usr/java/packages/lib/ext<br>
system:java.home=/usr/local/jdk1.7/jre<br>
system:java.io.tmpdir=/tmp<br>
system:java.library.path=/usr/local/hadoop-2.6.4/lib/native<br>
system:java.net.preferIPv4Stack=true<br>
system:java.runtime.name=Java(TM) SE Runtime Environment<br>
system:java.runtime.version=1.7.0_76-b13<br>
system:java.specification.name=Java Platform API Specification<br>
system:java.specification.vendor=Oracle Corporation<br>
system:java.specification.version=1.7<br>
system:java.vendor=Oracle Corporation<br>
system:java.vendor.url=http://java.oracle.com/<br>
system:java.vendor.url.bug=http://bugreport.sun.com/bugreport/<br>
system:java.version=1.7.0_76<br>
system:java.vm.info=mixed mode<br>
system:java.vm.name=Java HotSpot(TM) 64-Bit Server VM<br>
system:java.vm.specification.name=Java Virtual Machine Specification<br>
system:java.vm.specification.vendor=Oracle Corporation<br>
system:java.vm.specification.version=1.7<br>
system:java.vm.vendor=Oracle Corporation<br>
system:java.vm.version=24.76-b04<br>
system:line.separator=<br><br><br>
system:os.arch=amd64<br>
system:os.name=Linux<br>
system:os.version=2.6.32-573.el6.x86_64<br>
system:path.separator=:<br>
system:sun.arch.data.model=64<br>
system:sun.boot.class.path=/usr/local/jdk1.7/jre/lib/resources.jar:/usr/local/jdk1.7/jre/lib/rt.jar:/usr/local/jdk1.7/jre/lib/sunrsasign.jar:/usr/local/jdk1.7/jre/lib/jsse.jar:/usr/local/jdk1.7/jre/lib/jce.jar:/usr/local/jdk1.7/jre/lib/charsets.jar:/usr/local/jdk1.7/jre/lib/jfr.jar:/usr/local/jdk1.7/jre/classes<br>
system:sun.boot.library.path=/usr/local/jdk1.7/jre/lib/amd64<br>
system:sun.cpu.endian=little<br>
system:sun.cpu.isalist=<br>
system:sun.io.unicode.encoding=UnicodeLittle<br>
system:sun.java.command=org.apache.hadoop.util.RunJar /usr/local/hive/lib/hive-cli-1.2.1.jar org.apache.hadoop.hive.cli.CliDriver<br>
system:sun.java.launcher=SUN_STANDARD<br>
system:sun.jnu.encoding=UTF-8<br>
system:sun.management.compiler=HotSpot 64-Bit Tiered Compilers<br>
system:sun.os.patch.level=unknown<br>
system:user.country=US<br>
system:user.dir=/usr/local/start_sh<br>
system:user.home=/root<br>
system:user.language=en<br>
system:user.name=root<br>
system:user.timezone=US/Pacific-New
            </div>
                </div>