---
layout:     post
title:      spark基本命令
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/suixinlun/article/details/81630842				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h3 id="一spark所在目录">一、spark所在目录</h3>

<pre><code>cd usr/local/spark
</code></pre>

<p><img src="https://i.imgur.com/5fZuwrj.png" alt="" title=""></p>



<h3 id="二启动spark">二、启动spark</h3>

<pre><code>/usr/local/spark/sbin/start-all.sh
</code></pre>

<p><img src="https://i.imgur.com/0vlI8S7.png" alt="" title=""></p>

<p>启动Hadoop以<em>**</em>及Spark：</p>

<pre><code>bash ./starths.sh
</code></pre>

<p><img src="https://i.imgur.com/CDRY2PP.png" alt="" title=""></p>

<p>浏览器查看：</p>

<pre><code>172.16.31.17:8080
</code></pre>

<p><img src="https://i.imgur.com/oZv1wWe.png" alt="" title=""></p>

<p>停止Hadoop以及Spark</p>

<pre><code>bash ./stophs.sh
</code></pre>



<h3 id="三基础使用">三、基础使用</h3>

<p>参考链接：<a href="https://www.cnblogs.com/dasn/articles/5644919.html" rel="nofollow" title="Spark基础使用">https://www.cnblogs.com/dasn/articles/5644919.html</a></p>



<h4 id="1运行spark示例sparkpi">1.运行Spark示例（SparkPi）</h4>

<blockquote>
  <p>在 ./examples/src/main 目录下有一些 Spark 的示例程序，有 Scala、Java、Python、R 等语言的版本。</p>
</blockquote>

<p><img src="https://i.imgur.com/RBwTbNi.png" alt="" title=""></p>

<blockquote>
  <p>我们可以先运行一个示例程序 SparkPi（即计算 π 的近似值），执行如下命令：</p>
</blockquote>

<pre><code>./bin/run-example SparkPi 2&gt;&amp;1 | grep "Pi is roughly"
</code></pre>

<p><img src="https://i.imgur.com/uTNSJVo.png" alt="" title=""></p>

<blockquote>
  <p>执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 grep 命令进行过滤（命令中的 2&gt;&amp;1 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）</p>
</blockquote>

<p>Python 版本的 SparkPi 则需要通过 spark-submit 运行：</p>

<pre><code>./bin/spark-submit examples/src/main/python/pi.py
</code></pre>

<p><img src="https://i.imgur.com/Skcy5Ui.png" alt="" title=""></p>



<h4 id="2通过spark-shell进行交互分析">2.通过Spark-shell进行交互分析</h4>

<blockquote>
  <p>Spark Shell 支持 Scala 和 Python</p>
  
  <p>Scala 运行于 Java 平台（JVM，Java 虚拟机），并兼容现有的 Java 程序。</p>
  
  <p>Scala 是 Spark 的主要编程语言，如果仅仅是写 Spark 应用，并非一定要用 Scala，用 Java、Python 都是可以的。 <br>
  使用 Scala 的优势是开发效率更高，代码更精简，并且可以通过 Spark Shell 进行交互式实时查询，方便排查问题。</p>
</blockquote>

<p><strong>2.1 启动Spark Shell</strong></p>

<pre><code>./bin/spark-shell
</code></pre>

<p><img src="https://i.imgur.com/9dB44vO.png" alt="" title=""></p>

<p><strong>2.2 基础操作</strong></p>

<blockquote>
  <p>Spark 的主要抽象是分布式的元素集合（distributed collection of items），称为<strong>RDD</strong>（Resilient Distributed Dataset，弹性分布式数据集），它可被分发到集群各个节点上，进行<strong>并行操作</strong>。RDDs 可以通过 Hadoop InputFormats 创建（如 HDFS），或者从其他 RDDs 转化而来。</p>
</blockquote>

<p><strong><em>2.2.1 RDD创建方式</em></strong></p>

<p>参考链接：</p>

<p><a href="https://blog.csdn.net/sundujing/article/details/51397209" rel="nofollow" title="RDD创建方式">https://blog.csdn.net/sundujing/article/details/51397209</a> <br>
<a href="https://blog.csdn.net/u012834750/article/details/81020163" rel="nofollow" title="集合创建RDD">https://blog.csdn.net/u012834750/article/details/81020163</a> <br>
<a href="https://www.iteblog.com/archives/1512.html" rel="nofollow" title="makeRDD和parallelize函数区别">https://www.iteblog.com/archives/1512.html</a> <br>
<a href="https://www.jianshu.com/p/a7503b26cb73" rel="nofollow" title="加载本地文件和hdfs文件，并保存结果">https://www.jianshu.com/p/a7503b26cb73</a></p>

<p>（1）使用本地文件创建RDD：</p>

<pre><code>val textFile = sc.textFile("file:///usr/local/spark/README.md")
</code></pre>

<p><img src="https://i.imgur.com/9A7NaoU.png" alt="" title=""></p>

<blockquote>
  <p>代码中通过 “file://” 前缀指定读取本地文件。</p>
</blockquote>

<p>（2）使用hdfs文件创建RDD：</p>

<pre><code>val textfile = sc.textFile("/winnie/htest/test01.txt")
</code></pre>

<p><img src="https://i.imgur.com/XdYgW8X.png" alt="" title=""></p>

<blockquote>
  <p>Spark shell 默认是读取 HDFS 中的文件，需要先上传文件到 HDFS 中，否则会有“org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://localhost:9000/user/local/spark/README.md”的错误。</p>
</blockquote>

<p><img src="https://i.imgur.com/FU0NTDS.png" alt="" title=""></p>

<p>（3）使用集合创建RDD：</p>

<blockquote>
  <p>从集合中创建RDD主要有下面两个方法：makeRDD 和 parallelize</p>
</blockquote>

<p>makeRDD示例：</p>

<pre><code>sc.makeRDD(1 to 50)
</code></pre>

<p><img src="https://i.imgur.com/jPeZjLR.png" alt="" title=""></p>

<pre><code>sc.makeRDD(Array("1", "2", "3"))
</code></pre>

<p><img src="https://i.imgur.com/PpqdWRI.png" alt="" title=""></p>

<pre><code>sc.makeRDD(List(1,3,5))
</code></pre>

<p><img src="https://i.imgur.com/Jt9TSbc.png" alt="" title=""></p>

<p>parallelize示例：</p>

<p>同上</p>

<p><strong><em>2.2.2 RDD两种操作</em></strong></p>

<p>参考链接： <br>
<a href="https://blog.csdn.net/HANLIPENGHANLIPENG/article/details/53508746" rel="nofollow" title="RDD基本操作">https://blog.csdn.net/HANLIPENGHANLIPENG/article/details/53508746</a></p>

<p>（1）RDDs支持两种类型的操作：</p>

<blockquote>
  <p>actions: 执行操作，在数据集上运行计算后返回值</p>
  
  <p>transformations: 转化操作，从现有数据集创建一个新的数据集</p>
  
  <p>转化操作并不会立即执行，而是到了执行操作才会被执行</p>
</blockquote>

<p>转化操作：</p>

<pre><code>map()          参数是函数，函数应用于RDD每一个元素，返回值是新的RDD 
flatMap()      参数是函数，函数应用于RDD每一个元素，将元素数据进行拆分，变成迭代器，返回值是新的RDD 
filter()       参数是函数，函数会过滤掉不符合条件的元素，返回值是新的RDD 
distinct()     没有参数，将RDD里的元素进行去重操作 
union()        参数是RDD，生成包含两个RDD所有元素的新RDD 
intersection() 参数是RDD，求出两个RDD的共同元素 
subtract()     参数是RDD，将原RDD里和参数RDD里相同的元素去掉 
cartesian()    参数是RDD，求两个RDD的笛卡儿积
</code></pre>

<p>行动操作：</p>

<pre><code>collect()       返回RDD所有元素 
count()         RDD里元素个数，对于文本文件，为总行数
first()         RRD中的第一个item，对于文本文件，为首行内容 
countByValue()  各元素在RDD中出现次数 
reduce()        并行整合所有RDD数据，例如求和操作 
fold(0)(func)   和reduce功能一样，不过fold带有初始值 
aggregate(0)(seqOp,combop) 和reduce功能一样，但是返回的RDD数据类型和原RDD不一样 
foreach(func)   对RDD每个元素都是使用特定函数
</code></pre>

<blockquote>
  <p>行动操作每次的调用是不会存储前面的计算结果的，若想要存储前面的操作结果,要把需要缓存中间结果的RDD调用cache(),cache()方法是把中间结果缓存到内存中，也可以指定缓存到磁盘中（也可以只用persisit()）</p>
</blockquote>

<p>（2）实例</p>

<pre><code>textFile.count()
textFile.first()
</code></pre>

<p><img src="https://i.imgur.com/sanE1hl.png" alt="" title=""></p>

<pre><code>val linesWithSpark = textFile.filter(line =&gt; line.contains("Spark")) // 筛选出包含 Spark 的行
linesWithSpark.count() // 统计行数
</code></pre>

<p><img src="https://i.imgur.com/Q63snND.png" alt="" title=""></p>

<blockquote>
  <p>action 和 transformation 可以用链式操作的方式结合使用，使代码更为简洁：</p>
</blockquote>

<pre><code>textFile.filter(line =&gt; line.contains("Spark")).count()
</code></pre>

<p>找到包含单词最多的那一行内容共有几个单词：</p>

<pre><code>textFile.map(line =&gt; line.split(" ").size).reduce((a, b) =&gt; if (a &gt; b) a else b)
</code></pre>

<p><img src="https://i.imgur.com/WBfkrUU.png" alt="" title=""></p>

<blockquote>
  <p>代码首先将每一行内容 map 为一个整数，这将创建一个新的 RDD，并在这个 RDD 中执行 reduce 操作，找到最大的数。map()、reduce() 中的参数是 Scala 的函数字面量（function literals，也称为闭包 closures），并且可以使用语言特征或 Scala/Java 的库。</p>
</blockquote>

<p>Hadoop MapReduce 是常见的数据流模式，在 Spark 中同样可以实现（下面这个例子也就是 WordCount）：</p>

<pre><code>val wordCounts = textFile.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)  // 实现单词统计
wordCounts.collect() // 输出单词统计结果
</code></pre>

<p><img src="https://i.imgur.com/Wl9Gk4K.png" alt="" title=""></p>

<p><strong><em>2.2.3 Spark shell退出</em></strong></p>

<p>参考链接：<a href="https://blog.csdn.net/w5688414/article/details/78166176" rel="nofollow" title="退出操作及问题解决方法">https://blog.csdn.net/w5688414/article/details/78166176</a></p>

<pre><code>：quit
</code></pre>



<h4 id="3spark-sql-和-dataframes">3.Spark SQL 和 DataFrames</h4>

<blockquote>
  <p>Spark SQL 是 Spark 内嵌的模块，用于结构化数据。</p>
  
  <p>在 Spark 程序中可以使用 SQL 查询语句或 DataFrame API。</p>
  
  <p>DataFrames 和 SQL 提供了通用的方式来连接多种数据源，支持 Hive、Avro、Parquet、ORC、JSON、和 JDBC，并且可以在多种数据源之间执行 join 操作。</p>
</blockquote>

<p><strong>3.1 Spark SQL 基本操作</strong></p>

<blockquote>
  <p>Spark SQL 的功能是通过 SQLContext 类来使用的，而创建 SQLContext 是通过 SparkContext 创建的。</p>
</blockquote>

<p>在 Spark shell 启动时，输出日志的最后有这么几条信息:</p>

<p><img src="https://i.imgur.com/SZWohEZ.png" alt="" title=""></p>

<blockquote>
  <p>这些信息表明 SparkContent 和 SQLContext 都已经初始化好了，可通过对应的 sc、sqlContext 变量直接进行访问。</p>
  
  <p>使用 SQLContext 可以从现有的 RDD 或数据源创建 DataFrames。</p>
</blockquote>

<p>通过 Spark 提供的 JSON 格式的数据源文件 ./examples/src/main/resources/people.json 来进行演示，该数据源内容如下: <br>
<img src="https://i.imgur.com/xX6DcXz.png" alt="" title=""></p>

<hr>

<p>【补充：xshell上传文件】</p>

<pre><code>rz
</code></pre>

<p>【补充：xshell下载文件】</p>

<pre><code>sz
</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>