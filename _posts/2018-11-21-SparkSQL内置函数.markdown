---
layout:     post
title:      SparkSQL内置函数
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<pre><code>使用Spark SQL中的内置函数对数据进行分析，Spark SQL API不同的是，DataFrame中的内置函数操作的结果是返回一个Column对象，而DataFrame天生就是"A distributed collection of data organized into named columns.",这就为数据的复杂分析建立了坚实的基础并提供了极大的方便性，例如说，我们在操作DataFrame的方法中可以随时调用内置函数进行业务需要的处理，这之于我们构建附件的业务逻辑而言是可以极大的减少不必须的时间消耗（基于上就是实际模型的映射），让我们聚焦在数据分析上，
在Spark 1.5.x版本，增加了一系列内置函数到DataFrame API中，并且实现了code-generation的优化。与普通的函数不同，DataFrame的函数并不会执行后立即返回一个结果值，而是返回一个Column对象，用于在并行作业中进行求值。Column可以用在DataFrame的操作之中，比如select，filter，groupBy等。函数的输入值，也可以是Column。
</code></pre>



<h2 id="聚合函数">聚合函数</h2>

<pre><code>approxCountDistinct, avg, count, countDistinct, first, last, max, mean, min, sum, sumDistinct
</code></pre>



<h2 id="集合函数">集合函数</h2>

<pre><code>array_contains, explode, size, sort_array
</code></pre>

<h2 id="日期时间函数">日期/时间函数</h2>

<pre><code>日期时间转换：
unix_timestamp, from_unixtime, to_date, quarter, day, dayofyear, weekofyear, from_utc_timestamp, to_utc_timestamp
从日期时间中提取字段：
year, month, dayofmonth, hour, minute, second
日期/时间计算：
datediff, date_add, date_sub, add_months, last_day, next_day, months_between
获取当前时间等：
current_date, current_timestamp, trunc, date_format
</code></pre>



<h2 id="数学函数">数学函数</h2>

<pre><code>abs, acros, asin, atan, atan2, bin, cbrt, ceil, conv, cos, sosh, exp, expm1, factorial, floor, hex, hypot, log, log10, log1p, log2, pmod, pow, rint, round, shiftLeft, shiftRight, shiftRightUnsigned, signum, sin, sinh, sqrt, tan, tanh, toDegrees, toRadians, unhex
</code></pre>



<h2 id="混合函数">混合函数</h2>

<pre><code>array, bitwiseNOT, callUDF, coalesce, crc32, greatest, if, inputFileName, isNaN, isnotnull, isnull, least, lit, md5, monotonicallyIncreasingId, nanvl, negate, not, rand, randn, sha, sha1, sparkPartitionId, struct, when
</code></pre>



<h2 id="字符串函数">字符串函数</h2>

<pre><code>ascii, base64, concat, concat_ws, decode, encode, format_number, format_string, get_json_object, initcap, instr, length, levenshtein, locate, lower, lpad, ltrim, printf, regexp_extract, regexp_replace, repeat, reverse, rpad, rtrim, soundex, space, split, substring, substring_index, translate, trim, unbase64, upper
</code></pre>



<h2 id="窗口函数">窗口函数</h2>

<pre><code>cumeDist, denseRank, lag, lead, ntile, percentRank, rank, rowNumber
</code></pre>



<h2 id="案例实战">案例实战：</h2>

<pre><code>第一步：创建Spark的配置对象SparkConf，设置Spark程序的运行时的配置信息，例如说通过setMaster来设置程序要链接的Spark集群的Master的URL,Spark程序在本地运行
</code></pre>



<pre class="prettyprint"><code class=" hljs fsharp">    <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> SparkConf() <span class="hljs-comment">//创建SparkConf对象  </span>
    conf.setAppName(<span class="hljs-string">"SparkSQL"</span>) <span class="hljs-comment">//设置应用程序的名称，在程序运行的监控界面可以看到名称  </span>
    <span class="hljs-comment">//conf.setMaster("spark://DaShuJu-040:7077") //此时，程序在Spark集群  </span>
    conf.setMaster(<span class="hljs-string">"local"</span>)  </code></pre>

<pre><code>第二步：创建SparkContext对象
  SparkContext是Spark程序所有功能的唯一入口，无论是采用Scala、Java、Python、R等都必须有一个SparkContext
  SparkContext核心作用：初始化Spark应用程序运行所需要的核心组件，包括DAGScheduler、TaskScheduler、SchedulerBackend
  同时还会负责Spark程序往Master注册程序等
  SparkContext是整个Spark应用程序中最为至关重要的一个对象。
</code></pre>



<pre class="prettyprint"><code class=" hljs scala"><span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> SparkContext(conf) <span class="hljs-comment">//创建SparkContext对象，通过传入SparkConf实例来定制Spark运行的具体参数和配置信息  </span>
<span class="hljs-keyword">val</span> sqlContext = <span class="hljs-keyword">new</span> SQLContext(sc) 

<span class="hljs-comment">//注意：要使用Spark SQL的内置函数，就一定要导入SQLContext下的隐式转换  </span>
<span class="hljs-keyword">import</span> sqlContext.implicits._  </code></pre>

<pre><code>第三步：模拟数据，最后生成RDD
</code></pre>

<pre class="prettyprint"><code class=" hljs fsharp"><span class="hljs-keyword">val</span> userData = Array(  
      <span class="hljs-string">"2016-3-27,001,http://spark.apache.org/,1000"</span>,  
      <span class="hljs-string">"2016-3-27,001,http://hadoop.apache.org/,1001"</span>,  
      <span class="hljs-string">"2016-3-27,002,http://fink.apache.org/,1002"</span>,  
      <span class="hljs-string">"2016-3-28,003,http://kafka.apache.org/,1020"</span>,  
      <span class="hljs-string">"2016-3-28,004,http://spark.apache.org/,1010"</span>,  
      <span class="hljs-string">"2016-3-28,002,http://hive.apache.org/,1200"</span>,  
      <span class="hljs-string">"2016-3-28,001,http://parquet.apache.org/,1500"</span>,  
      <span class="hljs-string">"2016-3-28,001,http://spark.apache.org/,1800"</span>  
    )
    <span class="hljs-comment">//并行化成RDD</span>
    <span class="hljs-keyword">val</span> userDataRDD = sc.parallelize(userData)  </code></pre>

<pre><code>第四步：根据业务需要对数据进行预处理生成DataFrame，要想把RDD转换成DataFrame，需要先把RDD中的元素类型变成Row类型，于此同时要提供DataFrame中的Columns的元数据信息描述。
</code></pre>



<pre class="prettyprint"><code class=" hljs fsharp">    <span class="hljs-keyword">val</span> userDataRDDRow = userDataRDD.map(row =&gt; {<span class="hljs-keyword">val</span> splited = row.split(<span class="hljs-string">","</span>) ;Row(splited(<span class="hljs-number">0</span>),splited(<span class="hljs-number">1</span>).toInt,splited(<span class="hljs-number">2</span>),splited(<span class="hljs-number">3</span>).toInt)})  
    <span class="hljs-keyword">val</span> structTypes = StructType(Array(  
          StructField(<span class="hljs-string">"time"</span>, StringType, <span class="hljs-keyword">true</span>),  
          StructField(<span class="hljs-string">"id"</span>, IntegerType, <span class="hljs-keyword">true</span>),  
          StructField(<span class="hljs-string">"url"</span>, StringType, <span class="hljs-keyword">true</span>),  
          StructField(<span class="hljs-string">"amount"</span>, IntegerType, <span class="hljs-keyword">true</span>)  
    ))  

    <span class="hljs-keyword">val</span> userDataDF = sqlContext.createDataFrame(userDataRDDRow,structTypes)

    第五步：使用Spark SQL提供的内置函数对DataFrame进行操作，特别注意：内置函数生成的Column对象且自定进行CG</code></pre>

<pre><code>userDataDF.groupBy("time").agg('time, countDistinct('id)).map(row=&gt;Row(row(1),row(2))).collect.foreach {  println  } 
userDataDF.groupBy("time").agg('time, sum('amount)).show()  
</code></pre>

<p>“`</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>