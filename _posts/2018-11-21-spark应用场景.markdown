---
layout:     post
title:      spark应用场景
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
问题 1  Spark怎么会那么快 2 Spark的适用场景。苦苦搜索，总得结果。<br>
spark是对MapReduce计算模型的改进，可以说没有HDFS，MapReduce，就没有spark.尽管spark官网很少提到MapReduce.<br><span style="color:#FF0000;">Spark可用于迭代，主要思想是内存计算，即将数据存到内存中，以提高迭代效率。</span><br>
（我认为就是通过缓存数据的方式减少IO的消耗，从而提高了性能）<br>
Spark可以将过程数据存在内存中（从文件中读取，或经过map处理的数据），方便其他模块重复调用。<br>
相比hadoop MapReduce 主要的改进：<br><span style="color:#FF0000;">1迭代运算，一次创建数据集。多次使用，减少了IO的开销。<br>
2允许多种计算模型（包含map-reduce）<br>
3 支持非OO式算法实现，对机器学习算法，图计算能力有很好的支持。</span><br><br>
仔细研读下面的文章，就能理解我的总结<br>
迭代式MapReduce框架介绍<br><a href="http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/" rel="nofollow">http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/</a><br>
传统MapReduce框架  <br><a href="http://dongxicheng.org/mapreduce/traditional-mapreduce-framework/" rel="nofollow">http://dongxicheng.org/mapreduce/traditional-mapreduce-framework/</a><br>
选择Spark on Yarn的三个理由<br>
[url] http://storage.it168.com/a2013/1123/1564/000001564111.shtml[/url]<br>
注：迭代运算： 这里指对同一个数据集的多次使用<br>            </div>
                </div>