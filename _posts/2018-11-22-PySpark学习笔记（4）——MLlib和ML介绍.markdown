---
layout:     post
title:      PySpark学习笔记（4）——MLlib和ML介绍
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>Spark <strong>MLlib</strong>是Spark中专门用于处理机器学习任务的库，但在最新的Spark 2.0中，大部分机器学习相关的任务已经转移到Spark <strong>ML</strong>包中。两者的区别在于MLlib是基于RDD源数据的，而ML是基于DataFrame的更抽象的概念，可以<span class="RichText CopyrightRichText-richText">创建包含从数据清洗到特征工程再到模型训练等一系列机器学习工作。所以，未来在用Spark处理机器学习任务时，将以Spark ML为主。</span><br></p><p>Spark ML主要包括如下三个部分的功能：</p><p>（1）<strong>数据准备</strong>：特征提取、变换、选择和一些自然处理方法。</p><p>（2）<strong>机器学习算法</strong>： 常见的分类、聚类、回归算法。<br></p><p>（3）<strong>实用程序</strong>：常见的统计方法和模型评估方法。<br></p><p>Spark ML软件包主要包括三个抽象类：<strong>转换器（Transformer</strong>）、<strong>评估器（Estimator）</strong>和<strong>管道（Pipeline）</strong>。</p><h1>1.转换器</h1><p>PySpark ML中的转换器的实现通常需要将一个新列附加到DataFrame来转换数据。当从<strong>转换器基类</strong>（pyspark.ml.Transformer）派生时，每个新的转换器类需要实现.transform()方法，该方法要求传递一个要被转换的DataFrame(强制性要求)。</p><p><strong>pyspark.ml.feature</strong>模块提供了许多种转换器，部分常见转换器的简单介绍如下所示：<br></p><p>（1）<strong>Binarizer</strong>：根据指定的阈值将<strong>连续变量</strong>转换为对应的<strong>二进制值</strong>。</p><p>（2）<strong>Bucketizer</strong>:根据阈值列表将<strong>连续变量</strong>转换为<strong>多项值（</strong>即将连续变量离散化到指定的范围区间）。</p><p>（3）<strong>ChiSqSelector</strong>:此功能可以通过卡方（Chi-Square）检验的方法来完成分类模型中数量特征的选择。</p><p>（4）<strong>countVectorizer</strong>:该方法主要用于标记文本。</p><p>（5）<strong>HashingTF</strong>: 哈希转换器，输入为标记文本的列表，返回一个带有计数的有预定长度的向量。<br></p><p>（6）<strong>IDF</strong>：该方法计算文档列表的逆向文件频率。（需要提前用HashingTF或CountVectorizer将文档转换为向量表示）</p><p>（7）<strong>OneHotEncoder</strong>：该方法将分类编码为二进制向量列。</p><p>（8）<strong>PCA</strong>： 使用主成分分析执行数据降维。<br></p><p>（9）<strong>StopWordRemover</strong>： 从标记文本中删除停用词。<br></p><p>（10）<strong>Tokenizer</strong>： 分词器，将该默认分词器将字符串转成小写，然后以空格为分隔符分词。</p><p>（11）<strong>VectorIndexer</strong>: 该方法为类别列生成索引向量。</p><p>（12）<strong>VectorSlicer</strong>： 给定一个索引列表，它从特征向量中提取值。</p><p>（13）<strong>Word2Vec</strong>： 该方法将一个字符串（或句子）作为输入，将其转换为{string，vector}格式的映射、<br></p><h1>2.评估器</h1><p>PySpark ML中的评估器是指<strong>使用各种统计模型观测对象做预测或分类</strong>。如果是从<strong>抽象评估器类</strong>（pyspark.ml.Estimator）派生的，新模型必须实现,fit()方法，该方法用<strong>DataFrame中的数据</strong>和某些<strong>参数</strong>（默认或自定义的）来拟合模型。</p><p>PySpark ML中的评估器模型主要有分类模型、回归模型、聚类模型和推荐模型等。</p><h2>2.1 分类模型(pyspark.ml.classification)<br></h2><p>（1）<strong>LogisticRegression：</strong> (支持二项和多项（softmax）逻辑回归)</p><p>        逻辑回归使用一个对数函数，将输入变量和分类类别进行关联，可用于解决二类（Sigmoid函数）和多类分类问题。<br></p><p>（2）<strong>DecisionTreeClassifier</strong>：（支持二进制和多类标签）</p><p>       该分类器通过构建一个决策树来预测一个观察对象的所属类别。<br></p><p>（3）<strong>GBTClassifier</strong>: （支持二进制标签、连续特征和分类特征）</p><p>     <strong>梯度提升决策树</strong>（Gradient Boosting Decision Tree）是集成分类方法的一种，其基学习器是分类回归树CART。通过Boosting方法将多个弱分类器组合成一个强分类器。<br></p><p>（4）<strong>RandomForestClassifier</strong>: （支持二元标签和多项标签）<br></p><p>     <strong>随机森林决策树</strong>是另一种集成分类方法，其基学习器是决策树。通过Bagging方法（投票法）多个弱分类器的分类众数作为最终的分类结果。<br></p><p>（5）<strong>NaiveBayes:</strong> （支持二元标签和多项标签）</p><p>    该模型基于贝叶斯原理，使用条件概率理论对观测进行分类。<br></p><p>（6）<strong>MultilayerPerceptronClassifier</strong>:</p><p>    多层感知分类器至少包含三个完全相连的人造神经元层（创建模型时需要指定参数）：<strong>输入层</strong>（神经元个数需要和输入数据集中的特征数量一样），<strong>隐藏层</strong>（至少一个，包含有非线性变换函数）和<strong>输出层</strong>（神经元个数需要与输出类别个数一样）。<br></p><p>（7）<strong>OneVsRest:</strong></p><p>   该模型将多类问题转换为二类问题。<br></p><h2>2.2 回归模型(pyspark.ml.regression)</h2><p>（1）<strong>DecisionTreeRegressor</strong>: 用于处理连续标签的决策树。<br></p><p>（2）<strong>GBTRegressor</strong>: 用于处理连续标签的梯度提升树。<br></p><p>（3）<strong>RandomForestRegressor</strong>：用于处理连续标签的随机森林决策。<br></p><p>（4）<strong>LinearRegression</strong>： 简单的线性回归模型，它假设输入特征与连续的输出标签之间存在某种线性关系。<br></p><p>（5）<strong>GeneralizedLinearRegression</strong>:</p><p>       广义线性回归模型根据其所使用的内核（gaussian、binomial、gamma、possion）的不同，能够解决不同类型的线性回归问题。<br></p><h2>2.3 聚类模型(pyspark.ml.clustering)</h2><p>聚类是一系列无监督的模型，用于查找数据中的<strong>隐含模式</strong>。</p><p>（1）<strong>KMeans</strong>：k均值算法，该算法将数据分为k个簇，迭代地搜索那些使每个观察点和它所属簇的质点之间距离平方和最新的那些质点。</p><p>（2）<strong>BisectingKMeans</strong>：二分k均值算法，结合 了k均值算法和层次聚类算法。最初该算法将所有观察点作为一个簇，然后将数据迭代地分解为k个簇。<br></p><p>（3）<strong>GaussianMixture</strong>：高斯混合模型，该方法使用具有未知参数的k个高斯分布来剖析数据集。使用<strong>期望最大化算法</strong>，通过最大化对数似然函数找到高斯函数。</p><p>（4）<strong>LDA</strong>：潜在狄利克雷分配模型（Latent Dirichlet Allocation），可以用来识别大规模文档集或语料库中潜藏的<strong>主题信息</strong>。</p><h2>2.4 推荐模型(pyspark.ml.recommendation)<code class="descclassname"></code></h2><p>（1）<strong>ALS</strong>：交替最小二乘(Alternating Least Squares)，是一种基于协同过滤原理的推荐算法。在已知用户（User）数量m和物品（Item）数量n的前提下，可以通过奇异值分解（Singular Value Decomposition，SVD）的方法，将用户—商品矩阵A_mxn分解为矩阵U和V，如下所示：</p><p align="center"><img src="https://img-blog.csdn.net/20180411103051831" alt=""></p><p align="left">然后，通过ALS方法对其进行优化（先固定其中一个变量，优化另一个；然后再固定另一个，优化前一个，交替迭代进行），最后可以据此得出用户对特定物品的偏好（评分或偏好值）。<br></p><h1>3.管道<br></h1><p>PySpark ML中的管道是指<strong>从转换到评估的端到端的过程</strong>，这个过程可以<strong>对输入的DataFrame原始数据执行必要的数据转换操作</strong>，最后评估统计模型。</p><p><br></p><p><br></p><p></p><p></p>            </div>
                </div>