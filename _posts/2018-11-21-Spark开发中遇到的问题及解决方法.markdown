---
layout:     post
title:      Spark开发中遇到的问题及解决方法
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><span style="font-size:18px;">1、数据来源于HDFS，处理完成后同样写回HDFS的时候，遇到了HDFS的权限问题，提交程序的用户为root ，而root对HDFS没有写权限，这时可以通过伪造程序的使用者的方法解决：</span></p>
<p></p>
<pre><code class="language-java"><span style="font-size:18px;">	val conf = new SparkConf().setAppName("TestLogic")
			.set("fs.default.name","hdfs://udh-cluster-4:8020"）
               	 	.setMaster("spark://udh-cluster-4:7077")
               		.set("HADOOP_USER_NAME", "hdfs")</span></code></pre><br><p></p>
<p></p>
<p><span><span style="font-size:18px;">2、使用Standalone模式提交集群的时候报<span style="background-color:rgb(255,255,255);">错<span style="font-family:'Microsoft Yahei', Hei, Tahoma, SimHei, sans-serif;">All masters are unresponsive! Giving up 时，检查程序中conf指定的master是否</span></span></span></span></p>
<p><span><span style="background-color:rgb(255,255,255);"><span style="font-family:'Microsoft Yahei', Hei, Tahoma, SimHei, sans-serif;"><span style="font-size:18px;"><span></span>跟集群上的master的地址是相同的</span></span></span></span></p>
<p><span><span style="background-color:rgb(255,255,255);"><span style="font-family:'Microsoft Yahei', Hei, Tahoma, SimHei, sans-serif;"><span style="font-size:18px;">3、运行时出现FileSystemClose的情况时，需要在conf中指定conf.</span></span></span></span><span style="background-color:rgb(240,240,240);font-family:'Microsoft Yahei', Hei, Tahoma, SimHei, sans-serif;"><span style="font-size:18px;">.set("fs.default.name","hdfs://udh-cluster-4:8020"）</span></span></p>
<p><span><span style="font-size:18px;">并且最后需要执行SparkContext对象的Stop()方法.</span></span></p>
            </div>
                </div>