---
layout:     post
title:      3.数据采集 - 文件内容断点续采[flume1.6 + kafka0.10.2.0]
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本博客文章均由博主从互联网及各类书本资料收集加以自己理解、整理，如需转载请注明出处！					https://blog.csdn.net/feloxx/article/details/70788812				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p><span style="font-family:'宋体';">数据采集,目前两个方式是socket采集,文件内容采集</span></p>
<p><span style="font-family:'宋体';">因为socket采集，如果要实现高可用的话，需要双方有个心跳机制，才能保证高可用；</span></p>
<p><span style="font-family:'宋体';">文件采集的话,输出方和采集方相互独立，只要双方各种保证自己的高可用即可；</span></p>
<p><span style="font-family:'宋体';">这里主要使用flume来对日志文件进行采集；</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:18pt;"><strong>简单介绍下flume</strong></span></p>
<p><span style="font-family:'宋体';">Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</span></p>
<p><span style="font-family:'宋体';">flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent，agent本身是一个Java进程，运行在日志收集节点—所谓日志收集节点就是服务器节点。
</span></p>
<p><span style="font-family:'宋体';">agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。
</span></p>
<p><span style="font-family:'宋体';">source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。
</span></p>
<p><span style="font-family:'宋体';">channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。
</span></p>
<p><span style="font-family:'宋体';">sink：sink组件是用于把数据发送到目的地的组件，目的地包括kafka、hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:18pt;"><strong>简单介绍下flume的运行机制</strong></span></p>
<p><span style="font-family:'宋体';">flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据的输入source，一个是数据的输出sink，sink负责将数据发送到外部指定的目的地。</span></p>
<p><span style="font-family:'宋体';">source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方—-例如HDFS等</span></p>
<p><span style="font-family:'宋体';">注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:18pt;"><strong>目前使用的采集 存储流程</strong></span></p>
<p><span style="font-family:'宋体';">source 文件采集 -&gt; channel kafka 存储</span></p>
<p><span style="font-family:'宋体';">（为啥这里没有sink了？）</span></p>
<p><span style="font-family:'宋体';">在老版本中一般是这样做，source文件采集，memory&amp;file channel 内存缓存或者硬盘缓存， sink kafka 输出kafka</span></p>
<p><span style="font-family:'宋体';">但是这样多一层中间缓存，高可用会多出现一层问题的可能性。</span></p>
<p><span style="font-family:'宋体';">在flume1.6之后的新特性，kafka channel的出现可以直接将后面的两层，合成一层。</span></p>
<p><span style="font-family:'宋体';">也不搞什么中间缓存了，采集了数据就赶快往后面存，加强了安全性，提升了灵活性。</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:18pt;"><strong>采集后数据存储为kafka(介绍下为什么要使用kafka)</strong></span></p>
<p><span style="font-family:'宋体';">构建实时的流数据管道，可靠地获取系统和应用程序之间的数据；</span></p>
<p><span style="font-family:'宋体';">构建实时流的应用程序，对数据流进行转换或反应；</span></p>
<p><span style="font-family:'宋体';">主要是为了流式计算中，使用消息队列将接收的数据异步一下，防止计算入口造成大量压力，业界同样还有很多消息队列例如MQ</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:18pt;"><strong>flume采集高可用的主要实现</strong></span></p>
<p><span style="font-family:'宋体';">采集程序挂了之后，保证重启日志数据不丢失，并且不重复发送；</span></p>
<p><span style="font-family:'宋体';">实现flume采集的断点续传,接着崩溃的最后采集索引继续采集；</span></p>
<p><span style="font-family:'宋体';">对收集过的历史文件进行备份处理；</span></p>
<p><span style="font-family:'宋体';">实现小时级的文件自动更新采集：</span></p>
<p><span style="font-family:'宋体';">例如2017010115生成日志文件，采集程序对该文件进行采集，</span></p>
<p><span style="font-family:'宋体';">当时间走到2017010116时，会生成一个新的日志文件，</span></p>
<p><span style="font-family:'宋体';">采集程序会自动将2017010115 文件备份,并去采集2017010116文件。</span></p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';font-size:22pt;"><strong>第一步 检查部分</strong></span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>1.1 jps检查</strong></span></p>
<p><span style="font-family:'宋体';">因为flume采集的存储是kafka，所以先检查集群中的kafka是否在工作。</span></p>
<p><span style="font-family:'宋体';">在每台机器上执行jps命令，能看到运行的进程，判断其中是否有kafka进程。</span></p>
<p><span style="font-family:'宋体';">[hadoop1@hadoop4 ~]$ jps</span></p>
<p><span style="font-family:'宋体';">2162 QuorumPeerMain</span></p>
<p><span style="font-family:'宋体';">31587 Jps</span></p>
<p><span style="font-family:'宋体';">2375 NodeManager</span></p>
<p><span style="font-family:'宋体';">27544 Kafka</span></p>
<p><span style="font-family:'宋体';">23276 Worker</span></p>
<p><span style="font-family:'宋体';">2253 DataNode</span></p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>1.2 topic检查</strong></span></p>
<p><span style="font-family:'宋体';">topic为kafka的存储消息的主题，外部（生产，消费）调用的入口。</span></p>
<p><span style="font-family:'宋体';">这里我需要使用的是topic filetest10</span></p>
<p><span style="font-family:'宋体';">[hadoop1@hadoop2 ~]$ kafka-topics.sh --list --zookeeper hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181,hadoop5:2181</span></p>
<p><span style="font-family:'宋体';">filetest1</span></p>
<p><span style="font-family:'宋体';">filetest10</span></p>
<p><span style="font-family:'宋体';">filetest2</span></p>
<p><span style="font-family:'宋体';">filetest3</span></p>
<p><span style="font-family:'宋体';">filetest4</span></p>
<p><span style="font-family:'宋体';">filetest5</span></p>
<p><span style="font-family:'宋体';">filetest6</span></p>
<p><span style="font-family:'宋体';">filetest7</span></p>
<p><span style="font-family:'宋体';">filetest8</span></p>
<p><span style="font-family:'宋体';">filetest9</span></p>
<p> </p>
<p><span style="font-family:'宋体';">（顺便说一下，--zookeeper参数可以把每个节点都写全，或者写一个也可以，只要能连接zookeeper就行。）</span></p>
<p> </p>
<p><span style="font-family:'宋体';">光这么检查还不够，还需要看一下物理存储中topic是否存在</span></p>
<p><span style="font-family:'宋体';">[hadoop1@hadoop3 ~]$ ll /home/hadoop1/softs/kafka-2.11-0.10.2.0/kafka-logs/ | grep filetest10</span></p>
<p><span style="font-family:'宋体';">drwxrwxr-x 2 hadoop1 hadoop1 4096 Apr 11 17:47 filetest10-0</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>1.3 物理空间检查</strong></span></p>
<p><span style="font-family:'宋体';">往kafka中生产数据是需要占空间的。</span></p>
<p><span style="font-family:'宋体';">保险起见，每台服务器都检查下空间是否足够。</span></p>
<p><span style="font-family:'宋体';">一般没到75%可以暂时不用管。</span></p>
<p><span style="font-family:'宋体';">[hadoop1@hadoop4 ~]$ df -h</span></p>
<p><span style="font-family:'宋体';">Filesystem Size Used Avail Use% Mounted on</span></p>
<p><span style="font-family:'宋体';">/dev/mapper/hdvg-rootlv</span></p>
<p><span style="font-family:'宋体';">14G 7.4G 5.4G 58% /</span></p>
<p><span style="font-family:'宋体';">tmpfs 1.9G 0 1.9G 0% /dev/shm</span></p>
<p><span style="font-family:'宋体';">/dev/sda1 477M 42M 410M 10% /boot</span></p>
<p><span style="font-family:'宋体';">/dev/mapper/hdvg-homelv</span></p>
<p><span style="font-family:'宋体';">112G 74G 34G 69% /home</span></p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';font-size:22pt;"><strong>第二步 flume配置部分</strong></span></p>
<p><span style="font-family:'宋体';">flume的工作相对hadoop spark这些属于比较简单的。</span></p>
<p><span style="font-family:'宋体';">根据需要配置好配置文件后，直接运行就可以采集数据了。</span></p>
<p><span style="font-family:'宋体';">所有的难点基本就在配置文件中。</span></p>
<p> </p>
<p><span style="font-family:'宋体';">在集群安装部分，基本已经贴出来我们这套实时计算flume安装配置的所有部分。</span></p>
<p><span style="font-family:'宋体';">这里主要讲解一下配置文件。（知道怎么用，最好还要多知道一些原理，这样出了问题我们有解决思路嘛）</span></p>
<p> </p>
<p><span style="font-family:'宋体';">配置文件主要在 $FLUME_HOME/conf/ 中</span></p>
<p> </p>
<p><span style="font-family:'宋体';">[hadoop1@hadoop2 ~]$ ll /home/hadoop1/softs/flume-1.6.0/conf/</span></p>
<p><span style="font-family:'宋体';">total 28</span></p>
<p><span style="font-family:'宋体';">-rw-r----- 1 hadoop1 hadoop1 822 Apr 5 10:55 18888.conf</span></p>
<p><span style="font-family:'宋体';">-rw-r--r-- 1 hadoop1 hadoop1 1661 Mar 29 11:32 flume-conf.properties.template</span></p>
<p><span style="font-family:'宋体';">-rw-r--r-- 1 hadoop1 hadoop1 1110 Mar 29 11:32 flume-env.ps1.template</span></p>
<p><span style="font-family:'宋体';">-rw-r--r-- 1 hadoop1 hadoop1 1257 Apr 5 10:58 flume-env.sh</span></p>
<p><span style="font-family:'宋体';">-rw-r--r-- 1 hadoop1 hadoop1 1214 Mar 29 11:32 flume-env.sh.template</span></p>
<p><span style="font-family:'宋体';">-rw-r----- 1 hadoop1 hadoop1 1501 Apr 11 16:07 log010.conf</span></p>
<p><span style="font-family:'宋体';">-rw-r--r-- 1 hadoop1 hadoop1 3107 Apr 5 11:05 log4j.properties</span></p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';">可以参考模板文件flume-conf.properties.template</span></p>
<p><span style="font-family:'宋体';">也可以将模板文件改名成一个新的配置文件，log010.conf那个就是我的配置文件</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>2.1 配置文件讲解</strong></span></p>
<p><span style="font-family:'宋体';">目前我们用的到的配置文件中，几个主要地方</span></p>
<p> </p>
<p><span style="font-family:'宋体';">名字部分</span></p>
<p><span style="font-family:'宋体';">source采集部分</span></p>
<p><span style="font-family:'宋体';">channel管道部分</span></p>
<p><span style="font-family:'宋体';">sink输出部分</span></p>
<p><span style="font-family:'宋体';">采集输出部分</span></p>
<p> </p>
<p> </p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 名字部分，将各个flume的组件进行命名</strong></span></p>
<p><span style="font-family:'宋体';"># Name the components on this agent</span></p>
<p><span style="font-family:'宋体';">a1.sources = r1</span></p>
<p><span style="font-family:'宋体';">a1.sinks = k1</span></p>
<p><span style="font-family:'宋体';">a1.channels = c1</span></p>
<p> </p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># source采集部分（标红了几个主要配置的地方，其他使用默认即可）</strong></span></p>
<p><span style="font-family:'宋体';"># Describe/configure the source</span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.type = com.github.ningg.flume.source.SpoolDirectoryTailFileSource
<span style="color:#1f497d;"><strong>#采集插件的名字</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.spoolDir = /home/hadoop1/testdata/ <span style="color:#1f497d;">
<strong>#被采集的目录</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.fileSuffix = .COMPLETED <span style="color:#1f497d;">
<strong>#采集完成的文件后缀名</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.deletePolicy = never <span style="color:#1f497d;">
<strong>#采集完成后是否删除</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.ignorePattern = ^$ </span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.targetPattern = .*(\\d){4}(\\d){2}(\\d){2}(\\d){2}.*
<span style="color:#1f497d;"><strong>#被采集文件的正则匹配 （这两条匹配必须结果一致）</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.targetFilename = yyyyMMddHH <span style="color:#1f497d;">
<strong>#被采集文件的时间匹配（这两条匹配必须结果一致）</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.trackerDir = .flumespooltail <span style="color:#1f497d;">
<strong>#异常退出，采集崩溃；采集索引存放地点</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.consumeOrder = oldest</span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.batchSize = 100</span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.inputCharset = UTF-8</span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.decodeErrorPolicy = REPLACE</span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.deserializer = LINE</span></p>
<p> </p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># sink输出部分的描述</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 默认情况下，参数是logger，会在运行的时候显示传输数据的标题，例如下面这样</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 12/06/19 15:32:19 INFO source.NetcatSource: Source starting</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 12/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 12/06/19 15:32:34 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D Hello world!. }</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 但是这样，在实际运行中会出现特别多条这样的INFO sink.LoggerSink，我测试是传输10万条数据，这样的信息刷了100万条（这里有点搞不明白）</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 配置成null，就不会显示传输数据的标题，只会显示传输多少条数据的事件信息，例如下面这样</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 17/04/12 11:36:18 INFO producer.SyncProducer: Connected to hadoop3:9092 for producing</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 17/04/12 11:36:20 INFO sink.NullSink: Null sink k1 successful processed 10000 events.</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 17/04/12 11:37:02 INFO sink.NullSink: Null sink k1 successful processed 20000 events.</strong></span></p>
<p><span style="font-family:'宋体';"># Describe the sink</span></p>
<p><span style="font-family:'宋体';">a1.sinks.k1.type = null</span></p>
<p> </p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># channel管道部分</strong></span></p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 官方推荐一般是缓存在内存里，或者是在hdfs中，同样我们也可以把数据直接缓存到kafka中</strong></span></p>
<p><span style="font-family:'宋体';"># Use a channel which buffers events in memory</span></p>
<p><span style="font-family:'宋体';">a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
<span style="color:#1f497d;"><strong># flume1.6中已经带了kafka缓存的插件，这里直接调用即可</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.channels.c1.capacity = 10000</span></p>
<p><span style="font-family:'宋体';">a1.channels.c1.transactionCapacity = 1000</span></p>
<p><span style="font-family:'宋体';">a1.channels.c1.brokerList = hadoop1:9092,hadoop2:9092,hadoop3:9092,hadoop4:9092,hadoop5:9092
<span style="color:#1f497d;"><strong># kafka的broker地址，逗号分隔</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.channels.c1.topic = filetest10 <span style="color:#1f497d;">
<strong># topic地址</strong></span></span></p>
<p><span style="font-family:'宋体';">a1.channels.c1.zookeeperConnect = hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181,hadoop5:2181
<span style="color:#1f497d;"><strong># zookeeper地址，逗号分割</strong></span></span></p>
<p> </p>
<p><span style="color:#1f497d;font-family:'宋体';"><strong># 采集输出的管道的衔接信息</strong></span></p>
<p><span style="font-family:'宋体';"># Bind the source and sink to the channel</span></p>
<p><span style="font-family:'宋体';">a1.sources.r1.channels = c1</span></p>
<p><span style="font-family:'宋体';">a1.sinks.k1.channel = c1</span></p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';">主要配置地方就是source采集和channel的缓存</span></p>
<p><span style="font-family:'宋体';">其中source采集使用的是二次开发的插件，顺便说一下，flume的source、channel、sink都支持二次开发</span></p>
<p> </p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';font-size:22pt;"><strong>第三步 flume运行调试</strong></span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>3.1 数据准备</strong></span></p>
<p><span style="font-family:'宋体';">目前我们是采集文件的信息，这里我写了个小程序，来模拟往文件里生成数据。</span></p>
<p><span style="font-family:'宋体';">这个地方要明白一点，数据在哪生成，flume就必须部署在哪，文件采集我现在还没明白是否可以实现远程采集，如果有套路的盆友可以联系我，重金求子。</span></p>
<p> </p>
<p><span style="font-family:'宋体';">scala -classpath /home/hadoop1/testjar/FileDataCreateNumber.jar com.kuangshuai.KafkaStreaming.FileDataCreate /home/hadoop1/testdata/010_2017041212 1000000</span></p>
<p><span style="font-family:'宋体';">后面两个参数分别是生成文件的名字，数据条数（支持文件追加）</span></p>
<p> </p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>3.2 flume运行</strong></span></p>
<p><span style="font-family:'宋体';">执行命令 注意最后 --name的配置 一定要与配置文件中的名字对应，而且这个配置不加也会出错，下面为不加 --name的错误</span></p>
<p><img src="https://img-blog.csdn.net/20170426104925244" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">正确的运行命令</span></p>
<p><span style="font-family:'宋体';">flume-ng agent --conf conf --conf-file /home/hadoop1/softs/flume-1.6.0/conf/log010.conf --name a1</span></p>
<p> </p>
<p><span style="font-family:'宋体';">运行成功出现的日志几个关注点</span></p>
<p><img src="https://img-blog.csdn.net/20170426104925712" alt=""><span style="font-family:'宋体';"></span></p>
<p><img src="https://img-blog.csdn.net/20170426104926806" alt=""><span style="font-family:'宋体';"></span></p>
<p><img src="https://img-blog.csdn.net/20170426104927822" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>3.3 生成数据</strong></span></p>
<p><img src="https://img-blog.csdn.net/20170426104928494" alt=""><span style="font-family:'宋体';"></span></p>
<p><img src="https://img-blog.csdn.net/20170426104928931" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">这证明flume已经采集到这1万条数据，并存到kafka中。</span></p>
<p><span style="font-family:'宋体';">至于怎么验证kafka确实收到了这1万条数据，我们在后面的spark streaming计算的时候讲解。</span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>3.4 文件更新</strong></span></p>
<p><img src="https://img-blog.csdn.net/20170426104929166" alt=""><span style="font-family:'宋体';"></span></p>
<p><span style="font-family:'宋体';">从12点更新到13点</span></p>
<p><span style="font-family:'宋体';">flume的操作日志，会去备份12点的日志，采集13点的</span></p>
<p><img src="https://img-blog.csdn.net/20170426104929587" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';font-size:12pt;"><strong>3.5 模拟flume宕机</strong></span></p>
<p><span style="font-family:'宋体';">创建个新的topic</span></p>
<p><span style="font-family:'宋体';">[hadoop1@hadoop3 ~]$ kafka-topics.sh --create --zookeeper hadoop1:2181 --replication-factor 3 --partitions 3 --topic filetest11</span></p>
<p><span style="font-family:'宋体';">Created topic "filetest11".</span></p>
<p> </p>
<p><span style="font-family:'宋体';">开启flume采集</span></p>
<p><img src="https://img-blog.csdn.net/20170426104930150" alt=""><span style="font-family:'宋体';"></span></p>
<p><img src="https://img-blog.csdn.net/20170426104930447" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">生成100万数据</span></p>
<p><img src="https://img-blog.csdn.net/20170426104930634" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">开始收数据了</span></p>
<p><img src="https://img-blog.csdn.net/20170426104930947" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">手动结束flume</span></p>
<p><img src="https://img-blog.csdn.net/20170426104932197" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">等待若干秒后，再开始flume</span></p>
<p><span style="font-family:'宋体';">接着上次的偏移度开始采集了</span></p>
<p><img src="https://img-blog.csdn.net/20170426104932697" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">采集结束</span></p>
<p><img src="https://img-blog.csdn.net/20170426104933619" alt=""><span style="font-family:'宋体';"></span></p>
<p> </p>
<p><span style="font-family:'宋体';">另一边去检查kafka</span></p>
<p><span style="font-family:'宋体';">暂时只知道在控制台执行一个消费者的命令，我感觉肯定还有更高级的套路</span></p>
<p><span style="font-family:'宋体';">kafka-console-consumer.sh --zookeeper hadoop1:2181 --from-beginning --topic filetest11</span></p>
<p><img src="https://img-blog.csdn.net/20170426104934025" alt=""><span style="font-family:'宋体';"></span></p>
<p><span style="font-family:'宋体';">最后我怀疑这1条估计就是那个在中断采集的时候，产生的垃圾数据</span></p>
<p><span style="font-family:'宋体';"><span style="color:#1f497d;"><strong>猜测</strong></span>（在采集到某一条数据的时候，数据中断了，这一条的一半已经发送过去了，重启之后偏移度接着崩溃后的索引开始继续采集，把那一条又重新采集了）</span></p>
<p> </p>
<p><span style="font-family:'宋体';">采集到这个地方就已经结束了，难点基本就是source二次开发的那个断点续传的插件了。</span></p>
<p><span style="font-family:'宋体';">目前只实现了一个点的采集，后续整体完成了，更新个压力测试的。</span></p>
            </div>
                </div>