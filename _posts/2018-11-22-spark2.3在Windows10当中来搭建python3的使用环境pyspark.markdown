---
layout:     post
title:      spark2.3在Windows10当中来搭建python3的使用环境pyspark
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/shiheyingzhe/article/details/80714301				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>在python中编写spark的程序，需要安装好Java、spark、hadoop、python这些环境才可以，spark、hadoop都是依赖Java的，spark的开发语言是Scala，支持用Java、Scala、python这些语言来编写spark程序，本文讲述python语言调用pyspark的安装配置过程，文中的Java版本是Java SE10.0.1，spark版本是2.3.1，python版本用的是Anaconda3-5.2.0-Windows-x86_64（python版本是3.6）。为了方便编写和调试，这里安装的IDE是Pycharm。<br>
 </p>

<ul><li><strong>首先需要安装Java</strong></li>
</ul><p>到官网下载并安装<span style="color:#333333;">Java Standard Edition即Java SE10.0.1版本，这里下载的是window64位版本JDK，<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk10-downloads-4416644.html" rel="nofollow">点击打开链接</a>，</span></p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/20180616222551521?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#333333;"><strong>设置环境变量</strong></span></p>

<p><span style="color:#333333;">安装过程中按照默认配置就好，安装好以后，配置Java的环境变量，右键我的电脑，依次点击属性-高级系统设置-环境变量</span></p>

<p><span style="color:#333333;">新建用户变量: JAVA_HOME；C:\Program Files\Java\jdk-10.0.1</span></p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/2018061622311620?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#333333;">在系统变量中找到Path，点击按钮新建，然后添加文字%JAVA_HOME%\bin，最后按回车Enter，一直点击确定，就保存了更改，这样就将bin文件夹中的Java程序放到了系统变量中。</span></p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/20180616223443474?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#333333;">检查Java环境变量配置是否成功</span><span style="color:#333333;">，使用快捷键Ctrl+R打开命令行，输入cmd打开命令行工具，输入java -version</span></p>

<p>当出现图中所示时Java安装配置完成</p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/20180616223803610?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p> </p>

<ul><li><strong>下载安装并配置Spark</strong></li>
</ul><p>从官方网站<a href="http://spark.apache.org/downloads.html" rel="nofollow">Download Apache Spark™</a>下载相应版本的spark，因为spark是基于hadoop的，需要下载对应版本的hadoop才行，这个页面有对hadoop的版本要求，点击<span style="color:#555555;">Download Spark:</span><span style="color:#555555;"> </span><span style="color:#555555;"><a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz" rel="nofollow">spark-2.3.1-bin-hadoop2.7.tgz</a>就可以下载压缩包了，对应的hadoop版本要在Hadoop2.7及其以后。</span></p>

<p> </p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180616175513608?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>这里解压到D:\spark-2.3.1-bin-hadoop2.7，为了后续操作简便，这里将解压以后的文件夹名称改为spark，这样解压的路径就是D:\spark</p>

<p> </p>

<p><strong>配置环境变量</strong></p>

<p><span style="color:#333333;">右键我的电脑，依次点击属性-高级系统设置-环境变量</span></p>

<p>新建用户变量 SPARK_HOME    D:\spark</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180616225553484?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>找到系统变量Path  点击按钮新建，然后添加文本%SPARK_HOME%\bin，按回车enter，继续新建一个，添加文本%SPARK_HOME%\sbin，按键回车，<span style="color:#333333;">一直点击确定，就保存了更改，这样就将bin、sbin文件夹中的程序放到了系统变量中</span></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180616225826213?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><strong>pyspark：</strong>到这里spark的配置完成了一部分，还有pyspark需要配置，pyspark等anaconda安装后在下文中讨论，pyspark的安装有几种方式，其中解压以后的spark文件夹中就有pyspark库，可以安装到python的库当中去；还可以不复制，pyspark可以通过pip单独安装，还有一种是单独下载pyspark的安装包，解压以后安装到python库当中去。</p>

<p> </p>

<ul><li><strong>安装并配置Hadoop</strong></li>
</ul><p>上面安装spark的时候有对hadoop的版本要求，这里要求的是2.7及以后的版本，进入官方网站<a href="http://hadoop.apache.org/releases.html" rel="nofollow">Apache Hadoop Releases</a>下载2.7.6 binary版本，如果后续<strong>下载页面的版本有变化</strong>，下载2.7以后的版本就可以的。其中source版本是该版本hadoop的源代码，下载以后解压到D:\hadoop-2.7.6，为了后续操作方便，解压以后修改文件夹名称为hadoop，这样文件夹就是D:\hadoop</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180616231942836?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><strong>配置环境变量：</strong></p>

<p><span style="color:#333333;">右键我的电脑，依次点击属性-高级系统设置-环境变量</span></p>

<p><span style="color:#333333;">新增用户变量 HADOOP_HOME D:\hadoop</span></p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/20180616232557971?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#333333;">然后找到系统变量Path  点击按钮新建，然后添加文本%HADOOP%\bin，按回车enter，继续新建一个，添加文本%HADOOP%\sbin，按键回车，<span style="color:#333333;">一直点击确定，就保存了更改，这样就将bin、sbin文件夹中的程序放到了系统变量中</span></span></p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/20180617083609407?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p><span style="color:#333333;">从网站中下载<a href="https://github.com/srccodes/hadoop-common-2.2.0-bin" rel="nofollow">点击打开链接</a>一个压缩包，然后解压出来，复制其中的winutils.exe和winutils.pdb到hadoop的安装文件夹中，复制目录为：D:\hadoop\bin，复制到这个目录中</span></p>

<p><span style="color:#333333;">当输入命令pyspark出现以下结果时表明spark安装配置完成了</span></p>

<p><span style="color:#333333;"><img alt="" class="has" src="https://img-blog.csdn.net/20180617183933845?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></span></p>

<p> </p>

<ul><li><strong>安装并配置anaconda</strong></li>
</ul><p>在<a href="https://www.anaconda.com/download/#windows" rel="nofollow">anaconda官方网站</a>中下载并安装对应版本的anaconda，安装路径这里的是C:\Anaconda3.5.2.0，其中需要注意的一点是，需要勾选第一个将anaconda加入环境变量的选项，这样就不需要我们自己将它的路径加入到环境变量中去了。</p>

<p>安装anaconda不是必须的，必须安装的是python，单独只安装python也是可以的，但是anaconda当中集成了很多需要用到的库，为了方便起见，这里安装的是anaconda。</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617085153854?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">      <img alt="" class="has" src="https://img-blog.csdn.net/20180617085224458?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617084843417?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p> </p>

<ul><li><strong>配置pyspark库</strong></li>
</ul><p>之前在安装spark的时候，提到过pyspark库的安装有几种方法，一种方法是直接将spark自带的pyspark库安装到python的库当中去；一种是使用命令pip install pyspark安装；还有一种是单独下载pyspark的安装包，解压以后安装到python库当中去。这几种方法，这里都会进行讲解。</p>

<p><strong>将spark自带的pyspark库安装到python：</strong></p>

<p>以管理员身份打开cmd，按win键，依次选中Windows 系统，右键命令提示符，点击更多，点击以管理员身份运行</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617102324597?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>进入spark安装目录的python文件夹，cd D:\spark\python<br>
C:\&gt;cd D:\spark\python<br>
C:\&gt;d:</p>

<p>D:\spark\python&gt;</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617101407453?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>输入命令 python setup.py install，等待安装完成，<br>
D:\spark\python&gt;python setup.py install</p>

<p>出现这个图时pyspark就安装好了</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617102621591?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><strong>pip install pyspark命令行方式安装：</strong></p>

<p>同上面打开cmd的方式相同，需要以管理员身份运行，按win键，依次选中Windows 系统，右键命令提示符，点击更多，点击以管理员身份运行</p>

<p>输入命令 pip install pyspark，等待安装完成，这里需要注意的是，pyspark本身的安装包占用磁盘空间很多，有几百M，这种方式安装需要在线下载pyspark，网速不错的话，是非常推荐的，这种方式最简单，只需要一行命令就行了。</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617103633974?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h2> </h2>

<p><strong>单独下载安装pyspark：</strong></p>

<p>进入<a href="https://pypi.org/project/pyspark/#files" rel="nofollow">pyspark的PyPI的网站</a>，点击左侧的Download files，下载pyspark的安装包，然后解压好，这里解压的路径是D:\pyspark-2.3.1</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617104036714?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p> </p>

<p>同上面打开cmd的方式相同，需要以管理员身份运行，按win键，依次选中Windows 系统，右键命令提示符，点击更多，点击以管理员身份运行</p>

<p>进入解压以后文件夹的目录<br><img alt="" class="has" src="https://img-blog.csdn.net/20180617104343272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>输入命令行 python setup.py install ，等待安装完成，pyspark就安装完成了</p>

<p>D:\pyspark-2.3.1&gt;python setup.py install</p>

<p>以上几种方式都可以安装pyspark，其中最方便的方式是使用命令行 pip install pyspark，下面将讲解pycharm的安装配置过程，并演示一个python编写spark的示例。</p>

<h2> </h2>

<ul><li><strong>安装并配置Pycharm</strong></li>
</ul><p>在<a href="http://www.jetbrains.com/pycharm/download/#section=windows" rel="nofollow">Pycharm的官方网站</a>中下载pycharm的community版本，这个版本是免费的，按照默认配置安装就可以</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617092343882?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>安装好以后打开pycharm，根据自己的喜好配置界面，到这一步时，可以安装一些插件，这里安装的是Markdown</p>

<p> </p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617092634321?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>进入打开界面时打开settings</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093003127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>选择好Project Interpreter，点击右侧的下拉链，然后点击show all</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093058524?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093205912?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>点击+号，添加项目解释器，选中其中的Conda Environment，然后点击Existing environment，点击右侧的选择按钮，进入目录C:\Anaconda3.5.2.0，选中其中的python.exe文件，然后一直点击OK</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093315594?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093445379?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093614220?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">       <img alt="" class="has" src="https://img-blog.csdn.net/2018061709364137?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/2018061709342165?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093805372?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>等待库载入完成以后，点击OK，就完成了Project Interpreter的配置，等待更新完成，或者让它在后台运行</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617093939143?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>这个是在最开始的时候配置Project Interpreter，进入界面以后，可以在File-Settings或者File-Default_Settings中设置</p>

<p>设置自己的字体，在File-Settings-Editor-Font当中设置</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617095713774?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<h3> </h3>

<ul><li>使用python来编写spark的WordCount程序实例流程</li>
</ul><p>新建一个项目，编辑好项目的存放目录以后，需要注意选择Existing interpreter，而不是New interpreter，上一步就是在配置Project interpreter，需要点击选择已经配置好的解释器。新建一个项目还依次点击按钮File-Setting-New Project</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617094218927?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617094309241?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>等待pycharm配置好，右下角会有提示的，等这个任务完成以后，就可以新建python文件了</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617095233695?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>点击Create就创建好了一个项目，鼠标放在左侧项目然后右键，依次点击New-Python File，创建一个python文件WordCount.py</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617094737141?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20180617095329631?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"> <img alt="" class="has" src="https://img-blog.csdn.net/20180617095403735?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWhleWluZ3poZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>

<p>进入WordCount.py文件写入如下代码，就是中文版WordCount，很经典的分布式程序，需要用到中文分词库jieba，去除停用词再进行计数</p>

<pre class="has">
<code class="language-python">from pyspark.context import SparkContext
import jieba
# from pyspark.sql.session import SparkSession
# from pyspark.ml import Pipeline
# from pyspark.ml.feature import StringIndexer, VectorIndexer
sc = SparkContext("local", "WordCount")   #初始化配置
data = sc.textFile(r"D:\WordCount.txt")   #读取是utf-8编码的文件
with open(r'd:\中文停用词库.txt','r',encoding='utf-8') as f:
    x=f.readlines()
stop=[i.replace('\n','') for i in x]
stop.extend(['，','的','我','他','','。',' ','\n','？','；','：','-','（','）','！','1909','1920','325','B612','II','III','IV','V','VI','—','‘','’','“','”','…','、'])#停用标点之类
data=data.flatMap(lambda line: jieba.cut(line,cut_all=False)).filter(lambda w: w not in stop).\
    map(lambda w:(w,1)).reduceByKey(lambda w0,w1:w0+w1).sortBy(lambda x:x[1],ascending=False)
print(data.take(100))</code></pre>

<p>输出结果为：</p>

<pre class="has">
<code class="language-python">C:\Anaconda3.5.2.0\python.exe D:/Project/WordCount.py
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/D:/spark/jars/hadoop-auth-2.7.3.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2100-01-01 10:00:00 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2100-01-01 10:00:00 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:&gt;                                                          (0 + 1) / 1]Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\AppData\Local\Temp\jieba.cache
Loading model cost 0.9 seconds.
Prefix dict has been built succesfully.
[('小王子', 419), ('说', 360), ('没有', 200), ('一个', 199), ('说道', 120), ('星星', 119), ('星球', 104), ('会', 98), ('回答', 91), ('地方', 80), ('国王', 78), ('画', 74), ('狐狸', 72), ('知道', 68), ('中', 67), ('花', 64), ('羊', 62), ('一只', 61), ('道', 57), ('非常', 56), ('看到', 53), ('命令', 52), ('有点', 50), ('这是', 48), ('不会', 48), ('朋友', 47), ('沙漠', 46), ('走', 46), ('地理学家', 46), ('.', 45), ('时', 43), ('想', 42), ('事', 42), ('感到', 42), ('行星', 42), ('问题', 41), ('可能', 40), ('真', 40), ('重要', 39), ('猴面包树', 38), ('&amp;#', 38), ('39', 38), (';', 38), ('时间', 37), ('象', 36), ('问', 36), ('笑', 36), ('地球', 36), ('里', 35), ('爱', 34), ('花儿', 34), ('这种', 32), ('喜欢', 32), ('做', 32), ('蛇', 32), ('驯服', 32), ('一点', 31), (':', 31), ('看着', 30), ('一种', 30), ('发现', 30), ('一定', 30), ('一颗', 30), ('\u3000', 30), ('你好', 30), ('点灯', 30), ('探察', 30), ('大人', 29), ('家', 29), ('东西', 28), ('看见', 28), ('好象', 28), ('这位', 28), ('提出', 28), ('问道', 28), ('应该', 28), ('吃', 28), ('一天', 28), ('请', 27), ('住', 27), ('起来', 27), ('现在', 27), ('奇怪', 26), ('从来', 26), ('已经', 26), ('明白', 26), ('朵花', 26), ('路灯', 26), ('寻找', 26), ('十分', 24), ('小家伙', 24), ('是从', 24), ('地说', 24), ('年', 24), ('自言自语', 24), ('虚荣', 24), ('生活', 22), ('严肃', 22), ('工作', 22), ('想要', 22)]

Process finished with exit code 0</code></pre>

<p>最终结果是：</p>

<pre class="has">
<code class="language-python">[('小王子', 419), ('说', 360), ('没有', 200), ('一个', 199), ('说道', 120), ('星星', 119), ('星球', 104), ('会', 98), ('回答', 91), ('地方', 80), ('国王', 78), ('画', 74), ('狐狸', 72), ('知道', 68), ('中', 67), ('花', 64), ('羊', 62), ('一只', 61), ('道', 57), ('非常', 56), ('看到', 53), ('命令', 52), ('有点', 50), ('这是', 48), ('不会', 48), ('朋友', 47), ('沙漠', 46), ('走', 46), ('地理学家', 46), ('.', 45), ('时', 43), ('想', 42), ('事', 42), ('感到', 42), ('行星', 42), ('问题', 41), ('可能', 40), ('真', 40), ('重要', 39), ('猴面包树', 38), ('&amp;#', 38), ('39', 38), (';', 38), ('时间', 37), ('象', 36), ('问', 36), ('笑', 36), ('地球', 36), ('里', 35), ('爱', 34), ('花儿', 34), ('这种', 32), ('喜欢', 32), ('做', 32), ('蛇', 32), ('驯服', 32), ('一点', 31), (':', 31), ('看着', 30), ('一种', 30), ('发现', 30), ('一定', 30), ('一颗', 30), ('\u3000', 30), ('你好', 30), ('点灯', 30), ('探察', 30), ('大人', 29), ('家', 29), ('东西', 28), ('看见', 28), ('好象', 28), ('这位', 28), ('提出', 28), ('问道', 28), ('应该', 28), ('吃', 28), ('一天', 28), ('请', 27), ('住', 27), ('起来', 27), ('现在', 27), ('奇怪', 26), ('从来', 26), ('已经', 26), ('明白', 26), ('朵花', 26), ('路灯', 26), ('寻找', 26), ('十分', 24), ('小家伙', 24), ('是从', 24), ('地说', 24), ('年', 24), ('自言自语', 24), ('虚荣', 24), ('生活', 22), ('严肃', 22), ('工作', 22), ('想要', 22)]
</code></pre>

<p>到这里pyspark的运行环境已经配置完成</p>

<p>参考内容：<br><a href="https://blog.csdn.net/hjxinkkl/article/details/57083549?winzoom=1" rel="nofollow">在windows安装部署spark(python版)</a><br><a href="https://www.jianshu.com/p/7b325155edab" rel="nofollow">Spark在Windows下的环境搭建</a><br><a href="http://www.cnblogs.com/zq-inlook/p/4386216.html" rel="nofollow">WIN7下运行hadoop程序报：Failed to locate the winutils binary in the hadoop binary path</a><br><a href="http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=sorted" rel="nofollow">pyspark package-PySpark sortBy</a><br><a href="http://spark.apache.org/docs/latest/api/python/index.html" rel="nofollow">http://spark.apache.org/docs/latest/api/python/index.html</a></p>            </div>
                </div>