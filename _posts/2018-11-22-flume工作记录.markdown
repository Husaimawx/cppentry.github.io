---
layout:     post
title:      flume工作记录
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>启动flume</h1>
<p>flume启动master 日志输出路径</p>
<p>starting master, logging to/home/demo/flume_output/log/flume/flume-demo-master-master.out</p>
<p> </p>
<p>如果将master，node，和collector分出来的话，需要按照功能区分的服务器上启动不同的命令，具体命令如下：</p>
<p>Flume master //启动master</p>
<p>Flume node //启动node Flumenode_nowatch //启动node 但不启动watchdog。</p>
<p>Flume node/node_nowatch –n collector //启动collector</p>
<p> </p>
<h2>1.在远程登录服务器的时候，开了3个ssh客户端</h2>
<p>分别启动master，node，最后一个负责执行flume shell(用于连接master[flume是基于zookeeper文件系统的分布式日志采集器，所以flume shell操作跟zookeeper非常相似])</p>
<p>2.执行启动 flume master 和 flume node报错。(已解决)[原因是未设置环境变量]</p>
<p>设置环境变量为：flume-env.sh</p>
<p>内容如下：</p>
<p>export FLUME_HOME=/opt/flume</p>
<p>export FLUME_CONF_DIR=/opt/flume/conf</p>
<p>exportFLUME_LOG_DIR=/home/demo/flume_output/log/flume</p>
<p>exportFLUME_PID_DIR=/home/demo/flume_output/run/flume</p>
<h1>Flume web页面查看端口号</h1>
<p><a href="http://localhost:35871/flumemaster.jsp%20%20/" rel="nofollow">http://localhost:35871/flumemaster.jsp  //</a> 访问主机页面</p>
<p><a name="OLE_LINK2"></a><a name="OLE_LINK1"></a><a href="http://localhost:35862/flumeagent.jsp" rel="nofollow">http://localhost:35862/flumeagent.jsp</a>   // 访问node管理界面</p>
<p><a href="http://localhost:35863/flumeagent.jsp" rel="nofollow">http://localhost:35863/flumeagent.jsp</a>   // 访问collector</p>
<p> </p>
<h1>监听、收集文件</h1>
<p align="left"><span style="color:#464646;">exec mapDataNode1(</span><span style="color:#464646;">物理节点名称</span><span style="color:#464646;">) fooNode1(</span><span style="color:#464646;">逻辑节点名称</span><span style="color:#464646;">) //</span><span style="color:#464646;">首先映射物理节点和逻辑节点，也就是先将要给搜集日志的物理节点设置一个逻辑节点名称</span></p>
<p align="left"><span style="color:#464646;"> </span></p>
<p align="left"><span style="color:#464646;">//</span><span style="color:#464646;">然后设置逻辑节点要搜集日志的文件夹或者文件</span></p>
<p align="left"><span style="color:#464646;">exec configfooNode1 fooflow'tail("/opt/flume/testFlumeFileInfo/testAgentLog")'  'agentE2EChain("node1")'</span></p>
<p align="left"><span style="color:#464646;"><br>
exec config fooNode1 fooflow'tailDir("/home/hadoop/system/logs/flume")''agentE2EChain("DataNode1","DataNode2","DataNode3")'</span></p>
<p align="left"><span style="color:#464646;"> </span></p>
<p align="left"><span style="color:#464646;">//</span><span style="color:#464646;">最后把搜集到的日志发送到目标文件夹或者</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">上</span></p>
<p align="left"><span style="color:#464646;">exec configDataNode1 fooflow collectorSource 'collectorSink("hdfs://NameNode:9000/flume/collected","foo")'</span></p>
<p align="left"><span style="color:#464646;"> </span></p>
<p align="left"><span style="color:#464646;">备注：一个物理节点可以映射多个逻辑节点，也就是说一个服务器上可以采集多块日志文件目录。然后到具体的跟踪日志文件的时候就能同时进行对多个</span><span style="color:#464646;">agent</span><span style="color:#464646;">节点进行日志采集。最后执行收集文件，就可以把搜集的日志文件汇总到最终的目标地点。</span></p>
<p align="left"><span style="color:#464646;"> </span></p>
<p align="left"><span style="color:#464646;">值得注意的是。在没有更改配置文件的时候，默认导入</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">的格式是非常乱的。里面有很多暂时用不上的参数，比如说要采集的原日志文件中只有</span><span style="color:#464646;">nihao</span><span style="color:#464646;">这几个字母，但是导入到</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">中后就会出现一大堆东西，比如说：</span><span style="color:#464646;">
 /u000,…body….timestamp…</span><span style="color:#464646;">等</span></p>
<p align="left"><span style="color:#464646;">如果只想在</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">中写入原有日志的内容，就把</span><span style="color:#464646;">collector format</span><span style="color:#464646;">的参数改成</span><span style="color:#464646;">”raw”</span><span style="color:#464646;">即可</span></p>
<p align="left"><span style="color:#464646;"> </span></p>
<p align="left"><span style="color:#464646;">如果要取消</span><span style="color:#464646;">getconfigs</span><span style="color:#464646;">里面的</span><span style="color:#464646;">agent
</span><span style="color:#464646;">到</span><span style="color:#464646;"> collector</span><span style="color:#464646;">上的映射。执行</span><span style="color:#464646;"> exec decommission node1</span></p>
<p align="left"><span style="color:#464646;">即可</span></p>
<h1><span style="color:#464646;">flume</span><span style="color:#464646;">日志动态跟踪</span></h1>
<p><span style="color:#464646;">当被监控的日志文件夹里新增一个文件的时候：</span></p>
<p><span style="color:#464646;">如果该文件内容为空，那么将不写入最终的目标路径</span><span style="color:#464646;">(hdfs)</span></p>
<p><span style="color:#464646;">如果创建了一个新文件，并且新文件里有内容的话，那么最终</span><span style="color:#464646;">collector</span><span style="color:#464646;">将收集被监控的文件夹里的新文件，然后将文件以数据流的形式做传递</span><span style="color:#464646;">agent-&gt;collector-&gt;hdfs</span></p>
<p><span style="color:#464646;"> </span></p>
<p><span style="color:#464646;"> </span></p>
<p><span style="color:#464646;">跟踪写入到</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">时的状态发现：</span></p>
<p><span style="color:#464646;">当被跟踪的文件夹里有新文件的时候，</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">没有创建文件，当新创建的文件夹写入内容后，</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">状态开始发生变化，首先观察到</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">会新增一个</span><span style="color:#464646;">tmp</span><span style="color:#464646;">临时文件，当文件写入完成后，</span><span style="color:#464646;">tmp</span><span style="color:#464646;">文件会被移除，那么</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">里将出现一个新增的</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">文件。</span></p>
<h1><span style="color:#464646;">Flume</span><span style="color:#464646;">格式化输入的日志文件</span></h1>
<p><span style="color:#464646;">参考过有关博客资料后，发现如果想要格式化输入的日志文件信息导入到</span><span style="color:#464646;">hdfs</span><span style="color:#464646;">中，需要修改配置文件，并且自己手写格式化</span><span style="color:#464646;">jar</span><span style="color:#464646;">包，并将</span><span style="color:#464646;">jar</span><span style="color:#464646;">包相应的部署到服务器上。</span></p>
<p><span style="color:#464646;"> </span></p>
<p><span style="color:#464646;">以下是参考的资料：</span></p>
<p><span style="color:#464646;"> </span></p>
<p align="left"><span style="color:#52706C;">提取</span><span style="color:#52706C;">tailSrcFile</span><span style="color:#52706C;">中字串，将其添加到</span><span style="color:#52706C;">Event</span><span style="color:#52706C;">的元数据中，在</span><span style="color:#52706C;">Hadoop</span><span style="color:#52706C;">中建立符合</span><span style="color:#52706C;">Hive</span><span style="color:#52706C;">分区规范的目录层次。</span></p>
<p align="left"><span style="color:#52706C;">以</span><span style="color:#52706C;">PaserTailFile</span><span style="color:#52706C;">为例：</span></p>
<p align="left"><span style="color:#52706C;">1</span><span style="color:#52706C;">、</span><span style="color:#52706C;">Hive</span><span style="color:#52706C;">创建表指定</span><span style="color:#52706C;">Hadoop</span><span style="color:#52706C;">中文件目录（分区上层目录），设置好分区信息</span></p>
<p align="left"><span style="color:#52706C;">eg:</span></p>
<p align="left"><span style="color:#52706C;">create table if not exists test(</span></p>
<p align="left"><span style="color:#52706C;">     id int,</span></p>
<p align="left"><span style="color:#52706C;">     name string</span></p>
<p align="left"><span style="color:#52706C;">)<br>
partitioned by(school string,class int ) <br>
row format delimited fields terminated by "\t" <br>
location "/data/hivedata/student/";</span></p>
<p align="left"><span style="color:#52706C;">2</span><span style="color:#52706C;">、</span><span style="color:#52706C;">collector</span><span style="color:#52706C;">配置</span><span style="color:#52706C;"><br>
exec config ******* 'collectorSource''{ParserTailFileDecorator("(.+)_(.+)_(.+)_(") =&gt;collectorSink("hdfs://*****:9000/******/%{gameName}/%{logType}/rid=%{serverId}/logdate=%{dateStr}","%{tailSrcFile}-")}'</span></p>
<p align="left"><span style="color:#52706C;">其中</span><span style="color:#52706C;">ParserTailFileDecorator</span><span style="color:#52706C;">第一个参数为正则表达式，其余参数为对应元组的名字</span></p>
<p align="left"><span style="color:#52706C;">3</span><span style="color:#52706C;">、</span><span style="color:#52706C;">agent</span><span style="color:#52706C;">配置</span></p>
<p align="left"><span style="color:#52706C;">exec config *******'tailDir("*******","(.+)_(.+)_(.+)_(.+)('{value("gameName","******") =&gt;agentE2EChain("******","*****","******")}'</span></p>
<p align="left"><span style="color:#52706C;">4</span><span style="color:#52706C;">、</span><span style="color:#52706C;">plugin</span></p>
<p align="left"><span style="color:#52706C;">build.xml:</span></p>
<p align="left"><span style="color:#52706C;">&lt;?xml version="1.0"?&gt;<br>
&lt;project name="flume-ParserTailFile" default="jar"&gt;<br>
    &lt;property name="javac.debug"value="on"/&gt;<br>
    &lt;property name="flume.base"value="/home/hadoop/flume"/&gt;</span></p>
<p align="left"><span style="color:#52706C;">    &lt;pathid="classpath"&gt;<br>
        &lt;pathelementlocation="${flume.base}/build/classes"/&gt;<br>
        &lt;filesetdir="${flume.base}/lib"&gt;<br>
            &lt;includename="**/guava*.jar"/&gt;<br>
            &lt;includename="**/log4j-*.jar"/&gt;<br>
            &lt;includename="**/slf4j-*.jar"/&gt;<br>
        &lt;/fileset&gt;</span></p>
<p align="left"><span style="color:#52706C;">        &lt;filesetdir="${flume.base}"&gt;<br>
            &lt;includename="flume-*.jar"/&gt;<br>
        &lt;/fileset&gt;<br>
        &lt;pathelementlocation="${flume.base}/lib/"/&gt;<br>
    &lt;/path&gt;</span></p>
<p align="left"><span style="color:#52706C;">    &lt;targetname="jar"&gt;<br>
        &lt;mkdir dir="build"/&gt;<br>
        &lt;mkdirdir="build/classes"/&gt;</span></p>
<p align="left"><span style="color:#52706C;">       &lt;javac srcdir="./src" destdir="build/classes"debug="${javac.debug}"&gt;<br>
           &lt;classpath refid="classpath"/&gt;<br>
        &lt;/javac&gt;</span></p>
<p align="left"><span style="color:#52706C;">        &lt;jarjarfile="ParserTailFile.jar" basedir="build/classes"/&gt;<br>
     &lt;/target&gt;</span></p>
<p align="left"><span style="color:#52706C;">     &lt;targetname="clean"&gt;<br>
         &lt;echo message="Cleaninggenerated files and stuff"/&gt;<br>
         &lt;deletedir="build"/&gt;<br>
         &lt;deletefile="ParserTailFile.jar"/&gt;<br>
     &lt;/target&gt;<br>
&lt;/project&gt;</span></p>
<p align="left"><span style="color:#52706C;">src\ParserTailFile\ParserTailFileDecorator.java</span><span style="color:#52706C;">：</span></p>
<p align="left"><span style="color:#52706C;">package ParserTailFile;<br>
import java.io.IOException;<br>
import java.util.ArrayList;<br>
import java.util.List;<br>
import java.util.regex.*;<br>
import java.util.HashMap; <br>
import java.util.Map;</span></p>
<p align="left"><span style="color:#52706C;">import com.cloudera.flume.conf.Context;<br>
import com.cloudera.flume.conf.SinkFactory.SinkDecoBuilder;<br>
import com.cloudera.flume.core.Event;<br>
import com.cloudera.flume.core.EventImpl;<br>
import com.cloudera.flume.core.EventSink;<br>
import com.cloudera.flume.core.EventSinkDecorator;<br>
import com.cloudera.util.Pair;<br>
import com.google.common.base.Preconditions;</span></p>
<p align="left"><span style="color:#52706C;">public class ParserTailFileDecorator&lt;S extendsEventSink&gt; extends EventSinkDecorator&lt;S&gt; {<br>
  public ParserTailFileDecorator(S s,String ...argvs) {<br>
    super(s);<br>
    _valArr = argvs.clone();<br>
  }</span></p>
<p align="left"><span style="color:#52706C;">  @Override<br>
  public void append(Event e) throws IOException, InterruptedException {<br>
     Map&lt;String,byte[]&gt; parserAttrs = newHashMap&lt;String,byte[]&gt;();  // </span><span style="color:#52706C;">创建一个新的</span><span style="color:#52706C;">Map,<br>
     parserAttrs.putAll(e.getAttrs()); // e.getAttrs</span><span style="color:#52706C;">为只读</span><span style="color:#52706C;"><br>
        <br>
     if (e.getAttrs().containsKey("tailSrcFile"))<br>
     {<br>
         String tailSrcFile = newString(parserAttrs.get("tailSrcFile"));<br>
         int len = _valArr.length;<br>
         Pattern pattern =Pattern.compile(_valArr[0]);<br>
         Matcher matcher =pattern.matcher(tailSrcFile);<br>
        <br>
         if (matcher.matches()&amp;&amp;  matcher.groupCount() &gt;= len - 1)<br>
         {<br>
             for (inti=1; i &lt; len; i++)<br>
             {<br>
                parserAttrs.put(_valArr[i],matcher.group(i).getBytes());<br>
            } <br>
         } <br>
     }</span></p>
<p align="left"><span style="color:#52706C;">     EventImpl e2 = newEventImpl(e.getBody(),<br>
        e.getTimestamp(),e.getPriority(),e.getNanos(), e.getHost(),<br>
        parserAttrs);<br>
     super.append(e2); <br>
  }</span></p>
<p align="left"><span style="color:#52706C;">  public static SinkDecoBuilder builder() {<br>
    return new SinkDecoBuilder() {<br>
      @Override<br>
      public EventSinkDecorator&lt;EventSink&gt;build(Context context,<br>
          String... argv) {<br>
        Preconditions.checkArgument(argv.length&gt;= 2,<br>
            "usage:ParserDecorator(\"regex\",[groupName1,[groupName2,...]])");</span></p>
<p align="left"><span style="color:#52706C;">        returnnew ParserTailFileDecorator&lt;EventSink&gt;(null,argv);<br>
      }<br>
    };<br>
  }<br>
  <br>
  public static List&lt;Pair&lt;String, SinkDecoBuilder&gt;&gt;getDecoratorBuilders() {<br>
    List&lt;Pair&lt;String, SinkDecoBuilder&gt;&gt; builders =<br>
      new ArrayList&lt;Pair&lt;String,SinkDecoBuilder&gt;&gt;();<br>
    builders.add(new Pair&lt;String,SinkDecoBuilder&gt;("ParserTailFileDecorator",<br>
        builder()));<br>
    return builders;<br>
  }<br>
  private String[] _valArr;<br>
}</span></p>
<p align="left"><span style="color:#52706C;">代码参考</span><span style="color:#52706C;">HelloWorld</span><span style="color:#52706C;">和</span><span style="color:#52706C;">flume</span><span style="color:#52706C;">源码自身相关插件修改</span></p>
<p align="left"><span style="color:#52706C;">编译后，将得到的</span><span style="color:#52706C;">ParserTailFile.jar
</span><span style="color:#52706C;">置于</span><span style="color:#52706C;">flume/lib</span><span style="color:#52706C;">下，并修改</span><span style="color:#52706C;">conf/flume-site.xml</span><span style="color:#52706C;">文件</span><span style="color:#52706C;"><br>
  &lt;property&gt;<br>
      &lt;name&gt;flume.plugin.classes&lt;/name&gt;<br>
     &lt;value&gt;ParserTailFile.ParserTailFileDecorator&lt;/value&gt;<br>
      &lt;description&gt;Comma separated list ofplugins&lt;/description&gt;<br>
  &lt;/property&gt;</span></p>
<p align="left"><span style="color:#52706C;">由于插件只在</span><span style="color:#52706C;">collector</span><span style="color:#52706C;">中使用，故</span><span style="color:#52706C;">ParserTailFile.jar
</span><span style="color:#52706C;">只需放到</span><span style="color:#52706C;">master</span><span style="color:#52706C;">和</span><span style="color:#52706C;">collector</span><span style="color:#52706C;">端，而</span><span style="color:#52706C;">agent</span><span style="color:#52706C;">端不用放</span></p>
<p><span style="color:#464646;"> </span></p>
            </div>
                </div>