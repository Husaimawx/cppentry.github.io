---
layout:     post
title:      《Spark快速大数据分析》笔记Ch1、2
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/Sugar_girl/article/details/77864340				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p><strong>1.Spark数据分析导论</strong> <br>
<strong>1.1 Spark是什么</strong> <br>
Spark是一个用来实现快速而通用的集群计算的平台。在速度方面，Spark扩展了广泛使用的MapReduce计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。总的来说，Spark适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。Spark所提供的接口非常丰富。除了提供基于Python、Java、Scala和SQL的简单易用的API以及内建的丰富的程序库以外，Spark还能和其他大数据工具密切配合使用。 <br>
<strong>1.2一个大一统的软件栈</strong> <br>
Spark项目包含多个紧密集成的组件。Spark的核心是一个对由很多计算任务组成的、运行在多个工作机器或者是一个计算集群上的应用进行调度、分发以及监控的计算引擎。 <br>
各组件间密切结合的设计原理有这样几个优点。首先，软件栈中所有的程序和高级组件都可以从下层的改进中获益。其次，运行整个软件栈的代价变小了。最后，我们能够构建出无缝整合不同处理模型的应用。 <br>
<img src="https://img-blog.csdn.net/20170906095817085?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VnYXJfZ2lybA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
<strong>1.2.1 Spark Core</strong> <br>
Spark Core实现了Spark的基本功能，包括任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集（RDD）的API定义。RDD表示分布在多个计算节点上可以并行操作的元素集合，是Spark主要的编程抽象。Spark Core 提供了创建和操作这些集合的多个API。 <br>
<strong>1.2.2 Spark SQL</strong> <br>
Spark SQL是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。 <br>
<strong>1.2.3 Spark Streaming</strong> <br>
Spark Streaming是Spark提供的对实时数据进行流式计算的组件。比如生产环境中的网页服务器日志，或是网络服务中用户提交的状态更新组成的消息队列，都是数据流。Spark Streaming提供了用来操作数据流的API，并且与Spark Core中的RDD API高度对应。 <br>
<strong>1.2.4 MLlib</strong> <br>
Spark中还包含一个提供常见的机器学习功能的程序库，叫作MLlib。MLlib提供了很多种机器学习算法，包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能。 <br>
<strong>1.2.5 GraphX</strong> <br>
GraphX是用来操作图（比如社交网络的朋友关系图）的程序库，可以进行并行的图计算。与Spark Streaming和Spark SQL类似，GraphX也扩展了Spark的RDD API，能用来创建一个顶点和边都包含任意属性的有向图。GraphX还支持针对图的各种操作，以及一些常用图算法。 <br>
<strong>1.2.6 集群管理器</strong> <br>
就底层而言，Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器上运行，包括Hadoop YARN、Apache Mecos，以及Spark自带的一个简易调度器，叫作独立调度器。 <br>
<strong>1.3 Spark的用户和用途</strong> <br>
Spark是一个用于集群计算的通用计算框架，因此被用于各种各样的应用程序。Spark用例分为两类：数据科学应用和数据处理应用。 <br>
<strong>1.3.1数据科学任务</strong> <br>
数据科学家就是主要负责分析数据并建模的人。 <br>
<strong>1.3.2数据处理应用</strong> <br>
<strong>1.4 Spark简史</strong> <br>
<strong>1.5 Spark的版本和发布</strong> <br>
<strong>1.6 Spark的存储层次</strong> <br>
Spark不仅可以将任何Hadoop分布式文件系统上的文件读取为分布式数据集，也可以支持其他支持Hadoop接口的系统。Hadoop并非Spark的必要条件，Spark支持任何实现了Hadoop接口的存储系统。 <br>
<strong>2. Spark下载与入门</strong> <br>
Spark可以通过Python、Java或Scala来使用。Spark本身是用Scala写的，运行在Java虚拟机（JVM）上。要在你的电脑或集群上运行Spark，安装Java 6以上的版本，如果希望使用Python接口，还需要一个Python解释器（2.6解释器），Spark尚不支持Python3。 <br>
<strong>2.1 下载Spark</strong> <br>
<strong>2.2 Spark中Python和Scala的shell</strong> <br>
Spark带有交互式的shell，可以作即时数据分析。Spark shell可用来与分布式存储在很多机器的内存或者硬盘上的数据进行交互，并且处理过程的分发由Spark自动控制完成。 <br>
在Spark中，我们通过对分布式数据集的操作来表达我们的计算意图，这些计算会自动地在集群上并行进行。这样的数据集被称为弹性分布式数据集，RDD是Spark对分布式数据和计算的基本抽象。 <br>
要退出任一shell，按Ctrl-D。 <br>
<strong>2.3 Spark核心概念简介</strong> <br>
从上层来看，每个Spark应用都由一个驱动器程序来发起集群上的各种并行操作。驱动器程序包含应用的main函数，并且定义了集群上的分布式数据集，还对这些分布式数据集应用了相关操作。在前面的例子里，实际的驱动器程序就是Spark shell本身，你只需要输入想要运行的操作就可以了。 <br>
驱动器程序通过一个 SparkContext 对象来访问 Spark。这个对象代表对计算集群的一个连接。 shell 启动时已经自动创建了一个 SparkContext 对象，是一个叫作 sc 的变量。 <br>
一旦有了SparkContext，你就可以用它来创建RDD。调用sc.textFile() 来创建一个代表文件中各行文本的RDD。我们可以在这些行上进行各种操作，比如count()。要执行这些操作，驱动器程序一般要管理多个执行器（ executor）节点。比如，如果我们在集群上运行 count() 操作，那么不同的节点会统计文件的不同部分的行数。由于我们刚才是在本地模式下运行 Spark shell，因此所有的工作会在单个节点上执行，但你可以将这个 shell连接到集群上来进行并行的数据分析。 <br>
<strong>2.4独立应用</strong> <br>
如何在独立程序中使用 Spark。除了交互式运行之外，Spark也可以在Java、Scala或Python的独立程序中被连接使用。这与在 shell 中使用的主要区别在于你需要自行初始化 SparkContext。接下来，使用的 API 就一样了。 <br>
连接Spark的过程在各语言中并不一样。在Java和Scala中，只需要给你的应用添加一个对于spark-core工件的Maven依赖。 <br>
<strong>2.4.1 初始化SparkContext</strong> <br>
一旦完成了应用与 Spark 的连接，接下来就需要在你的程序中导入 Spark 包并且创建SparkContext。你可以通过先创建一个 SparkConf 对象来配置你的应用，然后基于这个SparkConf 创建一个 SparkContext 对象。</p>



<pre class="prettyprint"><code class=" hljs avrasm">import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.SparkConf</span><span class="hljs-comment">;</span>
import org<span class="hljs-preprocessor">.apache</span><span class="hljs-preprocessor">.spark</span><span class="hljs-preprocessor">.api</span><span class="hljs-preprocessor">.java</span><span class="hljs-preprocessor">.JavaSparkContext</span><span class="hljs-comment">;</span>
SparkConf conf = new SparkConf()<span class="hljs-preprocessor">.setMaster</span>(<span class="hljs-string">"local"</span>)<span class="hljs-preprocessor">.setAppName</span>(<span class="hljs-string">"My App"</span>)<span class="hljs-comment">;</span>
JavaSparkContext sc = new JavaSparkContext(conf)<span class="hljs-comment">;</span></code></pre>

<p>创建 SparkContext 的最基本的方法，只需传递两个参数： <br>
• 集群 URL：告诉 Spark 如何连接到集群上。在这几个例子中我们使用的是 local，这个特殊值可以让 Spark 运行在单机单线程上而无需连接到集群。 <br>
• 应用名：在例子中我们使用的是 My App。当连接到一个集群时，这个值可以帮助你在集群管理器的用户界面中找到你的应用。 <br>
<strong>2.4.2构建独立应用</strong> <br>
用sbt以及Maven来构建并打包一个简单的单词数统计的例程。</p>



<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-comment">// 创建一个Java版本的Spark Context</span>
SparkConf conf <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> SparkConf()<span class="hljs-built_in">.</span>setAppName(<span class="hljs-string">"wordCount"</span>);
JavaSparkContext sc <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> JavaSparkContext(conf);
<span class="hljs-comment">// 读取我们的输入数据</span>
JavaRDD<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span><span class="hljs-subst">&gt;</span> input <span class="hljs-subst">=</span> sc<span class="hljs-built_in">.</span>textFile(inputFile);
<span class="hljs-comment">// 切分为单词</span>
JavaRDD<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span><span class="hljs-subst">&gt;</span> words <span class="hljs-subst">=</span> input<span class="hljs-built_in">.</span>flatMap(
    <span class="hljs-literal">new</span> FlatMapFunction<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span>, <span class="hljs-built_in">String</span><span class="hljs-subst">&gt;</span>() {
        <span class="hljs-keyword">public</span> Iterable<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span><span class="hljs-subst">&gt;</span> call(<span class="hljs-built_in">String</span> x) {
            <span class="hljs-keyword">return</span> Arrays<span class="hljs-built_in">.</span>asList(x<span class="hljs-built_in">.</span>split(<span class="hljs-string">" "</span>));
        }});
<span class="hljs-comment">// 转换为键值对并计数</span>
JavaPairRDD<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span>, <span class="hljs-built_in">Integer</span><span class="hljs-subst">&gt;</span> counts <span class="hljs-subst">=</span> words<span class="hljs-built_in">.</span>mapToPair(
    <span class="hljs-literal">new</span> PairFunction<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span>, <span class="hljs-built_in">String</span>, <span class="hljs-built_in">Integer</span><span class="hljs-subst">&gt;</span>(){
        <span class="hljs-keyword">public</span> Tuple2<span class="hljs-subst">&lt;</span><span class="hljs-built_in">String</span>, <span class="hljs-built_in">Integer</span><span class="hljs-subst">&gt;</span> call(<span class="hljs-built_in">String</span> x){
            <span class="hljs-keyword">return</span> <span class="hljs-literal">new</span> Tuple2(x, <span class="hljs-number">1</span>);
        }})<span class="hljs-built_in">.</span>reduceByKey(<span class="hljs-literal">new</span> Function2<span class="hljs-subst">&lt;</span><span class="hljs-built_in">Integer</span>, <span class="hljs-built_in">Integer</span>, <span class="hljs-built_in">Integer</span><span class="hljs-subst">&gt;</span>(){
            <span class="hljs-keyword">public</span> <span class="hljs-built_in">Integer</span> call(<span class="hljs-built_in">Integer</span> x, <span class="hljs-built_in">Integer</span> y){ <span class="hljs-keyword">return</span> x <span class="hljs-subst">+</span> y;}});
<span class="hljs-comment">// 将统计出来的单词总数存入一个文本文件，引发求值</span>
counts<span class="hljs-built_in">.</span>saveAsTextFile(outputFile);</code></pre>

<p>可以使用非常简单的sbt或 Maven构建文件来构建这些应用。由于Spark Core包已经在各个工作节点的classpath中了，所以我们把对Spark Core的依赖标记为 provided，这样当我们稍后使用assembly方式打包应用时，就不会把spark-core包也打包到assembly包中。 <br>
一旦敲定了构建方式，我们就可以轻松打包并且使用 bin/spark-submit 脚本执行我们的应用了。 spark-submit 脚本可以为我们配置 Spark 所要用到的一系列环境变量。我们可以使用 Scala或者 Java进行构建。</p>



<pre class="prettyprint"><code class=" hljs avrasm">mvn clean &amp;&amp; mvn compile &amp;&amp; mvn package
$SPARK_HOME/bin/spark-submit \
    --class <span class="hljs-keyword">com</span><span class="hljs-preprocessor">.oreilly</span><span class="hljs-preprocessor">.learningsparkexamples</span><span class="hljs-preprocessor">.mini</span><span class="hljs-preprocessor">.java</span><span class="hljs-preprocessor">.WordCount</span> \
    ./target/learning-spark-mini-example-<span class="hljs-number">0.0</span><span class="hljs-number">.1</span><span class="hljs-preprocessor">.jar</span> \
    ./README<span class="hljs-preprocessor">.md</span> ./wordcounts</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>