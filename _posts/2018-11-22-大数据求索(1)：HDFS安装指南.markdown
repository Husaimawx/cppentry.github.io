---
layout:     post
title:      大数据求索(1)：HDFS安装指南
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/wen_fei/article/details/82888073				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1><a id="1HDFS_1"></a>大数据求索(1)：HDFS安装指南</h1>
<h2><a id="_3"></a>背景</h2>
<p>HDFS是分布式文件系统，是Hadoop的基础，本章主要介绍如何安装HDFS。</p>
<h2><a id="_7"></a>环境参数</h2>
<ul>
<li>centos 6.5</li>
<li>hadoop2.6.0-cdh5.7.0</li>
<li>jdk8</li>
</ul>
<h2><a id="_13"></a>伪分布式安装</h2>
<h3><a id="JDK_15"></a>一、JDK安装</h3>
<ul>
<li>
<p>下载  wget  <a href="http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-linux-x64.tar.gz" rel="nofollow">http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-linux-x64.tar.gz</a></p>
</li>
<li>
<p>解压并输出到app目录下 tar -zxvf xxx.tar.gz  -C ~/app</p>
</li>
<li>
<p>添加到系统环境变量</p>
<pre class=" language-bash"><code class="prism  language-bash">vim ~/.bash_profile
<span class="token function">export</span> JAVA_HOME<span class="token operator">=</span>/home/wds/jdk8
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$JAVA_HOME</span><span class="token keyword">:</span><span class="token variable">$PATH</span>
</code></pre>
</li>
<li>
<p>使得环境变量生效 source ~/.bash_profile</p>
</li>
<li>
<p>验证java是否配置成功 java -v</p>
</li>
</ul>
<h3><a id="SSH_33"></a>二、安装SSH并配置免密登录</h3>
<p>因为实在centos下，所以安装命令如下：</p>
<pre class=" language-bash"><code class="prism  language-bash"><span class="token function">sudo</span> yum <span class="token function">install</span> <span class="token function">ssh</span>
<span class="token function">sudo</span> yum <span class="token function">install</span> <span class="token function">rsync</span>
ssh-keygen -t dsa -P <span class="token string">''</span> -f ~/.ssh/id_dsa
<span class="token function">cat</span> ~/.ssh/id_dsa.pub <span class="token operator">&gt;&gt;</span> ~/.ssh/authorized_keys
</code></pre>
<p>然后使用ssh localhost检查能否免密登录。如果成功会连接到自己，使用exit退出即可。</p>
<h3><a id="Hadoop_46"></a>三、下载Hadoop</h3>
<p>这里推荐使用cdh版本，具体这个版本和官方版本的区别，可以自己查阅资料，一般推荐使用cdh版本。</p>
<p>下载地址为：<a href="https://archive.cloudera.com/cdh5/cdh/5/%EF%BC%8C%E7%84%B6%E5%90%8E%E6%90%9C%E7%B4%A2hadoop2.6.0-cdh5.7.0%EF%BC%8C%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA" rel="nofollow">https://archive.cloudera.com/cdh5/cdh/5/，然后搜索hadoop2.6.0-cdh5.7.0，如下图所示</a></p>
<p><img src="http://otw7his7z.bkt.clouddn.com/hdfs-1.png" alt="hdfs-1"></p>
<p>注意这里应该下载hadoop-2.6.0-cdh5.7.0.tar.gz，而不是带src的。</p>
<p>下载完成后解压 tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app</p>
<h3><a id="hadoop_58"></a>四、hadoop配置文件的修改</h3>
<p>因为是单机版，即伪分布式，所以这里只需要配置三个文件</p>
<p>etc/hadoop/hadoop-env.sh(大概25行)</p>
<p>注释掉原来的JAVA_HOME，改为自己系统下的目录，如下图所示:</p>
<p><img src="http://otw7his7z.bkt.clouddn.com/hdfs-2.png" alt="hdfs-2"></p>
<p>etc/hadoop/core-site.xml:</p>
<pre class=" language-xml"><code class="prism  language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>hdfs://主机名:8020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/home/stu/app/tmp<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">&gt;</span></span>
</code></pre>
<blockquote>
<p>说明：</p>
<p>fs.defaultFS : 默认文件系统的名称 ，uri用来确定主机、端口等</p>
<p>dfs.name.dir : 用于确定HDFS文件系统的元信息保存在说明目录下，默认的是保存在linux系统的/tmp目录下，但是每次重启系统后数据会被清除，所以这里自己配置一个新的目录</p>
</blockquote>
<p>etc/hadoop/hdfs-site.xml:</p>
<pre class=" language-xml"><code class="prism  language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">&gt;</span></span>
</code></pre>
<blockquote>
<p>说明：</p>
<p>dfs.replication : 用于指定备份的数量，默认是3份，因为这里是伪分布式，所以改为1</p>
</blockquote>
<p>为了方便命令行操作，配置hadoop环境变量</p>
<pre class=" language-bash"><code class="prism  language-bash">vim ~/.bash_profile
<span class="token function">export</span> Hadoop_HOME<span class="token operator">=</span>/home/wds/app/hadoop
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$Hadoop_HOME</span><span class="token keyword">:</span><span class="token variable">$PATH</span>
</code></pre>
<p>这样在linux shell里直接输入hadoop可以看到有命令提示，如下图所示</p>
<p><img src="http://otw7his7z.bkt.clouddn.com/hdfs-3.png" alt="hdfs-3"></p>
<p>到这里，配置基本完成。</p>
<h3><a id="HDFS_120"></a>五、启动HDFS</h3>
<p>1） 第一次启动的时候，需要先进行格式化操作</p>
<p>​	bin/hdfs namenode -format</p>
<p>2）启动</p>
<p>​	sbin/start-dfs.sh</p>
<p>启动日志如下</p>
<p><img src="http://otw7his7z.bkt.clouddn.com/hdfs-4.png" alt="hdfs-4"></p>
<ol start="3">
<li>验证启动是否成功</li>
</ol>
<ul>
<li>
<p>JPS命令</p>
<p><img src="http://otw7his7z.bkt.clouddn.com/hdfs-5.png" alt="hdfs-5"></p>
</li>
<li>
<p>浏览器访问，在浏览器里输入https://ip:50070</p>
<p><img src="http://otw7his7z.bkt.clouddn.com/hdfs-5.png" alt="hdfs-6"></p>
<p>浏览器里有很多方便的可视化操作，非常方便</p>
</li>
</ul>
<h3><a id="_146"></a>六、遇到的问题解决</h3>
<p>1） 浏览器访问打不开</p>
<p>可能的原因：</p>
<ul>
<li>
<p>没有格式化文件系统</p>
</li>
<li>
<p>没有配置JAVA_HOME</p>
</li>
<li>
<p>防火墙没有关闭（或者配置规则）</p>
</li>
<li>
<p>查看本地端口，netstat -ant，查看端口50070是否已经开放</p>
</li>
<li>
<p>查看本地端口，如果50070前面ip是127.0.0.1(默认绑定)，会造成回环，改成0.0.0.0或者本机ip，具体做法是修改/etc/hosts文件，删除多余文件并添加一行 ip 主机名</p>
<p>或者在hdfs-site.xml文件修改</p>
<pre class=" language-xml"><code class="prism  language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.http.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>0.0.0.0:50070<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre>
</li>
</ul>
<p>2） datanode结点无法启动</p>
<p>如果在第一次格式化文件系统以后又格式化了一次，会造成这种情况，主要原因是datanode的clusterID 和 namenode的clusterID 不匹配。当我们执行文件系统格式化时，会在namenode数据文件夹（即配置文件中dfs.name.dir在本地系统的路径）中保存一个current/VERSION文件，记录namespaceID，标志了所有格式化的namenode版本。如果我们频繁的格式化namenode，那么datanode中保存（即dfs.data.dir在本地系统的路径）的current/VERSION文件只是你地第一次格式化时保存的namenode的ID，因此就会造成namenode和datanode之间的ID不一致。</p>
<h4><a id="_175"></a>解决方法一：（推荐）</h4>
<p>删除DataNode的所有资料及将集群中每个datanode节点的/dfs/data/current中的VERSION删除，然后重新执行hadoop namenode -format进行格式化，重启集群，错误消失。</p>
<h4><a id="_179"></a>解决方法二：</h4>
<p>将name/current下的VERSION中的clusterID复制到data/current下的VERSION中，覆盖掉原来的clusterID</p>
<p>然后重启即可。</p>
<h3><a id="_185"></a>七、参考</h3>
<ol>
<li><a href="https://blog.csdn.net/baidu_16757561/article/details/53698746" rel="nofollow">https://blog.csdn.net/baidu_16757561/article/details/53698746</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" rel="nofollow">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a></li>
</ol>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>