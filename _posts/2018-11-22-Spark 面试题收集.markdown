---
layout:     post
title:      Spark 面试题收集
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/WYpersist/article/details/80316283				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <blockquote>
<h3><strong>Spark 算子有哪些，项目用到哪些算子</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 广播变量</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark内存溢出</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark OOM问题解决办法</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 任务执行速度倾斜问题解决方案</strong></h3>
</blockquote>

<p><a href="https://blog.csdn.net/lsshlsw/article/details/52025949" rel="nofollow"><u><span style="color:#800080;">https://blog.csdn.net/lsshlsw/article/details/52025949</span></u></a></p>

<blockquote>
<h3><strong>Spark与Hadoop MapReduce的异同</strong></h3>
</blockquote>

<p>首先Spark是借鉴了mapreduce并在其基础上发展起来的，继承了其分布式计算的优点并改进了mapreduce</p>

<p>明显的缺陷，但是二者也有不少的差异具体如下：</p>

<p>1、spark把运算的中间数据存放在内存，迭代计算效率更高；mapreduce的中间结果需要落地，需要保存到磁盘，</p>

<p>这样必然会有磁盘io操做，影响性能。</p>

<p> </p>

<p>2、spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的</p>

<p>只读性质的数据集，这些集合是弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系</p>

<p>来实现重建；mapreduce的话容错可能只能重新计算了，成本较高。</p>

<p> </p>

<p>3、spark更加通用，spark提供了transformation和action这两大类的多个功能api，另外还有流式处理</p>

<p>sparkstreaming模块、图计算GraphX等等；mapreduce只提供了map和reduce两种操作，流计算以及其他</p>

<p>模块的支持比较缺乏。</p>

<p> </p>

<p>4、spark框架和生态更为复杂，首先有RDD、血缘lineage、执行时的有向无环图DAG、stage划分等等，</p>

<p>很多时候spark作业都需要根据不同业务场景的需要进行调优已达到性能要求；mapreduce框架及其生态</p>

<p>相对较为简单，对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行。</p>

<p> </p>

<p>最后总结：</p>

<p>spark生态更为丰富，功能更为强大、性能更佳，适用范围更广；mapreduce更简单、稳定性好、适合离线海量数据挖掘计算。</p>

<blockquote>
<h3><strong>Spatk streaming的数据来源</strong></h3>
</blockquote>

<p>基于offset消费数据</p>

<blockquote>
<h3><strong>Spark和Hadoop的异同</strong></h3>
</blockquote>

<p><a href="https://blog.csdn.net/WYpersist/article/details/79982749" rel="nofollow"><u><span style="color:#800080;">https://blog.csdn.net/WYpersist/article/details/79982749</span></u></a></p>

<blockquote>
<h3><strong>Spark与hadoop的shuffle有何异同</strong></h3>
</blockquote>

<p><a href="https://blog.csdn.net/WYpersist/article/details/79982627" rel="nofollow"><u><span style="color:#800080;">https://blog.csdn.net/WYpersist/article/details/79982627</span></u></a></p>

<p> </p>

<blockquote>
<h3><strong>Spark RDD操作map与flatmap的区别</strong></h3>
</blockquote>

<p><a href="https://blog.csdn.net/wypersist/article/details/80220211" rel="nofollow"><u><span style="color:#0000ff;">https://blog.csdn.net/wypersist/article/details/80220211</span></u></a></p>

<p> </p>

<blockquote>
<h3><strong>Spark  RDD的理解</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 有哪些算子</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 的stage的理解</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 宽依赖和窄依赖</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Sparkcore 的广播变量</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 累加变量</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark 二次排序</strong></h3>
</blockquote>

<blockquote>
<h3><strong>Spark <span style="color:#333333;">reduceByKey和groupByKey区别</span></strong></h3>
</blockquote>

<p>reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。</p>

<p> </p>

<p><span style="color:#333333;">groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。</span></p>

<p><span style="color:#333333;"> </span></p>

<p>groupByKey在方法shuffle之间不会合并原样进行shuffle。reduceByKey进行shuffle之前会先做合并,这样就减少了shuffle的io传送，所以效率高一点。</p>

<p><span style="color:#333333;"> </span></p>

<p><span style="color:#ff0000;">### groupByKey是这样实现的</span></p>

<p><span style="color:#ff0000;">combineByKeyWithClassTag[CompactBuffer[V]](createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span></p>

<p><span style="color:#ff0000;">### reduceByKey是这样实现的</span></p>

<p><span style="color:#ff0000;">combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</span></p>

<p><span style="color:#ff0000;">对比上面发现，</span><span style="color:#ff0000;">groupByKey设置了</span><span style="color:#ff0000;">mapSideCombine = false，在map端不进行合并，那就是在shuffle前不合并。而reduceByKey没有设置</span></p>

<p><span style="color:#ff0000;">通过查看</span><span style="color:#ff0000;">combineByKeyWithClassTag的，发现reduceByKey默认在map端进行合并，</span><span style="color:#ff0000;">那就是</span></p>

<p> </p>            </div>
                </div>