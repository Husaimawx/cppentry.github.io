---
layout:     post
title:      Flume.apache.org 官方文档学习笔记 part five
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>     kafka 源：</p>

<p>        Kafka 源是Apache Kafka 消耗者，读取来自kafka主题的信息。如果你有多个Kafka源在运行，你可以给他们配置一样的使用者群组，以便每个源都读取一组唯一的主题分区。</p>

<p>       </p>

<p>        <br><img alt="" class="has" height="613" src="https://img-blog.csdn.net/20181021134617525?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzA5MTgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<p>       要注意的是：Kafka源重写了两个kafka消费者属性，auto-commit可以被此源设置为“false",并且每一批都是这样坚定。 Kafka资源保证至少曾经的信息可以被检索。复本可以被展现出来当此源启动的时候。  Kafka源也为key.deserializer(org.apache.kafka.common.serialization.StingSerializer）和value.apache.org.common.serilization.ByteASRR提供默认值.修改这些参数是不推荐的。</p>

<p>     </p>

<p><img alt="" class="has" height="481" src="https://img-blog.csdn.net/20181021141444788?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzA5MTgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<p>        </p>

<p>安全的Kafka源：<br>
            安全认证还有数据加密支持flume和kafka之间的交流信道。 安全认证SASL/GSSAPI(Kerberos V5) 或者 SSL(甚至实际协议是TLS实现，但是参数名字是SSL) 可以用于Fafka版本0.9.0</p>

<p>            截至目前，数据加密仅有SSL/TLS提供</p>

<p>            将kafka.consumer.security.protocol设置为以下值意味着：<br>
            SASL_PLAINTEXT -Kerberos 或者没有数据加密的明文验证。<br>
            SASL_SSL -Kerberos 或者有数据加密的明文验证。<br>
            SSL -基于加密的TLS，带有可选验证。 </p>

<p>        警告：当SSL协议被启用时，会有一个性能退化，这个的大小会由CPU类型和JVM实现来决定。<br>
        参考：Kafka 安全概述还有追踪此问题的jira：KAFKA-2561</p>

<p><br>
        TLS和Kafka源：<br>
            请阅读Configuring Kafka Clients SSL中描述的步骤，去了解如何附加配置进行调优，接下来列举出来的一些：安全，提供者，密文组，可用协议，档案或者密钥库类型。</p>

<p>            服务器验证和数据加密的示例配置：<br>
            a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource<br>
        a1.sources.source1.kafka.bootstrap.servers = kafka-1:9093,kafka-2:9093,kafka-3:9093<br>
        a1.sources.source1.kafka.topics = mytopic<br>
        a1.sources.source1.kafka.consumer.group.id = flume-consumer<br>
        a1.sources.source1.kafka.consumer.security.protocol = SSL<br>
        a1.sources.source1.kafka.consumer.ssl.truststore.location=/path/to/truststore.jks<br>
        a1.sources.source1.kafka.consumer.ssl.truststore.password=&lt;password to access the truststore&gt;</p>

<p>        注意：在默认的情况下，属性ssl.endpoint.identification.alogorithm 没有被定义，所以主机验证没有实现。为了实现主机验证，设置下面的属性：</p>

<p>        a1.sources.source1.kafka.consumer.ssl.endpoint.identification.algorithm=HTTPS</p>

<p>        一旦成功了，客户端就会验证这个服务器域名是否完全合格，靠下面两个字段的其中一个：</p>

<p>        a1.sources.source1.kafka.consumer.ssl.keystore.location=/path/to/client.keystore.jks<br>
        a1.sources.source1.kafka.consumer.ssl.keystore.password=&lt;password to access the keystore&gt;</p>

<p>        如果密钥库和密钥使用了不同的密码，那么 ssl.key.password 属性应该提供 所需的附加密钥 给两个消费者密钥库。</p>

<p>        a1.sources.source1.kafka.consumer.ssl.key.password = &lt;password to access the key&gt;</p>

<p><br>
        Kerberos 和 Kafka源：<br>
        为了用一个被Kerberos保护的kafka集群来使用Kafka源，为消费者设置 上面提到过的‘consumer.security.protocol’ 属性。  与Kafka代理一起使用的Kerberos密钥表和主体在JAAS文件的”KafkaClient“部分中指定。 ”客户端“部分描述了Zookeeper连接。有关JAAS文件内容的信息，请参阅Kafka doc. 可以通过flume-env.sh中的Java——OPTS指定此JAAS的位置以及可选的系统范围的kerberos配置：</p>

<p><img alt="" class="has" height="749" src="https://img-blog.csdn.net/20181021145502290?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzA5MTgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>            </div>
                </div>