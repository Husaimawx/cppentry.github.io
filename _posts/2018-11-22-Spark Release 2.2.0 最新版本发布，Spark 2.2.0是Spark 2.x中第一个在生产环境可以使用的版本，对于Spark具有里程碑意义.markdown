---
layout:     post
title:      Spark Release 2.2.0 最新版本发布，Spark 2.2.0是Spark 2.x中第一个在生产环境可以使用的版本，对于Spark具有里程碑意义
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：王家林大咖2018年新书《SPARK大数据商业实战三部曲》清华大学出版，清华大学出版社官方旗舰店（天猫）https://qhdx.tmall.com/?spm=a220o.1000855.1997427721.d4918089.4b2a2e5dT6bUsM					https://blog.csdn.net/duan_zhihua/article/details/75269994				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1>第2章 Spark 2.X技术及原理 </h1><p><span style="color:#FF0000;">Apache</span><span style="color:#FF0000;">官方网站于</span><span style="color:#FF0000;">2017</span><span style="color:#FF0000;">年</span><span style="color:#FF0000;">7</span><span style="color:#FF0000;">月</span><span style="color:#FF0000;">11</span><span style="color:#FF0000;">日发布了</span><span style="color:#FF0000;">Spark Release 2.2.0</span><span style="color:#FF0000;">版本，</span><span style="color:#FF0000;"> Apache Spark 2.2.0</span><span style="color:#FF0000;">版本是</span><span style="color:#FF0000;">Spark 2.x</span><span style="color:#FF0000;">系列上的第三个版本。</span><span style="color:#FF0000;">Spark 2.2.0</span><span style="color:#FF0000;">是</span><span style="color:#FF0000;">Spark 2.x</span><span style="color:#FF0000;">中第一个在生产环境可以使用的版本，对于</span><span style="color:#FF0000;">Spark</span><span style="color:#FF0000;">具有里程碑意义。</span><span style="color:#FF0000;">Spark 2.2.0</span><span style="color:#FF0000;">版本中</span><span style="color:#FF0000;"> Structured Streaming </span><span style="color:#FF0000;">的实验性标记（</span><span style="color:#FF0000;">experimental tag</span><span style="color:#FF0000;">）已经被移除，</span><span style="color:#FF0000;">此版本更多地侧重于系统的可用性（</span><span style="color:#FF0000;">usability</span><span style="color:#FF0000;">）、稳定性（</span><span style="color:#FF0000;">stability</span><span style="color:#FF0000;">）以及代码的</span><span style="color:#FF0000;">polish</span><span style="color:#FF0000;">，解决了</span><span style="color:#FF0000;">1100</span><span style="color:#FF0000;">个</span><span style="color:#FF0000;">tickets</span><span style="color:#FF0000;">。此外，只要安装</span><span style="color:#FF0000;">pyspark</span><span style="color:#FF0000;">，在</span><span style="color:#FF0000;">Spark 2.2.0</span><span style="color:#FF0000;">版本中</span><span style="color:#FF0000;">PySpark</span><span style="color:#FF0000;">可用于</span><span style="color:#FF0000;">pypi</span><span style="color:#FF0000;">。</span><span style="color:#FF0000;">Spark 2.2.0</span><span style="color:#FF0000;">版本移除了对</span><span style="color:#FF0000;"> Java 7 </span><span style="color:#FF0000;">以及</span><span style="color:#FF0000;"> Hadoop 2.5</span><span style="color:#FF0000;">及其之前版本的支持，移除了对</span><span style="color:#FF0000;">Python 2.6</span><span style="color:#FF0000;">的支持。</span></p><p><span style="color:#FF0000;">Apache Spark2.2.0</span><span style="color:#FF0000;">版本的一些新功能：</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">Core and Spark SQL</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">Structured Streaming</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">MLlib</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SparkR</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">GraphX</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">Deprecations</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">Changes of behavior</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">Known Issues</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">Credits</span></p><p><span style="color:#FF0000;">如无特殊说明，本书所有内容都基于最新最稳定的</span><span style="color:#FF0000;">Spark 2.2.0</span><span style="color:#FF0000;">版本的源代码编写，为体现</span><span style="color:#FF0000;">Spark</span><span style="color:#FF0000;">源代码的演进过程，部分核心源代码在</span><span style="color:#FF0000;">Spark 1.6.x</span><span style="color:#FF0000;">、</span><span style="color:#FF0000;">Spark 2.1.x</span><span style="color:#FF0000;">源码的基础上，新增</span><span style="color:#FF0000;">Spark 2.2.0</span><span style="color:#FF0000;">版本的源代码，便于读者系统比对、研习</span><span style="color:#FF0000;">Spark</span><span style="color:#FF0000;">源码。</span></p><h2>2.1   Spark2.X综述   </h2><p>Spark 2.0中更新发布了新的流处理框架（Structured Streaming）；对于API的更新，Spark 2.0版本API的更新主要包括DataFrame、DataSet、SparkSession、累加器API、Aggregator API等API的变动。</p><h3 align="left">2.1.1 连续应用程序   </h3><p>自从Spark得到广泛使用以来，其流处理框架Spark Streaming也逐渐地吸引到了很多用户，得益于其易用的高级API和一次性语义，使其成为使用最广泛的流处理框架之一。但是我们不仅仅需要流处理来构建实时应用程序，很多时候我们的应用程序只有一部分需要用到流处理，对于这种应用程序，Databricks公司把它称为ContinuousApplication（实时响应数据的端到端的应用程序），也就是连续的应用程序。在Continuous Application中会有许多难点，比如数据交互的完整性、流数据与离线数据的结合使用、在线机器学习等。</p><p>Spark2.0最重磅的更新是新的流处理框架——Structured Streaming。它允许用户使用DataFrame/DataSetAPI编写与离线批处理几乎相同的代码，便可以作用到流数据和静态数据上，引擎会自动增量化流数据计算，同时保证了数据处理的一致性，并且提供了和存储系统的事务集成。</p><h3 align="left">2.1.2 新的API   </h3><p>在Spark2.0版本的API中共有如下几个API的变动：</p><p>1）统一了DataFrame和DataSet。现在DataFrame不再是一个独立的类，而是作为DataSet[Row]的别名定义在org.apache.spark.sql这个包对象中。</p><p>sql\package.scala源代码如下：</p><p>1.           packageobject sql {</p><p>2.          </p><p>3.           /**</p><p>4.            * Converts a logical plan into zero or moreSparkPlans.  This API is exposed forexperimenting</p><p>5.            * with the query planner and is not designedto be stable across spark releases. Developers</p><p>6.            * writing libraries should instead considerusing the stable APIs provided in</p><p>7.            * [[org.apache.spark.sql.sources]]</p><p>8.            */</p><p>9.           @DeveloperApi</p><p>10.        @InterfaceStability.Unstable</p><p>11.        type Strategy = SparkStrategy</p><p>12.       </p><p>13.        type DataFrame = Dataset[Row]</p><p>14.      }</p><p>2）加入了SparkSession用于替换DataFrame和Dataset API的SQLContext和HiveContext（这两个API仍然可以使用）。</p><p>3）为SparkSession为SparkSQL加入一个新的，精简的配置参数–RuntimeConfig，可以用来设置和获得跟SparkSQL有关的Spark或者Hadoop设置。</p><p>SparkSession.scala源代码：</p><p>1.           /**</p><p>2.            * Runtime configuration interface for Spark.</p><p>3.            *</p><p>4.            * This is the interface through which theuser can get and set all Spark and Hadoop</p><p>5.            * configurations that are relevant to SparkSQL. When getting the value of a config,</p><p>6.            * this defaults to the value set in theunderlying `SparkContext`, if any.</p><p>7.            *</p><p>8.            * @since 2.0.0</p><p>9.            */</p><p>10.        @transient lazy val conf: RuntimeConfig = newRuntimeConfig(sessionState.conf)</p><p align="center"> </p><p>4）更简单，更高性能的累加器API。</p><p>5）用于DataSet中类型化聚合的新的改进的AggregatorAPI。</p><h2>2.2   Spark2.X Core   </h2><p>         本节讲解Tungsten引擎的新特性；SparkSession使用方法；以及一个更加简单和更高性能的累加器API的使用。</p><h3 align="left">2.2.1 第二代Tungsten引擎 </h3><p>Spark备受瞩目的原因之一在于它的高性能，Spark开发者为了保持这个优势一直在不断的进行各种层次的优化，其中最令人兴奋的莫过于钨丝计划（ProjectTungsten），因为钨丝计划的提出给Spark带来了极大的性能提升，并且在一定程度上引导了Spark的发展方向。</p><p>Spark是使用Scala和Java语言开发的，不可避免的运行JVM之上，当然内存管理也是依赖于JVM的内存管理机制，而对于大数据量的基于内存的处理，JVM对象模型对内存的额外开销，以及频繁的GC和Full GC都是非常致命的问题。另外，随着网络带宽和磁盘IO的不断提升，内存和CPU又重新作为性能瓶颈受到关注，JVM对象的序列化、反序列化带来的性能损耗急需解决。Spark1.5版本加入的钨丝计划从3大方面着手解决这些问题：</p><p>1） 统一内存管理模型和二进制处理（BinaryProcessing）。统一内存管理模型来代替</p><p>之前基于JVM的静态内存管理，引入Page来管理堆内存和堆外内存（on-heap和off-heap），并且直接操作内存中的二进制数据而不是Java对象，很大程度上摆脱了JVM内存管理的限制。</p><p>2） 基于缓存感知的计算（Cache-aware Computation）。Spark内存读取操作也会带来</p><p>一部分性能损耗，钨丝计划便设计了缓存友好的算法和数据结构来提高缓存命中率，充分利用L1/L2/L3三级缓存，大幅提高了内存读取速度，进而缩短了内存中的整个计算过程的时间。</p><p>3） 代码生成（Code Generation）。在JVM中，所有代码的执行由解释器来一步步的</p><p>解释执行，CodeGeneration这一功能则在Spark运行时动态生成用于部分算子求值的bytecode，减少了对基础数据类型的封装，并且缓解了调用虚函数的额外开销。</p><p>Spark2.0升级了第二代Tungsten引擎。其中最重要的一点是把CodeGeneration作用于全阶段的SparkSQL和DataFrame之上，（即全阶段代码生成Whole Stage Code Generation），为常见的算子带来了十倍左右的性能提升！</p><h3 align="left">2.2.2 SparkSession   </h3><p>加入SparkSession，取代原来的SQLContext和HiveContext，为了兼容两者仍然保留。SparkSession使用方法如下：</p><p>1.           SparkSession.builder()</p><p>2.               .master("local")</p><p>3.               .appName("Word Count")</p><p>4.               .config("spark.some.config.option","some-value")</p><p>5.               .getOrCreate()</p><p> </p><p>首先获得SparkSession的Builder，然后使用Builder来为SparkSession设置参数，最后使用getOrCreate方法来检测当前线程是否有一个已经存在的Thread-local级别的SparkSession，如果有则返回它，没有则检测是否有全局级别的SparkSession，有则返回没有则创建新的SparkSession。</p><p>在程序中如果要使用SparkContext时可以调用sparkSession.sparkContext即可。在程序的最后我们需要调用sparkContext.stop方法，这个方法会调用sparkContext.stop来关闭sparkContext。</p><p>从Spark2.0开始，DataFrame和DataSet既可以容纳静态、有限的数据，也可以容纳无限的流数据，所以用户也可以使用SparkSession像创建静态数据集一样来创建流式数据集，并且可以使用相同的操作算子。这样整合了实时流处理和离线处理的框架，结合其它容错、扩展等特性形成了完整的Lambda架构。</p><p> </p><h3 align="left">2.2.3 Accumulator API  </h3><p>Spark2.0引入了一个更加简单和更高性能的累加器API，比如在1.X版本中可以这样使用累加器：</p><p>1.         //定义累加器，这里直接使用SparkContext内置的累加器，设置初始值为0，名字为"My Accumulator"</p><p>2.         val accum = sc.accumulator(0,"My Accumulator")</p><p>3.         //计算值</p><p>4.         sc.parallelize(Array(1, 2, 3,4)).foreach(x =&gt; accum += x)</p><p>5.         //获取累加器的值，（Executor上面只能对累加器进行累加操作，只有Driver才能读取累加器的值，Driver读取值的时候会把各个Executor上存储的本地累加器的值加起来），这里结果是10。</p><p>6.         accum.value </p><p>在2.X版本里使用SparkContext里内置的累加器：</p><p>1.          //与1.X不同的是需要指定累加器的类型，目前SparkContext有Long类型和Double类型的累加器可以直接使用（不需要指定初始值）。</p><p>2.         val accum =sc.longAccumulator("My Accumulator")</p><p>3.         sc.parallelize(Array(1, 2, 3,4)).foreach(x =&gt; accum.add(x))</p><p>4.         print(accum.value) </p><p> </p><p>只使用SparkContext里内置的累加器功能肯定不能满足略微复杂的业务类型，此时我们就可以自定义累加器。在1.X版本的做法是（下面是官网的例子）：</p><p>1.          //继承AccumulatorParam[Vector]，返回类型为Vector。</p><p>2.         object VectorAccumulatorParamextends AccumulatorParam[Vector] {</p><p>3.         //定义“零”值，这里把传入的初始值的size作为“零”值。</p><p>4.         def zero(initialValue: Vector):Vector = {</p><p>5.             Vector.zeros(initialValue.size)</p><p>6.           }</p><p>7.         //定义累加操作的计算方式</p><p>8.           def addInPlace(v1: Vector, v2: Vector):Vector = {</p><p>9.             v1 += v2</p><p>10.        }</p><p>11.      }</p><p> </p><p>上面的累加器元素和返回类型是相同的，在Scala中还有另外一种方式来自定义累加器，用户只需要继承Accumulable，就可以把元素和返回值定义为不同的类型，这样我们就可以完成添加操作（比如像Int类型的List里添加整数，此时元素为Int类型，而返回类型为List）。</p><p>在Spark2.X中，加入了一个新的抽象类--AccumulatorV2，继承这个类要实现几个方法：</p><p>add方法：指定元素相加操作。</p><p>copy方法：指定对自定义的累加器的拷贝操作。</p><p>isZero方法：返回该累加器的值是否为“零”。</p><p>merge方法：合并两个相同类型的累加器。</p><p>reset方法：重置累加器。</p><p>value方法：返回累加器当前的值。</p><p>     在重写这几个方法之后，只需实例化自定义累加器，并连同累加器名字一起传给sparkContext.register方法即可。</p><p>我们来简单实现一个把字符串合并为数组的累加器：</p><p>1.          //首先要继承AccumulatorV2，并指定输入为String类型，输出为ArrayBuffer[String]</p><p>2.         class MyAccumulator extendsAccumulatorV2[String, ArrayBuffer[String]] {</p><p>3.         //设置累加器的结果，类型为ArrayBuffer[String]</p><p>4.           private var result = ArrayBuffer[String]()</p><p>5.          </p><p>6.         //判断累加器当前值是否为“零值”，这里我们指定如果result的size为0则累加器的当前值是“零值”</p><p>7.           override def isZero: Boolean =this.result.size == 0</p><p>8.          </p><p>9.         //copy方法设置为新建本累加器，并把result赋给新的累加器</p><p>10.        override def copy(): AccumulatorV2[String,ArrayBuffer[String]] = {</p><p>11.          val newAccum = new MyAccumulator</p><p>12.          newAccum.result = this.result</p><p>13.          newAccum</p><p>14.        }</p><p>15.      //reset方法设置为把result设置为新的ArrayBuffer</p><p>16.      override def reset(): Unit =this.result == new ArrayBuffer[String]()</p><p>17.       </p><p>18.      //add方法是把传进来的字符串添加到result内</p><p>19.      override def add(v: String):Unit = this.result += v</p><p>20.       </p><p>21.      //merge方法：把两个累加器的result合并起来</p><p>22.      override def merge(other:AccumulatorV2[String, ArrayBuffer[String]]): Unit = {</p><p>23.            result.++=:(other.value)</p><p>24.          }</p><p>25.      //value方法返回result</p><p>26.        override def value: ArrayBuffer[String] =this.result</p><p>27.      }</p><p>28.      接着在main方法里使用累加器：</p><p>29.      val Myaccum = newMyAccumulator()</p><p>30.       </p><p>31.      //向SparkContext注册累加器</p><p>32.          sc.register(Myaccum)</p><p>33.       </p><p>34.      //把“a”,“b”“c”“d”添加进累加器的result这个数组并打印出来</p><p>35.          sc.parallelize(Array("a","b","c","d")).foreach(x=&gt; Myaccum.add(x))</p><p>36.          println(Myaccum.value)</p><p> </p><p>运行结果显示的ArrayBuffer里的值顺序是不固定的，取决于各个Executor的值到达Driver的顺序。</p><p> </p><h2>2.3   Spark2.X SQL    </h2><p>Spark 2.0通过对SQL2003的支持增强了SQL功能，Catalyst新引擎提升了Spark查询优化的速度；本节对DataFrame和Dataset API、时间窗口进行了讲解。    </p><p><span style="color:#FF0000;">Apache Spark2.2.0</span><span style="color:#FF0000;">版本中核心和</span><span style="color:#FF0000;">Spark SQL</span><span style="color:#FF0000;">的更新：</span></p><p><span style="color:#FF0000;">1、  </span><span style="color:#FF0000;">API</span><span style="color:#FF0000;">更新</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19107: Supportcreating hive table with DataFrameWriter and Catalog</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-13721: Add support forLATERAL VIEW OUTER explode()</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18885: Unify CREATE TABLEsyntax for data source and hive serde tables</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-16475: Added BroadcastHints BROADCAST, BROADCASTJOIN, and MAPJOIN, for SQL Queries</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18350: Support sessionlocal timezone</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19261: Support ALTERTABLE table_name ADD COLUMNS</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-20420: Add events tothe external catalog</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18127: Add hooks andextension points to Spark</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-20576: Support generichint function in Dataset/DataFrame</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-17203: Data sourceoptions should always be case insensitive</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19139: AES-basedauthentication mechanism for Spark</span></p><p><span style="color:#FF0000;">2、  </span><span style="color:#FF0000;">性能优化和系统稳定性</span></p><p><span style="color:#FF0000;">基于成本的优化：</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-17075 SPARK-17076SPARK-19020 SPARK-17077 SPARK-19350: Cardinality estimation for filter, join,aggregate, project and limit/sample operators</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-17080: Cost-based joinre-ordering</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-17626: TPC-DS performanceimprovements using star-schema heuristics</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-17949: Introduce a JVMobject based aggregate operator</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18186: Partialaggregation support of HiveUDAFFunction</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18362 SPARK-19918:File listing/IO improvements for CSV and JSON</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18775: Limit the maxnumber of records written per file</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18761: Uncancellable /unkillable tasks shouldn’t starve jobs of resources</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-15352: Topology awareblock replication</span></p><p><span style="color:#FF0000;">3、  </span><span style="color:#FF0000;">其他的一些变化</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18352: Support forparsing multi-line JSON files</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19610: Support forparsing multi-line CSV files</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-21079: Analyze TableCommand on partitioned tables</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18703: Drop StagingDirectories and Data Files after completion of Insertion/CTAS againstHive-serde Tables</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18209: More robustview canonicalization without full SQL expansion</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-13446: [SPARK-18112]Support reading data from Hive metastore 2.0/2.1</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-18191: Port RDD API touse commit protocol</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-8425:Add blacklistmechanism for task scheduling</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19464: Remove supportfor Hadoop 2.5 and earlier</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19493: Remove Java 7support</span></p><p><span style="color:#FF0000;"> </span></p><h3 align="left">2.3.1 Spark SQL  </h3><p>Spark 2.0通过对SQL2003的支持大幅增强了SQL功能，现在可以运行所有99个TPC-DS查询。这个版本中的SparkSQL主要有以下几点改进：</p><p>1)  引入了支持ANSISQL和HiveSQL的本地解析器。</p><p>2)  本地实现DDL命令。</p><p>3)  支持非相关标量子查询。</p><p>4)  在Where与having条件中，支持(not)in和(not)exists。</p><p>5)  即使Spark没有和Hive集成搭建，SparkSQL也支持它们一起搭建时的除了Hive连接、Hive UDF（UserDefinedFunction用户自定义函数）和脚本转换之外的大部分功能。</p><p>6)  Hive式的分桶方式的支持。</p><p> </p><p>另外Catalyst查询优化器对于常见的工作负载也有了很多提升，对比如nullability propagation之类的查询做了更好的优化。Catalyst查询优化器从最早的应用于SparkSQL到现在应用于DataSetAPI，对Spark程序的高效率运行起到了非常重要的作用，并且随着DataSetAPI的流行，以及优化器自身的不断演进，未来肯定会对Spark的所有框架带来更高的执行效率。</p><h3 align="left">2.3.2 DataFrame和Dataset API  </h3><p>在Spark 1.x版本中，DataFrame的API存在很多问题，比如说DataFrame不是类型安全的(nottype-safe)、不是面向对象的（notobject-oriented），为了克服这些问题，Spark在1.6版本引入了Dataset并在2.0版本的Scala和Java中将二者进行了统一（在Python和R中，由于缺少类型安全性，DataFrame仍是主要的编程接口），DataFrame成为了DataSet[Row]的别名，而且Spark2.0版本为DataSet的类型化聚合加入了一个新的聚合器，让基于DataSet的聚合更加高效。</p><p>在2.1版本中DataFrame和Dataset API晋升为稳定的API，也就是说可以在生产环境中使用它们，且后续会基于向后兼容的前提下不断强化。</p><p>DataSetAPI是High-LevelAPI，有更高的抽象级别，与RDDAPI这样的Low-LevelAPI相比更加易用，它对于提升用户的工作效率，以及提高程序的可读性而言意义非凡。由于WholeStageCodeGeneration的引入，SparkSQL和DataSetAPI中的常见算子的性能提升了2到10倍。加上Catalyst查询优化器和Tungsten的帮助，用户可以不用过多的关注对程序优化，也能获得很好的执行效率。</p><p>所以毋庸置疑地，这样一种简单高效的API将成为Spark未来主流的编程接口！</p><p> </p><h3 align="left">2.3.3 Timed window     </h3><p>对于经常用到复杂SQL的用户而言，窗口函数一直以来都是不可或缺的，在Spark2.X版本中通过对Hive中的窗口函数的本地化实现，来使用spark的内存管理机制，从而提升了窗口函数的性能。</p><h2>2.4 Spark 2.X Streaming  </h2><p>Spark2.0为我们带来了一个新的流处理框架Structured Streaming，这是一个基于Spark SQL和Catalyst优化器构建的高级流API。它允许用户使用与操作静态数据的DataFrame / Dataset API对流数据进行编程，利用Catalyst优化器自动地增量化查询计划。并且它不但支持流数据的不断写入，还支持其他的静态数据的插入。</p><p><span style="color:#FF0000;">Apache Spark2.2.0</span><span style="color:#FF0000;">版本中</span><span style="color:#FF0000;">Structured Streaming</span><span style="color:#FF0000;">的更新：</span></p><p><span style="color:#FF0000;">1、    </span><span style="color:#FF0000;">整体可用性：</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-20844: The StructuredStreaming APIs are now GA and is no longer labeled experimental</span></p><p><span style="color:#FF0000;">2、  </span><span style="color:#FF0000;">Kafka </span><span style="color:#FF0000;">提升</span><span style="color:#FF0000;">：</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19719: Support forreading and writing data in streaming or batch to/from Apache Kafka</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19968: Cached producerfor lower latency kafka to kafka streams.</span></p><p><span style="color:#FF0000;">3、    </span><span style="color:#FF0000;">API </span><span style="color:#FF0000;">更新：</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19067: Support forcomplex stateful processing and timeouts using [flat]MapGroupsWithState</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-19876: Support for onetime triggers</span></p><p><span style="color:#FF0000;">4、    </span><span style="color:#FF0000;">其它的一些变化：</span></p><p><span style="color:#FF0000;">·          </span><span style="color:#FF0000;">SPARK-20979: Rate source fortesting and benchmarks</span></p><p><span style="color:#FF0000;"> </span></p>            </div>
                </div>