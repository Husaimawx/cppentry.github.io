---
layout:     post
title:      Hadoop小知识点
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1><span style="font-size:18px;"><br></span></h1>
<h1><span style="font-size:18px;">hdfs命令行</span></h1>
<p><span style="font-size:18px;">上传  hadoop fs -put 文件名 hdfs://主机名:9000/...</span></p>
<p><span style="font-size:18px;">下载  hadoop fs -get hdfs://主机名:9000/... 文件名</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">/hadoop/share/hadoop/mapreduce 文件夹下有测试程序</span></p>
<p><span style="font-size:18px;">提交MapReduce任务命令</span></p>
<p><span style="font-size:18px;">#hadoop jar hadoop-mapreduce-examples-2.4.1.jar pi 5 5</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">hadoop fs -mkdir /wordcount</span></p>
<p><span style="font-size:18px;">hadoop fs -mkdir /wordcount/imput</span></p>
<p><span style="font-size:18px;">hadoop fs -put test.txt /wordcount/input</span></p>
<p><span style="font-size:18px;">hadoop jar hadoop-mapreduce-examples-2.4.1.jar wordcount /wordcount/input /wordcount/output</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">hadoop fs -ls /wordcount/output</span></p>
<p><span style="font-size:18px;">hadoop fs -cat /wordcount/output/part-r-00000</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">hadoop fs -appendToFile 源 目的    HDFS支持追加，不支持修改</span></p>
<p><br></p>
<p><br></p>
<p><span style="font-size:18px;"></span></p>
<h1><span style="font-size:18px;">元数据：</span></h1>
<p></p>
<p><span style="font-size:18px;">/test/a.log, 3, {blk_1, blk_2}, [{blk_1:[h0, h1, h2]},{blk_2:[h1, h2, h3]}]</span></p>
<p><span style="font-size:18px;">文件名  副本数  分块  块的副本所在的结点</span></p>
<br><p><span style="font-size:18px;">(1)客户端上传文件时，NN首先往edits log文件中记录元数据操作日志；</span></p>
<p><span style="font-size:18px;">(2)上传完成后，NN在内存中写入这次上传操作的元数据信息；</span></p>
<p><span style="font-size:18px;">(3)edits满或时间到，将edits刷入fsimage中，此步操作由secondnamenode来完成（checkpoint操作）。</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><br></p>
<h1><span style="font-size:18px;">checkpoint配置</span></h1>
<p><span style="font-size:18px;">hdfs-site.xml</span></p>
<p><span style="font-size:18px;">fs.checkpoint.period    default=3600s</span></p>
<p><span style="font-size:18px;">fs.checkpoint.size    default=64M</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;"><br></span></p>
<h1><span style="font-size:18px;">datanode配置</span></h1>
<p><span style="font-size:18px;">dfs.block.size    default=128M</span></p>
<p><span style="font-size:18px;">dfs.replication    default=3</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">Linux  du -sh *    查看目录中文件大小</span></p>
<p><span style="font-size:18px;">hadoop spark 等基于大数据的产品，在Linux下开发更方便</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;"><br></span></p>
<h1><span style="font-size:18px;">Eclipse导包</span></h1>
<p><span style="font-size:18px;">1、新建project；</span></p>
<p><span style="font-size:18px;">2、Project--&gt;Properties--&gt;Java Build Path--&gt;Libraries--&gt;AddLibrary--&gt;User Library--&gt;User Libraries--&gt;New--&gt;Add External JARs</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">hadoop/share/hadoop/...</span></p>
<p><span style="font-size:18px;">hdfs hadoop-hdfs-2.4.1.jar，依赖于</span></p>
<p><span style="font-size:18px;">hdfs/lib/*</span></p>
<p><span style="font-size:18px;">common hadoop-common-2.4.1.jar，依赖于</span></p>
<p><span style="font-size:18px;">common/lib/*</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">RUN--&gt;Run Configuration--&gt;Arguments--&gt;VM arguments</span></p>
<p><span style="font-size:18px;">设置 -DHADOOP_USER_NAME=...</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">Ctrl+T查看继承结构</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;"><br></span></p>
<h1><span style="font-size:18px;">MapReduce中自定义排序的实现：</span></h1>
<p><span style="font-size:18px;">依据key类中的compareTo方法排序map task产生的中间输出结果。</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">自定义Partitioner：</span></p>
<p><span style="font-size:18px;">自定义的Partitioner类将所需的key值放入同一个reduce进程中处理，每个reduce进程将产生一个输出文件。</span></p>
<p><span style="font-size:18px;">Reduce进程的个数可通过job.setNumReduceTasks(int i)设置，默认reduce进程为1个。</span></p>
<p><br></p>
<p><span style="font-size:18px;">ruduce number &gt;= partitioner    OK  多余将产生空文件结果</span></p>
<p><span style="font-size:18px;">reduce number &lt; partitioner    ERROR</span></p>
<p><span style="font-size:18px;">reduce number = 1    OK</span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;"><br></span></p>
<p><span style="font-size:18px;">block  文件块</span></p>
<p><span style="font-size:18px;">split  切片</span></p>
<p><span style="font-size:18px;">map task的数量由切片的数量决定</span></p>
<p><span style="font-size:18px;">io.sort.mb  缓冲大小，默认100M</span></p>
<p><span style="font-size:18px;">io.sort.spill.percent  阈值，默认0.8</span></p>
<p><span style="font-size:18px;">mapred.local.dir  写入磁盘位置</span></p>
            </div>
                </div>