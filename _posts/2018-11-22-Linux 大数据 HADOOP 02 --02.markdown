---
layout:     post
title:      Linux 大数据 HADOOP 02 --02
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><br>
++++++++++++++++ | HDFS 基 本 使 用 | +++++++++++++++++++++<br>
++++++++++++++++ | HDFS 基 本 使 用 | +++++++++++++++++++++</p>

<p>HDFS 使用   与shell 相似 只是没有shell参数多<br>
• HDFS 基本命令<br>
– ./bin/hadoop fs –ls /<br>
– 对应 shell 命令 ls /<br>
– ./bin/hadoop fs –mkdir /abc<br>
– 对应 shell 命令 mkdir /abc<br>
– ./bin/hadoop fs –rmdir /abc<br>
– 对应 shell 命令 rmdir /abcHDFS 使用</p>

<p>• HDFS 基本命令<br>
– ./bin/hadoop fs –touchz /urfile<br>
– 对应 shell 命令 touch /urfile<br>
– ./bin/hadoop fs –cat /urfile<br>
– 对应 shell 命令 cat /urfile<br>
– ./bin/hadoop fs –rm /urfile<br>
– 对应 shell 命令 rm /urfileHDFS 使用</p>

<p>• HDFS 基本命令<br>
– 上传文件<br>
– ./bin/hadoop fs –put localfile /remotefile<br>
– 下载文件<br>
– ./bin/hadoop fs –get /remotefile</p>

<p>----------------------------------------------------------------------------<br>
hadoop词频统计<br>
• 1、在集群文件系统里创建文件夹<br>
• 2、上传要分析的文件到目录中<br>
• 3、分析上传文件<br>
• 4、展示结果</p>

<p>Hadoop 验证<br>
• 创建文件夹<br>
– ./bin/hadoop fs -mkdir /input<br>
• 上传要分析的文件<br>
– ./bin/hadoop fs -put *.txt /input</p>

<p>Hadoop 验证<br>
• 提交分析作业<br>
– ./bin/hadoop<br>
jar ./share/hadoop/mapreduce/hadoop-<br>
mapreduce-examples-2.7.3.jar wordcount /input<br>
/output<br>
• 查看结果<br>
– ./bin/hadoop fs –cat output/*</p>

<p>示例演示：：——————&gt;&gt;<br>
[root@nn01 hadoop]# ./bin/hadoop fs -mkdir /oo</p>

<p>[root@nn01 hadoop]# ./bin/hadoop fs -put *.txt /oo/<br>
[root@nn01 hadoop]# <br>
[root@nn01 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount /oo /xx</p>

<p>查看结果！！<br>
[root@nn01 hadoop]# ./bin/hadoop fs -cat /xx/*</p>

<p>----------------------------------------------------------------------------------------</p>

<p>++++++++++++| 节点管理|++++++++++++++++<br>
++++++++++++| 节点管理|++++++++++++++++<br>
HDFS节点管理              yarn节点管理<br>
-------------------------------------------<br>
HDFS节点管理    <br>
Hadoop 节点管理<br>
【 1 】• HDFS 增加节点<br>
– 1、启劢一个新的系统,禁用 selinux、禁用 firewalld<br>
– 2、设置 ssh 免密码登录<br>
– 3、在所有节点增加新新节点的主机信息 /etc/hosts<br>
– 4、安装 java openjdk 运行环境<br>
– 5、拷贝namnode的 /usr/local/hadoop 到本机<br>
– 6、修改namenode的slaves文件增加该节点<br>
– 7、在该节点启劢Datanode<br>
./sbin/hadoop-daemon.sh start datanode</p>

<p><br>
实验环境：：——————&gt;&gt;<br>
新建一台虚拟机node4<br>
确保nn01能免密钥连接node4<br>
[root@nn01 ~]# cd /root/.ssh/<br>
[root@nn01 .ssh]# ls<br>
authorized_keys  id_rsa  id_rsa.pub  known_hosts<br>
[root@nn01 .ssh]# ssh-copy-id -i id_rsa.pub 192.168.1.14<br>
[root@nn01 .ssh]# vim /etc/hosts<br>
192.168.1.10 nn01<br>
192.168.1.14 newnode<br>
192.168.1.11 node1<br>
192.168.1.12 node2<br>
192.168.1.13 node3</p>

<p>[root@node4 ~]# yum -y install rsync  确保node4安装了rsync  方便同步数据</p>

<p>  //然后把nn01的hosts 同步给node1 2 3 newnode<br>
[root@nn01 ~]# for i in node{1..3} newnode;do rsync -av /etc/hosts ${i}:/etc/; done</p>

<p>[root@nn01 ~]# ssh newnode            ##连接新系统node4<br>
[root@node5 ~]# hostname  newnode        ##// 更改主机名<br>
[root@node5 ~]# logout<br>
Connection to newnode closed.<br>
[root@nn01 ~]# ssh newnode</p>

<p>##安装java环境<br>
[root@newnode ~]# yum install -y java-1.8.0-openjdk-devel</p>

<p>在nn01添加slaves <br>
[root@nn01 ~]# cd /usr/local/hadoop/<br>
[root@nn01 hadoop]# ls<br>
bin  etc  include  lib  libexec  LICENSE.txt  logs  NOTICE.txt  README.txt  sbin  share<br>
[root@nn01 hadoop]# vim etc/hadoop/slaves <br>
[root@nn01 hadoop]# cat etc/hadoop/slaves<br>
node1<br>
node2<br>
node3<br>
newnode</p>

<p>hadoop同步到其他主机<br>
[root@nn01 ~]# for i in node{1..3};do rsync -aSH --delete /usr/local/hadoop ${i}:/usr/local/ -e 'ssh' &amp; done<br>
[1] 3308<br>
[2] 3309<br>
[3] 3310<br>
或者用脚本：：<br>
[root@nn01 hadoop]# rrr node{1..3}          ##昨天写好的脚本！</p>

<p><br>
[root@nn01 hadoop]# ssh newnode<br>
Last login: Fri Aug  3 10:33:40 2018 from 192.168.1.254<br>
[root@newnode ~]# cd /usr/local/<br>
[root@newnode local]# ls<br>
bin  etc  games  include  lib  lib64  libexec  sbin  share  src<br>
[root@newnode local]# </p>

<p>把nn01上/usr/local/hadoop 拷贝道自己的目录下<br>
[root@newnode local]# rsync -a nn01:/usr/local/hadoop /usr/local/</p>

<p>[root@newnode local]# ls        ##查看一下<br>
[root@newnode local]# cd hadoop/<br>
[root@newnode hadoop]# ls<br>
bin  etc  include  lib  libexec  LICENSE.txt  logs  NOTICE.txt  README.txt  sbin  share<br>
[root@newnode hadoop]# rm -rf logs        //# 删除logs 因为这个logs是nn01上的 防止冲突  删除之后它会自动再创建<br>
[root@newnode hadoop]# <br>
[root@newnode hadoop]# ls<br>
[root@newnode hadoop]# mkdir /var/hadoop         （创建hadoop）  ##现在节点创建好了<br>
[root@newnode hadoop]# </p>

<p>[root@newnode hadoop]# jps       ##看一下自己的角色，没有角色<br>
1224 Jps</p>

<p>[root@newnode hadoop]# ./sbin/hadoop-daemon.sh      // 启动角色（这个命令的作用是 单独启动一个角色的命令）<br>
[root@newnode hadoop]# ./sbin/hadoop-daemon.sh start datanode   ##启用datanode角色！！<br>
[root@newnode hadoop]# jps    ##角色起来了，<br>
1264 DataNode<br>
1338 Jps</p>

<p>##角色起来了，那么与集群是否连接成功呢<br>
[root@newnode hadoop]# logout  回到nn01 上<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report     ## newnode加入进来了<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report<br>
Configured Capacity: 85853126656 (79.96 GB)<br>
Present Capacity: 79092408320 (73.66 GB)<br>
DFS Remaining: 79092379648 (73.66 GB)<br>
DFS Used: 28672 (28 KB)<br>
DFS Used%: 0.00%<br>
Under replicated blocks: 0<br>
Blocks with corrupt replicas: 0<br>
Missing blocks: 0<br>
Missing blocks (with replication factor 1): 0</p>

<p>-------------------------------------------------<br>
Live datanodes (4):</p>

<p>Name: 192.168.1.12:50010 (node2)<br>
Hostname: node2<br>
Decommission Status : Normal<br>
Configured Capacity: 21463281664 (19.99 GB)<br>
DFS Used: 8192 (8 KB)<br>
Non DFS Used: 1690165248 (1.57 GB)<br>
DFS Remaining: 19773108224 (18.42 GB)<br>
DFS Used%: 0.00%<br>
DFS Remaining%: 92.13%<br>
Configured Cache Capacity: 0 (0 B)<br>
Cache Used: 0 (0 B)<br>
Cache Remaining: 0 (0 B)<br>
Cache Used%: 100.00%<br>
Cache Remaining%: 0.00%<br>
Xceivers: 1<br>
Last contact: Wed Aug 08 16:05:29 CST 2018</p>

<p><br>
Name: 192.168.1.11:50010 (node1)<br>
Hostname: node1<br>
Decommission Status : Normal<br>
Configured Capacity: 21463281664 (19.99 GB)<br>
DFS Used: 8192 (8 KB)<br>
Non DFS Used: 1690370048 (1.57 GB)<br>
DFS Remaining: 19772903424 (18.41 GB)<br>
DFS Used%: 0.00%<br>
DFS Remaining%: 92.12%<br>
Configured Cache Capacity: 0 (0 B)<br>
Cache Used: 0 (0 B)<br>
Cache Remaining: 0 (0 B)<br>
Cache Used%: 100.00%<br>
Cache Remaining%: 0.00%<br>
Xceivers: 1<br>
Last contact: Wed Aug 08 16:05:29 CST 2018</p>

<p><br>
Name: 192.168.1.13:50010 (node3)<br>
Hostname: node3<br>
Decommission Status : Normal<br>
Configured Capacity: 21463281664 (19.99 GB)<br>
DFS Used: 8192 (8 KB)<br>
Non DFS Used: 1690206208 (1.57 GB)<br>
DFS Remaining: 19773067264 (18.42 GB)<br>
DFS Used%: 0.00%<br>
DFS Remaining%: 92.13%<br>
Configured Cache Capacity: 0 (0 B)<br>
Cache Used: 0 (0 B)<br>
Cache Remaining: 0 (0 B)<br>
Cache Used%: 100.00%<br>
Cache Remaining%: 0.00%<br>
Xceivers: 1<br>
Last contact: Wed Aug 08 16:05:28 CST 2018</p>

<p><br>
Name: 192.168.1.14:50010 (newnode)            ## 新加入的节点！！newnode<br>
Hostname: newnode<br>
Decommission Status : Normal<br>
Configured Capacity: 21463281664 (19.99 GB)<br>
DFS Used: 4096 (4 KB)<br>
Non DFS Used: 1689976832 (1.57 GB)<br>
DFS Remaining: 19773300736 (18.42 GB)<br>
DFS Used%: 0.00%<br>
DFS Remaining%: 92.13%<br>
Configured Cache Capacity: 0 (0 B)<br>
Cache Used: 0 (0 B)<br>
Cache Remaining: 0 (0 B)<br>
Cache Used%: 100.00%<br>
Cache Remaining%: 0.00%<br>
Xceivers: 1<br>
Last contact: Wed Aug 08 16:05:31 CST 2018</p>

<p>[root@nn01 hadoop]# </p>

<p>+++++++++++++++++---------------------------<br>
【 2 】 设置同步带宽,并同步数据<br>
Hadoop节点管理<br>
• HDFS 节点管理<br>
– 增加节点 续 ... ...<br>
– 8、设置同步带宽,并同步数据<br>
./bin/hdfs dfsadmin -setBalancerBandwidth 67108864<br>
./sbin/start-balancer.sh<br>
– 9、查看集群状态<br>
./bin/hdfs dfsadmin -report</p>

<p><br>
[root@nn01 hadoop]# pwd<br>
/usr/local/hadoop<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin   ##如果同步带宽的命令记不住，就输入此命令 回车</p>

<p>[root@nn01 hadoop]# ./bin/hdfs dfsadmin -setBalancerBandwidth 60000000</p>

<p>[root@nn01 hadoop]# ./sbin/start-balancer.sh    ##同步</p>

<p>+++++++++++++++++++++++++-----------------------------------</p>

<p>Hadoop节点管理<br>
【 3 】• HDFS 修复节点<br>
– 修复节点比较简单,不增加节点基本一致<br>
– 需要注意新节点的 ip 不 主机名 要不损坏节点一致  ！！！！！<br>
– 启劢服务<br>
./sbin/hadoop-daemon.sh start datanode<br>
– 数据恢复是自劢的<br>
– 我们上线以后会自劢恢复数据,如果数据量非常巨大,<br>
可能需要一定的时间<br>
（一定要等到数据同步完了，才能修复节点 上线等时间，下线等时间，导入也要等时间，）<br>
++++++++++++++++++++++++++++++++++++++++++++++++++</p>

<p>Hadoop 节点管理<br>
【4 】• HDFS 删除节点<br>
– 配置NameNode的 hdfs-site.xml<br>
– 增加 dfs.hosts.exclude 配置<br>
&lt;property&gt;<br>
&lt;name&gt;dfs.hosts.exclude&lt;/name&gt;<br>
&lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt;<br>
&lt;/property<br>
– 增加 exclude 配置文件,写入要删除的节点 主机名<br>
– 更新数据<br>
./bin/hdfs dfsadmin -refreshNodes</p>

<p>------------------<br>
Hadoop 节点管理<br>
• HDFS 删除节点状态<br>
– 查看状态 ./bin/hdfs dfsadmin -report<br>
– Normal 正常状态<br>
– Decommissioned in Program 数据正在迁移<br>
– Decommissioned 数据迁移完成<br>
– 注意:只有当状态变成 Decommissioned 才能 down<br>
机下线<br>
-------------</p>

<p>删除节点遵循两个原则<br>
1，无数据删除， 2，必须查看状态<br>
-----<br>
++==## 删除节点分两个过程：<br>
1，配置要删除的节点有哪些；增加 dfs.hosts.exclude 配置<br>
2，– 增加 exclude 配置文件,写入要删除的节点 主机名<br>
– 更新数据<br>
做完这两步之后等 等时间，等数据迁移完成！ 然后才能 删除！（如果数据量很大，有的甚至要等一周 两周...）<br>
--------------<br>
导个数据，方便看状态！<br>
[root@nn01 ~]# lftp 192.168.1.254<br>
lftp 192.168.1.254:/&gt; cd public/<br>
lftp 192.168.1.254:/public&gt; ls<br>
lftp 192.168.1.254:/public&gt; get CentOS-7-x86_64-DVD-1708.iso <br>
lftp 192.168.1.254:/public&gt; exit<br>
[root@nn01 ~]# ls<br>
[root@nn01 ~]# mv CentOS-7-x86_64-DVD-1708.iso centos.iso</p>

<p>[root@nn01 ~]# cd /usr/local/hadoop/<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report |grep -P -A 14 "newnode" #只看newnode 后面14行<br>
Name: 192.168.1.15:50010 (newnode)<br>
Hostname: newnode<br>
Decommission Status : Normal  //状态</p>

<p>[root@nn01 hadoop]# vim etc/hadoop/hdfs-site.xml   //删除节点<br>
&lt;configuration&gt;<br>
............<br>
    &lt;property&gt;                        ## 这组配置的 意思是删除这组主机，要删除的组机写在这个文件<br>
        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;<br>
        &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt;<br>
    &lt;/property&gt;<br>
&lt;/configuration&gt;</p>

<p>[root@nn01 hadoop]# touch /usr/local/hadoop/etc/hadoop/exclude  ##创建这个文件 <br>
[root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/exclude   ## 在这个文件里面添加要删除的节点名称<br>
newnode<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report  #查看节点状态  此时还是 Normal   正常状态</p>

<p>[root@nn01 hadoop]# ./bin/hdfs dfsadmin -refreshNodes  ##开始导出数据，数据会被迁移到其它主机上<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -refreshNodes<br>
Refresh nodes successful<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report |grep -P -A 14 "newnode"<br>
Name: 192.168.1.15:50010 (newnode)<br>
Hostname: newnode<br>
Decommission Status : Decommission in progress            ###数据正在迁移 看newnode状态，这个状态是它的数据在分配到其它机器上 这时候这台主机是不能 下线的  这时候就是等等！ </p>

<p>[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report<br>
.............<br>
Decommission Status : Decommissioned            #数据迁移完成<br>
删除节点（下线节点）<br>
[root@nn01 hadoop]# ssh newnode<br>
[root@newnode ~]# cd /usr/local/hadoop/<br>
[root@newnode hadoop]# ls<br>
[root@newnode hadoop]# ./sbin/hadoop-daemon.sh stop datanode    ##下线完成了！！！<br>
stopping datanode<br>
[root@newnode hadoop]#<br>
==============================================<br>
-<br>
+++++++++++++++| yarn节点管理 |+++++++++++++++<br>
+++++++++++++++| yarn节点管理 |+++++++++++++++</p>

<p>Hadoop节点管理<br>
• Yarn 的相关操作<br>
– 由于在 2.x hadoop 引入了 yarn 框架,对于计算节点<br>
的操作已经变得非常简单<br>
– 增加节点<br>
sbin/yarn-daemon.sh start nodemanager<br>
– 删除节点<br>
sbin/yarn-daemon.sh stop nodemanager<br>
– 查看节点 (Resourcemanager)<br>
./bin/yarn node -list<br>
------------------</p>

<p>Hadoop节点管理<br>
• yarn 的系统环境配置不 hdfs 的基础环境配置是相同<br>
的,这里就丌重复列出了<br>
• 由于 yarn 不包含数据,所以在增加删除修复节点的<br>
时候比较简单,hdfs 要注意数据安全</p>

<p>实验演示：：------&gt;&gt;&gt;<br>
增加节点<br>
[root@nn01 hadoop]# pwd<br>
/usr/local/hadoop<br>
[root@nn01 hadoop]# ./bin/yarn node -list<br>
18/08/03 15:04:01 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.1.10:8032<br>
Total Nodes:3<br>
         Node-Id         Node-State    Node-Http-Address    Number-of-Running-Containers<br>
     node3:34287            RUNNING           node3:8042                               0<br>
     node1:39152            RUNNING           node1:8042                               0<br>
     node2:42983            RUNNING           node2:8042                               0<br>
[root@nn01 hadoop]# </p>

<p>[root@nn01 hadoop]# ssh newnode<br>
[root@newnode ~]# cd /usr/local//hadoop/<br>
[root@newnode hadoop]# <br>
[root@newnode hadoop]# ./sbin/yarn-daemon.sh start nodemanager #启动节点<br>
starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-newnode.out<br>
[root@newnode hadoop]# </p>

<p>[root@nn01 hadoop]# ./bin/yarn node -list        //## 看一下新节点出来了！<br>
18/08/03 15:06:37 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.1.10:8032<br>
Total Nodes:4<br>
         Node-Id         Node-State    Node-Http-Address    Number-of-Running-Containers<br>
   newnode:42328            RUNNING         newnode:8042                               0<br>
     node3:34287            RUNNING           node3:8042                               0<br>
     node1:39152            RUNNING           node1:8042                               0<br>
     node2:42983            RUNNING           node2:8042                               0<br>
[root@nn01 hadoop]# </p>

<p>##删除节点 <br>
注意：确定在这个节点没有计算任务的时候（时间）<br>
[root@newnode hadoop]# ./sbin/yarn-daemon.sh stop nodemanager<br>
[root@nn01 hadoop]# ./bin/yarn node -list<br>
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>
————————————————————————————————————————————<br>
++++++++++| N F S 网 关 |+++++++++++++++++++=<br>
++++++++++| N F S 网 关 |+++++++++++++++++++=</p>

<p>NFS 网关<br>
• NFS 网关用途<br>
– 用户可以通过操作系统兼容的本地NFSv3客户端来阅<br>
览HDFS文件系统<br>
– 用户可以从HDFS文件系统下载文档到本地文件系统<br>
– 用户可以通过挂载点直接流化数据。支持文件附加,<br>
但是丌支持随机写<br>
– NFS 网关支持NFSv3和允许HDFS 作为客户端文件系<br>
统的一部分被挂载</p>

<p>NFS网关<br>
• 配置代理用户<br>
– 在 namenode 和 nfsgw 上添加代理用户</p>

<p>– 代理用户的 uid gid 用户名 必须完全相同<br>
– 如果因特殊原因客户端的用户和NFS网关的用户 uid<br>
不能保持一致需要我们配置 nfs.map 的静态映射关系<br>
– nfs.map<br>
uid 10 100 # Map the remote UID 10 the local UID 100<br>
gid 11 101 # Map the remote GID 11 to the local GID 101</p>

<p>步骤++步骤++步骤++》》：：：如下！！！<br>
1# 给客户端nfsgw 配置hosts<br>
[root@nfsgw ~]# vim /etc/hosts<br>
192.168.1.10 nn01<br>
192.168.1.14 newnode<br>
192.168.1.11 node1<br>
192.168.1.12 node2<br>
192.168.1.13 node3<br>
192.168.1.15 nfsgw</p>

<p>2# uid gid 用户名 必须完全相同<br>
给客户端nfsgw  uid gid 用户名 必须完全相同<br>
[root@nfsgw ~]# groupadd -g 200 nsd1803<br>
[root@nfsgw ~]# useradd -u 200 -g 200 nsd1803<br>
[root@nfsgw ~]# id nsd1803<br>
uid=200(nsd1803) gid=200(nsd1803) 组=200(nsd1803)<br>
[root@nfsgw ~]#  </p>

<p> #在nn01上也配置<br>
[root@nn01 hadoop]# groupadd -g 200 nsd1803<br>
[root@nn01 hadoop]# useradd -u 200 -g 200 nsd1803<br>
[root@nn01 hadoop]# id nsd1803<br>
uid=200(nsd1803) gid=200(nsd1803) 组=200(nsd1803)</p>

<p>3 # • 核心配置 core-site.xml<br>
– hadoop.proxyuser.{代理用户}.groups<br>
– hadoop.proxyuser.{代理用户}.hosts<br>
– 这里的 {代理用户} 是你机器上真实运行 nfs3 的用户<br>
– 在非安全模式,运行nfs网关的用户为代理用户<br>
– groups 为挂载点用户所使用的组<br>
– hosts 为挂载点主机地址<br>
 • 核心配置 core-site.xml<br>
... ...<br>
&lt;property&gt;<br>
&lt;name&gt;hadoop.proxyuser.nsd1802.groups&lt;/name&gt;<br>
&lt;value&gt;*&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hadoop.proxyuser.nsd1802.hosts&lt;/name&gt;<br>
&lt;value&gt;*&lt;/value&gt;<br>
&lt;/property&gt;<br>
... ...</p>

<p>3##  注意：：在配置core-site.xml之前 先停止集群！</p>

<p>[root@nn01 ~]# cd /usr/local/hadoop/        ## 停止集群！<br>
[root@nn01 hadoop]# ./sbin/stop-all.sh</p>

<p>4 #  配置 核心配置 core-site.xml<br>
[root@nn01 hadoop]# ls<br>
bin         etc      lib      LICENSE.txt  NOTICE.txt  README.txt  share<br>
centos.iso  include  libexec  logs         oo          sbin        xx<br>
[root@nn01 hadoop]# cd etc/hadoop/<br>
[root@nn01 hadoop]# vim core-site.xml <br>
... ...<br>
&lt;property&gt;<br>
&lt;name&gt;hadoop.proxyuser.nsd1802.groups&lt;/name&gt;<br>
&lt;value&gt;*&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hadoop.proxyuser.nsd1802.hosts&lt;/name&gt;<br>
&lt;value&gt;*&lt;/value&gt;<br>
&lt;/property&gt;<br>
... ...</p>

<p>[root@nn01 hadoop]# cat slaves   #查看节点是否正确，把之前配置的newnode 删除了 防止影响接下来的实验<br>
node1<br>
node2<br>
node3<br>
[root@nn01 hadoop]# &gt;exclude   清除exclude</p>

<p>5## 同步到其它节点！<br>
[root@nn01 hadoop]# rrr node{1..3}    ## 同步到其它节点上<br>
[root@nn01 hadoop]# </p>

<p>6## 启动集群！<br>
[root@nn01 hadoop]# pwd<br>
/usr/local/hadoop/etc/hadoop<br>
[root@nn01 hadoop]# cd ../../<br>
[root@nn01 hadoop]# pwd<br>
/usr/local/hadoop<br>
[root@nn01 hadoop]# ./sbin/start-dfs.sh<br>
7 ## 查看一下状态！！<br>
[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report  #查看节点都正常！<br>
此时集群部分准备工作做完了！</p>

<p>++++++++++++++| NFSGW配置 | ++++++++++++++<br>
++++++++++++++| NFSGW配置 | ++++++++++++++<br>
实验接上面：：：————&gt;&gt;</p>

<p>NFS 网关<br>
• 配置步骤<br>
– 1、启劢一个新的系统,禁用 selinux、禁用 firewalld<br>
– 2、卸载 rpcbind 、nfs-utils<br>
– 3、配置 /etc/hosts,添加所有 namenode 和<br>
datanode 的主机名不 ip 对应关系<br>
– 4、安装 java openjdk 的运行环境<br>
– 5、同步 namenode 的 /usr/local/hadoop 到本机<br>
– 6、配置 hdfs-site.xml<br>
– 7、启动服务<br>
---------------------</p>

<p>1 #、启劢一个新的系统,禁用 selinux、禁用 firewalld<br>
上面实验已经做过<br>
2# 卸载 rpcbind 、nfs-utils  先查看是否安装<br>
[root@nfsgw ~]# rpm -qa |grep rpcbind<br>
[root@nfsgw ~]# rpm -qa |grep nfs-utils</p>

<p>3# 已经做过</p>

<p>4# [root@nfsgw ~]# yum -y install java-1.8.0-openjdk-devel</p>

<p>5# 同步 nn01 的 /usr/local/hadoop 到NFSGW的相同目录下<br>
[root@nfsgw local]# rsync -aSH --delete nn01:/usr/local/hadoop /usr/local/</p>

<p>6 ## NFSGW: hdfs-site.xml 增加配置<br>
[root@nfsgw ~]# cd /usr/local/hadoop/<br>
[root@nfsgw hadoop]# cd etc/hadoop/<br>
[root@nfsgw hadoop]# pwd<br>
/usr/local/hadoop/etc/hadoop<br>
[root@nfsgw hadoop]# vim hdfs-site.xml <br>
&lt;configuration&gt;<br>
........</p>

<p>    &lt;property&gt;<br>
        &lt;name&gt;nfs.exports.allowed.hosts&lt;/name&gt;<br>
        &lt;value&gt;* rw&lt;/value&gt;<br>
    &lt;/property&gt;<br>
    &lt;property&gt;<br>
        &lt;name&gt;nfs.dump.dir&lt;/name&gt;<br>
        &lt;value&gt;/var/nfstmp&lt;/value&gt;<br>
    &lt;/property&gt;<br>
&lt;/configuration&gt;<br>
-------------<br>
– 配置完该属性后要创建 /var/nfstmp 文件夹<br>
mkdir /var/nfstmp<br>
– 并且把该文件夹的属组改成 代理用户</p>

<p>[root@nfsgw hadoop]# mkdir /var/nfstmp<br>
[root@nfsgw hadoop]# id nsd1803<br>
uid=200(nsd1803) gid=200(nsd1803) 组=200(nsd1803)<br>
[root@nfsgw hadoop]# chown 200.200 /var/nfstmp<br>
[root@nfsgw hadoop]# ll -d /var/nfstmp/        ## 修改属主属组 保证写的权限！<br>
drwxr-xr-x 2 nsd1803 nsd1803 6 8月   3 17:28 /var/nfstmp/<br>
[root@nfsgw hadoop]# </p>

<p>NFS 网关<br>
• 启动与挂载<br>
– 设置 /usr/local/hadoop/logs 权限,为代理用户赋予<br>
读写执行的权限<br>
setfacl -m user:proxyuser:rwx /usr/local/hadoop/logs</p>

<p><br>
[root@nfsgw hadoop]# cd /usr/local/hadoop/<br>
[root@nfsgw hadoop]# <br>
[root@nfsgw hadoop]# ll -d logs/<br>
drwxr-xr-x 2 root root 6 8月   3 17:16 logs/<br>
[root@nfsgw hadoop]# <br>
[root@nfsgw hadoop]# setfacl -m u:nsd1803:rwx logs<br>
[root@nfsgw hadoop]# getfacl logs<br>
创建数据根目录 /var/hadoop<br>
[root@nfsgw hadoop]# mkdir /var/hadoop        </p>

<p>------验证一下nsd1803 用户的权限<br>
[root@nfsgw hadoop]# <br>
[root@nfsgw hadoop]# sudo -u nsd1803 -s<br>
[nsd1803@nfsgw hadoop]$ cd /var/nfstmp/<br>
[nsd1803@nfsgw nfstmp]$ ls<br>
[nsd1803@nfsgw nfstmp]$ touch aaa<br>
[nsd1803@nfsgw nfstmp]$ ls<br>
aaa<br>
[nsd1803@nfsgw nfstmp]$ rm -f aaa</p>

<p>[nsd1803@nfsgw nfstmp]$ cd /usr/local/hadoop/logs/<br>
[nsd1803@nfsgw logs]$ ls<br>
[nsd1803@nfsgw logs]$ touch fff<br>
[nsd1803@nfsgw logs]$ ls<br>
fff<br>
[nsd1803@nfsgw logs]$ rm -f fff<br>
[nsd1803@nfsgw logs]$ ls</p>

<p>[nsd1803@nfsgw logs]$ ll<br>
总用量 0<br>
[nsd1803@nfsgw logs]$ exit<br>
exit<br>
[root@nfsgw hadoop]# <br>
-----------</p>

<p>#使用 root 用户启劢 portmap 服务  <br>
#使用代理用户启劢 nfs3<br>
NFS 网关<br>
• 这里要特别注意:<br>
– 启劢 portmap 需要使用 root 用户<br>
– 启劢 nfs3 需要使用 core-site 里面设置的代理用户<br>
– 必须先启劢 portmap 之后再启劢 nfs3<br>
– 如果 portmap 重启了,在重启之后 nfs3 也需要重启</p>

<p>【nfs端口 2049  RPC端口111】<br>
rpc 在rhel7 叫 rpcbind  在rhel6 叫portmap<br>
启动服务  有两个服务nfs ，portmap  ；   nfs 依赖于portmap<br>
nfs启动的时候需要往rpc进行注册  <br>
启动顺序：：<br>
必须先启动portmp 再启动nfs <br>
重启的时候 ： 先停nfs再停portmp  然后再启动portmap 然后再启动nfs<br>
而且启动portmap必须用root启动（停）  nfs必须用代理用户启动（停）</p>

<p>[root@nfsgw ~]# cd /usr/local/hadoop/<br>
[root@nfsgw hadoop]# ./sbin//hadoop-daemon.sh --script ./bin/hdfs start portmap<br>
[root@nfsgw hadoop]# jps<br>
1321 Jps<br>
1274 Portmap                #//看到 Portmap<br>
[root@nfsgw hadoop]# <br>
[root@nfsgw hadoop]# su -l nsd1803<br>
[nsd1803@nfsgw ~]$ cd /usr/local/hadoop/<br>
[nsd1803@nfsgw hadoop]$ ./sbin/hadoop-daemon.sh --script ./bin/hdfs start nfs3<br>
starting nfs3, logging to /usr/local/hadoop/logs/hadoop-nsd1803-nfs3-nfsgw.out<br>
[nsd1803@nfsgw hadoop]$ <br>
[nsd1803@nfsgw hadoop]$ jps<br>
1368 Nfs3                    ## 能看到Nfs3<br>
1421 Jps<br>
[nsd1803@nfsgw hadoop]$ <br>
[nsd1803@nfsgw hadoop]$ exit        ## 退回到root下 再看<br>
logout<br>
[root@nfsgw hadoop]# <br>
[root@nfsgw hadoop]# jps            ## 同时看到 Nfs3   Portmap<br>
1368 Nfs3<br>
1433 Jps<br>
1274 Portmap<br>
[root@nfsgw hadoop]# </p>

<p>[root@nfsgw hadoop]# netstat -ntlup<br>
端口号 2049   111 都应该出现</p>

<p>[root@nfsgw hadoop]# cd /usr/local/hadoop/<br>
[root@nfsgw hadoop]# ls<br>
bin  etc  include  lib  libexec  LICENSE.txt  logs  NOTICE.txt  README.txt  sbin  share<br>
[root@nfsgw hadoop]# <br>
[root@nfsgw hadoop]# cd logs/<br>
[root@nfsgw logs]# ls</p>

<p>[root@nfsgw logs]# cat hadoop-nsd1803-nfs3-nfsgw.log   正确应该是都是INFO<br>
2018-08-03 17:46:37,481 INFO org.apache.hadoop.hdfs.nfs.nfs3<br>
2018-08-03 17:46:37,481 INFO org.apache.hadoop.hdfs.nf<br>
2018-08-03 17:46:37,481 INFO org.apache.hadoop.hdf<br>
2018-08-03 17:46:37,481 INFO org.apache.hadoop.hdfs.nfs.nfs3<br>
2018-08-03 17:46:37,481 INFO org.apache.hadoop.hdfs.nf<br>
2018-08-03 17:46:37,481 INFO org.apache.hadoop.hdf<br>
----------</p>

<p>• 启动与挂载<br>
– 目前NFS 只能使用v3版本<br>
– vers=3<br>
– 仅使用TCP作为传输协议。<br>
– proto=tcp<br>
– 不支持NLM<br>
– nolock<br>
– 禁用 access time 的时间更新<br>
– noatime<br>
---------------<br>
NFS 网关<br>
• 启动与挂载<br>
– 强烈建议使用安装选项“sync”,因为它可以最小化<br>
戒避免重新排序写入,这将导致更可预测的吞吏量。<br>
未挃定同步选项可能会导致上传大文件时出现丌可靠<br>
的行为<br>
– 启劢一台机器安装 nfs-utils<br>
yum install nfs-utils<br>
– 挂载 nfs<br>
mount -t nfs -o vers=3,proto=tcp,noatime,nolock,sync 192.168.1.15:/ /mnt/</p>

<p>###实验————&gt;&gt; 找一台机器 不需要hadoop 任意一台<br>
[root@newnode ~]# <br>
[root@newnode ~]# cd /usr/local/<br>
[root@newnode local]# ls<br>
[root@newnode local]# rm -rf hadoop      #因为在上面做的实验中这台机器是newnode节点，所以删除hadoop！<br>
[root@newnode local]# cd<br>
 <br>
[root@newnode ~]# yum install -y nfs-utils</p>

<p>[root@newnode ~]# mount -t nfs -o vers=3,proto=tcp,nolock,noatime,sync,noacl 192.168.1.15:/  /mnt/<br>
[root@newnode ~]# cd /mnt/<br>
[root@newnode mnt]# ls</p>

<p>[root@newnode mnt]# showmount -e 192.168.1.15<br>
Export list for 192.168.1.16:<br>
/ *<br>
[root@newnode mnt]# rpcinfo -p 192.168.1.15<br>
   program vers proto   port  service<br>
    100005    3   udp   4242  mountd<br>
    100005    1   tcp   4242  mountd<br>
    100000    2   udp    111  portmapper<br>
    100000    2   tcp    111  portmapper<br>
    100005    3   tcp   4242  mountd<br>
    100005    2   tcp   4242  mountd<br>
    100003    3   tcp   2049  nfs<br>
    100005    2   udp   4242  mountd<br>
    100005    1   udp   4242  mountd<br>
[root@newnode mnt]# </p>

<p><br>
～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～#关于同时启动hdfs  yarn的脚本#<br>
[root@nn01 hadoop]# ls<br>
bin  etc  include  lib  libexec  LICENSE.txt  logs  NOTICE.txt  oo  README.txt  sbin  share  xx<br>
[root@nn01 hadoop]# cat ./sbin/start-all.sh <br>
#!/usr/bin/env bash</p>

<p># Licensed to the Apache Software Foundation (ASF) under one or more<br>
# contributor license agreements.  See the NOTICE file distributed with<br>
# this work for additional information regarding copyright ownership.<br>
# The ASF licenses this file to You under the Apache License, Version 2.0<br>
# (the "License"); you may not use this file except in compliance with<br>
# the License.  You may obtain a copy of the License at<br>
#<br>
#     http://www.apache.org/licenses/LICENSE-2.0<br>
#<br>
# Unless required by applicable law or agreed to in writing, software<br>
# distributed under the License is distributed on an "AS IS" BASIS,<br>
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>
# See the License for the specific language governing permissions and<br>
# limitations under the License.</p>

<p><br>
# Start all hadoop daemons.  Run this on master node.</p>

<p>echo "This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh"</p>

<p>bin=`dirname "${BASH_SOURCE-$0}"`<br>
bin=`cd "$bin"; pwd`</p>

<p>DEFAULT_LIBEXEC_DIR="$bin"/../libexec<br>
HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}<br>
. $HADOOP_LIBEXEC_DIR/hadoop-config.sh</p>

<p># start hdfs daemons if hdfs is present<br>
if [ -f "${HADOOP_HDFS_HOME}"/sbin/start-dfs.sh ]; then<br>
  "${HADOOP_HDFS_HOME}"/sbin/start-dfs.sh --config $HADOOP_CONF_DIR<br>
fi</p>

<p># start yarn daemons if yarn is present<br>
if [ -f "${HADOOP_YARN_HOME}"/sbin/start-yarn.sh ]; then<br>
  "${HADOOP_YARN_HOME}"/sbin/start-yarn.sh --config $HADOOP_CONF_DIR<br>
fi<br>
[root@nn01 hadoop]# pwd<br>
/usr/local/hadoop<br>
[root@nn01 hadoop]# </p>

<p><br>
～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～</p>            </div>
                </div>