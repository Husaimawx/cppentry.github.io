---
layout:     post
title:      spark－概念
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主编写文章，未经博主允许转载，转载请注明出处。					https://blog.csdn.net/u012373815/article/details/53728332				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>本文长篇介绍了spark基本概念和spark Streaming  、spark sql 请仔细阅读，红色标注的是我认为比较重要的部分。</p>

<h1 id="运行环境">运行环境</h1>

<p><img src="https://img-blog.csdn.net/20161218172313957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>

<h1 id="基本概念">基本概念</h1>

<p><img src="https://img-blog.csdn.net/20161218172334442?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>

<h1 id="spark生态圈">Spark生态圈</h1>

<p>以Spark Core为核心，从HDFS、Amazon S3和HBase等持久层读取数据，以MESS、YARN和自身携带的Standalone为资源管理器调度Job完成Spark应用程序的计算。 这些应用程序可以来自于不同的组件，如Spark Shell/Spark Submit的批处理、Spark Streaming的实时处理应用、Spark SQL的即席查询、BlinkDB的权衡查询、MLlib/MLbase的机器学习、GraphX的图处理和SparkR的数学计算等等。</p>

<p><img src="https://img-blog.csdn.net/20161218163823722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>

<p>spark官方文档的翻译精读。基于1.6版本。</p>

<h1 id="1-spark概述">1. spark概述</h1>

<p><img src="https://img-blog.csdn.net/20161218163909692?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>



<h1 id="2-rdd-基本概念">2. Rdd 基本概念</h1>

<p><img src="https://img-blog.csdn.net/20161218163948505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>Spark更适合于迭代运算比较多的ML和DM运算。因为在Spark里面，有RDD的概念。</p>

<p>RDD可以cache到内存中，那么每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法来说，效率提升比较大</p>

<p>•   RDD(Resilient Distributed Dataset) 弹性分布数据集介绍  <br>
弹性分布式数据集（基于Matei的研究论文）或RDD是Spark框架中的核心概念。可以将RDD视作数据库中的一张表。其中可以保存任何类型的数据。Spark将数据存储在不同分区上的RDD之中。  <br>
RDD可以帮助重新安排计算并优化数据处理过程。  <br>
此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。 </p>

<p>RDD是不可变的。你可以用变换（Transformation）修改RDD，但是这个变换所返回的是一个全新的RDD，而原有的RDD仍然保持不变。 </p>

<p>RDD支持两种类型的操作： <br>
•   变换（Transformation） <br>
•   行动（Action）  <br>
变换：变换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。 </p>

<p>行动：行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。  <br>
行动操作包括：reduce，collect，count，first，take，countByKey以及foreach。 <br>
•   共享变量（Shared varialbes）  <br>
o   广播变量（Broadcast variables） <br>
o     累加器（Accumulators）</p>

<hr>



<h2 id="21-并行集合">2.1   并行集合</h2>

<p><img src="https://img-blog.csdn.net/20161218164038615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="22外部数据集">2.2.外部数据集</h2>

<p><img src="https://img-blog.csdn.net/20161218164153820?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>



<h3 id="221-rdd操作">2.2.1    RDD操作</h3>

<p><img src="https://img-blog.csdn.net/20161218164229539?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h4 id="传递函数导spark集群">传递函数导spark集群</h4>

<p>spark API很大程度上需要依靠在驱动程序里传递函数导集群上运行</p>

<ul>
<li>匿名函数，可以在短代码中实现</li>
<li>全局单利对象的静态方法</li>
</ul>

<p>这里需要传递包含方法的整个对象到每个work，而不是一个方法引用</p>

<p><img src="https://img-blog.csdn.net/20161218164323196?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h3 id="222-rdd-持久化">2.2.2    RDD 持久化</h3>

<p><img src="https://img-blog.csdn.net/20161218164430446?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218164506587?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="23-共享变量">2.3   共享变量</h2>

<p><img src="https://img-blog.csdn.net/20161218164549696?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218164612482?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>



<h1 id="3-sprakstream基本概念">3. SprakStream基本概念</h1>

<p><img src="https://img-blog.csdn.net/20161218164743812?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218164820001?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>容错性：对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。</p>

<p>对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。</p>

<p><img src="https://img-blog.csdn.net/20161218165121615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="31-离散流dstream">3.1   离散流DStream</h2>

<p>此处介绍一个wordcount的小李子</p>

<p><img src="https://img-blog.csdn.net/20161218165159636?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
通过spark引擎计算这些隐含的RDD算子，DStream操作隐藏了大部分的细节 <br>
<img src="https://img-blog.csdn.net/20161218165225929?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="32-输入dstream和receive">3.2   输入DStream和Receive</h2>

<p><img src="https://img-blog.csdn.net/20161218165254730?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>感悟：这就是为什么在kafka的那个小李子中，只会拿取数据而不处理，等到stop 任务的时候才处理，是因为我只是给他本地运行的一个核心，只有当任务停止的时候核心才会从接受消息的receiver中空余出来进行数据处理。</p>



<h3 id="321-基本源">3.2.1    基本源</h3>

<p><img src="https://img-blog.csdn.net/20161218165345762?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>针对于第三点也就是说不能持续的写此文件。持续写，后来写入的数据不被处理，这个功能测试使用，比较鸡肋。不太管用。可以把每个文件当作一个RDD因为RDD不可改变。所以这个文件改变了，也不会生成新的RDD 所以改变新加的数据，不会被处理。</p>



<h3 id="322-高级源">3.2.2    高级源</h3>

<p><img src="https://img-blog.csdn.net/20161218165412418?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>这些高级源在spark-shell中不能被使用。</p>

<p><img src="https://img-blog.csdn.net/20161218165442370?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="33-dstream-的转换transformation">3.3   DStream 的转换（transformation）</h2>

<p><img src="https://img-blog.csdn.net/20161218165509622?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>union 不去重</p>

<p><img src="https://img-blog.csdn.net/20161218165531981?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218165643982?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218165703623?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="34-dstream的输出">3.4   DStream的输出</h2>

<p><img src="https://img-blog.csdn.net/20161218165728687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="341-foreachrdd">3.4.1 foreachRDD</h2>

<p><img src="https://img-blog.csdn.net/20161218165754935?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218165841342?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218165909998?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="35-缓存和持久化">3.5   缓存和持久化</h2>

<p><img src="https://img-blog.csdn.net/20161218170004112?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218170040817?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218170057874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="36-streaming配置升级和监控">3.6   Streaming配置、升级和监控</h2>

<p><img src="https://img-blog.csdn.net/20161218170126740?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218170147031?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218170246672?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="37-性能调优">3.7   性能调优</h2>

<p>提高sparkStreaming应用程序的性能，需要考虑两件事</p>

<pre><code>高效的利用集群资源减少批数据的处理时间
设置正确的批处理容量（size）使数据的处理速度能跟上数据的接收速度 
减少批数据的执行时间
内存调优
</code></pre>



<h3 id="371-减少批处理时间">3.7.1    减少批处理时间</h3>

<p><img src="https://img-blog.csdn.net/20161218170312992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
<img src="https://img-blog.csdn.net/20161218170344540?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>此处的粗粒度指的是什么？ <br>
Mesos 是一个集群资源管理器，粗粒度和细粒度是两种管理模式，粗粒度是直接把人物所需资源准备好，然后执行任务。细粒度是拿到任务对任务按照优先级排序。然后再给资源安优先级执行任务。</p>



<h3 id="372-设置正确的批处理容量">3.7.2    设置正确的批处理容量</h3>

<p><img src="https://img-blog.csdn.net/20161218170408884?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h3 id="373-内存调优">3.7.3    内存调优</h3>

<p><img src="https://img-blog.csdn.net/20161218170431422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="38-容错语义">3.8   容错语义</h2>

<p><img src="https://img-blog.csdn.net/20161218170452500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
 <img src="https://img-blog.csdn.net/20161218170541782?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<hr>



<h1 id="4-spark-sql">4  .Spark Sql</h1>



<h2 id="41-sql">4.1   SQL</h2>

<p>Spark SQL的一种用法是直接执行SQL查询语句，你可使用最基本的SQL语法，也可以选择HiveQL语法。Spark SQL可以从已有的Hive中读取数据。更详细的请参考Hive Tables 这一节。如果用其他编程语言运行SQL，Spark SQL将以DataFrame返回结果。你还可以通过命令行command-line 或者 JDBC/ODBC 使用Spark SQL。</p>



<h2 id="42-dataframes">4.2   DataFrames</h2>

<p>DataFrame是一种分布式数据集合，每一条数据都由几个命名字段组成。概念上来说，她和关系型数据库的表 或者 R和Python中的data frame等价，只不过在底层，DataFrame采用了更多优化。DataFrame可以从很多数据源（sources）加载数据并构造得到，如：结构化数据文件，Hive中的表，外部数据库，或者已有的RDD。 <br>
DataFrame API支持Scala, Java, Python, and R。</p>



<h2 id="43-datasets">4.3   Datasets</h2>

<p>Dataset是Spark-1.6新增的一种API，目前还是实验性的。Dataset想要把RDD的优势（强类型，可以使用lambda表达式函数）和Spark SQL的优化执行引擎的优势结合到一起。Dataset可以由JVM对象构建（constructed ）得到，而后Dataset上可以使用各种transformation算子（map，flatMap，filter 等）。</p>

<p>Dataset API 对 Scala 和 Java的支持接口是一致的，但目前还不支持Python，不过Python自身就有语言动态特性优势（例如，你可以使用字段名来访问数据，row.columnName）。对Python的完整支持在未来的版本会增加进来。</p>

<p>spark 1.2升级到1.3 SchemaRDD重命名为DataFrame</p>



<h2 id="44-创建dataframe">4.4   创建DataFrame</h2>

<p>Spark应用可以用SparkContext创建DataFrame，所需的数据来源可以是已有的RDD（existing RDD），或者Hive表，或者其他数据源（data sources.） <br>
以下是一个从JSON文件创建DataFrame的小栗子：</p>



<h2 id="45-dataframe操作">4.5   DataFrame操作</h2>

<p>DataFrame提供了结构化数据的领域专用语言支持，包括Scala, Java, Python and R. <br>
这里我们给出一个结构化数据处理的基本示例：</p>



<h2 id="46-创建dataset">4.6   创建Dataset</h2>

<p>Dataset API和RDD类似，不过Dataset不使用Java序列化或者Kryo，而是使用专用的编码器（Encoder ）来序列化对象和跨网络传输通信。如果这个编码器和标准序列化都能把对象转字节，那么编码器就可以根据代码动态生成，并使用一种特殊数据格式，这种格式下的对象不需要反序列化回来，就能允许Spark进行操作，如过滤、排序、哈希等。</p>



<h2 id="47-和rdd互操作">4.7   和RDD互操作</h2>

<p>Spark SQL有两种方法将RDD转为DataFrame。</p>

<ol>
<li>使用反射机制，推导包含指定类型对象RDD的schema。这种基于反射机制的方法使代码更简洁，而且如果你事先知道数据schema，推荐使用这种方式；</li>
<li>编程方式构建一个schema，然后应用到指定RDD上。这种方式更啰嗦，但如果你事先不知道数据有哪些字段，或者数据schema是运行时读取进来的，那么你很可能需要用这种方式。</li>
</ol>



<h3 id="471-利用反射推导schema">4.7.1    利用反射推导schema</h3>

<p>Spark SQL的Scala接口支持自动将包含case class对象的RDD转为DataFrame。对应的case class定义了表的schema。case class的参数名通过反射，映射为表的字段名。case class还可以嵌套一些复杂类型，如Seq和Array。RDD隐式转换成DataFrame后，可以进一步注册成表。随后，你就可以对表中数据使用SQL语句查询了。</p>



<h3 id="472-编程方式定义schema">4.7.2    编程方式定义Schema</h3>

<p>如果不能事先通过case class定义schema（例如，记录的字段结构是保存在一个字符串，或者其他文本数据集中，需要先解析，又或者字段对不同用户有所不同），那么你可能需要按以下三个步骤，以编程方式的创建一个DataFrame：</p>

<ol>
<li>从已有的RDD创建一个包含Row对象的RDD</li>
<li>用StructType创建一个schema，和步骤1中创建的RDD的结构相匹配</li>
<li>把得到的schema应用于包含Row对象的RDD，调用这个方法来实现这一步：SQLContext.createDataFrame</li>
</ol>



<h2 id="48-数据源">4.8   数据源</h2>



<h3 id="481-通用加载保存函数">4.8.1    通用加载/保存函数</h3>

<p>在最简单的情况下，所有操作都会以默认类型数据源来加载数据（默认是Parquet，除非修改了spark.sql.sources.default 配置）。</p>



<pre class="prettyprint"><code class=" hljs avrasm">val df = sqlContext<span class="hljs-preprocessor">.read</span><span class="hljs-preprocessor">.load</span>(<span class="hljs-string">"examples/src/main/resources/users.parquet"</span>)
df<span class="hljs-preprocessor">.select</span>(<span class="hljs-string">"name"</span>, <span class="hljs-string">"favorite_color"</span>)<span class="hljs-preprocessor">.write</span><span class="hljs-preprocessor">.save</span>(<span class="hljs-string">"namesAndFavColors.parquet"</span>)</code></pre>



<h3 id="482-手动指定选项">4.8.2    手动指定选项</h3>

<p>你也可以手动指定数据源，并设置一些额外的选项参数。数据源可由其全名指定（如，org.apache.spark.sql.parquet），而对于内建支持的数据源，可以使用简写名（json, parquet, jdbc）。任意类型数据源创建的DataFrame都可以用下面这种语法转成其他类型数据格式。</p>



<pre class="prettyprint"><code class=" hljs avrasm">val df = sqlContext<span class="hljs-preprocessor">.read</span><span class="hljs-preprocessor">.format</span>(<span class="hljs-string">"json"</span>)<span class="hljs-preprocessor">.load</span>(<span class="hljs-string">"examples/src/main/resources/people.json"</span>)
df<span class="hljs-preprocessor">.select</span>(<span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>)<span class="hljs-preprocessor">.write</span><span class="hljs-preprocessor">.format</span>(<span class="hljs-string">"parquet"</span>)<span class="hljs-preprocessor">.save</span>(<span class="hljs-string">"namesAndAges.parquet"</span>)</code></pre>



<h3 id="483-直接对文件使用sql">4.8.3    直接对文件使用SQL</h3>

<p>Spark SQL还支持直接对文件使用SQL查询，不需要用read方法把文件加载进来。</p>



<pre class="prettyprint"><code class=" hljs sql">val df = sqlContext.sql("<span class="hljs-operator"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> parquet.<span class="hljs-string">`examples/src/main/resources/users.parquet`</span><span class="hljs-string">")</span></span></code></pre>



<h3 id="484-保存模式">4.8.4    保存模式</h3>

<p>Save操作有一个可选参数SaveMode，用这个参数可以指定如何处理数据已经存在的情况。很重要的一点是，这些保存模式都没有加锁，所以其操作也不是原子性的。另外，如果使用Overwrite模式，实际操作是，先删除数据，再写新数据。</p>

<p><img src="https://img-blog.csdn.net/20161218170934473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h3 id="485-保存到持久化表">4.8.5    保存到持久化表</h3>

<p>在使用HiveContext的时候，DataFrame可以用saveAsTable方法，将数据保存成持久化的表。与registerTempTable不同，saveAsTable会将DataFrame的实际数据内容保存下来，并且在HiveMetastore中创建一个游标指针。持久化的表会一直保留，即使Spark程序重启也没有影响，只要你连接到同一个metastore就可以读取其数据。读取持久化表时，只需要用用表名作为参数，调用SQLContext.table方法即可得到对应DataFrame。</p>

<p>默认情况下，saveAsTable会创建一个”managed table“，也就是说这个表数据的位置是由metastore控制的。同样，如果删除表，其数据也会同步删除。</p>



<h2 id="49-数据集">4.9   数据集</h2>

<p>两种实现方式，</p>

<p><img src="https://img-blog.csdn.net/20161218171039985?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
  <img src="https://img-blog.csdn.net/20161218171108361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="410-性能调优">4.10  性能调优</h2>

<p><img src="https://img-blog.csdn.net/20161218171128533?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h3 id="parquet列式存储">Parquet列式存储</h3>

<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目</p>



<h4 id="列式存储">列式存储</h4>

<p>列式存储和行式存储相比有哪些优势呢？</p>

<ol>
<li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li>
<li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</li>
<li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
</ol>



<h2 id="411-数据类型">4.11  数据类型</h2>

<p><img src="https://img-blog.csdn.net/20161218171205974?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""> <br>
<img src="https://img-blog.csdn.net/20161218171223365?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjM3MzgxNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>