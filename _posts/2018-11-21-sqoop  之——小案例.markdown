---
layout:     post
title:      sqoop  之——小案例
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p><strong>注意:</strong><br>
在用sqoop操作的时候:1.注意cdh版本要一致<br>
2.虽然一致,但是还是需要导入一些报到 $SQOOP_HOME<br>
1)从 $HIVE_HOME/lib下导入 hive-common-1.1.0-cdh5.7.0.jar,<br>
hive-shims-*,<br>
2)导入 java-connector-jar<br>
3)导入 json的jar包<br>
其中需要从外部导入的包下载地址:<br>
<a href="https://download.csdn.net/download/huonan_123/10746467" rel="nofollow">https://download.csdn.net/download/huonan_123/10746467</a></p>
<h2><a id="_MySQLHDFS_12"></a>一、 MySQL==&gt;HDFS</h2>
<ol>
<li>不指定导入路径</li>
</ol>
<pre><code>[hadoop@hadoop001 bin]$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop --password root --username 123456 --table emp 

[hadoop@hadoop001 bin]$ hadoop fs -ls   /user/hadoop/
18/10/25 05:16:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2018-10-25 05:14 /user/hadoop/emp
</code></pre>
<ol start="2">
<li>–<strong>target-dir</strong> ===》指定hdfs路径<br>
<strong>注意</strong>：指定路径–target-dir sqoop_emp(不指定会以读取的表名自动生成一个目录)</li>
</ol>
<pre><code>[hadoop@hadoop001 bin]$ sqoop import \
&gt; --connect jdbc:mysql://localhost:3306/sqoop \
&gt; --password root \
&gt; --username 123456 \
&gt; --table emp \
&gt; --target-dir sqoop_emp

</code></pre>
<ol start="3">
<li><strong>delete-target-dir</strong> ===》如果已经存在目录就删除</li>
</ol>
<pre><code>sqoop import \
--connect jdbc:mysql://localhost:3306/sqoop \
--username root \
--password root \
--table emp \
--delete-target-dir 
</code></pre>
<h2><a id="HDFS__MySQL_45"></a>二、HDFS =&gt; MySQL</h2>
<pre><code>sqoop export \
--connect jdbc:mysql://localhost:3306/sqoop \
--username root \
--password 123456 \
--table emp_sqoop \
--export-dir emp 

</code></pre>
<pre><code>[hadoop@hadoop001 sqoop-1.4.6-cdh5.7.0]$ sqoop export \
&gt; --connect jdbc:mysql://localhost:3306/sqoop \
&gt; --username root \
&gt; --password 123456 \
&gt; --table emp_sqoop \
&gt; --export-dir emp 
Warning: /home/hadoop/app/sqoop-1.4.6-cdh5.7.0/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /home/hadoop/app/sqoop-1.4.6-cdh5.7.0/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /home/hadoop/app/sqoop-1.4.6-cdh5.7.0/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /home/hadoop/app/sqoop-1.4.6-cdh5.7.0/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
18/10/27 23:56:10 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
18/10/27 23:56:10 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
18/10/27 23:56:10 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
18/10/27 23:56:10 INFO tool.CodeGenTool: Beginning code generation
18/10/27 23:56:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `emp_sqoop` AS t LIMIT 1
18/10/27 23:56:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `emp_sqoop` AS t LIMIT 1
18/10/27 23:56:11 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/app/hadoop-2.6.0-cdh5.7.0
Note: /tmp/sqoop-hadoop/compile/74c212a715892b55898d0c44514495ad/emp_sqoop.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
18/10/27 23:56:13 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/74c212a715892b55898d0c44514495ad/emp_sqoop.jar
18/10/27 23:56:13 INFO mapreduce.ExportJobBase: Beginning export of emp_sqoop
18/10/27 23:56:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/10/27 23:56:14 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
18/10/27 23:56:14 INFO Configuration.deprecation: mapred.map.max.attempts is deprecated. Instead, use mapreduce.map.maxattempts
18/10/27 23:56:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
18/10/27 23:56:15 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
18/10/27 23:56:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
18/10/27 23:56:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
18/10/27 23:56:19 INFO input.FileInputFormat: Total input paths to process : 4
18/10/27 23:56:19 INFO input.FileInputFormat: Total input paths to process : 4
18/10/27 23:56:19 INFO mapreduce.JobSubmitter: number of splits:3
18/10/27 23:56:19 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
18/10/27 23:56:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540701601006_0031
18/10/27 23:56:21 INFO impl.YarnClientImpl: Submitted application application_1540701601006_0031
18/10/27 23:56:21 INFO mapreduce.Job: The url to track the job: http://hadoop001:8088/proxy/application_1540701601006_0031/
18/10/27 23:56:21 INFO mapreduce.Job: Running job: job_1540701601006_0031
18/10/27 23:56:35 INFO mapreduce.Job: Job job_1540701601006_0031 running in uber mode : false
18/10/27 23:56:35 INFO mapreduce.Job:  map 0% reduce 0%
18/10/27 23:56:47 INFO mapreduce.Job:  map 100% reduce 0%
18/10/27 23:56:47 INFO mapreduce.Job: Job job_1540701601006_0031 completed successfully
18/10/27 23:56:47 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=417579
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1513
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=21
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=3
		Data-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=27910
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=27910
		Total vcore-seconds taken by all map tasks=27910
		Total megabyte-seconds taken by all map tasks=28579840
	Map-Reduce Framework
		Map input records=14
		Map output records=14
		Input split bytes=594
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=286
		CPU time spent (ms)=4520
		Physical memory (bytes) snapshot=527609856
		Virtual memory (bytes) snapshot=4675698688
		Total committed heap usage (bytes)=452460544
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
18/10/27 23:56:47 INFO mapreduce.ExportJobBase: Transferred 1.4775 KB in 32.3879 seconds (46.7149 bytes/sec)
18/10/27 23:56:47 INFO mapreduce.ExportJobBase: Exported 14 records.

</code></pre>
<p>结果:</p>
<pre><code>mysql&gt; select * from emp_sqoop;
+-------+--------+-----------+------+------------+------+------+--------+
| EMPNO | ENAME  | JOB       | MGR  | HIREDATE   | SAL  | COMM | DEPTNO |
+-------+--------+-----------+------+------------+------+------+--------+
|  7369 | SMITH  | CLERK     | 7902 | 1980-12-17 |  800 | NULL |     20 |
|  7499 | ALLEN  | SALESMAN  | 7698 | 1981-02-20 | 1600 |  300 |     30 |
|  7521 | WARD   | SALESMAN  | 7698 | 1981-02-22 | 1250 |  500 |     30 |
|  7566 | JONES  | MANAGER   | 7839 | 1981-04-02 | 2975 | NULL |     20 |
|  7654 | MARTIN | SALESMAN  | 7698 | 1981-09-28 | 1250 | 1400 |     30 |
|  7698 | BLAKE  | MANAGER   | 7839 | 1981-05-01 | 2850 | NULL |     30 |
|  7782 | CLARK  | MANAGER   | 7839 | 1981-06-09 | 2450 | NULL |     10 |
|  7788 | SCOTT  | ANALYST   | 7566 | 1982-12-09 | 3000 | NULL |     20 |
|  7839 | KING   | PRESIDENT | NULL | 1981-11-17 | 5000 | NULL |     10 |
|  7844 | TURNER | SALESMAN  | 7698 | 1981-09-08 | 1500 |    0 |     30 |
|  7876 | ADAMS  | CLERK     | 7788 | 1983-01-12 | 1100 | NULL |     20 |
|  7900 | JAMES  | CLERK     | 7698 | 1981-12-03 |  950 | NULL |     30 |
|  7902 | FORD   | ANALYST   | 7566 | 1981-12-03 | 3000 | NULL |     20 |
|  7934 | MILLER | CLERK     | 7782 | 1982-01-23 | 1300 | NULL |     10 |
+-------+--------+-----------+------+------------+------+------+--------+
14 rows in set (0.01 sec)

</code></pre>
<h2><a id="Mysql_hive_168"></a>三、Mysql =&gt;hive</h2>
<p>执行:</p>
<pre><code>[hadoop@hadoop001 sqoop-1.4.6-cdh5.7.0]$ sqoop import \
&gt; --connect jdbc:mysql://localhost:3306/sqoop \
&gt; --username root \
&gt; --password 123456 \
&gt; --table emp \
&gt; --delete-target-dir \
&gt; --hive-import  \
&gt; --hive-table d5_emp_test_p \
&gt; --fields-terminated-by '\t' \
&gt; --columns 'EMPNO,ENAME,JOB,SAL,COMM' \
&gt; --hive-overwrite \
&gt; --hive-partition-key 'pt' \
&gt; --hive-partition-value 'rz'
</code></pre>
<p>报错:<br>
<img src="https://img-blog.csdnimg.cn/20181028153034156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1b25hbl8xMjM=,size_27,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>原因是:hdfs目录的权限问题<br>
删除:hadoop fs -rmr /tmp/hive<br>
再次执行会生成这个目录<br>
再次执行<br>
结果:<br>
<img src="https://img-blog.csdnimg.cn/20181028153346795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1b25hbl8xMjM=,size_27,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="Mysql_hive_194"></a>四、增量Mysql =&gt;hive</h2>
<ol>
<li>在原来基础上插入一条数据到mysql的emp表</li>
</ol>
<pre><code>insert into emp (empno,ename,job,mgr,hiredate,sal,comm,deptno) values(9999,"lxp","bb",9999,"2018-12-12",16000,30000,40);

</code></pre>
<ol start="2">
<li>追加导入</li>
</ol>
<pre><code>sqoop import \
--connect jdbc:mysql://192.168.52.130:3306/sqoop \
--username root \
--password 123456 \
-m 1 \
--table emp \
--hive-database 'ruozedata_test' \
--hive-import \
--direct \
--hive-table 'emp_sqopp_test' \
--fields-terminated-by '\t' \
--input-null-non-string '0' \
--input-null-string '' \
--check-column empno \
--incremental append \
--last-value 9999
</code></pre>
<ul>
<li><em><strong>常用命令</strong></em></li>
</ul>

<table>
<thead>
<tr>
<th>命令</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>–connect</td>
<td>mysql d的url  如：jdbc:mysql://localhost:3306/sqoop</td>
</tr>
<tr>
<td>–username</td>
<td>用户名</td>
</tr>
<tr>
<td>–password</td>
<td>密码</td>
</tr>
<tr>
<td>–table</td>
<td>读取mysql的表名称</td>
</tr>
<tr>
<td>–export-dir</td>
<td>导出hdfs路径</td>
</tr>
<tr>
<td>–hive-database</td>
<td>导入指定hive数据库</td>
</tr>
<tr>
<td>–hive-import</td>
<td>导入表到hive</td>
</tr>
<tr>
<td>–hive-overwrite</td>
<td>使导入数据覆盖原有数据</td>
</tr>
<tr>
<td>–incremental append</td>
<td>表示 在原来基础上追加数据</td>
</tr>
<tr>
<td>–check-column</td>
<td>表示以哪个列作为追加标识</td>
</tr>
<tr>
<td>–last-value</td>
<td>追击字段的value大于这个值就追加</td>
</tr>
<tr>
<td>–mapreduce-job-name</td>
<td>指定mr job作业的时候job名称  可以在web的50070端口查看</td>
</tr>
<tr>
<td>-fields-terminated-by</td>
<td>指定导入的字段</td>
</tr>
<tr>
<td>–where</td>
<td>拼写sql条件</td>
</tr>
<tr>
<td>-m</td>
<td>Use ‘n’ map tasks to import in parallel      ，指定几个task执行</td>
</tr>
<tr>
<td>-e</td>
<td>使用sql</td>
</tr>
</tbody>
</table>
            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>