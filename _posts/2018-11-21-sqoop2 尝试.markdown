---
layout:     post
title:      sqoop2 尝试
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
进入客户端  sqoop2<br><br><br>
show connector<br>
show link<br>
show job<br><br><br>
update link -lid 5<br>
update job -jid 3<br><br><br>
1.create link <br><span></span>hdfs link<br><span></span>create link --cid 3  <br><span></span>Name: hdfs<br><span></span>HDFS URI: -&gt;hdfs://CDH1:8020<br><span></span><br><span></span>oracle link<br><span></span>create link --cid 4<br><span></span>Name: oracle<br><span></span>JDBC Driver Class: oracle.jdbc.driver.OracleDriver<br><span></span>JDBC Connection String: jdbc:oracle:thin:@10.10.10.10:1521:sid1<br><span></span>Username: ccd<br><span></span>Password: ******<br><span></span><br>
2.create job<br><span></span>create job -f 5 -t 4  [from link_id to link_id]<br><span></span><br><span></span>Name: test_import<br><span></span>Schema name:<br><span></span>Table name: table_name<br><span></span>Table SQL statement:<br><span></span>Table column names:<br><span></span>Partition column name: table_id<br><span></span>Null value allowed for the partition column:<br><span></span>Boundary query:<br><span></span>Override null value:<br><span></span>Null value:<br><span></span>Output format:<br><span></span>  0 : TEXT_FILE<br><span></span>  1 : SEQUENCE_FILE<br><span></span>Choose: 0<br><span></span>Compression format:<br><span></span>  0 : NONE<br><span></span>  1 : DEFAULT<br><span></span>  2 : DEFLATE<br><span></span>  3 : GZIP<br><span></span>  4 : BZIP2<br><span></span>  5 : LZO<br><span></span>  6 : LZ4<br><span></span>  7 : SNAPPY<br><span></span>  8 : CUSTOM<br><span></span>Choose: 0<br><span></span>Custom compression format:<br><span></span>Output directory: /warehouse/table_name<br><span></span>Extractors: 2  [mapper num]<br><span></span>Loaders: 2     [reduce num]<br><br><br>
3.run job<br><span></span>start job --jid 2<br><span></span><br><span></span><br>
遇到的问题<br>
1.驱动找不到<br><span></span>sqoop1 将驱动放在/opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.11/lib/sqoop/lib<br><span></span>sqoop2 将驱动放在/var/lib/sqoop2<span>
</span><br>
2.There is no column found in the target table， <br><span></span>sqoop1中table name 需要大写<br>
3.com.google.protobuf.InvalidProtocolBufferException CDH端口是8020,不是9000，<br><span></span>sqoop1的问题，sqoop2指定HDFS地址<br>
4.No column is found to partition data<br><span></span>没有指定partition，会自动拿到表的primary key做为partition(从源码上看到的)。<br><span></span>如果表配置有错误，比如下面这种情况，直接抛出GENERIC_JDBC_CONNECTOR_0005异常。修改完下面这个问题，也就不会抛该异常了。<br>
5. table or view does not exist,SELECT MIN(driver_id), MAX(driver_id) FROM ccd.tablename<br><span></span>在JDBC Connection String(jdbc:oracle:thin:@10.10.10.10:1521:sid1)中以及有sid的了，<br><span></span>所以在配置job的时候Schema name设置为空即可。<br><br><br>
sqoop2[1.99.5]  默认分隔符是","，不能自定义，字段前后有"'"包住，看上去不适合作为hive数据。
            </div>
                </div>