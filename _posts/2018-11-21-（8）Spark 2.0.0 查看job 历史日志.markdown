---
layout:     post
title:      （8）Spark 2.0.0 查看job 历史日志
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/LoveCarpenter/article/details/78793015				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><div class="toc">
<ul>
<li><a href="#1%E5%BC%95%E8%A8%80" rel="nofollow">引言</a></li>
<li><a href="#2-%E5%90%AF%E5%8A%A8spark%E5%8E%86%E5%8F%B2%E6%97%A5%E5%BF%97" rel="nofollow">启动Spark历史日志</a><ul>
<li><a href="#21-%E5%AE%89%E8%A3%85spark" rel="nofollow">1 安装Spark</a></li>
<li><a href="#22-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6spark-defaultsconf" rel="nofollow">2 修改配置文件spark-defaultsconf</a></li>
<li><a href="#23-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6spark-envsh" rel="nofollow">3 修改配置文件spark-envsh</a></li>
<li><a href="#24-%E5%88%9B%E5%BB%BA%E6%97%A5%E5%BF%97%E7%9B%AE%E5%BD%95" rel="nofollow">4 创建日志目录</a></li>
<li><a href="#25-%E5%90%AF%E5%8A%A8spark%E5%8E%86%E5%8F%B2%E6%97%A5%E5%BF%97web%E7%AB%AF" rel="nofollow">5 启动spark历史日志web端</a></li>
<li><a href="#26-%E6%B5%8F%E8%A7%88%E5%99%A8%E6%9F%A5%E7%9C%8B%E6%98%AF%E5%90%A6%E5%90%AF%E5%8A%A8%E6%88%90%E5%8A%9F" rel="nofollow">6 浏览器查看是否启动成功</a></li>
</ul>
</li>
</ul>
</div>
</div>




<h1 id="1引言">1.引言</h1>

<p>      在使用<code>Spark</code>的时候，有时候我们会关注<code>job</code>的历史日志，但是在<code>Spark</code>中默认情况下，历史日志是关闭的，在本篇博客中主要介绍一下如何启动<code>spark</code>的历史日志。</p>

<p>      博主的环境为：</p>

<ul>
<li>操作系统为<code>Centos6.7</code></li>
<li><code>Hadoop</code>版本为<code>2.6.1</code></li>
<li><code>Hadoop</code>安装目录为：<code>/usr/local/hadoop</code></li>
<li><code>HDFS</code>的地址为：<code>hdfs://localhost:9000</code></li>
<li><code>Spark</code>的版本为：<code>2.0.0</code>，且Spark是伪分布安装。</li>
</ul>

<h1 id="2-启动spark历史日志">2. 启动Spark历史日志</h1>



<h2 id="21-安装spark">2.1 安装Spark</h2>

<p>      如何安装<code>Spark</code>的伪分布，请参考博文：<a href="http://blog.csdn.net/LoveCarpenter/article/details/78787415" rel="nofollow" target="_blank">Spark 2.0.0 伪分布安装</a></p>



<h2 id="22-修改配置文件spark-defaultsconf">2.2 修改配置文件<code>spark-defaults.conf</code></h2>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-comment">//将模板文件修改为配置文件</span>
cp spark-defaults.conf.template spark-defaults.conf
<span class="hljs-comment">//修改的配置信息</span>
spark.eventLog.enabled <span class="hljs-keyword">true</span> 
<span class="hljs-comment">//设置hdfs的目录，需要和自己hadoop的目录匹配</span>
spark.eventLog.dir hdfs:<span class="hljs-comment">//localhost:9000/var/log/spark </span>
spark.eventLog.compress <span class="hljs-keyword">true</span></code></pre>

<p><img src="https://img-blog.csdn.net/20171213163135787?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTG92ZUNhcnBlbnRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p><img src="https://img-blog.csdn.net/20171213163208149?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTG92ZUNhcnBlbnRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<h2 id="23-修改配置文件spark-envsh">2.3 修改配置文件spark-env.sh</h2>



<pre class="prettyprint"><code class=" hljs rust"><span class="hljs-comment">//配置文件最后加入</span>
<span class="hljs-keyword">export</span> SPARK_HISTORY_OPTS=<span class="hljs-string">"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs:/localhost:9000/var/log/spark"</span></code></pre>

<p><img src="https://img-blog.csdn.net/20171213163417051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTG92ZUNhcnBlbnRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>



<h2 id="24-创建日志目录">2.4 创建日志目录</h2>

<p>      Spark不会自动创建日志文件夹，因此需要我们自己创建</p>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-comment">//首先进入hadoop的bin目录</span>
cd /usr/local/hadoop/bin
<span class="hljs-comment">//创建文件夹命令</span>
./hadoop fs -mkdir /var
./hadoop fs -mkdir /var/log
./hadoop fs -mkdir /var/log/spark</code></pre>



<h2 id="25-启动spark历史日志web端">2.5 启动spark历史日志web端</h2>



<pre class="prettyprint"><code class="language-java hljs "><span class="hljs-comment">//进入spark的bin目录</span>
cd /usr/local/spark/sbin
<span class="hljs-comment">//如果spark集群正在启动，首先停止集群</span>
./stop-all.sh
<span class="hljs-comment">//重新启动集群</span>
./start-all.sh
<span class="hljs-comment">//启动job历史端口</span>
./start-history-server.sh</code></pre>

<h2 id="26-浏览器查看是否启动成功">2.6 浏览器查看是否启动成功</h2>

<ul>
<li>浏览器输入：<a href="http://localhost:18080" rel="nofollow">http://localhost:18080</a>查看（在linux机器上查看）</li>
</ul>

<p><img src="https://img-blog.csdn.net/20171213164336265?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTG92ZUNhcnBlbnRlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>