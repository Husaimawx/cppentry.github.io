---
layout:     post
title:      hadoop基础----hadoop实战(二)-----hadoop操作hdfs---hdfs文件系统常用命令
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/q383965374/article/details/52170218				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;">我们在前面已经介绍过了 hadoop 1.0是由2大部分组成的:</span></p><p><span style="font-size:18px;">hdfs + mapreduce</span></p><p><span style="font-size:18px;">也对hdfs做了相关介绍。</span></p><p><span style="font-size:18px;"><a href="http://blog.csdn.net/zzq900503/article/details/50782389" rel="nofollow">hadoop基础----hadoop理论(三)-----hadoop分布式文件系统HDFS详解</a></span></p><p><br></p><p><span style="font-size:18px;">因为上一章我们已经安装好了hadoop1.0的环境</span></p><p><span style="font-size:18px;"><a href="http://blog.csdn.net/zzq900503/article/details/51597978" rel="nofollow">hadoop基础----hadoop实战(一)-----hadoop环境安装---手动安装官方1.0版本</a><br></span></p><p><span style="font-size:18px;">本章就来尝试实际操作 hdfs的命令。</span></p><p><br></p><p><br></p><p><span style="font-size:18px;">如果熟悉linux的命令的话 你会发现hdfs会有很多相似的命令。</span></p><p><span style="font-size:18px;">hdfs模拟了很多linux命令，用法也很类似:<br>hadoop fs +类似linux命令<br></span></p><p><span style="font-size:18px;">更多命令详细可参考官网。</span></p><p><span style="font-size:18px;"><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" rel="nofollow">http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html</a></span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;">这里尝试常用的一些命令:</span></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p></p><h1><span><span><span style="color:#ff0000;">HDFS命令格式</span></span></span></h1><p></p><p style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:20px;"><span style="font-family:'Courier new';font-size:18px;">hadoop fs -cmd <span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:20px;">args</span></span></p><p style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:20px;"><span style="font-family:'Courier new';font-size:18px;">cmd:具体的操作，基本上与UNIX的命令行相同</span></p><p style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:20px;"><span style="font-family:'Courier new';font-size:18px;">args:参数</span></p><br><p><br></p><p><br></p><h1><span style="color:#ff0000;">HDFS资源URI格式</span></h1><h2><span style="color:#cc33cc;">格式</span></h2><p><span style="font-size:18px;">也就是命令格式中的args参数，我们有时候需要填写路径path。这里讲解一下hdfs中path的格式。</span></p><p><span style="font-size:18px;">比如:</span></p><p><span style="font-size:18px;">hadoop fs -cat hdfs://<span style="font-size:18px;">host1</span>:port1/file1 hdfs://host2:port2/file2<br>hadoop fs -cat file:///file3 /user/hadoop/file4<br></span></p><p><span style="font-size:18px;">这里<span style="font-size:18px;">hdfs://hadoop0:port1/file1就是路径path。</span></span></p><p><span style="font-size:18px;">hdfs中没有当前工作目录这样一个概念，也没有cmd这样的命令。所以一般需要用绝对路径。格式如下:</span></p><p></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">scheme://authority/path</span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">scheme：协议名（file或hdfs）</span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">authority：namenode主机名(hadoop0:9000或者192.168.30.180:9000)</span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">path：路径 </span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;"><br></span></span></p><h2 style="border:0px;list-style:none;"><span><span style="color:#cc33cc;">示例</span></span></h2><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">hdfs://<span style="font-size:18px;">192.168.30.180</span>:9000/user/joe/test.txt</span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">假设已经在core-site.xml里配置了 fs.default.name=hdfs://<span style="font-size:18px;">192.168.30.180</span><span style="font-size:18px;">:9000</span>，则仅使用/user/joe/test.txt即可。</span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;">我们回头看看安装篇中的设置，我们是有设置的。如图:</span></span></p><p style="border:0px;list-style:none;"><span><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20160810160740302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p style="border:0px;list-style:none;"><span style="font-size:18px;">hdfs默认工作目录为 /user/$USER，$USER是当前的登录用户名。<span style="text-align:justify;">只有将文件放入HDFS上(也就是工作目录)后，才可以运行Hadoop程序来处理它。</span></span></p><br><p><br></p><p><br></p><p><br></p><h1><span style="color:#ff0000;">常用命令</span></h1><h2><span style="color:#cc33cc;">进入bin目录</span></h2><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">现在我们来实际操作常用命令。</span><span style="font-size:18px;">未配置全局路径时必须进入hadoop安装路径的bin目录下</span><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">才能运行常用hdfs命令。</span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><img src="https://img-blog.csdn.net/20160810172332083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">否则会报 command not found.</span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"></span></p><pre><code class="language-plain">cd /home/joe/hadoop/hadoop-1.2.1/bin/</code></pre><p><br></p><p><br></p><p><br></p><p></p><h2><span style="color:#cc33cc;">查看hdfs支持的所有命令fs</span></h2><p></p><pre><code class="language-plain">hadoop fs</code></pre><p></p><p><span style="font-size:18px;">如图所示:</span></p><p><img src="https://img-blog.csdn.net/20160815182412305?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><br><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">查看某个命令的帮助文档fs -help</span></h2><p></p><pre><code class="language-plain">./hadoop fs -help ls</code></pre><p></p><p><span style="font-size:18px;">查看命令ls的帮助文档。</span></p><p><img src="https://img-blog.csdn.net/20160816180102291?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">列出目录和文件fs -ls</span></h2><p><span style="text-align:justify;"><span style="font-size:18px;">列出hadoop工作目录下的内容--目录和文件-<span style="text-align:justify;">包括文件名，权限，所有者，大小和修改时间。</span></span></span><br></p><p></p><pre><code class="language-plain">./hadoop fs -ls /</code></pre><p></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><img src="https://img-blog.csdn.net/20160810182540848?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">ps:hadoop工作目录的内容是集群一致的，也就是说在集群中的任意机子中hadoop0或者hadoop1或者hadoop2中运行，显示的内容都是一样的。</span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p></p><pre><code class="language-plain">./hadoop dfs -ls /</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><img src="https://img-blog.csdn.net/20160811093437031?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="font-size:18px;">PS:  hadoop dfs  -ls 作用跟hadoop fs -ls 作用类似，也是列出<span style="text-align:justify;">工作目录下的内容。区别可见下一节。</span></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">fs与dfs的区别</span></h2><p><span style="font-size:18px;">fs涉及到一个通用的文件系统，可以指向任何的文件系统如local，HDFS等。但是dfs仅是针对HDFS的。</span></p><p><span style="font-size:18px;">那么什么时候用FS呢？可以在本地与hadoop分布式文件系统的交互操作中使用。特定的DFS指令与HDFS有关。<br></span></p><p><span style="font-size:18px;">这两个命令依赖于模式的配置。当使用绝对URI（如scheme://a/b）这两个命令的效果是相同的。</span></p><p><span style="font-size:18px;">只有默认的模式配置参数对dfs和fs起作用。<br></span></p><p></p><p><span style="font-size:18px;">hadoop fs：使用面最广，可以操作任何文件系统。</span></p><p><span style="font-size:18px;">hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。<span style="font-size:18px;">hdfs dfs是hadoop2.0版本中的命令。我们现在的1.0版本是没有该命令的。反正用法是类似的。</span></span></p><p><span style="font-size:18px;">详细分析可参考:</span><a href="http://blog.csdn.net/pipisorry/article/details/51340838" rel="nofollow">http://blog.csdn.net/pipisorry/article/details/51340838</a><br></p><br><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">循环列出目录、子目录及文件信息 fs -lsr</span></h2><p></p><pre><code class="language-plain">./hadoop fs -lsr   /</code></pre><img src="https://img-blog.csdn.net/20160816112435819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br><p></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">建立目录fs -mkdir</span></h2><p></p><pre><code class="language-plain">./hadoop fs -mkdir   /joe/examples</code></pre><p></p><p><br></p><p><img src="https://img-blog.csdn.net/20160816114745287?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;">查看一下是否创建成功，我们在hadoop0创建目录，在hadoop1中查看。证明了hadoop的工作目录是集群式的，不是本机的。也就是 在hadoop0创建和hadoop1中创建效果是一样的。</span><br></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20160816114754623?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="font-size:18px;">ps:Hadoop的mkdir命令会自动创建父目录。</span><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">上传文件fs -put</span></h2><p><span style="font-size:18px;">我们在使用hadoop的过程中，经常会需要把本地文件上传到hdfs中进行处理分析。</span></p><p><span style="font-size:18px;">hdfs可以把本地文件上传到hdfs中，也就是把本地文件变成集群式的文件。</span></p><p><span style="font-size:18px;">我们先在本地当前目录（也就是bin目录下）创建一个test.txt</span></p><p></p><pre><code class="language-plain">vim test.txt</code></pre><p></p><p><span style="font-size:18px;">输入内容hello,hadoop</span></p><p><img src="https://img-blog.csdn.net/20160816152738068?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><img src="https://img-blog.csdn.net/20160816152747021?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><span style="font-size:18px;">开始上传 </span></p><p></p><pre><code class="language-plain">./hadoop fs -put  test.txt   . </code></pre><p></p><p><span style="font-size:18px;">最后一个参数是句点，相当于放入了默认的工作目录，等价于hadoop fs -put test.txt   /user/joe。<br></span></p><p><span style="font-size:18px;">把test.txt文件放入hdfs默认文件夹中（也就是默认工作目录 /user/joe）。</span></p><p><span style="font-size:18px;">我们也可以指定给文件重命名，例如test.txt上传成test2.txt。</span></p><p></p><pre><code class="language-plain">./hadoop fs -put  test.txt  test2.txt </code></pre><p></p><p><span style="font-size:18px;">也可以不放在默认工作目录，而是指定目录/joe/examples</span></p><p></p><pre><code class="language-plain">./hadoop fs -put  test.txt   /joe/examples</code></pre><br><br><p></p><p><img src="https://img-blog.csdn.net/20160816153855007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><span style="font-size:18px;">这个时候我们在hadoop1中查看也是看到同样的文件。所以文件已经在hdfs系统中了。也就是集群式的。</span></p><p><img src="https://img-blog.csdn.net/20160816154104523?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">上传文件fs -copyFromLocal</span></h2><p><span style="font-size:18px;">将本地文件上传到hdfs文件系统中，等同与fs -put命令。</span></p><p><span style="font-size:18px;">还是新建一个文件123.txt</span></p><p></p><pre><code class="language-plain">vim  123.txt</code></pre><span style="font-size:18px;">内容写上let‘s  play hadoop</span><p></p><p></p><pre><code class="language-plain">./hadoop -copyFromLocal  123.txt   .</code></pre><img src="https://img-blog.csdn.net/20160816181700343?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br><p></p><p><br></p><p><br><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">下载文件fs -get</span></h2><p><span style="font-size:18px;">我们刚才学习了上传文件到hdfs集群文件系统中，当然也有反向操作，把文件从集群中下载到本地。</span></p><p></p><pre><code class="language-plain">./hadoop fs -get  test2.txt     .</code></pre><p></p><p><span style="font-size:18px;">最后的参数是句点，代表当前目录，把hdfs默认目录(/user/joe)中的test2.txt下载到当前目录。</span></p><p><img src="https://img-blog.csdn.net/20160816165410046?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><span style="font-size:18px;">当然路径文件什么的我们都可以做相应调整，例如:</span></p><p></p><pre><code class="language-plain">./hadoop fs -get  /joe/examples/test.txt    /home/joe</code></pre><span style="font-size:18px;">把hdfs文件系统中 /joe/examples/test.txt 文件下载到本地  /home/joe目录中。</span><p></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">下载文件fs -copyToLocal </span></h2><p><span style="font-size:18px;">把hdfs文件系统中的文件下载到本地，等同于fs -get。</span></p><p></p><pre><code class="language-plain">./hadoop  fs -copyToLocal    123.txt    /home/joe</code></pre><p></p><p><span style="font-size:18px;">将hdfs文件系统中的123.txt下载到本地 /home/joe目录中。</span></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">列出指定名称的目录和文件fs -ls /目录或文件名</span></h2><p><span style="font-size:18px;">列出HDFS系统中下名为joe的目录</span><br></p><p></p><pre><code class="language-plain">./hadoop fs -ls  /joe</code></pre><p></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><span style="font-size:18px;">列出HDFS<span style="font-size:18px;">系统中</span>下名为test.txt的文件</span></span></p><p></p><pre><code class="language-plain">./hadoop fs -ls test.txt</code></pre><span style="font-size:18px;">ps:可以看到目录带/, 查看文件不带/</span><p></p><p><img src="https://img-blog.csdn.net/20160816164315067?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">查看hdfs中的文件内容fs -cat</span></h2><h3><span style="color:#3333ff;">查看文件内容</span></h3><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">比如上面放到集群中的test.txt文件，如果我们要看里面写的什么</span></p><p></p><pre><code class="language-plain">./hadoop fs -cat test.txt</code></pre><p></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><img src="https://img-blog.csdn.net/20160816173018125?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;">可以看到里面的内容 就是我们创建文件时写的hello,hadoop</span></span></p><p><br></p><h3><span style="color:#3333ff;">查看目录内容（</span><span style="color:rgb(51,51,255);font-size:12px;">*号</span><span style="color:rgb(51,51,255);font-size:12px;">）</span></h3><p><span style="font-size:18px;">我们还可以一次性查看某个目录下所有文件的内容,例如:<br></span></p><p><span style="font-size:18px;">/user/joe/目录中有test.txt和test2.txt这2个文件。</span></p><p></p><pre><code class="language-plain">./hadoop fs -cat  /user/joe/*</code></pre><br><img src="https://img-blog.csdn.net/20160816174528380?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><p></p><p><span style="font-size:18px;">用*号代表所有文件。</span></p><p><br></p><p><br></p><p><br></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">查看hdfs中的文件内容fs -text</span></h2><p></p><pre><code class="language-plain">./hadoop fs -text test.txt</code></pre><p></p><p><span style="font-size:18px;">显示文件的内容，当文件为文本文件时，等同于cat，文件为压缩格式（gzip以及hadoop的二进制序列文件格式）时，会先解压缩。</span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">查看hdfs中的文件内容fs -tail</span></h2><p><span style="font-size:18px;">查看文件尾部1K字节的内容，支持-f配置，也就是 持续输出。可以起到监控作用。</span></p><p></p><pre><code class="language-plain">./hadoop fs -tail -f test.txt</code></pre><br><br><p></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">改变目录或文件所属组fs -chgrp</span></h2><p><span style="text-align:justify;"><span style="font-size:18px;">设置hdfs文件系统中的文件或目录属于哪个组，路径递归设置的话使用参数 -R。命令的使用者必须是文件的所有者或者超级用户。</span></span><br></p><p></p><pre><code class="language-plain">./hadoop fs -chgrp  joe   test.txt  </code></pre><p></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:tahoma, '宋体';font-size:14px;line-height:22.4px;text-align:justify;background-color:rgb(250,250,252);"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">设置test.txt为joe组拥有。</span><br></span></span></p><p></p><pre><code class="language-plain">./hadoop fs -chgrp  -R  joe   /joe</code></pre><p></p><p><span style="font-size:18px;line-height:25.7143px;color:rgb(51,51,51);font-family:'Courier new';">递归设置目录joe和目录下的所有子目录examples和文件test.txt为joe组拥有。</span></p><p><span style="font-size:18px;line-height:25.7143px;color:rgb(51,51,51);font-family:'Courier new';">如下图:本来所属组都是supergroup 现在相应目录和文件的组已经修改成了joe。</span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><img src="https://img-blog.csdn.net/20160817144806525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h2><span style="color:#cc33cc;">改变目录或文件所属人fs -chown</span></h2><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><span style="font-size:18px;text-align:justify;">设置hdfs文件系统中的文件或目录属于哪个用户，路径递归设置的话使用参数-R。</span><span style="font-size:18px;text-align:justify;">命令的使用者必须是文件的所有者或者超级用户。</span><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -chown pig test.txt</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);">设置test.txt为pig用户拥有。</span><br></span></span></p><p></p><pre><code class="language-plain">./hadoop fs -chown -R pig  /joe</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);">递归设置joe目录和joe目录下的子目录和文件为pig用户拥有。</span><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817145721538?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">修改目录或文件的操作权限fs -chmod</span></h2><p><span style="text-align:justify;"><span style="font-size:18px;">设置hdfs文件系统中的文件或目录操作的权限是什么,路径递归设置的话使用参数-R。<span style="text-align:justify;">命令的使用者必须是文件的所有者或者超级用户。</span>相应权限的3位数或+/-{rwx}，跟linux的一样。详情可见:</span><span style="font-family:'Courier new';color:#333333;"></span></span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p><span style="text-align:justify;"><a href="http://blog.csdn.net/zzq900503/article/details/50237171" rel="nofollow">linux基础(七)----linux命令系统学习----系统安全相关命令</a>   <span style="font-size:18px;">这篇文章中的chmod参数详解。</span><span style="font-family:'Courier new';color:#333333;"><br></span></span></p><p style="text-align:justify;"></p><pre><code class="language-plain">./hadoop fs -chmod  666 test.txt</code></pre><p></p><p style="text-align:justify;"><span style="font-size:18px;">设置test.txt的权限为666,即所有用户所有组对它有读写权限。</span></p><p style="text-align:justify;"> <img src="https://img-blog.csdn.net/20160817151030257?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p style="text-align:justify;"><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;"><span style="color:rgb(51,51,51);font-family:'宋体';font-size:14px;line-height:28px;background-color:rgb(249,249,249);"><br></span></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">查看目录文件数量fs -count</span></h2><p></p><pre><code class="language-plain">./hadoop fs -count /</code></pre><p></p><p style="text-align:justify;"><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);">统计默认工作目录下的目录文件数量</span></span></p><p style="text-align:justify;"><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817151347606?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="text-align:justify;"><span style="font-size:18px;">显示为目录个数，文件个数，文件总计大小，输入路径。</span><span style="font-family:'Courier new';color:#333333;"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">复制文件fs -cp</span></h2><p><span style="font-size:18px;">cp命令是将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。</span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -cp test.txt  /joe/test.txt</code></pre><p></p><p style="text-align:justify;"><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);">把test.txt复制到joe目录下。</span></span></p><p></p><pre><code class="language-plain">./hadoop fs -cp test2.txt 123.txt  /joe</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);">把test2.txt和123.txt复制到joe目录下。因为这里是多个源路径 所以  目标路径必须是一个目录/joe。</span><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817154842070?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="text-align:justify;"><span style="color:#cc33cc;">查看目录或者文件大小fs -du</span></span></h2><p></p><pre><code class="language-plain">./hadoop fs -du /joe</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><span style="color:rgb(51,51,51);font-family:'宋体';line-height:28px;background-color:rgb(249,249,249);"><span style="font-size:18px;">显示joe目录中每个文件或目录的大小</span></span><span style="font-size:14px;color:rgb(51,51,51);font-family:'宋体';line-height:28px;background-color:rgb(249,249,249);">。</span><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817155113532?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="text-align:justify;"><span style="color:#cc33cc;">查看目录或者文件总大小fs -dus</span></span></h2><p></p><pre><code class="language-plain">./hadoop fs -dus /joe</code></pre><p></p><p><span style="text-align:justify;"><span style="font-size:18px;">类似于du，参数为目录时，会显示该目录的总大小 。</span><span style="font-family:'Courier new';color:#333333;"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817155339255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;text-align:justify;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">检测目录文件是否存在fs -test</span></h2><p><span style="font-size:18px;">hadoop fs -test   -[ezd]   PATH    </span></p><div style="border-width:0px;list-style:none;"><span style="font-size:18px;">对PATH进行如下类型的检查： </span><div style="border-width:0px;list-style:none;"><span style="font-size:18px;">-e PATH是否存在，如果PATH存在，返回0，否则返回1 </span><div style="border-width:0px;list-style:none;"><span style="font-size:18px;">-z 文件是否为空，如果长度为0，返回0，否则返回1 </span><div style="border-width:0px;list-style:none;"><span style="font-size:18px;">-d 是否为目录，如果PATH为目录，返回0，否则返回1 </span></div></div></div></div><p></p><p><span style="font-size:18px;">在 Linux 下，不管你是启动一个桌面程序也好，还是在控制台下运行命令，所有的程序在结束时，都会返回一个数字值，这个值叫做返回值，或者称为错误号 ( Error Number )。</span></p><p><span style="font-size:18px;">在控制台下，有一个特殊的环境变量 <span>$?</span>，保存着前一个程序的返回值。</span></p><p></p><pre><code class="language-plain">./hadoop fs -test -e /joe

echo $?

./hadoop fs -test -e test.txt

echo $?

./hadoop fs -test -z  test.txt

echo $?

./hadoop fs -test -d  test.txt

echo $?</code></pre><br><br><p></p><p><img src="https://img-blog.csdn.net/20160817174436467?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">合并下载文件fs -getmerge</span></h2><p><span style="font-family:'Courier new';color:#333333;"></span></p><p></p><div><span><span style="font-size:18px;">fs -getmerge命令用于接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件内容拼接保存成本地目标文件。</span></span></div><div><span><span style="font-size:18px;">使用方法：hadoop fs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]</span></span></div><div><span><span style="font-size:18px;">参数说明：addnl是可选的，用于指定在每个文件结尾添加一个换行符。</span></span></div><p></p><p></p><pre><code class="language-plain">./hadoop fs -getmerge /joe   total.txt </code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><span style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;line-height:25.2px;"><span style="font-size:18px;">把hdfs文件系统中的所有文件合并为文件total.txt,total.txt位于当前目录，不在hdfs中，是本地文件。</span></span><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"></span></p><pre><code class="language-plain">./hadoop fs -getmerge  /joe   total2.txt  true</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><span style="font-family:'microsoft yahei';line-height:26px;"><span style="font-size:18px;">加上addnl的true后，合并到local file中的hdfs文件之间会空出一行.</span></span><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);">ps:新版本中不用true了，改成了用-nl参数。</span></span></span></p><p></p><pre><code class="language-plain">./hadoop fs -getmerge -nl /joe  total2.txt</code></pre><p></p><p><span style="font-size:18px;">详情可见:</span></p><p></p><p style="z-index:1;"><span style="font-size:18px;"><a href="https://issues.apache.org/jira/browse/HADOOP-7348" rel="nofollow">Modify the option of FsShell getmerge from [addnl] to [-nl] for consistency</a></span></p><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817183011127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">创建空文件fs -touchz</span></h2><p><span style="font-family:'Courier new';color:#333333;"></span></p><p></p><pre><code class="language-plain">./hadoop fs -touchz /joe/blank.txt</code></pre><p></p><p><span style="font-size:18px;">在/joe目录下创建一个空文件blank.txt</span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817183512972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">删除空目录或者文件fs -rm</span></h2><p><span style="font-size:18px;">从HDFS文件系统删除test.txt文件，rm命令也可以删除空目录</span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -rm test.txt</code></pre><span style="font-size:18px;">可以看到test.txt已经被删除了。</span><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817183835337?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">删除目录以及子目录和文件fs -rmr</span></h2><p></p><pre><code class="language-plain">./hadoop fs -rmr /joe</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);">删除joe目录下的所有目录和文件。</span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160817184057325?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">清空回收站fs -expunge </span></h2><p><span style="font-family:'Courier new';color:#333333;"></span></p><p><span style="font-size:18px;">清空回收站，文件被删除时，它首先会移到临时目录.Trash/中，当超过延迟时间之后，文件才会被永久删除,该命令能马上进行删除。</span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -expunge </code></pre><br><br><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">上传文件并删除原文件fs -moveFromLocal</span></h2><p><span style="text-indent:28px;"><span style="font-size:18px;">和fs -put命令类似，上传目录或者源文件到hdfs中，但是源文件拷贝之后自身被删除。</span></span><br></p><p></p><pre><code class="language-plain">./hadoop  fs  -moveFromLocal  total.txt .</code></pre><p></p><p><span style="font-size:18px;">将total.txt文件上传到hdfs的当前工作目录。并删除本地的total.txt文件。</span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160818112405810?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><h2><span style="color:#cc33cc;">下载文件并删除hdfs中的文件fs -moveToLocal </span></h2><p><span style="text-align:justify;"><span style="font-size:18px;">与fs -get命令相似，下载hdfs中的文件到本地目录，但拷贝结束后，删除HDFS上原文件。</span></span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -moveToLocal  total.txt .</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);">将hdfs系统默认工作目录(/user/joe)中的total.txt下载到本地当前目录并删除原文件。</span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160818113647921?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);">当前1.0版本的hadoop中hdfs未实现该方法。后续版本中我们可以尝试。</span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><h2><span style="color:#cc33cc;">移动文件fs -mv</span></h2><p><span style="font-size:18px;">将hdfs系统中的文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。</span><span style="font-family:'Courier new';color:#333333;"><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -mv test2.txt  /home/test2.txt</code></pre><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);">将默认工作目录中的test2.txt移动到home目录下。</span></span></p><p></p><pre><code class="language-plain">./hadoop fs -mv total.txt  123.txt  /home</code></pre><span style="font-size:18px;line-height:25.7143px;color:rgb(51,51,51);font-family:'Courier new';background-color:rgb(250,250,252);"></span><p></p><p><span style="line-height:25.7143px;">将默认工作目录中的total.txt和123.txt</span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160818142426562?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><h2><span style="color:#cc33cc;">改变副本数fs -setrep</span></h2><h3><span style="color:#3333ff;">设置副本数</span></h3><p><span style="font-size:18px;">fs setrep命令可以改变一个文件的副本系数。加-R参数可以递归的改变目录下所有文件的副本系数。</span><br></p><p><img src="https://img-blog.csdn.net/20160818143245627?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><span style="font-size:18px;">我们在安装篇配置hdfs-site.xml配置文件的时候 其实已经对副本数做了配置，这个命令是修改某个目录或者文件的副本数。</span><span style="font-family:'微软雅黑';color:#333333;"></span></p><p></p><pre><code class="language-plain">./hadoop fs -setrep  2   /home/123.txt</code></pre><p></p><p><span style="font-size:18px;">将home目录中的123.txt的副本数设置为2。</span></p><p><span style="font-size:18px;"><br></span></p><p></p><pre><code class="language-plain">./hadoop fs -setrep  2  -R   /home</code></pre><p></p><p><span style="font-size:18px;">将home目录下的所有目录文件的副本数递归设置为2。</span><br></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;"><br></span></p><p></p><h3><span style="color:#3333ff;">查看当前hdfs的副本数</span></h3><pre><code class="language-plain">./hadoop fsck -locations</code></pre><p></p><p><span style="font-family:'微软雅黑';color:#333333;"><span style="font-size:14px;line-height:25.2px;background-color:rgb(255,255,204);"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;line-height:19.5px;"><img src="https://img-blog.csdn.net/20160818150125351?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br style="color:rgb(75,75,75);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:13px;line-height:20.8px;"><span style="color:rgb(75,75,75);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:13px;line-height:20.8px;"><br></span></span></span></span></span></span></p><p><span style="font-family:'微软雅黑';color:#333333;"><span style="font-size:14px;line-height:25.2px;background-color:rgb(255,255,204);"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;line-height:19.5px;"><span style="color:rgb(75,75,75);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:13px;line-height:20.8px;"><br></span></span></span></span></span></span></p><p><span style="font-family:'微软雅黑';color:#333333;"><span style="font-size:14px;line-height:25.2px;background-color:rgb(255,255,204);"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;line-height:19.5px;"><span style="color:rgb(75,75,75);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:13px;line-height:20.8px;"><br></span></span></span></span></span></span></p><h3><span style="color:#3333ff;">查看某个文件的副本数</span></h3><p><span style="font-size:18px;">可以通过ls中的文件描述符看到</span></p><p></p><pre><code class="language-plain">./hadoop  fs -ls  /home</code></pre><p></p><p><img src="https://img-blog.csdn.net/20160818150454043?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><span style="font-family:'微软雅黑';color:#333333;"><span style="font-size:14px;line-height:25.2px;background-color:rgb(255,255,204);"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;line-height:19.5px;"><br></span></span></span></span></span></p><p><span style="font-family:'微软雅黑';color:#333333;"><span style="font-size:14px;line-height:25.2px;background-color:rgb(255,255,204);"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;line-height:18px;"><span style="font-family:Verdana, Geneva, Arial, Helvetica, sans-serif;font-size:13px;line-height:19.5px;"><br></span></span></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></p><p><br></p><h2><span style="color:#cc33cc;">查看文件或者目录的统计信息fs -stat</span></h2><p></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">返回对应路径的状态信息。可以通过与C语言中的printf类似的格式化字符串定制输出格式，这里支持的格式字符有：</span></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">%b：文件大小</span></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">%o：Block大小</span></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">%n：文件名</span></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">%r：副本个数</span></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">%y或%Y：最后一次修改日期和时间</span></p><p style="border-width:0px;list-style:none;"><span style="font-size:18px;">默认情况输出最后一次修改日期和时间。</span></p><pre><code class="language-plain">./hadoop fs -stat  /home
./hadoop fs -stat    '%b  %o  %n  %r  %y'    /home</code></pre><br><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><img src="https://img-blog.csdn.net/20160818151959204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h2><span style="color:#cc33cc;">归档文件archive</span></h2><p></p><div><span style="font-size:18px;">    Hadoop并不擅长对小型文件的储存，原因取决于Hadoop文件系统的文件管理机制，Hadoop的文件存储的单元为一个块（block），block的数据存放在集群中的datanode节点上，由namenode对所有datanode存储的block进行管理。namenode将所有block的元数据存放在内存中，以方便快速的响应客户端的请求。那么问题来了，不管一个文件有多小，Hadoop都把它视为一个block，大量的小文件，将会把namenode的内存耗尽。</span></div><div><span style="font-size:18px;">     那么如何对大量的小文件进行有效的处理呢？Hadoop的优秀工程师们其实已经为我们考虑好了，Hadoop提供了一个叫Archive归档工具，Archive可以把多个文件归档成为一个文件，换个角度来看，Archive实现了文件的元数据整理，但是，归档的文件大小其实没有变化，只是压缩了文件的元数据大小。</span></div><p></p><p><br></p><pre><code class="language-plain">./hadoop  archive  -archiveName  gs.har  -p  /home   har     </code></pre><p></p><p><span style="font-size:18px;">命令中参数 -archiveName：压缩文件名，自己任意取； <br>-p：归档文件所在的父目录； <br>/home：要<span style="font-size:18px;">归档</span>的文件名----如果不加文件名则是归档整个目录。<br>har：<span style="font-size:18px;">归档</span>文件存放路径，这里用表示hdfs的默认目录也就是/user/joe下面的har目录。</span><br></p><p><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20160818175433543?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;">查看har文件是否生成</span></p><p><span style="font-size:18px;"></span></p><pre><code class="language-plain">./hadoop fs -ls har</code></pre><p></p><p><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20160818175511147?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="font-size:18px;">可以看到已经生成了gs.har文件。</span></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-size:18px;"><br></span></p><p><span style="font-family:'Courier new';color:#333333;"></span></p><p></p><p><span style="font-size:18px;">显示har的内容可以用如下命令</span></p><p style="color:rgb(51,51,51);font-family:'microsoft yahei';font-size:14px;line-height:26px;background-color:rgb(250,250,252);"></p><pre><code class="language-plain">./hadoop fs -ls har/gs.har</code></pre><img src="https://img-blog.csdn.net/20160818175605841?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><p></p><p style="color:rgb(51,51,51);font-family:'microsoft yahei';font-size:14px;line-height:26px;background-color:rgb(250,250,252);"><br></p><p style="color:rgb(51,51,51);font-family:'microsoft yahei';font-size:14px;line-height:26px;background-color:rgb(250,250,252);"><br><br></p><p><span style="font-size:18px;">显示har压缩的是那些文件可以用如下命令</span></p><p></p><pre><code class="language-plain">./hadoop fs -ls -R har:///user/joe/har/gs.har</code></pre><img src="https://img-blog.csdn.net/20160818175946904?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><p><span style="font-size:18px;"><span style="color:rgb(85,85,85);font-family:'微软雅黑';font-size:14px;line-height:31px;"><br></span></span></p><p><span style="font-size:18px;"><span style="color:rgb(85,85,85);font-family:'微软雅黑';font-size:14px;line-height:31px;"><br></span></span></p><p><span style="color:rgb(85,85,85);font-family:'微软雅黑';line-height:31px;"><span style="font-size:18px;">另外，我们可以像其它文件系统一样，操作har文件的下级目录,比如如上图中的joe目录:</span></span></p><p></p><pre><code class="language-plain">./hadoop fs -ls -R har:///user/joe/har/gs.har/joe</code></pre><img src="https://img-blog.csdn.net/20160818180058280?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br><p></p><p><span style="font-size:18px;"><span style="color:rgb(85,85,85);font-family:'微软雅黑';font-size:14px;line-height:31px;"><br></span></span></p><p></p><p><span style="font-size:18px;"><br></span></p><p><span style="color:rgb(85,85,85);font-family:'微软雅黑';line-height:31.5px;"><span style="font-size:18px;">远程操作归档文件也是很方便,只需要把主机名和端口写上就行了。---把hadoop0和端口以及路径换成自己对应的master使用的主机名和端口以及相应har路径就行。</span></span></p><p></p><pre><code class="language-plain">./hadoop fs -ls -R har://hdfs-hadoop0:9000/user/joe/har/gs.har/joe</code></pre><p><span style="font-size:18px;"><span style="color:rgb(51,51,51);font-family:'microsoft yahei';line-height:26px;"><img src="https://img-blog.csdn.net/20160818180325955?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><span style="font-size:18px;color:rgb(51,51,51);font-family:'microsoft yahei';line-height:26px;">注意</span><span style="font-size:18px;color:rgb(51,51,51);font-family:'microsoft yahei';line-height:26px;background-color:rgb(250,250,252);">：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。</span></p><p></p><p><br></p><p><br></p><p><br></p><p><span style="font-size:18px;">删除归档文件</span><br></p><p></p><pre><code class="language-plain">./hadoop fs -rmr har/gs.har</code></pre><img src="https://img-blog.csdn.net/20160818180640768?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br><p></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p></p><h1><span style="color:#ff0000;">hdfs管理与更新</span></h1><h2><span style="color:#cc33cc;">启动hdfs</span></h2><p></p><p></p><pre><code class="language-plain">./start-dfs.sh</code></pre><br><br><p></p><p><br></p><h2><span style="color:#cc33cc;">关闭hdfs</span></h2><p></p><pre><code class="language-plain">./stop-dfs.sh</code></pre><p></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">查看HDFS的基本统计信息</span></h2><p></p><pre><code class="language-plain">./hadoop dfsadmin -report</code></pre><p></p><p><span style="font-family:verdana, Arial, Helvetica, sans-serif;font-size:18px;color:#333333;"><span style="line-height:25.2px;">ps：这里的dfs不能换成fs。</span></span></p><p><span style="font-family:verdana, Arial, Helvetica, sans-serif;color:#333333;"><span style="font-size:14px;line-height:25.2px;"><img src="https://img-blog.csdn.net/20160818172658359?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></span></p><p><br></p><p><br></p><p><br></p><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><br></p><p></p><h2><span style="color:#cc33cc;">退出安全模式</span></h2><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;line-height:25.2px;"><span style="font-size:18px;">NameNode在启动时会自动进入安全模式。安全模式是NameNode的一种状态，在这个阶段，文件系统不允许有任何修改。</span></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;line-height:25.2px;"><span style="font-size:18px;">系统显示Name node in safe mode，说明系统正处于安全模式，这时只需要等待十几秒即可，也可通过下面的命令退出安全模式：</span></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"></p><pre><code class="language-plain">./hadoop dfsadmin  -safemode leave</code></pre><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><span style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:18px;line-height:25.2px;">ps：这里的dfs不能换成fs。</span><br></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><img src="https://img-blog.csdn.net/20160818172908172?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><br></p><p><br></p><p></p><h2><span style="color:#cc33cc;">进入安全模式</span></h2><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;line-height:25.2px;"><span style="font-size:18px;">在必要情况下，可以通过以下命令把HDFS置于安全模式：</span></p><pre><code class="language-plain">./hadoop  dfsadmin  -safemode enter</code></pre><p><span style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><span style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:18px;line-height:25.2px;">ps：这里的dfs不能换成fs。</span><br></span></p><p><span style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><img src="https://img-blog.csdn.net/20160818173104142?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></span></p><p><span style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><br></span></p><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><br></p><h2><span style="color:#cc33cc;">节点添加</span></h2><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;line-height:25.2px;"><span style="font-size:18px;">添加一个新的DataNode节点，先在新加节点上安装好Hadoop，要和NameNode使用相同的配置（可以直接从NameNode复制），修改$HADOOP_HOME/conf/master文件，加入NameNode主机名。然后在NameNode节点上修改$HADOOP_HOME/conf/slaves文件，加入新节点名，再建立新加节点无密码的SSH连接，运行启动命令为：</span></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"></p><pre><code class="language-plain">./start-all.sh</code></pre><br><br><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><br></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><br></p><p></p><h2><span style="color:#cc33cc;">负载均衡</span></h2><p></p><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;line-height:25.2px;"><span style="font-size:18px;">HDFS的数据在各个DataNode中的分布可能很不均匀，尤其是在DataNode节点出现故障或新增DataNode节点时。新增数据块时NameNode对DataNode节点的选择策略也有可能导致数据块分布不均匀。用户可以使用命令重新平衡DataNode上的数据块的分布：</span></p><pre><code class="language-plain">./start-balancer.sh</code></pre><p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;line-height:25.2px;"><img src="https://img-blog.csdn.net/20160818173245878?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><br></p><p><br></p><p><br></p><p><br></p><h2><span style="color:#cc33cc;">格式化hdfs系统</span></h2><p></p><pre><code class="language-plain">./hadoop namenode -format</code></pre><p></p><img src="https://img-blog.csdn.net/20160818173850579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;"><span style="line-height:25.7143px;background-color:rgb(250,250,252);"><br></span></span></span></p><h1><span style="color:#ff0000;">hdfs命令任意目录执行</span></h1><p><span style="font-size:18px;">我们之前操作hadoop命令首先第一步就是进入hadoop安装的bin目录。</span></p><p></p><p><span style="font-size:18px;">为了每次执行hadoop的时候，不用进入<span style="text-align:justify;">hadoop的安装的bin</span>目录，也不用加上hadoop的安装的bin路径，要做的事情就是将其安装路径加入到PATH（全局路径）中，这样就可以直接执行hadoop命令。</span></p><p><span style="font-size:18px;">操作如下：</span></p><p><span style="font-size:18px;">我这里的安装路径是</span></p><p><span style="font-size:18px;">/home/joe/hadoop/hadoop-1.2.1/bin/</span></p><p><span style="font-size:18px;">需要先切换到root角色</span></p><p></p><pre><code class="language-plain">su -</code></pre><span style="font-size:18px;">然后修改配置文件</span><p></p><p></p><pre><code class="language-plain">vim /etc/profile</code></pre><p></p><p><span style="font-size:18px;">在最后加入一句</span></p><p></p><pre><code class="language-plain">export HADOOP_INSTALL=/home/joe/hadoop/hadoop-1.2.1</code></pre><p></p><p><span style="font-size:18px;">这里的路径对应自己的安装路径。</span></p><p><span style="font-size:18px;">注意这里要写HADOOP_INSTALL，因为如果写HADOOP_HOME会在执行命令的时候提醒该已经deprecated</span></p><p><span style="font-size:18px;">然后将其bin加入到PATH中，再加上一句:</span></p><p></p><pre><code class="language-plain">export PATH=$HADOOP_INSTALL/bin:$PATH</code></pre><img src="https://img-blog.csdn.net/20160818181623501?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br><p></p><p><br></p><p><br></p><p><span style="font-size:18px;">让配置文件生效</span></p><p></p><pre><code class="language-plain">source /etc/profile</code></pre><br><br><p></p><p></p><p></p><br><p><br></p><p><br></p><p><span style="font-size:18px;">把用户切换回 joe</span></p><p></p><pre><code class="language-plain">su joe</code></pre><br><p></p><p><br></p><p><span style="font-size:18px;">这个时候在任意路径都可以尝试之前的操作命令。不需要在bin目录执行./hadoop了。</span></p><p><span style="font-size:18px;">任意命令执行hadoop 命令就行。</span></p><p><span style="font-size:18px;">如下:</span></p><p></p><pre><code class="language-plain">hadoop fs -ls /</code></pre><p></p><p><img src="https://img-blog.csdn.net/20160818203005581?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p><p><span style="font-size:18px;">我不在bin目录 也可以执行hadoop命令了。</span></p><p><br></p><p><br></p><p><br></p><h1><span style="color:#ff0000;"><br></span><span style="font-family:'Courier new';line-height:25.7143px;"><strong><span style="font-size:32px;color:#ff0000;">简化hdfs命令</span></strong></span></h1><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;">简化hdfs命令其实就是作一些简化的命令声明。</span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;">因为我们现在需要输入hadoop fs - 这样的格式才能执行hadoop命令。</span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;">命令比较长，如果我们想简化的话:</span></span></p><p><span style="font-family:'Courier new';color:#333333;"><span style="font-size:18px;line-height:25.7143px;">先切换到root 用户</span></span></p><p></p><pre><code class="language-plain">su -</code></pre><p></p><p><span style="font-size:18px;">在用户根目录下</span></p><p></p><pre><code class="language-plain"> vim  .bashrc</code></pre><br><p></p><p> </p><p></p><div><span style="font-size:18px;">在最后添加：</span></div><div><pre><code class="language-plain">alias fs='hadoop fs'
alias  fsa='hadoop dfsadmin'</code></pre><br></div><div><img src="https://img-blog.csdn.net/20160818204645213?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></div><div><br></div><div><br></div><div><span style="font-size:18px;">执行</span></div><div><pre><code class="language-plain">source .bashrc</code></pre></div><div><span style="font-size:18px;">即可生效</span></div><div> </div><div><br></div><div><br></div><div><span style="font-size:18px;">测试</span></div><div><pre><code class="language-plain">fs -ls 
fs -mkdir  input
fsa -safemode leave</code></pre></div><div><span style="font-size:18px;">等等命令</span></div><div><img src="https://img-blog.csdn.net/20160818204820863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></div><span style="font-family:'Courier new';color:#333333;"><br></span><p></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p><h1><span style="font-family:Arial;background-color:rgb(255,255,255);"><span style="font-size:32px;color:#ff0000;">组操作</span></span></h1><p><span style="color:rgb(51,51,51);font-family:Arial;font-size:14px;background-color:rgb(255,255,255);">sudo addgroup hadoop#</span><span style="color:rgb(51,51,51);font-family:Arial;font-size:14px;background-color:rgb(255,255,255);">添加一个hadoop组</span></p><p><span style="background-color:rgb(255,255,255);color:rgb(51,51,51);font-family:Arial;font-size:14px;">sudo usermod -a -G hadoop zzq  #将当前用户zzq加入到hadoop组</span></p><p><span style="background-color:rgb(255,255,255);color:rgb(51,51,51);font-family:Arial;font-size:14px;">sudo gedit etc/sudoers#将hadoop组加入到sudoer</span></p><p><span style="background-color:rgb(255,255,255);color:rgb(51,51,51);font-family:Arial;font-size:14px;">在root ALL=(ALL) ALL后 hadoop ALL=(ALL) ALL</span></p><p><span style="color:rgb(51,51,51);font-family:'Courier new';font-size:18px;line-height:25.7143px;"><br></span></p>            </div>
                </div>