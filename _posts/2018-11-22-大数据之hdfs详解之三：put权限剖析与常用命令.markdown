---
layout:     post
title:      大数据之hdfs详解之三：put权限剖析与常用命令
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<ul>
<li>
</ul>
<p>–无论是对于hdfs的读和写，对于用户来说都是无感知的、透明的操作，用户并不关心数据如何读出来如何写进去的，只要返回一个结果告诉用户数据读出来了或写进去了，至于怎么读怎么写，用户并不关心<br>
补充：<br>
读：hdfs dfs -ls / = hdfs dfs -ls hdfs://hadoop001:9000/<br>
hdfs dfs -ls /      /是hdfs文件系统的根目录  而不是Linux磁盘<br>
→<br>
hdfs dfs -ls hdfs://hadoop001:9000/       其中hdfs://hadoop001:9000是一个参数，来自core-site.xml文件的配置<br>
→<br>
hdfs dfs -ls    读取当前命令操作的用户的路径：/user/用户/<br>
hdfs dfs -ls  =    hdfs dfs -ls /user/hadoop</p>
<ul>
<li>
</ul>
<h2><a id="Hadoop_put_13"></a>Hadoop put权限剖析</h2>
<p>上传文件：将windows上的文件存放到linux上：hdfs dfs -put 目标文件 目标路径<br>
如果文件事先存在或由于权限问题上传不了就会抛错。<br>
→ 如果是文件已存在就抛出文件exists错误<br>
→ 如果是权限问题，抛出这样的错误：<br>
put: Permission denied: user=root, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x<br>
在这种情况下：<br>
建议:<br>
1.切换目录所需的用户  su  - hadoop<br>
2.强行修改对应的用户和用户组：要么是生产管控不严格 或者 是测试学习用<br>
→<br>
生产上一般不会改根目录的权限，而是将文件上传到一个新建的文件夹里面，修改该文件的用户和用户组权限<br>
<img src="https://img-blog.csdn.net/20181013134908234?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N5bHZpYV9ENTA3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>
在root用户下将文件上传到hadoop用户新建的文件夹里面：<br>
<img src="https://img-blog.csdn.net/20181013135028726?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N5bHZpYV9ENTA3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>
[root@hadoop001 hadoop-2.6.0-cdh5.7.0]# bin/hdfs dfs -put NOTICE.txt /hahadata<br>
18/10/13 11:12:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>
没有抛错！<br>
再返回hadoop用户查看这个新建的文件夹里面有没有上传的文件：是有的<br>
这样就解决了文件上传没有权限的问题了，但是生产上一般都不会这样做的！<br>
<img src="https://img-blog.csdn.net/20181013135112425?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N5bHZpYV9ENTA3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"></p>
<ul>
<li>
</ul>
<h2><a id="HDFS_37"></a>HDFS常用命令：</h2>
<p>命令帮助：hdfs dfs （回车）<br>
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs<br>
Usage: hadoop fs [generic options]<br>
[-appendToFile  … ]<br>
[-cat [-ignoreCrc]  …]<br>
[-checksum  …]<br>
[-chgrp [-R] GROUP PATH…]<br>
[-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>
[-chown [-R] [OWNER][:[GROUP]] PATH…]<br>
[-copyFromLocal [-f] [-p] [-l]  … ]<br>
[-copyToLocal [-p] [-ignoreCrc] [-crc]  … ]<br>
[-count [-q] [-h] [-v] <path> …]<br>
[-cp [-f] [-p | -p[topax]]  … ]<br>
[-createSnapshot  []]<br>
[-deleteSnapshot  ]<br>
[-df [-h] [<path> …]]<br>
[-du [-s] [-h] <path> …]<br>
[-expunge]<br>
[-find <path> …  …]<br>
[-get [-p] [-ignoreCrc] [-crc]  … ]<br>
[-getfacl [-R] <path>]<br>
[-getfattr [-R] {-n name | -d} [-e en] <path>]<br>
[-getmerge [-nl]  ]<br>
[-help [cmd …]]<br>
[-ls [-d] [-h] [-R] [<path> …]]<br>
[-mkdir [-p] <path> …]<br>
[-moveFromLocal  … ]<br>
[-moveToLocal  ]<br>
[-mv  … ]<br>
[-put [-f] [-p] [-l]  … ]<br>
[-renameSnapshot   ]<br>
[-rm [-f] [-r|-R] [-skipTrash]  …]<br>
[-rmdir [–ignore-fail-on-non-empty] </path></path></path></path></path></path></path></path></p><dir> …]<br>
[-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} <path>]|[–set &lt;acl_spec&gt; <path>]]<br>
[-setfattr {-n name [-v value] | -x name} <path>]<br>
[-setrep [-R] [-w]  <path> …]<br>
[-stat [format] <path> …]<br>
[-tail [-f] ]<br>
[-test -[defsz] <path>]<br>
[-text [-ignoreCrc]  …]<br>
[-touchz <path> …]<br>
[-usage [cmd …]]</path></path></path></path></path></path></path></dir><p></p>
<p>Generic options supported are<br>
-conf      specify an application configuration file<br>
-D &lt;property=value&gt;            use value for given property<br>
-fs &lt;local|namenode:port&gt;      specify a namenode<br>
-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager<br>
-files     specify comma separated files to be copied to the map reduce cluster<br>
-libjars     specify comma separated jar files to include in the classpath.<br>
-archives     specify comma separated archives to be unarchived on the compute machines.</p>
<p>The general command line syntax is<br>
bin/hadoop command [genericOptions] [commandOptions]</p>
<hr>
<p>hdfs dfs -ls / ：查看/路径下的文件和文件夹<br>
hdfs dfs -put  a.txt / ：上传a.txt文件到 / 目录路径下<br>
hdfs dfs -get  /a.txt ./ ：下载a.txt文件到 ./ 当前目录路径下<br>
hdfs dfs -copyFromLocal a.txt / ：上传a.txt文件到 / 目录路径下<br>
hdfs dfs -copyToLocal /a.txt ./ ：下载a.txt文件到 ./ 当前目录路径下<br>
hdfs dfs -cat /a.txt ：查看文件内容<br>
hdfs dfs -rm /a.txt ：删除文件<br>
hdfs dfs -rmr  /filename ：删除文件夹</p>
<ul>
<li>
</ul>
<p>注意：hadoop fs等价于 hdfs dfs，因为两者调用同一个代码结构<br>
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cat bin/hadoop | grep fs<br>
if [ ‘’$ COMMAND" = “fs” ] ; then<br>
CLASS=org.apache.hadoop.fs.FsShell<br>
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cat bin/hdfs<br>
elif [ “$COMMAND” = “dfs” ] ; then<br>
CLASS=org.apache.hadoop.fs.FsShell</p>
<ul>
<li>
</ul>
<h2><a id="2_114"></a>注意2个命令：</h2>
<p>（1）dfsadmin<br>
dfsadmin             run a DFS admin client<br>
报告hdfs的集群健康状况 (-report)<br>
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ <strong>hdfs dfsadmin</strong><br>
Usage: hdfs dfsadmin<br>
Note: Administrative commands can only be run as the HDFS superuser.<br>
[-report [-live] [-dead] [-decommissioning]]<br>
[-safemode &lt;enter | leave | get | wait&gt;]<br>
[-saveNamespace]<br>
[-rollEdits]<br>
[-restoreFailedStorage true|false|check]<br>
[-refreshNodes]<br>
[-setQuota  …]<br>
[-clrQuota …]<br>
[-setSpaceQuota  …]<br>
[-clrSpaceQuota …]<br>
[-finalizeUpgrade]<br>
[-rollingUpgrade [&lt;query|prepare|finalize&gt;]]<br>
[-refreshServiceAcl]<br>
[-refreshUserToGroupsMappings]<br>
[-refreshSuperUserGroupsConfiguration]<br>
[-refreshCallQueue]<br>
[-refresh <a>host:ipc_port</a>  [arg1…argn]<br>
[-reconfig &lt;datanode|…&gt; <a>host:ipc_port</a> &lt;start|status|properties&gt;]<br>
[-printTopology]<br>
[-refreshNamenodes datanode_host:ipc_port]<br>
[-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]<br>
[-setBalancerBandwidth ]<br>
[-fetchImage ]<br>
[-allowSnapshot ]<br>
[-disallowSnapshot ]<br>
[-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]<br>
[-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]<br>
[-metasave filename]<br>
[-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]<br>
[-help [cmd]]</p>
<p>[hadoop@hadoop001 ~]$ <strong>hdfs dfsadmin -report</strong><br>
18/10/13 11:46:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>
Configured Capacity: 42139451392 (39.25 GB)<br>
Present Capacity: 33108271104 (30.83 GB)<br>
DFS Remaining: 33107415040 (30.83 GB)<br>
DFS Used: 856064 (836 KB)<br>
DFS Used%: 0.00%<br>
Under replicated blocks: 0<br>
Blocks with corrupt replicas: 0<br>
Missing blocks: 0<br>
Missing blocks (with replication factor 1): 0</p>
<hr>
<p>Live datanodes (1):</p>
<p>Name: 172.31.95.246:50010 (hadoop001)<br>
Hostname: hadoop001<br>
Decommission Status : Normal<br>
Configured Capacity: 42139451392 (39.25 GB)<br>
DFS Used: 856064 (836 KB)<br>
Non DFS Used: 9031180288 (8.41 GB)<br>
DFS Remaining: 33107415040 (30.83 GB)<br>
DFS Used%: 0.00%<br>
DFS Remaining%: 78.57%<br>
Configured Cache Capacity: 0 (0 B)<br>
Cache Used: 0 (0 B)<br>
Cache Remaining: 0 (0 B)<br>
Cache Used%: 100.00%<br>
Cache Remaining%: 0.00%<br>
Xceivers: 1<br>
Last contact: Sat Oct 13 11:46:35 CST 2018<br>
这些信息都可以在客户端50070的web界面可以看到，以显示hdfs的集群健康状况</p>
<ul>
<li>
</ul>
<p>（2）安全模式：一种保护功能<br>
hdfs dfsadmin -safemode &lt;enter | leave | get | wait&gt;<br>
log日志说了一句话 NN节点处于 safemode， 只读的，就要离开：hdfs dfsadmin -safemode leave<br>
1、如果是手工让它进入安全模式，代表集群只读，可以做维护，做完维护，hdfs dfsadmin -safemode leave<br>
2、如果是异常情况，比如hdfs损坏（或部署有问题），NN节点自动进入安全模式<br>
a.手工尝试的去hdfs dfsadmin -safemode leave是成功的，进入正常的 -put正常操作<br>
b.手工尝试的去hdfs dfsadmin -safemode leave不成功，那么仔细去看NN节点的log日志，去分析问题出在哪里<br>
→可能是block块损坏</p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>