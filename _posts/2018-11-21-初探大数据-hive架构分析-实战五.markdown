---
layout:     post
title:      初探大数据-hive架构分析-实战五
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>官网地址hive.apache.org<br></p><p><img src="https://img-blog.csdn.net/20180305100245596?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGVub3N0YWxoag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p><br></p><p>架构解析：</p><p><br></p><p>一、Hive底层的执行引擎有：MapReduce、Tez、Spark</p>    Hive on MapReduce<br>    Hive on Tez<br>    Hive on Spark<br><br>压缩：GZIP、LZO、Snappy、BZIP2..<br>存储：TextFile、SequenceFile、RCFile、ORC、Parquet<br>UDF：自定义函数<br><br><p>二、Hive环境搭建</p>1）Hive下载：http://archive.cloudera.com/cdh5/cdh/5/<br>    wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz<br><br>2）解压<br>    tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz -C ~/app/<br><br>3）配置<br>    系统环境变量(~/.bahs_profile)<br>        export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0<br>        export PATH=$HIVE_HOME/bin:$PATH<br><br><p>    实现安装一个mysql， yum install xxx</p><p><br></p><p>    hive-env.sh里面改成实际存付的位置<br>        HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0<br></p>    hive-site.xml<br>    &lt;property&gt;<br>          &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>        &lt;value&gt;jdbc:mysql://localhost:3306/sparksql?createDatabaseIfNotExist=true&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br>        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br>       &lt;/property&gt;<br>    &lt;property&gt;<br>          &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br>        &lt;value&gt;root&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>          &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br>        &lt;value&gt;root&lt;/value&gt;<br>    &lt;/property&gt;<br><br>4）拷贝mysql驱动到$HIVE_HOME/lib/<br><br>5）启动hive: $HIVE_HOME/bin/hive<br><br>6）创建表<br>CREATE  TABLE table_name <br>  [(col_name data_type [COMMENT col_comment])]<br>create table hive_wordcount(context string);<br>7）加载数据到hive表<br>LOAD DATA LOCAL INPATH 'filepath' INTO TABLE tablename <br>load data local inpath '/home/hadoop/data/hello.txt' into table hive_wordcount;<br><p>8）查询</p><p>select word, count(1) from hive_wordcount lateral view explode(split(context,'\t')) wc as word group by word;</p>lateral view explode(): 是把每行记录按照指定分隔符进行拆解<br><br>hive ql提交执行以后会生成mr作业，并在yarn上运行<br>create table emp(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';<br><br>create table dept(<br>deptno int,<br>dname string,<br>location string<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';<br><br>load data local inpath '/home/hadoop/data/emp.txt' into table emp;<br>load data local inpath '/home/hadoop/data/dept.txt' into table dept;<br><br>求每个部门的人数<br>select deptno, count(1) from emp group by deptno;<br><p><br></p><p><br></p><p><br></p>            </div>
                </div>