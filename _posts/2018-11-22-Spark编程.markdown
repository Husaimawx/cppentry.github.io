---
layout:     post
title:      Spark编程
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/crazy_scott/article/details/83216925				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>介绍Spark编程的结构，以及Spark SQL。</p>
<!-- more -->
<h1><a id="Spark_shell_6"></a>Spark shell</h1>
<ul>
<li>Spark Shell 提供了简单的方式来学习Spark API</li>
<li>Spark Shell可以以实时、交互的方式来分析数据</li>
<li>Spark Shell支持Scala和Python</li>
<li>一个Driver就包括main方法和分布式数据集</li>
<li>Spark Shell本身就是一个Driver，里面已经包含了main方法</li>
</ul>
<h1><a id="Spark_RDD_14"></a>Spark RDD</h1>
<h2><a id="RDD_16"></a>RDD操作</h2>
<h3><a id="RDD_18"></a>RDD创建</h3>
<ul>
<li>
<p>从文件系统中加载数据创建RDD，并指定分区的个数</p>
<ul>
<li>本地文件系统</li>
<li>HDFS</li>
<li>其它</li>
</ul>
</li>
<li>
<p>通过并行集合（数组）创建RDD</p>
<ul>
<li>可以将本地的Java对象变为RDD</li>
</ul>
</li>
<li>
<p>创建RDD时手动指定分区个数</p>
<ul>
<li>
<p>在调用textFile()和parallelize()方法的时候手动指定分区个数即可，语法格式如下：</p>
<blockquote>
<p>sc.textFile(path, partitionNum)</p>
</blockquote>
</li>
<li>
<p>其中，path参数用于指定要加载的文件的地址，partitionNum参数用于指定分区个数。</p>
</li>
</ul>
</li>
</ul>
<h2><a id="RDD_Transformation_38"></a>RDD Transformation</h2>
<p><img src="http://wx2.sinaimg.cn/mw690/0060lm7Tly1fwezkqe4evj30ne0cvq71.jpg" alt="54004037642"></p>
<h3><a id="RDD_Repartition_42"></a>RDD Repartition</h3>
<ul>
<li>通过转换操作得到新RDD 时，直接调用repartition 方法或自定义分区方法</li>
<li>什么使用用到该方法？
<ul>
<li>当遇到某些操作，如<code>join</code>，则可将宽依赖Partition之后变为窄依赖，便于pipeline执行</li>
</ul>
</li>
</ul>
<h2><a id="RDD_Action_48"></a>RDD Action</h2>
<ul>
<li>惰性机制：整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，<strong>只有遇到action操作时，才会发生真正的计算</strong>，开始从血缘关系源头开始，进行物理的转换操作操</li>
</ul>
<h2><a id="RDD_52"></a>RDD保存</h2>
<ul>
<li>RDD写入到本地文本文件</li>
<li>RDD中的数据保存到HDFS文件中</li>
</ul>
<h1><a id="Spark_SQL_57"></a>Spark SQL</h1>
<ul>
<li>Spark SQL增加了DataFrame（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据</li>
<li>Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范</li>
</ul>
<h2><a id="Schema_62"></a>Schema</h2>
<ul>
<li>RDD的局限性：RDD是分布式Java对象的集合，但是对象内部结构，即数据Schema不可知</li>
<li><img src="http://wx2.sinaimg.cn/mw690/0060lm7Tly1fwezol7pm0j30gf09b0wc.jpg" alt="54004061559"></li>
</ul>
<h2><a id="DataFrame_67"></a>DataFrame</h2>
<ul>
<li>无论读取什么数据，都写成<code>DataSet&lt;Row&gt;</code>
<ul>
<li><img src="http://wx2.sinaimg.cn/mw690/0060lm7Tly1fwezpe9qgwj30j207yt9a.jpg" alt="54004065924"></li>
</ul>
</li>
</ul>
<h2><a id="DataSet_72"></a>DataSet</h2>
<ul>
<li>相比DataFrame，DataSet明确声明类型
<ul>
<li><img src="http://wx3.sinaimg.cn/mw690/0060lm7Tly1fwezq2f7inj30km07eq3e.jpg" alt="54004070261"></li>
</ul>
</li>
<li>在源码中，可以将DataFrame理解为<code>DataSet&lt;Row&gt;</code>的别名</li>
<li>若查询语句中有一列不存在，则可以在编译时检查出来</li>
</ul>
<h2><a id="SQL_Query_79"></a>SQL Query</h2>
<ul>
<li>sql(“”)括号中的SQL语句对于该函数来说仅仅是一条字符串
<ul>
<li><img src="http://wx1.sinaimg.cn/mw690/0060lm7Tly1fwezr7bw7uj30dz09jq3c.jpg" alt="54004076814"></li>
</ul>
</li>
<li>编译时不会进行任何语法检查</li>
</ul>
<h2><a id="_85"></a>比较</h2>
<p><img src="http://wx1.sinaimg.cn/mw690/0060lm7Tly1fwezs6or8zj30k608yt9i.jpg" alt="54004082500"></p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>