---
layout:     post
title:      第51课： Spark中的新解析引擎Catalyst源码SQL最终转化为RDD具体实现
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：王家林大咖2018年新书《SPARK大数据商业实战三部曲》清华大学出版，清华大学出版社官方旗舰店（天猫）https://qhdx.tmall.com/?spm=a220o.1000855.1997427721.d4918089.4b2a2e5dT6bUsM					https://blog.csdn.net/duan_zhihua/article/details/73274151				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>第51课： Spark中的新解析引擎Catalyst源码SQL最终转化为RDD具体实现</p><p></p><p>基于DataSet的代码转换为RDD之前需要一个Action的操作，基于Spark中的新解析引擎Catalyst进行优化，Spark中的Catalyst不仅限于SQL的优化，Spark的五大子框架（Spark Cores、Spark SQL、Spark Streaming、Spark GraphX、Spark Mlib）将来都会基于Catalyst基础之上。</p><p>Dataset.scala的collect方法源码：</p><p>1.            defcollect(): Array[T] = collect(needCallback = true)</p><p>进入collect(needCallback = true)方法：</p><p>1.           private def collect(needCallback: Boolean):Array[T] = {</p><p>2.             def execute(): Array[T] =withNewExecutionId {</p><p>3.               queryExecution.executedPlan.executeCollect().map(boundEnc.fromRow)</p><p>4.             }</p><p>5.          </p><p>6.             if (needCallback) {</p><p>7.               withCallback("collect",toDF())(_ =&gt; execute())</p><p>8.             } else {</p><p>9.               execute()</p><p>10.          }</p><p>11.        }</p><p><span style="color:#FF0000;"> </span></p><p align="left">其中关键的一行代码是queryExecution.executedPlan.executeCollect().map(boundEnc.fromRow)，我们看一下executedPlan。executedplan不用来初始化任何SparkPlan，仅用于执行。</p><p>QueryExecution.scala的源码：</p><p>1.         class QueryExecution(valsparkSession: SparkSession, val logical: LogicalPlan) {</p><p>2.         ……</p><p>3.         // executedPlan should not beused to initialize any SparkPlan. It should be</p><p>4.           // only used for execution.</p><p>5.           lazy val executedPlan: SparkPlan =prepareForExecution(sparkPlan)</p><p>6.         ……</p><p>7.              lazyval toRdd: RDD[InternalRow] = executedPlan.execute()</p><p>8.         ……</p><p> queryExecution.executedPlan.executeCollect()其中的executeCollect方法运行此查询，将结果作为数组返回。executeCollect方法调用了byteArrayRdd.collect()方法。</p><p>SparkPlan .scala的executeCollect源码如下：</p><p>1.           def executeCollect(): Array[InternalRow] = {</p><p>2.             val byteArrayRdd = getByteArrayRdd()</p><p>3.          </p><p>4.             val results = ArrayBuffer[InternalRow]()</p><p>5.             byteArrayRdd.collect().foreach { bytes=&gt;</p><p>6.               decodeUnsafeRows(bytes).foreach(results.+=)</p><p>7.             }</p><p>8.             results.toArray</p><p>9.           }   </p><p> </p><p>byteArrayRdd.collect()方法调用RDD.scala的collect方法，collect方法最终通过sc.runJob提交Spark集群运行。</p><p>RDD.scala的collect方法源码：</p><p>1.            defcollect(): Array[T] = withScope {</p><p>2.             val results = sc.runJob(this, (iter:Iterator[T]) =&gt; iter.toArray)</p><p>3.             Array.concat(results: _*)</p><p>4.           }</p><p> </p><p> 回到QueryExecution.scala中，executedPlan.execute()是关键性的代码。</p><p>1.              lazyval toRdd: RDD[InternalRow] = executedPlan.execute()</p><p>        进入SparkPlan.scala的execute返回查询结果类型为RDD[InternalRow]。调用`doExecute`</p><p>执行，SparkPlan应重写`doExecute`进行具体实现。在execute 方法就生成了RDD[InternalRow]。execute源码方法：</p><p>1.           final def execute(): RDD[InternalRow] =executeQuery {</p><p>2.             doExecute()</p><p>3.           }   </p><p>SparkPlan.scala的doExecute()抽象方法没有具体实现，通过SparkPlan具体实现重写。产生的查询结果作为RDD[InternalRow]。</p><p>1.              protected def doExecute(): RDD[InternalRow]</p><p> </p><p>InternalRow是通过语法树生成的一些数据结构。其子类包括BaseGenericInternalRow、JoinedRow、Row、UnsafeRow</p><p>InternalRow.scala源码：</p><p>1.         abstract class InternalRowextends SpecializedGetters with Serializable {</p><p>2.         ……   </p><p>3.           def setBoolean(i: Int, value: Boolean): Unit= update(i, value)</p><p>4.           def setByte(i: Int, value: Byte): Unit =update(i, value)</p><p>5.           def setShort(i: Int, value: Short): Unit =update(i, value)</p><p>6.           def setInt(i: Int, value: Int): Unit =update(i, value)</p><p>7.           def setLong(i: Int, value: Long): Unit =update(i, value)</p><p>8.           def setFloat(i: Int, value: Float): Unit =update(i, value)</p><p>9.           def setDouble(i: Int, value: Double): Unit =update(i, value)</p><p>10.      ……..</p><p> </p><p>DataSet的代码转化成为RDD的内部流程如下：</p><p>Parse SQL(DataSet) -&gt; AnalyzeLogical Plan -&gt; Optimize Logical Plan -&gt; Generate Physical Plan-&gt;Prepareed Spark Plan -&gt; Execute SQL -&gt; Generate RDD</p><p>基于DataSet的代码一步步转化成为RDD：最终是调用execute()生成RDD。</p><p> </p><br>            </div>
                </div>