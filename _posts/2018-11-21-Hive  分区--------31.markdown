---
layout:     post
title:      Hive  分区--------31
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><span style="font-size:12pt;font-family:'微软雅黑', sans-serif;">访问<span lang="en-us" xml:lang="en-us">hive</span>的另外一种方法：进入到<span lang="en-us" xml:lang="en-us">apache-hve-2.3.2-bin.tar/bin</span>目录下</span></p><p><span style="font-size:12pt;font-family:'微软雅黑', sans-serif;"><img src="https://img-blog.csdn.net/20180519091356644?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></span></p><p>复制一个ssh<span style="background-color:rgb(255,255,255);">通道，在输入   </span><strong> ./beeline -u jdbc:hive2://    注：第一次启动hive2和修改文件后进去需要输入hiveserve2，不然可直接输入./beeline -u jdbc:hive2://命令进入。</strong><span style="font-size:12pt;font-family:'微软雅黑', sans-serif;"></span></p><p style="background:#FFFFFF;"><strong><img src="https://img-blog.csdn.net/20180519091820614?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></strong></p><p style="background:#FFFFFF;">在jupyter里写SQL语句格式：spark.sql("SQL语句")<strong></strong></p><p align="left"><strong><span style="color:#2C3033;"><img src="https://img-blog.csdn.net/20180521094106119?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;">hive 语法（创建一个表）：设计分区，有内表和外表</span></strong></p><p align="left"><strong><span style="color:#2C3033;">CREATE [TEMPORARY] [EXTERNAL]（加上这个就是外表） TABLE [IF NOT EXISTS]（如果不存在我就去创建）</span></strong></p><p align="left"><span style="color:#2c3033;"><strong>[db_name.]（如果不写数据库名，默认当前数据库）table_name   </strong></span></p><p align="left"><span style="color:#2c3033;"><strong>[(col_name data_type[COMMENT col_comment],…</strong></span><span style="color:rgb(44,48,51);">[constraint_specification])]</span></p><p align="left"><strong><span style="color:#2C3033;">[COMMENT table_comment]</span></strong></p><p align="left"><strong><span style="color:#2C3033;">[PARTITIONED BY (col_name data_type[COMMENT col_comment],…)]   创建分区（相当于目录）、列名、描述</span></strong></p><p align="left"><strong><span style="color:#2C3033;">[CLUSTERED BY (col_name,col_name,…)[SORTED BY (col_name[ASC|DESC],…)] INTO num_buckets BUCKETS] 桶相当于是分区里面的文件</span></strong></p><p align="left"><span style="color:#2c3033;"><strong>[LOCATION hdfs_path]指定位置，存到什么地方</strong></span></p><p align="left"><span style="color:#2c3033;"><strong>create [临时表][外部表]（不写这个就是创建一个普通的内部表） table tablename</strong></span></p><p align="left"><strong><span style="color:#2C3033;">外部表特点：只是存储元数据，删除外部表只是删除了元数据，表并没有删；而内部表删除表全部都删掉。</span></strong></p><p align="left"><span style="color:#2c3033;"><strong>[ROW FORMAT row_format] 数据量大，不可能一条一条插入，行进行格式化，一条一条进行拆分。</strong></span></p><p align="left"><span style="color:#2c3033;"><strong>row format delimited fields terminated by ',';  以后往这个表插入数据按照，号进行拆分</strong></span></p><p align="left"><span style="color:#2c3033;"><strong>load data local inpath '/home/hadoop/student.data' into table tb_student; 导入本地文件</strong></span></p><p align="left"><strong><span style="color:#2C3033;"><img src="https://img-blog.csdn.net/20180521103052316?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;">可以这样单挑插入：</span></strong></p><p align="left"><strong><span style="color:#2C3033;"><img src="https://img-blog.csdn.net/20180521105332795?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></span></strong></p><p align="left"><span style="color:#2c3033;"><strong>插入本地文件到表里，不是本地文件就不用写local   ,在路径后 into前 可加OVERWRITE 覆盖存才数据</strong></span></p><p align="left"><img src="https://img-blog.csdn.net/2018052110573643?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" style="color:rgb(44,48,51);font-weight:bold;" alt=""></p><p align="left"><strong><span style="color:#2C3033;"><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;">create table student_python5(id int,name varchar(20),sex varchar(20),age int,grade_id int)partitioned by(par string) clustered by(age) sorted by (age desc) into 5 buckets row format delimited fieldsterminated by ',';<br></span></strong></p><p align="left"><strong><span style="color:#2C3033;"><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;"><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;"><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;"><br></span></strong></p><p align="left"><strong><span style="color:#2C3033;">HIVE</span><span style="color:#2C3033;">与mysql</span>的关系 hive常用命令整理 hive与hdfs整合过程</strong></p><p style="background:#FFFFFF;"><span style="color:#3D464D;">Hive</span><span style="color:#3D464D;">是一个基于hadoop</span>的数据仓库平台。通过hive，我们可以方便地进行ETL的工作。hive定义了一个类似于SQL的查询语言：HQL，能够将用户编写的QL转化为相应的Mapreduce程序基于Hadoop执行。</p><p><strong><strong><span style="color:#E53333;background:#FFFFFF;">Hive</span></strong><span style="color:#3D464D;background:#FFFFFF;"> </span><strong><span style="color:#E53333;background:#FFFFFF;">可以看成是从HQL</span>到Map-Reduce的</strong><span style="color:#3D464D;background:#FFFFFF;"> </span><strong><span style="color:#E53333;background:#FFFFFF;">映射器，hive并没有存贮数据</span></strong></strong></p><p><strong><strong><span style="color:#E53333;background:#FFFFFF;"></span></strong></strong></p><p style="background:#FFFFFF;"><span style="color:#3D464D;">hive数据在HDFS</span>的warehouse目录下，一个表对应一个子目录。</p><span style="color:#3D464D;">本地的/tmp</span>目录存放日志和执行计划<br><p><strong><span style="color:#3D464D;">内嵌模式：元数据保持在内嵌的Derby</span>模式，只允许一个会话连接</strong></p><p><strong></strong></p><p style="background:#FFFFFF;"><span style="color:#3D464D;">本地独立模式：在本地安装Mysql</span>，把元数据放到Mysql内</p><p><strong><span style="color:#3D464D;">远程模式：元数据放置在远程的Mysql</span>数据库。</strong></p><p><strong><strong><span style="color:#3D464D;">hive</span><span style="color:#3D464D;">的表分为两种，内表和外表。</span></strong><span style="color:#3D464D;"> <br>Hive </span><span style="color:#3D464D;">创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。 <br></span></strong></p><p><strong>在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</strong></p><p><strong>hadoop集群中有两种节点，一种是namenode，另一种是DataNode。其中DataNode主要负责数据存储，namenode主要负责三个功能，分别是1管理元数据2维护目录树3响应客户请求</strong></p><p><strong><img src="https://img-blog.csdn.net/2018052109271674?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hbmdndW95YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></strong></p><p><strong></strong></p><p style="background-color:rgb(255,255,255);">hdfs在外界看来就是普通的文件系统，可以通过路径进行数据的访问等操作，但在实际过程存储中，却是分布在各个节点上。如上图所示，是一条元数据，/test/a.log 是在hdfs文件系统中的路径，3是这个文件的副本数(副本数可以通过在配置文件中的配置来修改的)。在hdfs中，文件是进行分块存储的，如果文件过大，就要分成多块存储，每个块在文件系统中存储3个副本，以上图为例，就是分成blk_1和blk_2两个块，每个块在实际的节点中有3个副本，比如blk_1的3个副本分别存储在h0，h1，h3中。</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>现在由此引出一个问题，namenode中的元数据是存储在哪里的？首先，我们做个假设，如果存储在namenode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断点，元数据丢失，整个集群就无法工作了！！！因此必须在磁盘中有备份，在磁盘中的备份就是fsImage，存放在namenode节点对应的磁盘中。这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新fsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦namenode节点断点，就会产生数据丢失。因此，引入edits.log文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到edits.log中。这样，一旦namenode节点断电，可以通过fsImage和edits.log的合并，合成元数据。但是，如果长时间添加数据到edit.log中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行fsImage和edits.log的合并，如果这个操作有namenode节点完成，又会效率过低。因此，引入一个新的节点secondaryNamenode，专门用于fsImage和edits.log的合并。</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>检查点处理过程的具体步骤如下</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>1)namenode节点每隔一定时间请求secondaryNamenode合并操作</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>2)secondaryNamenode请求namenode进行edits.log的滚动，这样新的编辑操作就能够进入新的文件中</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>3)secondaryNamenode从namenode中下载fsImage和edits.log</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>4)secondaryNamenode进行fsImage和edits.log的合并,成为fsImage.checkpoint文件</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>5)namenode下载合并后的fsImage.checkpoin文件</p><p style="background-color:rgb(255,255,255);"><span style="white-space:pre;"></span>6)将fsImage.checkpoint和edits.new命名为原来的文件名(这样之后fsImage和内存中的元数据只差edits.new)</p><br><p><strong><br></strong></p><p><strong><br></strong></p>            </div>
                </div>