---
layout:     post
title:      hive的介绍及安装
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/firstchange/article/details/78567978				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2 id="hive">hive</h2>

<ul>
<li><p>hive简介</p>

<pre><code>hive的产生
    非java编程者对hdfs的数据做mapreduce操作
什么是hive
    Hive : 数据仓库。
    Hive：解释器，编译器，优化器等。
    Hive 运行时，元数据存储在关系型数据库里面。
</code></pre></li>
<li><p>hive架构</p>

<pre><code>用户接口
    CLI，Client 和 WUI。
        CLI
            其中最常用的是CLI，Cli启动的时候，会同时启动一个Hive副本。
        Client
            Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。 
        WUI
            WUI是通过浏览器访问Hive。
元数据的存储
    Hive将元数据存储在数据库中，如mysql、derby。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。
解释器、编译器、优化器
    解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行。
数据的存储
    Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（包含*的查询，比如select * from tbl不会生成MapRedcue任务）。
补充
    编译器
        编译器将一个Hive SQL转换操作符
    操作符
        操作符是Hive的最小的处理单元
        每个操作符代表HDFS的一个操作或者一道MapReduce作业
        图
    ANTLR词法语法分析工具解析hql
        EL表达式是用ANTLR解析的
        图
</code></pre></li>
<li><p>hive集群的三种模式</p>

<pre><code>划分依据
    元数据的存放及管理
本地模式
    local模式。
    此模式连接到一个In-memory 的数据库Derby，一般用于Unit Test。

单用户模式
    单用户模式。
    通过网络连接到一个数据库中，是最经常使用到的模式

多用户远程模式
    远程服务器模式/多用户模式。
    用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库
</code></pre></li>
<li><p>hive部署安装</p>

<pre><code>检查环境
    安装mysql
        使用yum安装
            yum  install mysql-server
        启动mysql服务
            service mysqld start
        设置开机启动
            chkconfig mysqld on
            chkconfig mysqld --list
    配置mysql
        修改权限
            进入mysql
            使用mysql库
            修改user表
                GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123' WITH GRANT OPTION;
                使其可以进行外部访问mysql的全部表
                删除表中的冲突信息
                    delete from user where host != '%';
                刷新权限
                    flush privileges;
    退出再登录
</code></pre></li>
<li><p>启动hadoop</p>

<pre><code>    启动zookeeper
        zkServer.sh start
        zkServer.sh status
    启动journalnode
        hadoop-deamon.sh start journalnode
    启动NameNode
        hadoop-daemon.sh start namenode
    同步NameNode
        hdfs namenode -bootstrapStandby
    格式化zkfc
        hdfs zkfc -formatZK
    启动集群
        start-dfs.sh
        start-yarn.sh
        start-all.sh
    启动resourcemanager
        yarn-daemon.sh start resourcemanager
    通过jps和浏览器检查集群启动情况
</code></pre></li>
<li><p>单用户模式</p>

<pre><code>    安装hive
        上传压缩包
        修改/etc/profile环境变量
            按tab检查是否生效
        配置文件
            重命名配置文件
            将原有配置删除
                :.,$-1d
            加入现有配置文件
                表的存放：

                元数据是否自身管理：

                mysql连接配置：

        jar包
            将mysql的驱动包放在lib下
            同步hadoop和hive的jline包
        启动hive并验证
        注意点
            地址和端口号
            数据库的用户的密码
</code></pre></li>
<li><p>多用户模式</p>

<pre><code>    上传压缩包
    修改/etc/profile环境变量
        按tab检查是否生效
    配置文件
        服务端
            文件存放路径

            mysql配置

        客户端
            文件存放路径

            是否开启本地

            服务端的地址和端口

    jar包
        服务端
            只需要mysql驱动包
        客户端
            只需要同步jline包
    开启
        先开服务端
            hive --service metastore
        再开客户端
    注意点
        地址和端口号
        数据库的用户的密码
</code></pre></li>
<li><p>hive SQL</p>

<pre><code>hive的官网
    hive.apache.org

DDL
    创建表
        hive内部表
            删除表时，元数据与数据都会被删除
        hive外部表
            删除外部表只删除metastore的元数据，不删除hdfs中的表数据
        使用like建表
            CREATE TABLE empty_key_value_store LIKE key_value_store;
        使用as select建表
            CREATE TABLE new_key_value_store 
                  AS
                SELECT columA, columB FROM key_value_store;
    分区
        必须在表定义时指定对应的partition字段
            单分区建表语句
                单分区表，按天分区，在表结构中存在id，content，dt三列。
                以dt为文件夹区分
            双分区建表语句
                双分区表，按天和小时分区，在表结构中新增加了dt和hour两列。
                先以dt为文件夹，再以hour子文件夹区分
        Hive添加分区表语法
            表已创建，在此基础上添加分区
            ALTER TABLE day_table ADD PARTITION (dt='2008-08-08', hour='08')
        Hive删除分区语法
            用户可以用 ALTER TABLE DROP PARTITION 来删除分区。
            内部表中、对应分区的元数据和数据将被一并删除。
            不需要指定全部的分区信息
        Hive向指定分区添加数据语法
            当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制至Hive表对应的位置。数据加载时在表下自动创建一个目录
        Hive查询执行分区语法
            分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。
DML
    Hive 查看表描述
        DESCRIBE [EXTENDED|FORMATTED] table_name
    from…insert…select 
        为什么from要写在前面
SerDe序列化反序列化
    SerDe 用于做序列化和反序列化。
    构建在数据存储和执行引擎之间，对两者实现解耦。
    Hive通过ROW FORMAT DELIMITED以及SERDE进行内容的读写
    案例
        hive正则匹配
Beeline
    Beeline 要与HiveServer2配合使用
    服务端启动hiveserver2
    客户的通过beeline两种方式连接到hive
        1、beeline -u jdbc:hive2://localhost:10000/default -n root
        2、beeline
            beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;db&gt;;auth=noSasl root 123
    默认 用户名、密码不验证
        为什么
            效率
            内网，安全性
JDBC
    Hive JDBC运行方式
        服务端启动hiveserver2后，在java代码中通过调用hive的jdbc访问默认端口10000进行连接、访问
</code></pre></li>
<li><p>一定要养成良好的习惯</p>

<pre><code>1、必须要看文档--最基本、标准
2、看日志，看报错信息，明白问题在哪里
</code></pre></li>
</ul>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>