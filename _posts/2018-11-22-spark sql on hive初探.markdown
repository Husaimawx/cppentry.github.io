---
layout:     post
title:      spark sql on hive初探
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<span style="color:#555555;"><span style="font-family:'宋体';">   前一段时间由于shark项目停止更新，sql on spark拆分为两个方向，一个是spark sql on hive,另一个是hive on spark。hive on spark达到可用状态估计还要等很久的时间，所以打算试用下spark sql on hive，用来逐步替代目前mr on hive的工作。</span></span><br><span style="color:#555555;"><span style="font-family:'宋体';">  当前试用的版本是spark1.0.0,如果要支持hive,必须重新进行编译，编译的命令有所变化  </span></span><br><div class="blockcode">
<div id="code_V61">
<ol><li>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"<br></li><li>mvn -Pyarn -Phive -Dhadoop.version=2.3.0-cdh5.0.0 -DskipTests clean package</li></ol></div>
<em>复制代码</em></div>
<span style="color:#555555;"><span style="font-family:'宋体';">  写了段比较简单的代码</span></span><br><div class="blockcode">
<div id="code_E0i">
<ol><li>val conf = new SparkConf().setAppName("SqlOnHive")<br></li><li>    val sc = new SparkContext(conf)<br></li><li>    val hiveContext = new HiveContext(sc)<br></li><li>    import hiveContext._<br></li><li>    hql("FROM tmp.test SELECT id limit 1").foreach(println)</li></ol></div>
<em>复制代码</em></div>
<span style="color:#555555;"><span style="font-family:'宋体';">编译后export出jar文件，使用standalone模式，故采用java -cp的方式提交，提交之前需要将hive-site.xml文件copy到$SPARK_HOME/conf目录下</span></span><br><div class="blockcode">
<div id="code_C8f">
<ol><li>java -XX:PermSize=256M -cp /home/hadoop/hql.jar com.yintai.spark.sql.SqlOnHive spark://h031:7077</li></ol></div>
<em>复制代码</em></div>
<span style="color:#555555;"><span style="font-family:'宋体';">提交后会报异常</span></span><br><div class="blockcode">
<div id="code_gEQ">
<ol><li>java.lang.RuntimeException: Error in configuring object<br></li><li>    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)<br></li><li>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)<br></li><li>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br></li><li>    at org.apache.spark.rdd.HadoopRDD.getInputFormat(HadoopRDD.scala:155)<br></li><li>    at org.apache.spark.rdd.HadoopRDD$anon$1.&lt;init&gt;(HadoopRDD.scala:187)<br></li><li>    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:181)<br></li><li>    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)<br></li><li>    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)<br></li><li>    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)<br></li><li>    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)<br></li><li>    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)<br></li><li>    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)<br></li><li>    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)<br></li><li>    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)<br></li><li>    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)<br></li><li>    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)<br></li><li>    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)<br></li><li>    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)<br></li><li>    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)<br></li><li>    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)<br></li><li>    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)<br></li><li>    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)<br></li><li>    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)<br></li><li>    at org.apache.spark.scheduler.Task.run(Task.scala:51)<br></li><li>    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)<br></li><li>    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)<br></li><li>    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)<br></li><li>    at java.lang.Thread.run(Thread.java:662)<br></li><li>Caused by: java.lang.reflect.InvocationTargetException<br></li><li>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br></li><li>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br></li><li>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br></li><li>    at java.lang.reflect.Method.invoke(Method.java:597)<br></li><li>    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)<br></li><li>    ... 27 more<br></li><li>Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.<br></li><li>    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135)<br></li><li>    at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175)<br></li><li>    at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)<br></li><li>    ... 32 more<br></li><li>Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzoCodec not found<br></li><li>    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801)<br></li><li>    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)<br></li><li>    ... 34 more<br></li><li><br></li></ol></div>
<em>复制代码</em></div>
<br><br><span style="color:#555555;"><span style="font-family:'宋体';">解决办法是需要设置相关的环境变量，在spark-env.sh中设置</span></span><br><div class="blockcode">
<div id="code_xVg">
<ol><li>SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/path/to/your/hadoop-lzo/libs/native<br></li><li>    SPARK_CLASSPATH=$SPARK_CLASSPATH:/path/to/your/hadoop-lzo/java/libs</li></ol></div>
<em>复制代码</em></div>
<br><br><span style="color:#555555;"><span style="font-family:'宋体';">修改过环境变量之后重新提交，继续报错</span></span><br><div class="blockcode">
<div id="code_I55">
<ol><li>14/07/23 10:25:19 ERROR RetryingHMSHandler: NoSuchObjectException(message:There is no database named tmp)<br></li><li>        at org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:431)<br></li><li>        at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:441)<br></li><li>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br></li><li>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br></li><li>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br></li><li>        at java.lang.reflect.Method.invoke(Method.java:597)<br></li><li>        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)<br></li><li>        at com.sun.proxy.$Proxy9.getDatabase(Unknown Source)<br></li><li>        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:628)<br></li><li>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br></li><li>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br></li><li>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br></li><li>        at java.lang.reflect.Method.invoke(Method.java:597)<br></li><li>        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)<br></li><li>        at com.sun.proxy.$Proxy10.get_database(Unknown Source)<br></li><li>        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:810)<br></li><li>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br></li><li>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br></li><li>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br></li><li>        at java.lang.reflect.Method.invoke(Method.java:597)<br></li><li>        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)<br></li><li>        at com.sun.proxy.$Proxy11.getDatabase(Unknown Source)<br></li><li>        at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)<br></li><li>        at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)<br></li><li>        at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)<br></li><li>        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)<br></li><li>        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)<br></li><li>        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:185)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:160)<br></li><li>        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd$lzycompute(HiveContext.scala:249)<br></li><li>        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd(HiveContext.scala:246)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scala:85)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:90)<br></li><li>        at com.yintai.spark.sql.SqlOnHive$.main(SqlOnHive.scala:20)<br></li><li>        at com.yintai.spark.sql.SqlOnHive.main(SqlOnHive.scala)<br></li><li><br></li><li>14/07/23 10:25:19 ERROR DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: tmp<br></li><li>        at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3480)<br></li><li>        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)<br></li><li>        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)<br></li><li>        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)<br></li><li>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:185)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:160)<br></li><li>        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd$lzycompute(HiveContext.scala:249)<br></li><li>        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd(HiveContext.scala:246)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scala:85)<br></li><li>        at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:90)<br></li><li>        at com.yintai.spark.sql.SqlOnHive$.main(SqlOnHive.scala:20)<br></li><li>        at com.yintai.spark.sql.SqlOnHive.main(SqlOnHive.scala)</li></ol></div>
<em>复制代码</em></div>
<span style="color:#555555;"><span style="font-family:'宋体';">造成这个错误的原因就是spark程序无法加载到hive-site.xml，从而无法获取到远程metastore服务的地址，只能在本地的derby数据库中查找，自然找不到相关库表的元数据信息。spark sql实际上是通过实例化HiveConf类来加载hive-site.xml文件的，这个跟hive cli的方式是一致的，代码如下</span></span><br><div class="blockcode">
<div id="code_sIR">
<ol><li>ClassLoader classLoader = Thread.currentThread().getContextClassLoader();<br></li><li>    if (classLoader == null) {<br></li><li>      classLoader = HiveConf.class.getClassLoader();<br></li><li>    }<br></li><li>    hiveDefaultURL = classLoader.getResource("hive-default.xml");<br></li><li>    // Look for hive-site.xml on the CLASSPATH and log its location if found.<br></li><li>    hiveSiteURL = classLoader.getResource("hive-site.xml");<br></li><li><br></li></ol></div>
<em>复制代码</em></div>
<span style="color:#555555;"><span style="font-family:'宋体';">使用java -cp提交的方式不能正确的设置环境变量，在1.0.0版本中，新增了使用spark-submit脚本进行提交的方式，后来改为使用此脚本来提交</span></span><br><div class="blockcode">
<div id="code_KNc">
<ol><li>/usr/lib/spark/bin/spark-submit --class com.yintai.spark.sql.SqlOnHive <br></li><li>    --master spark://h031:7077 <br></li><li>    --executor-memory 1g <br></li><li>    --total-executor-cores 1 <br></li><li>    /home/hadoop/hql.jar</li></ol></div>
<em>复制代码</em></div>
<p style="line-height:28px;text-align:left;"><span style="color:#555555;"><span style="font-family:'宋体';">这个脚本在提交过程中会设置SparkConf中的spark.executor.extraClassPath和spark.driver.extraClassPath属性，从而保证可以正确加载到所需的配置文件，到此测试成功。</span></span></p>
<p style="line-height:28px;text-align:left;"><span style="color:#555555;"><span style="font-family:'宋体';">    目前spark sql on hive兼容了大部分hive的语法和UDF，在SQL解析的时候使用了Catalyst框架，作业在运行效率上高出hive很多，不过目前的版本还存在一些BUG，稳定性上会有一些问题，需要等到新的稳定版发布再进行进一步的测试。</span></span></p>
<br><p style="line-height:28px;text-align:left;"><span style="color:#555555;"><span style="font-family:'宋体';">    参考资料<br></span></span></p>
<p style="line-height:28px;text-align:left;"><span style="color:#555555;"><span style="font-family:'宋体';">   
<span><a class="gj_safe_a" href="http://spark.apache.org/docs/1.0.0/sql-programming-guide.html" rel="nofollow">http://spark.apache.org/docs/1.0.0/sql-programming-guide.html</a></span></span></span></p>
<p style="line-height:28px;text-align:left;"><span style="color:#555555;"><span style="font-family:'宋体';"><span><a class="gj_safe_a" href="http://hsiamin.com/posts/2014/05/03/enable-lzo-compression-on-hadoop-pig-and-spark/" rel="nofollow">[url]http://hsiamin.com/posts/2014/05/03/enable-lzo-compression-on-hadoop-pig-and-spark/</a>[/url]</span></span></span></p>
<p style="line-height:28px;text-align:left;"><span style="color:#555555;"><span style="font-family:'宋体';">本文出自 “<span><a class="gj_safe_a" href="http://xiaowuliao.blog.51cto.com/" rel="nofollow">17的博客</a></span>”
 博客，请务必保留此出处<span><a class="gj_safe_a" href="http://xiaowuliao.blog.51cto.com/3681673/1441737" rel="nofollow">http://xiaowuliao.blog.51cto.com/3681673/1441737</a></span></span></span></p>
            </div>
                </div>