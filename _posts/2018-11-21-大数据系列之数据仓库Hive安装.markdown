---
layout:     post
title:      大数据系列之数据仓库Hive安装
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/mengfanzhundsc/article/details/81300326				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body"><p><span style="font-size:1em;">Hive系列博文，持续更新~~~</span></p>
<h4><a id="post_title_link_6684615" href="http://www.cnblogs.com/cnmenglang/p/6684615.html" rel="nofollow">大数据系列之数据仓库Hive原理</a></h4>
<h4><a id="post_title_link_6661488" href="http://www.cnblogs.com/cnmenglang/p/6661488.html" rel="nofollow">大数据系列之数据仓库Hive安装</a></h4>
<h4><a id="post_title_link_6683911" href="http://www.cnblogs.com/cnmenglang/p/6683911.html" rel="nofollow">大数据系列之数据仓库Hive中分区Partition如何使用</a></h4>
<h4><a id="post_title_link_6683897" href="http://www.cnblogs.com/cnmenglang/p/6683897.html" rel="nofollow">大数据系列之数据仓库Hive命令使用及JDBC连接</a></h4>
<p> </p>
<p><img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170403103523925-1397785631.png" alt="" width="676" height="155"></p>
<p>Hive主要分为以下几个部分</p>
<p>⽤户接口<br>1.包括CLI，JDBC/ODBC，WebUI<br>元数据存储（metastore）<br>1.默认存储在⾃带的数据库derby中，线上使⽤时⼀般换为MySQL<br>驱动器（Driver）<br>1.解释器、编译器、优化器、执⾏器<br>Hadoop<br>1.⽤MapReduce 进⾏计算，⽤HDFS 进⾏存储</p>
<p> </p>
<p>前提部分：Hive的安装需要在Hadoop已经成功安装且成功启动的基础上进行安装。若没有安装请移步至<a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/cnmenglang/p/6530719.html" rel="nofollow">大数据系列之Hadoop分布式集群部署</a>。</p>
<p>使用包: apache-hive-2.1.1-bin.tar.gz, <span style="font-family:'Courier New';font-size:12px;color:#000000;">mysql-connector-java-5.1.27-bin.jar</span></p>
<p> <a href="https://pan.baidu.com/s/1o7VcNDS" rel="nofollow">云盘</a>,密码:seni</p>
<p>本文将Hive安装在Hadoop Master节点上，以下操作仅在master服务器上进行操作。</p>
<p>1.　切换至普通用户 su mfz</p>
<p>2. 将gz包上传至目录下</p>
<p>　　/home/mfz</p>
<p>3.解压</p>
<div class="cnblogs_Highlighter">
<pre><code class="language-csharp">tar -xzvf apache-hive-2.1.1-bin.tar.gz
</code></pre>
</div>
<p> 4.目录：</p>
<p><img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170403104854066-177922726.png" alt="" width="447" height="209"></p>
<p>5.创建hive-site.xml</p>
<div class="cnblogs_Highlighter">
<p>&lt;?xml version="1.0"?&gt; &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</p>
<p>&lt;configuration&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;hive.metastore.uris&lt;/name&gt;</p>
<p>        &lt;value&gt;thrift://localhost:9083&lt;/value&gt;</p>
<p>        &lt;description&gt;ThriftURIfor theremotemetastore. Usedbymetastoreclientto connectto remotemetastore.&lt;/description&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</p>
<p>        &lt;value&gt;10000&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</p>
<p>        &lt;value&gt;jdbc:mysql://localhost:3306/hive_13?createDatabaseIfNotExist=true&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</p>
<p>        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</p>
<p>        &lt;value&gt;hadoop&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</p>
<p>        &lt;value&gt;hadoop&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</p>
<p>        &lt;value&gt;false&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</p>
<p>        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</p>
<p>        &lt;description&gt;locationofdefault databasefor thewarehouse&lt;/description&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;fs.defaultFS&lt;/name&gt;</p>
<p>        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;</p>
<p>        &lt;value&gt;true&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt;</p>
<p>        &lt;value&gt;SchemaTable&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;datanucleus.schema.autoCreateTables&lt;/name&gt;</p>
<p>        &lt;value&gt;true&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;beeline.hs2.connection.user&lt;/name&gt;</p>
<p>        &lt;value&gt;mfz&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>    &lt;property&gt;</p>
<p>        &lt;name&gt;beeline.hs2.connection.password&lt;/name&gt;</p>
<p>        &lt;value&gt;111111&lt;/value&gt;</p>
<p>    &lt;/property&gt;</p>
<p>&lt;/configuration&gt;</p>
</div>
<p> 　　5.1由配置文件可看出，我们需要mysql的数据库hive_13,数据库用户名为hadoop,数据库密码为hadoop.</p>
<p>6.安装mysql </p>
<p>　　6.1 安装参考文章：<a class="postTitle2" href="http://www.cnblogs.com/xiaoluo501395377/archive/2013/04/07/3003278.html" rel="nofollow">Linux学习之CentOS(十三)--CentOS6.4下Mysql数据库的安装与配置</a></p>
<p>　　6.2 建立mysql数据库、用户、权限 参考文章：<a class="postTitle2" href="http://www.cnblogs.com/penciler/p/4813157.html" rel="nofollow">使用MySQL命令行新建用户并授予权限的方法</a></p>
<p>7.启动验证Mysql是否安装配置成功 :使用hadoop用户登录　　</p>
<div class="cnblogs_Highlighter">
<pre><code class="language-csharp"> mysql -u hadoop -p 
</code></pre>
</div>
<p> <img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170403105552722-2062211487.png" alt="" width="513" height="205"></p>
<p>8.配置hive环境变量：</p>
<div class="cnblogs_Highlighter">
<pre><code class="language-csharp">vi /home/mfz/.bash_profile  
#Hive CONFIG
export HIVE_HOME=/home/mfz/apache-hive-2.1.1-bin
export PATH=$PATH:$HIVE_HOME/bin

#wq .bash_profile
#生效配置
source /home/mfz/.bash_profile   
#验证是否生效
echo $HIVE_HOME

[mfz@master apache-hive-2.1.1-bin]$ echo $HIVE_HOME                                                                                  
/home/mfz/apache-hive-2.1.1-bin </code></pre>
</div>
<p>9. 将mysql的java connector复制到依赖库中</p>
<div class="cnblogs_Highlighter">
<pre><code class="language-csharp">cp resources/msyql/mysql-connector-java-5.1.27-bin.jar apache-hive-2.1.1-bin/bin/
</code></pre>
</div>
<p>10.启动hive，命令: hive;  若出现如下几种错误请参照对应解决方案；</p>
<p>错误1：</p>
<p><img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170403110634222-1884888753.png" alt="" width="792" height="491"></p>
<p>　　原因：Hive metastore database is not initialized</p>
<p>　　解决方案：执行命令</p>
<div class="cnblogs_Highlighter">
<pre><code class="language-csharp">schematool -dbType mysql -initSchema
</code></pre>
</div>
<p> 错误2:</p>
<p><img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170403110855332-26053061.png" alt="" width="798" height="555"></p>
<p>　　原因：hadoop 安全模式打开导致</p>
<p>　　解决方案：执行命令</p>
<div class="cnblogs_Highlighter">
<pre><code class="language-csharp">#关闭hadoop安全模式
hadoop dfsadmin -safemode leave
</code></pre>
</div>
<p> 11.启动hive.</p>
<p>　　A.方式1： hive命令</p>
<p><img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170403111234910-242394250.png" alt="" width="1060" height="354"></p>
<p>　　B.方式2（重要）：</p>
<p>　　beeline</p>
<p>　　!connect jdbc:hive2://master:10000/default mfz 111111<span class="hljs-selector-tag"><br></span></p>
<p><img src="https://images2015.cnblogs.com/blog/941878/201704/941878-20170409095923753-417708775.png" alt="" width="620" height="629"></p>
<p>　　说明default是database名称，mfz是master服务器用户，111111是用户的登录密码.</p>
<p>　　因为beeline是取代hive客户端的新客户端，它访问HS2来发起hive操作，但是别急着敲下命令，继续往下看：这里涉及一个hadoop.proxy的概念：默认HS2是以user=anonymous身份访问Hdfs的，我们称HS2是hadoop的一个代理服务。但是，我们实际上希望以mfz身份去访问hdfs，因为此前创建的hive数据目录都是属于mfz用户的，anonymous是无法访问的，那么此时就需要给hadoop配置一个proxyuser，意思是HS2代理可以支持用户以mfz身份访问hdfs，而不是anonymous用户。</p>
<p>为了实现这个能力，需要修改hadoop项目的core-site.xml配置来实现（记得重启namenode和datanode）：</p>
<pre>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.mfz.groups&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.proxyuser.mfz.hosts&lt;/name&gt;<br>    &lt;value&gt;*&lt;/value&gt;<br>&lt;/property&gt;<br><br></pre>
<p>10.hive 使用命令.</p>
<p>数据定义语句DDL<br>Create/Drop/Alter Database<br>Create/Drop/Truncate Table<br>Alter Table/Partition/Column<br>Create/Drop/Alter View<br>Create/Drop/Alter Index<br>Create/Drop Function<br>Create/Drop/Grant/Revoke Roles and Privileges<br>Show<br>Describe</p>
<p>完~ 关于Hive的Nosql操作命令与Jdbc访问Hive方式见博文 <a id="link_post_title" class="link-post-title" href="http://www.cnblogs.com/cnmenglang/p/6683897.html" rel="nofollow">大数据系列之数据仓库Hive使用</a></p>
<p> </p>
<p>转载请注明出处：</p>
<p>作者：mengfanzhu</p>
<p>原文链接：<a id="Editor_Edit_hlEntryLink" title="view: 大数据系列之数据仓库Hive安装" href="http://www.cnblogs.com/cnmenglang/p/6661488.html" rel="nofollow">http://www.cnblogs.com/cnmenglang/p/6661488.html</a></p></div><div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory"></div>
<div id="EntryTag"></div>
<div id="blog_post_info">
</div>
<div class="clear"></div>
<div id="post_next_prev"></div>
</div>


		</div>            </div>
                </div>