---
layout:     post
title:      kafka-2.8-0.8.0安装
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>kafka是linkedin用于日志处理的分布式消息队列， 同时支持离线和在线日志处理。 kafka对消息保存时根据Topic进行归类， 发送消息者成为 Producer,消息接受者成为 Consumer,此外 kafka 集群有多个kafka实例组成， 每个实例(server)称为broker。 无论是kafka集群， 还是producer和consumer都依赖于zookeeper来保证系统可用性，为集群保存一些meta信息。 <br>
<img src="https://img-blog.csdn.net/20170101115928088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzAzMzM4NTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>

<p>一个 Topic 可以认为是一类消息，每个 topic 将被分成多个partition(区),每个 partition 在存储层面是 append log 文件。任何发布到此 partition 的消息都会被直接追加到log 文件的尾部， 每条消息在文件中的位置称为 offset（偏移量）， offset 为一个 long型数字， 它是唯一标记一条消息。 kafka 并没有提供其他额外的索引机制来存储 offset，因为在 kafka 中几乎不允许对消息进行“ 随机读写” 。 <br>
在kafka 中， 即使消息被消费,消息仍然不会被立即删除。 日志文件将会根据 broker 中的配置要求,保留一定的时间之后删除;比如log 文件保留 2 天,那么两天后,文件会被清除,无论其中的消息是否被消费。 kafka 通过这种简单的手段,来释放磁盘空间,以及减少消息消费之后对文件内容改动的磁盘IO 开支。 <br>
安装过程参考博文：<a href="http://www.tuicool.com/articles/uQzYfq" rel="nofollow">http://www.tuicool.com/articles/uQzYfq</a> <br>
安装 2.8.0-0.8.0 <br>
1.下载kafka_2.8.0-0.8.0.tar.gz  <br>
<a href="https://archive.apache.org/dist/kafka/0.8.0/kafka_2.8.0-0.8.0.tar.gz" rel="nofollow">https://archive.apache.org/dist/kafka/0.8.0/kafka_2.8.0-0.8.0.tar.gz</a>  <br>
2.解压缩  <br>
tar -vxf kafka_2.8.0-0.8.0.tar.gz  <br>
3.修改配置文件  <br>
修改conf/server.properties  <br>
host.name=10.10.224.12  （修改为主机ip,不然服务器返回给客户端的是主机的hostname,客户端并不一定能够识别）  <br>
修改conf/zookeeper.properties 属性文件  <br>
dataDir=/usr/local/tmp/zookeeper   (zookeeper临时数据文件)  <br>
4.启动zookeeper和kafka  <br>
cd bin  <br>
启动zookeeper  <br>
./zookeeper-server-start.sh ../config/zookeeper.properties &amp; (&amp;推出命令行，服务守护执行)  <br>
启动kafka  <br>
./kafka-server-start.sh ../config/server.properties &amp;  <br>
5.验证是否成功  <br>
*创建主题  <br>
./kafka-create-topic.sh –partition 1 –replica 1 –zookeeper localhost:2181 –topic test  <br>
检查是否创建主题成功  <br>
./kafka-list-topic.sh –zookeeper localhost:2181  <br>
*启动produce  <br>
./bin/kafka-console-producer.sh –broker-list 10.10.224.12:9092  –topic test  <br>
*启动consumer  <br>
./kafka-console-consumer.sh –zookeeper localhost:2181 –topic test  <br>
6.关闭kafka和zookeeper  <br>
./kafka-server-stop.sh ../config/server.properties  <br>
./zookeeper-server-stop.sh  <br>
心得总结：  <br>
1.produce启动的时候参数使用的是kafka的端口而consumer启动的时候使用的是zookeeper的端口；  <br>
2.必须先创建topic才能使用；  <br>
3.topic本质是以文件的形式储存在zookeeper上的。 </p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>