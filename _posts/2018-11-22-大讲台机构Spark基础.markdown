---
layout:     post
title:      大讲台机构Spark基础
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
课程大纲<br>
Spark概述<br>
1、Spark概述        1）Spark是什么 <br>
2）Spark组成---大一统的软件栈 <br>
3）Spark组成---Spark Core <br>
4）Spark组成---Spark SQL <br>
5）Spark组成---Spark Streaming <br>
6）Spark组成---Spark MLlib <br>
7）Spark组成---GraphX <br>
8）Spark与Hadoop的关系 <br>
9）Spark的竞争对手---Flink <br>
10）Spark的竞争对手---Storm/JStorm <br>
11）Spark的竞争对手---Hadoop3.x <br>
2、Spark生态系统        1）Spark自有生态圈(以Spark为核心) <br>
2）Spark 之外更大的生态圈 <br>
3）如何学习Spark <br>
3、Spark 编程实例        1）Wordcount之Java版 <br>
2）Wordcount之Scala版 <br>
Spark安装部署<br>
1、Spark源码编译        1）Spark版本选择 <br>
2）Spark下载—Apache版本 <br>
3）Spark下载—CDH版本 <br>
4）Spark下载—HDP版本 <br>
5）Spark build参考文档 <br>
6）下载Spark源代码 <br>
2、Spark最简安装        1）Spark预编译安装包下载—apache版 <br>
2）Spark目录详解 <br>
3）Spark最简安装 <br>
4）运行spark自带实例程序 <br>
3、Spark编程环境搭建        1）基于Intellij IDEA搭建Spark开发环境搭—参考文档 <br>
2）基于Intellij IDEA搭建Spark开发环境—maven vs sbt <br>
3）基于Intellij IDEA搭建Spark开发环境搭—maven构建scala项目 <br>
4）基于Intellij IDEA搭建Spark开发环境搭—spark依赖 <br>
5）基于Intellij IDEA搭建Spark开发环境搭—打包插件 <br>
6）Spark 实例程序开发 <br>
7）Spark 项目的打包与运行 <br>
4、Spark 编程模型        1）Spark 核心概念详解 <br>
2）Spark Application的组成 <br>
3）Spark Application基本概念 <br>
4）Spark Application编程模型 <br>
5）RDD的概念 <br>
6）RDD接口 <br>
7）RDD的本质特征 <br>
8）RDD--partitions、RDD-preferredLocations、dependencies、compute、partitioner、lineage <br>
9）典型RDD的特征 <br>
10）不同角度分析RDD <br>
11）Scheduler Optimizations <br>
12）如何创建RDD <br>
13）RDD transformation操作 <br>
14）RDD 控制操作 <br>
15）RDD action操作 <br>
16）创建Pair RDD <br>
17）Pair RDD transformation操作 <br>
18）Pair RDD action操作 <br>
19）Pair RDD 分区控制 <br>
5、Spark运行模式概述        1）解析Application program的组成部分 <br>
2）Spark 运行流程详解 <br>
3）Spark 具体执行流程 <br>
4）Spark 任务调度 <br>
5）Spark DAGScheduler <br>
6）Spark TaskScheduler <br>
7）Spark ScheduleBacked <br>
8）Spark 作业详细执行过程 <br>
9）Spark 实例解析 <br>
10）Spark 各种运行模式详解 <br>
6、Spark Standalone运行模式        1）Spark Standalone架构 <br>
2）手工启动Spark集群 <br>
3）通过脚本启动Spark集群 <br>
4）访问Web UI查看Spark集群 <br>
5）Job提交与运行 <br>
6）Spark Standalone HA高可用实现 <br>
7）Spark基本工作流程 <br>
8）Spark local模式 <br>
9）Spark local cluster模式 <br>
10）Spark standalone模式 <br>
11）Spark standalone详细过程解析 <br>
12）Spark Standalone 实例操作 <br>
7、Spark on YARN        1）YARN是什么 <br>
2）YARN在Hadoop生态系统中的位置 <br>
3）YARN产生的背景 <br>
4）YARN基本架构 <br>
5）Spark on YARN配置与部署 <br>
6）spark-shell运行在YARN上 <br>
7）提交Spark Job给YARN <br>
8）Spark on YARN运行架构解析 <br>
9）Spark on YARN配置详解和注意事项 <br>
HDFS核心技术精讲<br>
HDFS核心技术精讲        1、HDFS设计的前提和目标 <br>
2、HDFS体系结构<br>
3、HDFS特性与优点<br>
4、HDFS不合适场景<br>
5、HDFS2.0新特性<br>
Spark Core<br>
1、Spark交互式工具spark-shell        1）Spark REPL <br>
2）Spark shell <br>
3）spark-shell运行在YARN上 <br>
2、Spark应用程序部署工具spark-submit        1）打包Spark Application <br>
2）使用spark-submit启动Spark Application <br>
3）spark-submit 各种使用方式详解 <br>
4）spark-submit option各种配置选项详解 <br>
3、Spark存储管理机制        1）存储管理概述 <br>
2）RDD控制操作 <br>
3）RDD持久化级别 <br>
4）如何选择持RDD久化级别 <br>
5）缓存淘汰机制 <br>
6）Shuffle数据持久化 <br>
7）广播变量和累加器 <br>
4、Spark多语言编程        1）Spark多语言编程特点 <br>
2）Spark 编程模型 <br>
3）深入Spark 多语言编程 <br>
4）Spark 多语言编程综合实例 <br>
Spark SQL<br>
1、Spark SQL概述        1）Spark SQL是什么？ <br>
2）何为结构化数据 <br>
3）SparkSQL 与 Spark Core的关系 <br>
4）Spark SQL前世今生：由Shark发展而来 <br>
5）Spark SQL前世今生：可以追溯到Hive <br>
6）Spark SQL前世今生：Hive 到 Shark <br>
7）Spark SQL前世今生：Shark 到 Spark SQL <br>
8）Spark SQL前世今生：Hive 到 Hive on Spark <br>
2、Spark SQL基本原理        1）Spark SQL模块划分 <br>
2）Spark SQL架构--catalyst设计图 <br>
3）Spark SQL运行架构 <br>
4）Hive兼容性 <br>
3、Spark SQL编程详解        1）SparkSQL的依赖 <br>
2）SparkSQL的入口：SQLContext <br>
3）SparkSQL的入口： HiveContext <br>
4）SQLContext vs HiveContext <br>
5）Spark SQL的作用与使用方式 <br>
6）Spark SQL支持的API <br>
7）从程序中使用SparkSQL的基本套路 <br>
8）DataFrame—推荐使用 <br>
9）为什么要用DataFrame <br>
10）SparkSQL数据源：从各种数据源创建DataFrame <br>
11）SparkSQL数据源：RDD <br>
12）SparkSQL数据源：Hive <br>
13）SparkSQL数据源：Hive读写 <br>
14）SparkSQL数据源：访问不同版本的metastore <br>
15）SparkSQL数据源：Parquet <br>
16）SparkSQL数据源：Json <br>
17）SparkSQL数据源：JDBC <br>
4、Spark SQL分布式SQL引擎        1）SparkSQL分布式查询引擎：两种实现方式 <br>
2）SparkSQL分布式查询引擎：Thrift JDBC/ODBC服务 <br>
3）SparkSQL分布式查询引擎：beeline <br>
4）SparkSQL分布式查询引擎：Spark SQL CLI <br>
5、Spark SQL用户自定函数        1）注册UDF<br>
6、Spark SQL性能调优        1）开启缓存数据功能 <br>
2）参数调优<br>
Spark Streaming<br>
1、Spark Streaming概述        1）批处理 &amp; 流处理 <br>
2）为什么需要流处理---更多场景需要 <br>
3）Spark Core &amp; RDD本质上是离线运算 <br>
4）Spark Streaming是什么 <br>
5）Spark Streaming的竞争对手 <br>
6）Spark Streaming vs Storm <br>
2、Spark Streaming 实例操作        1）Spark Streaming 实例操作<br>
3、SparkStreaming运行原理与核心概念        1）SparkStreaming运行原理 <br>
2）SparkStreaming的高层抽象DStream <br>
3）Dstream与RDD的关系 <br>
4）Batch duration <br>
4、SparkStreaming编程模型        1）依赖管理 <br>
2）编程基本套路 <br>
3）Dstream输入源---input DStream <br>
4）Dstream输入源--- Receiver <br>
5）内置的input Dstream：Basic Sources <br>
6）内置的input Dstream：Advanced Sources <br>
7）Dstream输入源：multiple input DStream <br>
8）Dstream输入源：Custom Receiver <br>
9）无状态转换操作 <br>
10）有状态转换操作1-updateStateByKey <br>
11）有状态转换操作2-window操作—普通规约与增量规约 <br>
12）有状态转换操作2-window操作—理解增量规约 <br>
13）DStream输出 <br>
14）持久化操作 <br>
5、SparkStreaming性能调优        1）合理的并行度 <br>
2）减少任务启动开销 <br>
3）选择合适的batch Duration <br>
4）内存调优 <br>
5）设置合理的cpu数 <br>
6、Spark Streaming容错        1）检查点机制-checkpoint <br>
2）Driver节点容错 <br>
3）Worker节点容错 <br>
4）处理保证 <br><br><br>
下载地址：http://www.itsource.com.cn/thread-3033-1-1.html
            </div>
                </div>