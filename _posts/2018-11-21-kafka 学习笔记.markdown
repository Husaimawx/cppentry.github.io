---
layout:     post
title:      kafka 学习笔记
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1><a href="https://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow" id="cb_post_title_url">Apache Kafka系列(一) 起步</a></h1>

<ul><li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">Apache Kafka系列(一) 起步</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354183.html" rel="nofollow">Apache Kafka系列(三) Java API使用</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7355309.html" rel="nofollow">Apache Kafka系列(四) 多线程Consumer方案</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7407273.html" rel="nofollow">Apache Kafka系列(五) Kafka Connect及FileConnector示例</a></li>
</ul><h2><strong>摘要：</strong></h2>

<p>　　1.Apache Kafka基本概念</p>

<p>　　2.Kafka的安装</p>

<p>　　3.基本工具创建Topic</p>

<p>　本文基于centos7, Apache Kafka 0.11.0</p>

<h2><strong>一、基本概念</strong></h2>

<p>　　Apache Kafka是一个发布/订阅的消息系统，于2009年源自Linkedin，并与2011年开源。在架构方面，Kafka类似于其他的消息系统（ActiveMQ，RabbitMQ）。但是Kafka有诸多的特性使得越来越流行：</p>

<ul><li>Kafka本身支持分布式，很容易横向扩展</li>
	<li>高吞吐量，高性能</li>
	<li>高容错性，即使宕机</li>
</ul><p>　　Kafka在实时数据处理方面的应用场景越来越多</p>

<h2><strong>二、安装</strong></h2>

<p>　　1.安装jdk</p>

<pre>
[root@eb2c2d938924 /]# yum install java-1.8.0-openjdk.x86_64</pre>

<p>　    2.下载kafka_2.12-0.11.0.0.tgz到/tmp下</p>

<pre>
[root@eb2c2d938924 /]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/0.11.0.0/kafka_2.12-0.11.0.0.tgz  -P /tmp</pre>

<p>       3.解压压缩文件到/opt目录下</p>

<pre>
[root@eb2c2d938924 /]# tar -xvzf /tmp/kafka_2.12-0.11.0.0.tgz -C /opt/</pre>

<p>       4.解压后的目录结构如下</p>

<pre>
[root@eb2c2d938924 kafka_2.12-0.11.0.0]# pwd
/opt/kafka_2.12-0.11.0.0
[root@eb2c2d938924 kafka_2.12-0.11.0.0]# ls
LICENSE  NOTICE  bin  config  libs  site-docs
[root@eb2c2d938924 kafka_2.12-0.11.0.0]# </pre>

<ul><li>bin：包含Kafka运行的所有脚本，如：start/stop Zookeeper，start/stop Kafka</li>
	<li>libs：Kafka运行的依赖库</li>
	<li>config：zookeeper，Logger，Kafka等相关配置文件</li>
	<li>sit-docs：Kafka相关文档</li>
</ul><h2><strong>三、启动Kafka服务器</strong></h2>

<p>　　Kafka能够以3种方式部署集群</p>

<ul><li>单节点-单Broker集群：只在一个节点上部署一个Broker</li>
	<li>单节点-多Broker集群：在一个节点上部署多个Broker，只不过各个Broker以不同的端口启动</li>
	<li>多节点-多Broker集群：以上两种的组合，每个节点上部署一到多个Broker，且各个节点连接起来</li>
</ul><p>　　本次仅以第一种方式部署。启动Kafka需要两步：</p>

<ul><li>启动ZooKeeper</li>
</ul><p>　　　　Kafka使用ZooKeeper来存储集群元数据和Consumer信息。因此，有两种选项：</p>

<p>　　　　第一，独立部署ZooKeeper应用并配置Kafka集群到该ZooKeeper；</p>

<p>　　　　第二，使用Kafka自带的ZooKeeper</p>

<p>　　　　本文选择使用Kafka自带的ZooKeeper</p>

<ul><li> 启动Kafka服务</li>
</ul><p>    3.1 启动ZooKeeper</p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
[root@eb2c2d938924 kafka_2.12-0.11.0.0]# <strong>bin/zookeeper-server-start.sh config/</strong><strong>zookeeper.properties</strong> 
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
[2017-08-10 14:02:29,426] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
[2017-08-10 14:02:29,491] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2017-08-10 14:02:29,491] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2017-08-10 14:02:29,501] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p>　　　　ZooKeeper成功启动，并绑定到端口2181。该端口是ZooKeeper的默认端口，可以通过编辑文件config/zookeeper.properties 中的clientPort来修改监听端口。</p>

<p>　　3.2 启动Kafka Broker</p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
[root@eb2c2d938924 kafka_2.12-0.11.0.0]# bin/kafka-server-start.sh config/server.properties 
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
[2017-08-10 14:11:58,741] INFO KafkaConfig values: 
 .....
[2017-08-10 14:12:00,385] INFO Kafka version : 0.11.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2017-08-10 14:12:00,385] INFO Kafka commitId : cb8625948210849f (org.apache.kafka.common.utils.AppInfoParser)
[2017-08-10 14:12:00,386] INFO [Kafka Server 0], started (kafka.server.KafkaServer)</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p> </p>

<h2><strong>四、使用Kafka</strong></h2>

<p>　　4.1 创建一个Topic 名称为HelloWorld</p>

<pre>
[root@eb2c2d938924 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 \
&gt; --partitions 1 --topic HelloWorld</pre>

<p>　　   校验Topic是否创建成功</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --list --zookeeper localhost:2181
HelloWorld</pre>

<p>     4.2 启动一个Producer并发送消息</p>

<p>　　　可以使用Kafka命令行客户端（获取标准命令行输入并一Message形式发出）发送消息到Kafka集群。默认情况下，每行会单独算作一次消息发出。下例通过该命令行终端发送消息到HelloWorld这个Topic，命令如下：</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic HelloWorld
&gt;hello world!
&gt;this is the first greating
&gt;</pre>

<p>     4.3 启动一个Consumer并接受消息</p>

<p>　　    跟4.2中类似，同样可以使用Kafka命令行终端来启动一个Consumer（格式化消息到标准输出），命令如下</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topicelloWorld --from-beginning
hello world!
this is the first greating</pre>

<p>　　</p>

<h2><strong>五、结论</strong></h2>

<p>　　本文展示了如何一步一步安装Kafka到Linux，涵盖如何下载，启动ZooKeeper/Kafka，发送和接受来自Kafka服务器的消息</p>

<p> </p>

<h1><a href="https://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></h1>

<ul><li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">Apache Kafka系列(一) 起步</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354183.html" rel="nofollow">Apache Kafka系列(三) Java API使用</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7355309.html" rel="nofollow">Apache Kafka系列(四) 多线程Consumer方案</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7407273.html" rel="nofollow">Apache Kafka系列(五) Kafka Connect及FileConnector示例</a></li>
</ul><p>Apache Kafka命令行工具（Command Line Interface,CLI），下文简称CLI。</p>

<h3>1. 启动Kafka</h3>

<p>　　启动Kafka需要两步：</p>

<p>　　1.1. 启动ZooKeeper　</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/zookeeper-server-start.sh config/zookeeper.properties</pre>

<p>　　1.2. 启动Kafka Server</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-server-start.sh config/server.properties </pre>

<h3>2. 列出Topic</h3>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --list
HelloWorld</pre>

<h3>3. 创建Topic</h3>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic Demo1
Created topic "Demo1".</pre>

<p>　　上述命令会创建一个名为Demo1的Topic，并指定了replication-factor和partitions分别为1。其中replication-factor控制一个Message会被写到多少台服务器上，因此这个值必须小于或者</p>

<p>　　等于Broker的数量。</p>

<h3>4. 描述Topic</h3>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic Demo1
Topic:Demo1     PartitionCount:1        ReplicationFactor:1     Configs:
        Topic: Demo1    Partition: 0    Leader: 0       Replicas: 0     Isr: 0</pre>

<h3>5. 发布消息到指定的Topic</h3>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic Demo1
&gt;this
&gt;is 
&gt;the 
&gt;firest
&gt;input</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p>　　可以在控制台逐行输入任意消息。命令的终止符是：control + C组合键。</p>

<h3>6. 消费指定Topic上的消息</h3>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic Demo1
this
is 
the 
firest
input</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<h3>7. 修改Topic</h3>

<p>　　7.1 增加指定Topic的partition，在第3步中创建的Demo1的partition是1。如下命令将增加10个partition</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --alter --zookeeper localhost:2181 --partitions 11 --topic Demo1
WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected
Adding partitions succeeded!</pre>

<p>　　7.2. 删除指定Topic</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic Demo1
Topic Demo1 is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.</pre>

<p>　　　　Note中指出该Topic并没有真正的删除，如果真删除，需要把server.properties中的delete.topic.enable置为true</p>

<p>　　7.3 给指定的Topic增加配置项，如给一个增加max message size值为128000</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic Demo1 --config max.message.bytes=128000
WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.
         Going forward, please use kafka-configs.sh for this functionality
Updated config for topic "Demo1".</pre>

<p>　　　　warning中指出该命令已经过期，将来可能被删除，替代的命令是使用kafka-config.sh。新命令如下：</p>

<pre>
[root@Server1 kafka_2.12-0.11.0.0]# bin/kafka-configs.sh --alter --zookeeper localhost:2181 --entity-type topics --entity-name Demo1 --add-config max.message.bytes=12800
Completed Updating config for entity: topic 'Demo1'.</pre>

<p>　　　　需要使用entity-type置为topics，并在entity-name中指定对应的名称</p>

<h3>8. 结论</h3>

<p>　　本文展示了CLI所提供的一些常用的命令，这些基本的命令在运维Kafka过程中很实用。</p>

<p> </p>

<h1><a href="https://www.cnblogs.com/qizhelongdeyang/p/7354183.html" rel="nofollow">Apache Kafka系列(三) Java API使用</a></h1>

<ul><li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">Apache Kafka系列(一) 起步</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354183.html" rel="nofollow">Apache Kafka系列(三) Java API使用</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7355309.html" rel="nofollow">Apache Kafka系列(四) 多线程Consumer方案</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7407273.html" rel="nofollow">Apache Kafka系列(五) Kafka Connect及FileConnector示例</a></li>
</ul><h2><strong>摘要：</strong></h2>

<p>　　Apache Kafka Java Client API</p>

<h2>一、基本概念</h2>

<p>　　Kafka集成了Producer/Consumer连接Broker的客户端工具，但是在消息处理方面，这两者主要用于服务端（Broker）的简单操作，如：</p>

<p>　　　　1.创建Topic</p>

<p>　　　　2.罗列出已存在的Topic</p>

<p>　　　　3.对已有Topic的Produce/Consume测试</p>

<p>　　跟其他的消息系统一样，Kafka提供了多种不用语言实现的客户端API，如:Java，Python，Ruby，Go等。这些API极大的方便用户使用Kafka集群，本文将展示这些API的使用</p>

<h2>二、前提</h2>

<ul><li>在本地虚拟机中安装了Kafka 0.11.0版本，可以参照前一篇文章：  <a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">Apache Kafka系列(一) 起步</a></li>
	<li>本地安装有JDK1.8</li>
	<li>IDEA编译器</li>
	<li>Maven3</li>
</ul><h2>三、项目结构</h2>

<p>　　Maven pom.xml如下：</p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.randy&lt;/groupId&gt;
  &lt;artifactId&gt;kafka_api_demo&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;Maven&lt;/name&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
      &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
      &lt;version&gt;0.11.0.0&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p> </p>

<h2>四、源码</h2>

<p>　　<strong>4.1 Producer的源码　</strong>　　　</p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy;

import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;


/**
 * Author  : RandySun
 * Date    : 2017-08-13  16:23
 * Comment :
 */
public class ProducerDemo {

    public static void main(String[] args){
        Properties properties = new Properties();
        properties.put("bootstrap.servers", "192.168.1.110:9092");
        properties.put("acks", "all");
        properties.put("retries", 0);
        properties.put("batch.size", 16384);
        properties.put("linger.ms", 1);
        properties.put("buffer.memory", 33554432);
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        Producer&lt;String, String&gt; producer = null;
        try {
            producer = new KafkaProducer&lt;String, String&gt;(properties);
            for (int i = 0; i &lt; 100; i++) {
                String msg = "Message " + i;
                producer.send(new ProducerRecord&lt;String, String&gt;("HelloWorld", msg));
                System.out.println("Sent:" + msg);
            }
        } catch (Exception e) {
            e.printStackTrace();

        } finally {
            producer.close();
        }

    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p>　　可以使用KafkaProducer类的实例来创建一个Producer，KafkaProducer类的参数是一系列属性值，下面分析一下所使用到的重要的属性：</p>

<ul><li><em>bootstrap.servers</em></li>
</ul><pre>
properties.put("bootstrap.servers", "192.168.1.110:9092");</pre>

<p>　　　bootstrap.servers是Kafka集群的IP地址，如果Broker数量超过1个，则使用逗号分隔，如"192.168.1.110:9092,192.168.1.110:9092"。其中，192.168.1.110是我的其中一台虚拟机的</p>

<p>           IP地址，9092是所监听的端口</p>

<ul><li><em>key.serializer</em>   &amp;  <em>value.serializer</em></li>
</ul><pre>
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");</pre>

<p>　　   序列化类型。 Kafka消息是以键值对的形式发送到Kafka集群的，其中Key是可选的，Value可以是任意类型。但是在Message被发送到Kafka集群之前，Producer需要把不同类型的消</p>

<p>　　　息序列化为二进制类型。本例是发送文本消息到Kafka集群，所以使用的是StringSerializer。</p>

<ul><li><em>发送Message到Kafka集群</em></li>
</ul><pre>
   for (int i = 0; i &lt; 100; i++) {
      String msg = "Message " + i;
      producer.send(new ProducerRecord&lt;String, String&gt;("HelloWorld", msg));
      System.out.println("Sent:" + msg);
   }</pre>

<p>　　　上述代码会发送100个消息到HelloWorld这个Topic</p>

<p>　　<strong>4.2 Consumer的源码</strong></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy;

import java.util.Arrays;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

/**
 * Author  : RandySun
 * Date    : 2017-08-13  17:06
 * Comment :
 */
public class ConsumerDemo {

    public static void main(String[] args){
        Properties properties = new Properties();
        properties.put("bootstrap.servers", "192.168.1.110:9092");
        properties.put("group.id", "group-1");
        properties.put("enable.auto.commit", "true");
        properties.put("auto.commit.interval.ms", "1000");
        properties.put("auto.offset.reset", "earliest");
        properties.put("session.timeout.ms", "30000");
        properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(properties);
        kafkaConsumer.subscribe(Arrays.asList("HelloWorld"));
        while (true) {
            ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(100);
            for (ConsumerRecord&lt;String, String&gt; record : records) {
                System.out.printf("offset = %d, value = %s", record.offset(), record.value());
                System.out.println();
            }
        }

    }
}
</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p>　　可以使用<em>KafkaConsumer</em>类的实例来创建一个Consumer，<em>KafkaConsumer</em>类的参数是一系列属性值，下面分析一下所使用到的重要的属性：</p>

<ul><li><em>bootstrap.servers</em></li>
</ul><p>　　  和Producer一样，是指向Kafka集群的IP地址，以逗号分隔。</p>

<ul><li><em>group.id</em></li>
</ul><p>　　   Consumer分组ID</p>

<ul><li><em>key.deserializer and value.deserializer</em></li>
</ul><p><em>　　  </em> 发序列化。Consumer把来自Kafka集群的二进制消息反序列化为指定的类型。因本例中的Producer使用的是String类型，所以调用<em>StringDeserializer</em>来反序列化</p>

<p>　　Consumer订阅了Topic为HelloWorld的消息，Consumer调用poll方法来轮循Kafka集群的消息，其中的参数100是超时时间（Consumer等待直到Kafka集群中没有消息为止）： </p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
        kafkaConsumer.subscribe(Arrays.asList("HelloWorld"));
        while (true) {
            ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(100);
            for (ConsumerRecord&lt;String, String&gt; record : records) {
                System.out.printf("offset = %d, value = %s", record.offset(), record.value());
                System.out.println();
            }
        }</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<h2>五、总结</h2>

<p>　　本文展示了如何创建一个Producer并生成String类型的消息，Consumer消费这些消息。这些都是基于Apache Kafka 0.11.0 Java API。</p>

<p> </p>

<h1><a href="https://www.cnblogs.com/qizhelongdeyang/p/7355309.html" rel="nofollow">Apache Kafka系列(四) 多线程Consumer方案</a></h1>

<ul><li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">Apache Kafka系列(一) 起步</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354183.html" rel="nofollow">Apache Kafka系列(三) Java API使用</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7355309.html" rel="nofollow">Apache Kafka系列(四) 多线程Consumer方案</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7407273.html" rel="nofollow">Apache Kafka系列(五) Kafka Connect及FileConnector示例</a></li>
</ul><p><strong>本文的图片是通过<a href="http://files.cnblogs.com/files/qizhelongdeyang/kafka.pptx" rel="nofollow">PPT</a>截图出的，读者如果修改意见请联系我</strong></p>

<h3>一、Consumer为何需要实现多线程</h3>

<p>　　假设我们正在开发一个消息通知模块，该模块允许用户订阅其他用户发送的通知/消息。该消息通知模块采用Apache Kafka，那么整个架构应该是消息的发布者通过Producer调用API写入消息到Kafka Cluster中，然后消息的订阅者通过Consumer读取消息，刚开始的时候系统架构图如下：</p>

<p><img alt="" class="has" src="https://images2017.cnblogs.com/blog/314515/201708/314515-20170813232834617-1579907180.png"></p>

<p>          但是，随着用户数量的增多，通知的数据也会对应的增长。总会达到一个阈值，在这个点上，Producer产生的数量大于Consumer能够消费的数量。那么Broker中未消费的消息就会逐渐增多。即使Kafka使用了优秀的消息持久化机制来保存未被消费的消息，但是<em>Kafka的消息保留机制限制</em>（时间，分区大小，消息Key）也会使得始终未被消费的Message被永久性的删除。另一方面从业务上讲，一个消息通知系统的高延迟几乎算作是废物了。所以多线程的Consumer模型是非常有必要的。</p>

<h3>二、多线程的Kafka Consumer 模型类别</h3>

<p>　　基于Consumer的多线程模型有两种类型：</p>

<ul><li>模型一：多个Consumer且每一个Consumer有自己的线程，对应的架构图如下：</li>
</ul><p>                         <img alt="" class="has" src="https://images2017.cnblogs.com/blog/314515/201708/314515-20170814000037960-579515983.png"></p>

<p> </p>

<ul><li>模型二：一个Consumer且有多个Worker线程</li>
</ul><p>                         <img alt="" class="has" src="https://images2017.cnblogs.com/blog/314515/201708/314515-20170814000045617-792211891.png"></p>

<p>   　　两种实现方式的优点/缺点比较如下：</p>

<table border="0"><tbody><tr><th>名称</th>
			<th>优点</th>
			<th>缺点</th>
		</tr><tr><td>模型一</td>
			<td>
			<p>1.Consumer Group容易实现</p>

			<p>2.各个Partition的顺序实现更容易</p>
			</td>
			<td>
			<p>1.Consumer的数量不能超过Partition的数量，否则多出的Consumer永远不会被使用到</p>

			<p>2.因没个Consumer都需要一个TCP链接，会造成大量的系统性能损耗</p>
			</td>
		</tr><tr><td>模型二</td>
			<td>1.由于通过线程池实现了Consumer，横向扩展更方便</td>
			<td>
			<p>1.在每个Partition上实现顺序处理更困难。</p>

			<p>例如：同一个Partition上有两个待处理的Message需要被线程池中的2个线程消费掉，那这两个线程必须实现同步</p>
			</td>
		</tr></tbody></table><h3>三、代码实现</h3>

<p>3.1 前提</p>

<ul><li>
	<ul><li>Kafka Broker 0.11.0</li>
		<li>JDK1.8</li>
		<li>IDEA</li>
		<li>Maven3</li>
		<li>Kafka环境搭建及Topic创建修改等请参照本系列的前几篇文章。</li>
	</ul></li>
</ul><p> 3.2 源码结构</p>

<p>                 <img alt="" class="has" height="495" src="https://images2017.cnblogs.com/blog/314515/201708/314515-20170820165717006-858215211.png" width="350"></p>

<p>       其中，consumergroup包下面对应的是模型一的代码，consumerthread包下是模型二的代码。ProducerThread是生产者代码。</p>

<p> 3.3 pom.xml</p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;com.randy&lt;/groupId&gt;
  &lt;artifactId&gt;kafka_multithread_consumer_model&lt;/artifactId&gt;
  &lt;packaging&gt;war&lt;/packaging&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;kafka_multithread_consumer_model Maven Webapp&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;


  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
      &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
      &lt;version&gt;0.11.0.0&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;

  &lt;build&gt;
    &lt;finalName&gt;kafka_multithread_consumer_model&lt;/finalName&gt;
  &lt;/build&gt;
&lt;/project&gt;</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p> 3.4 方案一：Consumer Group</p>

<p>　　ProducerThread.java是一个生产者线程，发送消息到Broker</p>

<p>　　ConsumerThread.java是一个消费者线程，由于消费消息</p>

<p>　　ConsumerGroup.java用于产生一组消费者线程</p>

<p>　　ConsumerGroupMain.java是入口类     </p>

<p><strong>3.4.1 ProducerThread.java　</strong></p>

<p><img alt="" class="has" id="code_img_opened_b3fa77fd-7b94-4ed3-9388-411883945090" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy;

import org.apache.kafka.clients.producer.*;

import java.util.Properties;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  11:41
 * Comment :
 */
public class ProducerThread implements Runnable {
    private final Producer&lt;String,String&gt; kafkaProducer;
    private final String topic;

    public ProducerThread(String brokers,String topic){
        Properties properties = buildKafkaProperty(brokers);
        this.topic = topic;
        this.kafkaProducer = new KafkaProducer&lt;String,String&gt;(properties);

    }

    private static Properties buildKafkaProperty(String brokers){
        Properties properties = new Properties();
        properties.put("bootstrap.servers", brokers);
        properties.put("acks", "all");
        properties.put("retries", 0);
        properties.put("batch.size", 16384);
        properties.put("linger.ms", 1);
        properties.put("buffer.memory", 33554432);
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        return properties;
    }

    @Override
    public void run() {
        System.out.println("start sending message to kafka");
        int i = 0;
        while (true){
            String sendMsg = "Producer message number:"+String.valueOf(++i);
            kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(topic,sendMsg),new Callback(){

                @Override
                public void onCompletion(RecordMetadata recordMetadata, Exception e) {
                    if(e != null){
                        e.printStackTrace();
                    }
                    System.out.println("Producer Message: Partition:"+recordMetadata.partition()+",Offset:"+recordMetadata.offset());
                }
            });
            // thread sleep 3 seconds every time
            try {
                Thread.sleep(3000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            System.out.println("end sending message to kafka");
        }
    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p> <strong>3.4.2 ConsumerThread.java</strong></p>

<p><img alt="" class="has" id="code_img_opened_50bf2bb9-4cf5-4001-bf9a-d089734c04ca" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy.consumergroup;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.util.Arrays;
import java.util.Properties;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  12:03
 * Comment :
 */
public class ConsumerThread implements Runnable {
    private static KafkaConsumer&lt;String,String&gt; kafkaConsumer;
    private final String topic;

    public ConsumerThread(String brokers,String groupId,String topic){
        Properties properties = buildKafkaProperty(brokers,groupId);
        this.topic = topic;
        this.kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties);
        this.kafkaConsumer.subscribe(Arrays.asList(this.topic));
    }

    private static Properties buildKafkaProperty(String brokers,String groupId){
        Properties properties = new Properties();
        properties.put("bootstrap.servers", brokers);
        properties.put("group.id", groupId);
        properties.put("enable.auto.commit", "true");
        properties.put("auto.commit.interval.ms", "1000");
        properties.put("session.timeout.ms", "30000");
        properties.put("auto.offset.reset", "earliest");
        properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        return properties;
    }

    @Override
    public void run() {
        while (true){
            ConsumerRecords&lt;String,String&gt; consumerRecords = kafkaConsumer.poll(100);
            for(ConsumerRecord&lt;String,String&gt; item : consumerRecords){
                System.out.println("Consumer Message:"+item.value()+",Partition:"+item.partition()+"Offset:"+item.offset());
            }
        }
    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p><strong>3.4.3 ConsumerGroup.java</strong></p>

<p><img alt="" class="has" id="code_img_opened_23d6e338-7a27-4a31-9282-26301d7882e5" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy.consumergroup;

import java.util.ArrayList;
import java.util.List;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  14:09
 * Comment :
 */
public class ConsumerGroup {
    private final String brokers;
    private final String groupId;
    private final String topic;
    private final int consumerNumber;
    private List&lt;ConsumerThread&gt; consumerThreadList = new ArrayList&lt;ConsumerThread&gt;();

    public ConsumerGroup(String brokers,String groupId,String topic,int consumerNumber){
        this.groupId = groupId;
        this.topic = topic;
        this.brokers = brokers;
        this.consumerNumber = consumerNumber;
        for(int i = 0; i&lt; consumerNumber;i++){
            ConsumerThread consumerThread = new ConsumerThread(brokers,groupId,topic);
            consumerThreadList.add(consumerThread);
        }
    }

    public void start(){
        for (ConsumerThread item : consumerThreadList){
            Thread thread = new Thread(item);
            thread.start();
        }
    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p><strong>3.4.4 ConsumerGroupMain.java　</strong>　</p>

<p><img alt="" class="has" id="code_img_opened_1435cace-31af-41dc-ac3b-9afc5ef31b9d" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy.consumergroup;

import com.randy.ProducerThread;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  14:18
 * Comment :
 */
public class ConsumerGroupMain {

    public static void main(String[] args){
        String brokers = "Server2:9092";
        String groupId = "group01";
        String topic = "HelloWorld";
        int consumerNumber = 3;

        Thread producerThread = new Thread(new ProducerThread(brokers,topic));
        producerThread.start();

        ConsumerGroup consumerGroup = new ConsumerGroup(brokers,groupId,topic,consumerNumber);
        consumerGroup.start();
    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<h3>3.5 方案二：多线程的Consumer</h3>

<p>　　ConsumerThreadHandler.java用于处理发送到消费者的消息</p>

<p>　　ConsumerThread.java是消费者使用线程池的方式初始化消费者线程</p>

<p>　　ConsumerThreadMain.java是入口类</p>

<p><strong>3.5.1 ConsumerThreadHandler.java</strong></p>

<p><img alt="" class="has" id="code_img_opened_10a52ce3-599d-4567-b8d5-b31ecd5a8aa1" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy.consumerthread;

import org.apache.kafka.clients.consumer.ConsumerRecord;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  16:29
 * Comment :
 */
public class ConsumerThreadHandler implements Runnable {
    private ConsumerRecord consumerRecord;

    public ConsumerThreadHandler(ConsumerRecord consumerRecord){
        this.consumerRecord = consumerRecord;
    }

    @Override
    public void run() {
        System.out.println("Consumer Message:"+consumerRecord.value()+",Partition:"+consumerRecord.partition()+"Offset:"+consumerRecord.offset());
    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p><strong>3.5.2 ConsumerThread.java</strong></p>

<p><img alt="" class="has" id="code_img_opened_39fdebad-d9ff-431e-85e6-675e4a25d87a" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy.consumerthread;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.util.Arrays;
import java.util.Properties;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  16:42
 * Comment :
 */
public class ConsumerThread {

    private final KafkaConsumer&lt;String, String&gt; consumer;
    private final String topic;
    // Threadpool of consumers
    private ExecutorService executor;


    public ConsumerThread(String brokers, String groupId, String topic){
        Properties properties = buildKafkaProperty(brokers,groupId);
        this.consumer = new KafkaConsumer&lt;&gt;(properties);
        this.topic = topic;
        this.consumer.subscribe(Arrays.asList(this.topic));
    }

    public void start(int threadNumber){
        executor = new ThreadPoolExecutor(threadNumber,threadNumber,0L, TimeUnit.MILLISECONDS,
                new ArrayBlockingQueue&lt;Runnable&gt;(1000), new ThreadPoolExecutor.CallerRunsPolicy());
        while (true){
            ConsumerRecords&lt;String,String&gt; consumerRecords = consumer.poll(100);
            for (ConsumerRecord&lt;String,String&gt; item : consumerRecords){
                executor.submit(new ConsumerThreadHandler(item));
            }
        }
    }

    private static Properties buildKafkaProperty(String brokers, String groupId){
        Properties properties = new Properties();
        properties.put("bootstrap.servers", brokers);
        properties.put("group.id", groupId);
        properties.put("enable.auto.commit", "true");
        properties.put("auto.commit.interval.ms", "1000");
        properties.put("session.timeout.ms", "30000");
        properties.put("auto.offset.reset", "earliest");
        properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        return properties;
    }


}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p><strong>3.5.3 ConsumerThreadMain.java</strong></p>

<p><img alt="" class="has" id="code_img_opened_72df60b7-edc5-4f5b-a3c6-701775e2e1b0" src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif"></p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
package com.randy.consumerthread;

import com.randy.ProducerThread;

/**
 * Author  : RandySun (sunfeng152157@sina.com)
 * Date    : 2017-08-20  16:49
 * Comment :
 */
public class ConsumerThreadMain {

    public static void main(String[] args){
        String brokers = "Server2:9092";
        String groupId = "group01";
        String topic = "HelloWorld";
        int consumerNumber = 3;


        Thread producerThread = new Thread(new ProducerThread(brokers,topic));
        producerThread.start();

        ConsumerThread consumerThread = new ConsumerThread(brokers,groupId,topic);
        consumerThread.start(3);


    }
}</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<h3>四. 总结</h3>

<p>　　本篇文章列举了两种不同的消费者模式。两者各有利弊。所有代码都上传到了https://github.com/qizhelongdeyang/kafka_multithread_consumer_model.git ，如有疑问或者错误请指正</p>

<p> </p>

<h1><a href="https://www.cnblogs.com/qizhelongdeyang/p/7407273.html" rel="nofollow">Apache Kafka系列(五) Kafka Connect及FileConnector示例</a></h1>

<ul><li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">Apache Kafka系列(一) 起步</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7354183.html" rel="nofollow">Apache Kafka系列(三) Java API使用</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7355309.html" rel="nofollow">Apache Kafka系列(四) 多线程Consumer方案</a></li>
	<li><a href="http://www.cnblogs.com/qizhelongdeyang/p/7407273.html" rel="nofollow">Apache Kafka系列(五) Kafka Connect及FileConnector示例</a></li>
</ul><h3>一. Kafka Connect简介</h3>

<p>　　Kafka是一个使用越来越广的消息系统，尤其是在大数据开发中（实时数据处理和分析）。为何集成其他系统和解耦应用，经常使用Producer来发送消息到Broker，并使用Consumer来消费Broker中的消息。Kafka Connect是到0.9版本才提供的并极大的简化了其他系统与Kafka的集成。Kafka Connect运用用户快速定义并实现各种Connector(File,Jdbc,Hdfs等)，这些功能让大批量数据导入/导出Kafka很方便。</p>

<p>             <img alt="" class="has" src="https://images2017.cnblogs.com/blog/314515/201708/314515-20170821220746418-613069141.png"></p>

<p>如图中所示，左侧的Sources负责从其他异构系统中读取数据并导入到Kafka中；右侧的Sinks是把Kafka中的数据写入到其他的系统中。</p>

<h3>二. 各种Kafka Connector</h3>

<p>　　Kafka Connector很多，包括开源和商业版本的。如下列表中是常用的开源Connector</p>

<table><tbody><tr><td>Connectors</td>
			<td>References</td>
		</tr><tr><td>Jdbc</td>
			<td><a href="https://github.com/apache/ignite/tree/master/modules/kafka" rel="nofollow">Source</a>, <a href="https://github.com/apache/ignite/tree/master/modules/kafka" rel="nofollow">Sink</a></td>
		</tr><tr><td>Elastic Search</td>
			<td><a href="https://github.com/ksenji/kafka-connect-es" rel="nofollow">Sink1</a>, <a href="https://github.com/hannesstockner/kafka-connect-elasticsearch" rel="nofollow">Sink2</a>, <a href="https://github.com/DataReply/kafka-connect-elastic-search-sink" rel="nofollow">Sink3</a></td>
		</tr><tr><td>Cassandra</td>
			<td><a href="https://github.com/tuplejump/kafka-connect-cassandra" rel="nofollow">Source1</a>, <a href="https://github.com/datamountaineer/stream-reactor/tree/master/kafka-connect-cassandra" rel="nofollow">Source 2</a>, <a href="https://github.com/tuplejump/kafka-connect-cassandra" rel="nofollow">Sink1</a>, <a href="https://github.com/datamountaineer/stream-reactor/tree/master/kafka-connect-cassandra" rel="nofollow">Sink2 </a></td>
		</tr><tr><td>MongoDB</td>
			<td><a href="https://github.com/DataReply/kafka-connect-mongodb" rel="nofollow">Source</a></td>
		</tr><tr><td>HBase</td>
			<td><a href="https://github.com/mravi/kafka-connect-hbase" rel="nofollow">Sink</a></td>
		</tr><tr><td>Syslog</td>
			<td><a href="https://github.com/jcustenborder/kafka-connect-syslog" rel="nofollow">Source</a></td>
		</tr><tr><td>MQTT (Source)</td>
			<td><a href="https://github.com/evokly/kafka-connect-mqtt" rel="nofollow">Source</a></td>
		</tr><tr><td>Twitter (Source)</td>
			<td><a href="https://github.com/rollulus/twitter-kafka-connect" rel="nofollow">Source</a>, <a href="https://github.com/Eneco/kafka-connect-twitter" rel="nofollow">Sink</a></td>
		</tr><tr><td>S3</td>
			<td><a href="https://github.com/qubole/streamx" rel="nofollow">Sink1</a>, <a href="https://github.com/DeviantArt/kafka-connect-s3" rel="nofollow">Sink2</a><br>
			 </td>
		</tr></tbody></table><p>　　商业版的可以通过<a href="https://www.confluent.io/product/connectors/" rel="nofollow">Confluent.io</a>获得</p>

<h3>三. 示例</h3>

<p>3.1 FileConnector Demo</p>

<p>　本例演示如何使用Kafka Connect把Source(test.txt)转为流数据再写入到Destination(test.sink.txt)中。如下图所示：</p>

<p>          <img alt="" class="has" src="https://images2017.cnblogs.com/blog/314515/201708/314515-20170821225909980-1168791908.png"></p>

<p>      本例使用到了两个Connector:</p>

<ul><li>FileStreamSource：从test.txt中读取并发布到Broker中</li>
	<li>FileStreamSink：从Broker中读取数据并写入到test.sink.txt文件中</li>
</ul><p>　　其中的Source使用到的配置文件是${KAFKA_HOME}/config/connect-file-source.properties</p>

<pre>
name=local-file-source
connector.class=FileStreamSource
tasks.max=1
file=test.txt
topic=connect-test</pre>

<p>　　其中的Sink使用到的配置文件是${KAFKA_HOME}/config/connect-file-sink.properties</p>

<pre>
name=local-file-sink
connector.class=FileStreamSink
tasks.max=1
file=test.sink.txt
topics=connect-test</pre>

<p>　　Broker使用到的配置文件是${KAFKA_HOME}/config/connect-standalone.properties</p>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<pre>
bootstrap.servers=localhost:9092key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=trueinternal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false
offset.storage.file.filename=/tmp/connect.offsets
offset.flush.interval.ms=10000
</pre>

<p><a><img alt="复制代码" class="has" src="http://common.cnblogs.com/images/copycode.gif"></a></p>

<p> </p>

<p>3.2 运行Demo</p>

<p>　　需要熟悉Kafka的一些命令行，参考本系列之前的文章<a href="http://www.cnblogs.com/qizhelongdeyang/p/7354315.html" rel="nofollow">Apache Kafka系列(二) 命令行工具（CLI）</a></p>

<p> 3.2.1 启动Kafka Broker</p>

<pre>
[root@localhost bin]# cd /opt/kafka_2.11-0.11.0.0/
[root@localhost kafka_2.11-0.11.0.0]# ls
bin  config  libs  LICENSE  logs  NOTICE  site-docs
[root@localhost kafka_2.11-0.11.0.0]# ./bin/zookeeper-server-start.sh ./config/zookeeper.properties &amp;
[root@localhost kafka_2.11-0.11.0.0]# ./bin/kafka-server-start.sh ./config/server.properties &amp;</pre>

<p>3.2.2 启动Source Connector和Sink Connector</p>

<pre>
[root@localhost kafka_2.11-0.11.0.0]# ./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties </pre>

<p>3.3.3 打开console-consumer</p>

<pre>
./kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic connect-test</pre>

<p>3.3.4 写入到test.txt文件中，并观察3.3.3中的变化</p>

<pre>
[root@Server4 kafka_2.12-0.11.0.0]# echo 'firest line' &gt;&gt; test.txt
[root@Server4 kafka_2.12-0.11.0.0]# echo 'second line' &gt;&gt; test.txt
3.3.3中打开的窗口输出如下
{"schema":{"type":"string","optional":false},"payload":"firest line"}
{"schema":{"type":"string","optional":false},"payload":"second line"}</pre>

<p>3.3.5 查看test.sink.txt</p>

<pre>
[root@Server4 kafka_2.12-0.11.0.0]# cat test.sink.txt 
firest line
second line</pre>

<p> </p>

<h3>四. 结论</h3>

<p>本例仅仅演示了Kafka自带的File Connector，后续文章会完成JndiConnector，HdfsConnector，并且会使用CDC（Changed Data Capture）集成Kafka来完成一个ETL的例子</p>

<p> PS:</p>

<p>相比编译过Kafka-Manager都知道各种坑，经过了3个小时的努力，我终于把Kafka-Manager编译通过并打包了，并且新增了Kafka0.11.0版本支持。</p>

<p>附下载地址： 链接: https://pan.baidu.com/s/1miiMsAk 密码: 866q</p>

<p> </p>

<p>转载：<a href="http://www.cnblogs.com/qizhelongdeyang/p/7341954.html" rel="nofollow">http://www.cnblogs.com/qizhelongdeyang/p/7341954.html</a></p>            </div>
                </div>