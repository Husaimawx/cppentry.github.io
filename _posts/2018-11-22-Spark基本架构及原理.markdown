---
layout:     post
title:      Spark基本架构及原理
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>目标：</strong></span></div><ul><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(0,0,0);">Spark概述</li><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(0,0,0);">Spark<span style="color:rgb(51,51,51);background-color:rgb(255,255,255);">基本概念</span></li><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(0,0,0);">Spark四大运行模式、运行流程</li><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(51,51,51);"><span style="background-color:rgb(255,255,255);">spark 与 hadoop</span></li><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(51,51,51);"><span style="background-color:rgb(255,255,255);">RDD运行流程</span></li><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(0,0,0);">Spark三大类算子</li><li style="text-align:left;line-height:1.75;font-size:14px;list-style-position:inside;white-space:pre-wrap;font-family:'Microsoft YaHei', STXihei;color:rgb(0,0,0);">Spark<span style="color:rgb(51,51,51);background-color:rgb(255,255,255);"> Streaming</span></li></ul></div><div style="white-space:pre-wrap;line-height:1.625;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>Spark概述：</strong></span></div><div style="white-space:pre-wrap;text-indent:28px;line-height:1.625;font-size:14px;">Apache Spark是专为大规模数据处理而设计的快速通用的计算引擎。</div><div style="white-space:pre-wrap;text-indent:28px;line-height:1.625;font-size:14px;">Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类 Hadoop MapRedduce的通用并行框架，Spark拥有Hadoop MapReduce所具有的优点：但不同于MapReduce 的是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖据与机器学习等需要迭代的MapRedduce的算法。</div><div style="white-space:pre-wrap;text-indent:28px;line-height:1.625;font-size:14px;">Spark是一种与Hadoop相似的开源集群计算环境，但是两者之间有不间还存在一些不同之处，这些有用的不同之处使Spark 在某些工作负载方面表现得更加优越，换句话说，Spark启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。</div><div style="white-space:pre-wrap;text-indent:28px;line-height:1.625;font-size:14px;">Spark是在Scala语言中实现的，它将Scala 用作其应用程序框架。与Hadoop不同，Spark和Scala能够紧密集成，其中的Scala 可以像操作本地集合对象一样轻松地操作分布数据集。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">尽管创建Spark是为了支持分布式数据集上的选代作业，但是实际上它是对Hadoop的补充，可以在Hadoop文件系统中进行并行运行。通过名为Mesos 的第三方集群框架可以支持此行为。Spark由加州大学伯克利分校AMP安验室（Algrihms, Machines and</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">People Lab）开发，可用来构建大型的、低延迟的数据分析应用程序。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>Spark基本概念：</strong></span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">1、Spark特性</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">高可伸缩性</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">高容错</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">内存计算</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">2、Spark的生态体系</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark属于BDAS（DBAS，伯利克分析栈）生态体系。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">MapReduce属于Hadoop生态体系之一，Spark则属于BDAS生态体系之一。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">Hadoop包含了MapReduce、HDFS、HBase、Hive、Zookeeper、Pig、Sqoop等。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">BDAS包含了Spark、Shark(相当于Hive)、BlinkDB、Spark Streaming(消息实时处理框架，类似Storm)。</div><p></p><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">3、Spark的数据读取和存储</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark可以从以下系统访问数据</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;"> Hadoop HDFS 以及HIVE, HBASE 等生态圈部件</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">	Amazon S3</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">Cassandra, Mongodb</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">其他流工具如 Flume, Kafka所支持的各协议如 AVRO</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">另外Spark可以支持一下文件格式:  Text (包括CSV JSON 等)</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">SequenceFiles</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">AVRO</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">Parquet </div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="color:rgb(223,64,42);">*Spark可以独立于HADOOP单独运行</span></div><p></p><p></p><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">4、Spark部件和应用平台</div><img src="https://img-blog.csdn.net/20180527094744598" alt=""><br><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark 的主要部件</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">SPARK CORE:包含spark的主要基本功能。所有跟RDD有关的API都出自于SPARKCORE。</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">Spark SQL: Spark 中用于结构化数据处理的软件包。用户用户可以在Spark环境下用SQL语言处理数据。</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">Spark Streaming:Spark 中用来处理流数据的部件</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">MLlib:Spark 中用来进行机器学习和数学建模的软件包</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">GraphX:Spark 中用来进行图计算(如社交媒体关系) 的库函数</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">Cluster Managers:Spark 中用来管理机群或节点的软件平台.这包括Hadoop YARN, Apache Mesos, 和 Standalone Scheduler (Spark 自带的用于单机系统)</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">SPARK CORE</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">Spark生态圈的核心:</div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">负责从HDFS、Amazon S3和HBase等持久层读取数据</div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">在、YARN和Standalone为资源管理器调度Job完成分布式计算</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">包括两个重要部件</div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">有向无环图(DAG)的分布式并行计算框架</div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">容错分布式数据RDD (Resilient Distributed Dataset)</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">总结SPARK CORE </div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">总体来说SPARK CORE 就是 SPARK 功能调度中心，其中包括任务调动， 内存管理，容错管理及存储管理。同时也是一些				列应用程序的集中地。</div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">这些应用程序用来定义和管理RDD (Resilient DistributedDataset).</div><div style="white-space:pre-wrap;text-align:left;text-indent:56px;line-height:1.75;font-size:14px;">RDD代表了一系列数据集合分布在机群的内存中。SPARK CORE 的任务是对这些数据进行分布式计算。</div></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><p></p><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">5、Spark支持的API</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark支持的API包括Scala、Python、Java 、R。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong><br></strong></span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>Spark和Hadoop：</strong></span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">6、Spark和MapReduce</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">相对MapReduce，Spark具有如下优势:</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">MapReduce通常将中间结果放到HDFS上，Spark是基于内存并行大数据框架，中间结果存放到内存，对于迭代数据Spark效率高。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">MapReduce总是消耗大量时间排序，而有些场景不需要排序，Spark可以避免不必要的排序带来的开销。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">Spark 是一张有向无环图(从一个点出发最终无法回到该点的一个拓扑)，并对其进行优化。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">hadoop和Spark对比更多可参考知乎：<a href="https://www.zhihu.com/question/26568496" rel="nofollow"><span style="color:rgb(0,56,132);">https://www.zhihu.com/question/26568496</span></a></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>Spark四大运行模式、流程：</strong></span></div></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">7、<span style="font-size:14px;text-align:left;white-space:pre-wrap;">Spark四大</span>运行模式</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Local (用于测试、 开发)：Spark单机运行，一般用于开发测试。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Standlone (独立集群模式)：构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark on YARN (Spark在 YARN上)：Spark客户端直接连接Yarn。不需要额外构建Spark集群。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark on Mesos (Spark 在Mesos上)：Spark客户端直接连接Mesos。不需要额外构建Spark集群。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">详细启动方式可参考博客：<span style="color:rgb(0,56,132);"><a href="https://blog.csdn.net/zhangxinrun/article/details/54603585" rel="nofollow">https://blog.csdn.net/zhangxinrun/article/details/54603585</a></span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="color:rgb(57,57,57);">模式分类</span></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">批处理：用于大规模的分布式数据处理</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">spark-submit predict.py</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">spark-submit --class “SparkPi” target/scala-2.10/realtime-event_2.10-1.0.jar</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">流方式：Spark流用于传送和处理实时数据</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">交互方式：常用于处理在内存中的大块数据.较低的延迟性  </div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">spark-shell</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">pyspark</div><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"></div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">8、Spark运行时的步骤</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Driver程序启动多个Worker, Worker从文件系统加载数据并产生RDD (即数据放到RDD</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">中，RDD是一个数据结构)，并按照不同分区Cache到内存中。</div><p><img src="https://img-blog.csdn.net/20180526181415529" alt=""></p><p></p><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">一、构建Spark Application的运行环境，启动SparkContext</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">二、SparkContext向资源管理器（可以是Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackend</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">三、Executor向SparkContext申请Task</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">四、SparkContext将应用程序分发给Executor</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">五、SparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">六、Task在Executor上运行，运行完释放所有资源</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark运行特点：</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">一、每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行Task。这种Application隔离机制是有优势的，无论是从调度角度看（每个Driver调度他自己的任务），还是从运行角度看（来自不同Application的Task运行在不同JVM中），当然这样意味着Spark Application不能跨应用程序共享数据，除非将数据写入外部存储系统</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">二、Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">三、提交SparkContext的Client应该靠近Worker节点（运行Executor的节点），最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">四、Task采用了数据本地性和推测执行的优化机制</div><p></p><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>RDD：</strong></span></div></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">9、RDD</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">RDD英文名为Resilient Distributed Dataset，中文名为弹性分布式数据集。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">什么是RDD? RDD是一个只读、分区记录的集合，你可以把他理解为一个存储数据的数据结构，在Spark中一切操作基于 RDD。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">RDD可以通过以下几种方式创建</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">一、集合转换</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">二、从文件系统 (本地文件、HDFS.、HBase)输入</div><div style="white-space:pre-wrap;margin-left:28px;text-align:left;line-height:1.75;font-size:14px;">三、从父RDD转换(为什么需要父RDD呢?容错， 下面会提及)</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">RDD的计算类型:</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">一、Transformation:延迟执行，一个RDD通过该操作产生新的RDD时不会立即执行，只有等到Action操作才会真正执行。</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">二、Action:提交Spark作业，当Action时，Transformation 类型的操作才会真正执行计算操作，然后产生最终结果输出</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">三、Hadoop提供处理的数据接口有Map和Reduce,而Spark提供的不仅仅有Map和Reduce,还有更多对数据处理的接口</div><div style="white-space:pre-wrap;text-indent:28px;text-align:left;line-height:1.75;font-size:14px;">    </div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">10. 容错Lineage</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">每个RDD都会记录自己所依赖的父RDD, 一旦出现某个RDD的某些Partition丢失，可以通过并行计算迅速恢复，这就是容错。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">通过并行计算迅速恢复，这就是容错</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">RDD的依赖又分为Narrow Dependent(窄依赖)和Wide Dependent (宽依赖)</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">一、窄依赖:每个Partition最多只能给一个RDD使用，由于没有多重依赖，所以在一个节点上可以一次性将Partition处理完，且一旦数据发生丢失或者损坏，可以迅速从上一个RDD恢复</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">二、宽依赖:每个Partition可以给多个RDD使用，由于多重依赖，只有等到所有到达节点的数据处理完毕才能进行下一步处理，</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">一且发生数据丢失或者损坏则完蛋了，所以在此发生之前，必须将上一次所有节点的数据进行物化(存储到磁盘上)处理，这样达到恢复。</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">11、宽、 窄依赖缓存策略</div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;">Spark通过useDisk、 useMemory、deserialized、 replication4个参数组成11种缓存策略。</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">一、useDisk:使用磁盘缓存(boolean) .</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">二、useMemory:使用内存缓存(boolean).</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">三、deserialized:反序列化(序列化是为了网络将对象进行传输，boolean: true 反序列化false序列化) .</div><div style="white-space:pre-wrap;text-align:left;text-indent:28px;line-height:1.75;font-size:14px;">四、replication:副本数量(int).</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">通过StorageL evel类的构造传参的方式进行控制，结构如下:</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">class StorageLevel private (useDisk : Boolean ,useMemory : Boolean , deserialized : Boolean ,replication: Ini)</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">12.提交的方式</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">spark-submit(官方推荐)</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">sbt run</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">ava -jar</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">提交时指定各种参数：</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">1.num-executors 50~100</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><span style="color:rgb(51,51,51);background-color:rgb(255,255,255);">2.executor-memory 4G~8G  num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量,这个量是不能超过队列的最大内存量的</span></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><span style="color:rgb(51,51,51);background-color:rgb(255,255,255);">3.executor-cores 2~4   </span></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><span style="color:rgb(51,51,51);background-color:rgb(255,255,255);">4.spark.default.parallelism  用于设置每个stage的默认task数量,Spark作业的默认task数量为500~1000个较为合适,设置该参数为num-executors * executor-cores的2~3倍较为合适</span></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;">提交示例以参数可参考博客：</div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><a href="https://www.cnblogs.com/gnool/p/5643595.html" rel="nofollow"><span style="color:rgb(0,56,132);">https://www.cnblogs.com/gnool/p/5643595.html</span></a></div><div style="white-space:pre-wrap;line-height:1.75;font-size:14px;"><a href="https://blog.csdn.net/diannao720/article/details/73325703" rel="nofollow"><span style="color:rgb(0,56,132);">https://blog.csdn.net/diannao720/article/details/73325703</span></a></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><br></div><div style="white-space:pre-wrap;text-align:left;line-height:1.75;font-size:14px;"><span style="font-size:16px;background-color:rgb(255,255,255);"><strong>Spark三大算子操作参考笔者博客：</strong></span></div><p>Spark中RDD的Value型Transformation算子操作（一）：<a href="https://blog.csdn.net/zjh_746140129/article/details/80465312" rel="nofollow">Spark中RDD的Value型Transformation算子操作（一）</a></p><p>Spark中RDD的Key-Value型Transformation算子操作（二）：<a href="https://blog.csdn.net/zjh_746140129/article/details/80465413" rel="nofollow">Spark中RDD的Key-Value型Transformation算子操作（二）</a><br></p><p>Spark中Actionn算子操作（三）：<a href="https://blog.csdn.net/zjh_746140129/article/details/80465472" rel="nofollow">Spark中Actionn算子操作（三）</a><br></p><p></p><p><br></p>            </div>
                </div>