---
layout:     post
title:      flume1.8远程写hdfs+hadoop2.8集成
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><strong>       </strong></p>

<p id="main-toc"><strong>目录</strong></p>

<p id="%E4%B8%80%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87" rel="nofollow">一、环境准备</a></p>

<p id="%E4%BA%8C%E3%80%81%E9%9B%86%E6%88%90hadoop%E9%85%8D%E7%BD%AE-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E9%9B%86%E6%88%90hadoop%E9%85%8D%E7%BD%AE" rel="nofollow">二、集成hadoop配置</a></p>

<p id="%C2%A0%20%C2%A0%20%C2%A01%E3%80%81%E5%A4%8D%E5%88%B6hadoop%C2%A0%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%88%B0flume%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A-toc" style="margin-left:40px;"><a href="#%C2%A0%20%C2%A0%20%C2%A01%E3%80%81%E5%A4%8D%E5%88%B6hadoop%C2%A0%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%88%B0flume%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A" rel="nofollow"> 1、复制hadoop 相关配置文件到flume服务器上</a></p>

<p id="%C2%A02%E3%80%81%E6%96%B0%E5%BB%BA%2F%E4%BF%AE%E6%94%B9%20flume%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9A-toc" style="margin-left:40px;"><a href="#%C2%A02%E3%80%81%E6%96%B0%E5%BB%BA%2F%E4%BF%AE%E6%94%B9%20flume%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9A" rel="nofollow"> 2、新建/修改 flume的配置文件：</a></p>

<p id="%C2%A03%E3%80%81%E5%A4%8D%E5%88%B6%E7%9B%B8%E5%85%B3jar%E5%8C%85%E5%88%B0flume%20lib%E7%9B%AE%E5%BD%95(%E6%88%91%E7%9A%84%E6%98%AF%2Fopt%2Fflume%2Fapache-flume-1.8.0-bin%2Flib)-toc" style="margin-left:40px;"><a href="#%C2%A03%E3%80%81%E5%A4%8D%E5%88%B6%E7%9B%B8%E5%85%B3jar%E5%8C%85%E5%88%B0flume%20lib%E7%9B%AE%E5%BD%95(%E6%88%91%E7%9A%84%E6%98%AF%2Fopt%2Fflume%2Fapache-flume-1.8.0-bin%2Flib)" rel="nofollow"> 3、复制相关jar包到flume lib目录(我的是/opt/flume/apache-flume-1.8.0-bin/lib)</a></p>

<p id="4%E3%80%81%E5%90%AF%E5%8A%A8flume%3A-toc" style="margin-left:40px;"><a href="#4%E3%80%81%E5%90%AF%E5%8A%A8flume%3A" rel="nofollow">4、启动flume:</a></p>

<p id="5%E3%80%81%E6%B5%8B%E8%AF%95flume%E9%87%87%E9%9B%86%E5%88%B0HDFS%E4%BF%9D%E5%AD%98-toc" style="margin-left:40px;"><a href="#5%E3%80%81%E6%B5%8B%E8%AF%95flume%E9%87%87%E9%9B%86%E5%88%B0HDFS%E4%BF%9D%E5%AD%98" rel="nofollow">5、测试flume采集到HDFS保存</a></p>

<hr id="hr-toc"><p><strong>        近几天抽时间研究了一下flume的安装配置，flume的应用场景自行百度一下吧，这里不废话了。由于己经利用虚拟机成功安装了hadoop集群，这里主要是讲一下flume的安装。我是另外虚拟化了一台机器单独用于作flume agent服务器，这台机器上没有安装hadoop哦，所以是远程写hdfs，呵呵。如果是在己安装hadoop的机器上，应该会更简单吧。</strong></p>

<h1 id="%E4%B8%80%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><strong>一、环境准备</strong></h1>

<p><strong>  1、官网下载flume1.8， 这个就自己去下载吧，下载好以后，通过ftp放到系统里面。进入centos系统后，通过 tar -zxvf  -C 命令解压到指定的目录 ，我的是 <span style="color:#f33b45;">/opt/flume</span></strong></p>

<p><strong><span style="color:#f33b45;">  </span><span>2、安装JDK，自行百度，rpm安装、下载压缩文件再解决的都行，记得设置JAVA_HOME环境变量。我是rpm安装的，安装成功后，java -version是可以查看信息的，但是事实是启动flume的时候还是会有警告JAVA_HOME is not set ，所以还是在/etc/profile中配置一下环境变量吧。</span></strong></p>

<p><strong>  3、（不要在flume的服务器上作为节点）hadoop伪分布式集群安装（比较多，参考我之前的文章吧，这里不作重点讲解），我的namenode节点服务器Ip是：10.10.10.129，主机名是:hserver1，这里先写出来，后面会用到，避免刚接触的朋友直接复制出现问题，这里作出说明，需要根据自己的服务器信息修改；</strong></p>

<p><strong>4、在flume服务器上把/etc/host修改一下，将hadoop主节点的主机名配置 一下：hserver1  10.10.10.129  （建议添加，没测试过不加是否可以访问远程服务器。）</strong></p>

<h1 id="%E4%BA%8C%E3%80%81%E9%9B%86%E6%88%90hadoop%E9%85%8D%E7%BD%AE"><strong>二、集成hadoop配置</strong></h1>

<h2 id="%C2%A0%20%C2%A0%20%C2%A01%E3%80%81%E5%A4%8D%E5%88%B6hadoop%C2%A0%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%88%B0flume%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A"><strong> 1、复制hadoop 相关配置文件到flume服务器上</strong></h2>

<p><strong>在hadoop namenode节点服务器上，将core-site.xml,hdfs-site.xml（这两个是hadoop集群的配置文件），将文件通过ftp传到flume服务器上后，需要放到  /opt/flume/apache-flume-1.8.0-bin/conf 这个目录下(也就是flume的配置目录)</strong></p>

<p><strong><img alt="" class="has" height="238" src="https://img-blog.csdn.net/20180918172106392?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9sb25nZ3Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1103"></strong></p>

<p><strong><img alt="" class="has" height="209" src="https://img-blog.csdn.net/20180918172249810?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9sb25nZ3Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1111"></strong></p>

<h2 id="%C2%A02%E3%80%81%E6%96%B0%E5%BB%BA%2F%E4%BF%AE%E6%94%B9%20flume%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%9A"><strong> 2、新建/修改 flume的配置文件：</strong></h2>

<p><strong>   我这里是直接新建的一个了，在conf目录下执行 touch hdfs.properties，创建配置文件。我的配置文件内容如下：</strong></p>

<p><strong># Name the components on this agent<br>
 <br>
agent1.sources = spooldirSource<br>
 <br>
agent1.channels = fileChannel<br>
 <br>
agent1.sinks = hdfsSink<br>
 <br>
 <br>
 <br>
# Describe/configure the source<br>
 <br>
agent1.sources.spooldirSource.type=spooldir（source type有好几种，自行百度，我这里是用的这个)<br>
 <br>
agent1.sources.spooldirSource.spoolDir=<span style="color:#f33b45;">/usr/data/spooldir（自定义监控目录，需要先创建好，不然启动flume的时候会报错）</span><br>
 <br>
 <br>
 <br>
# Describe the sink<br>
 <br>
agent1.sinks.hdfsSink.type=hdfs<br>
 <br>
agent1.sinks.hdfsSink.hdfs.path=<span style="color:#f33b45;">hdfs://hserver1:9000/flume/%y-%m-%d/%H%M/%S （自定义hdfs上保存路径,hserver1为namenode的路径，是hadoop集群hdfs的访问路径，根据自己的hadoop集成安装情况修改，如果没有修改/etc/host文件，添加主机名映射，建议直接用ip地址，不要用hserver1这种主机名） </span><br>
 <br>
agent1.sinks.hdfsSink.hdfs.round = true<br>
 <br>
agent1.sinks.hdfsSink.hdfs.roundValue = 10<br>
 <br>
agent1.sinks.hdfsSink.hdfs.roundUnit = minute<br>
 <br>
agent1.sinks.hdfsSink.hdfs.useLocalTimeStamp = true<br>
 <br>
agent1.sinks.hdfsSink.hdfs.fileType=DataStream  <br>
 <br>
 <br>
 <br>
# Describe the channel<br>
 <br>
agent1.channels.fileChannel.type = file<br>
 <br>
agent1.channels.fileChannel.dataDirs=/hadoop/flume/datadir<br>
 <br>
 <br>
 <br>
# Bind the source and sink to the channel<br>
 <br>
agent1.sources.spooldirSource.channels=fileChannel<br>
 <br>
agent1.sinks.hdfsSink.channel=fileChannel</strong></p>

<h2 id="%C2%A03%E3%80%81%E5%A4%8D%E5%88%B6%E7%9B%B8%E5%85%B3jar%E5%8C%85%E5%88%B0flume%20lib%E7%9B%AE%E5%BD%95(%E6%88%91%E7%9A%84%E6%98%AF%2Fopt%2Fflume%2Fapache-flume-1.8.0-bin%2Flib)"><strong> 3、复制相关jar包到flume lib目录(我的是/opt/flume/apache-flume-1.8.0-bin/lib)</strong></h2>

<p><strong>由于flume服务器没有安装hadoop,但是要依赖hadoop的相关类来将文件保存到hdfs的，所以要复制依赖的jar包到 flume。中间经过N多的尝试，给大家理出来。从hadoop安装目录 的 /share/common   、 /share/hdfs目录下找，我的是hadoop2.8，其他版本的可能略有出入，大家启动flume的时候，根据不同的异常对照一下）</strong></p>

<p><strong>1、hadoop-common-2.8.0.jar、 hadoop-hdfs-2.8.0.jar、（没有的话会报NoClass Found :XXXXX.SequenceFile异常)</strong></p>

<p><strong>2、hadoop-auth-2.8.0.jar  (ClassNotFoundException: org.apache.hadoop.util.PlatformName)</strong></p>

<p><strong>3、commons-configuration-1.6.jar  （  ClassNotFoundException::XXXX.Configuration 异常)</strong></p>

<p><strong>3、htrace-core4-4.0.1-incubating.jar (  ClassNotFoundException::XXXX.Trace$Build异常)</strong></p>

<p><strong>4、hadoop-hdfs-client-2.8.0.jar  (</strong><a href="http://www.cnblogs.com/justinzhang/p/4983673.html" rel="nofollow" id="cb_post_title_url">java.io.IOException: No FileSystem for scheme: hdfs</a>，被这个异常差点玩死，百度一下，有位兄弟说要降版本才能解决，还有说要改jar包里面的core-default.xml文件，无意中看到有这个client包，仔细想想flume应该是作为客户端向远程的hdfs服务器写入文件，所以试着把这个jar包添上，果然成功解决)</p>

<p> </p>

<h2 id="4%E3%80%81%E5%90%AF%E5%8A%A8flume%3A">4、启动flume:</h2>

<p>这里有个地方要注意：   如果你是在flume安装目录下的bin目录内（例如我的：<strong>/opt/flume/apache-flume-1.8.0-bin/bin）启动flume</strong>，启动以后，可能控制台是没有显示实时的log信息的，这个是正常的情况，不要以为没有成功启动。建议大家在上一级目录(例如我的：<strong>/opt/flume/apache-flume-1.8.0-bin</strong>)下使用以下命令启动：</p>

<p>bin/flume-ng agent --conf /opt/flume/apache-flume-1.8.0-bin/conf(这个改成自己的目录位置） --conf-file conf/hdfs.properties（改成自己flume配置文件的名字) --name agent1 -Dflume.root.logger=DEBUG,console</p>

<p><img alt="" class="has" height="533" src="https://img-blog.csdn.net/20180918182336381?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9sb25nZ3Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1083"></p>

<h2 id="5%E3%80%81%E6%B5%8B%E8%AF%95flume%E9%87%87%E9%9B%86%E5%88%B0HDFS%E4%BF%9D%E5%AD%98">5、测试flume采集到HDFS保存</h2>

<p>    5.1  再打开一个虚拟终端，连接到flume服务器，利用vim，在监控目录( <strong><span style="color:#f33b45;">/usr/data/spooldir)</span></strong> 中新建一个 test.log，内容随意；</p>

<p>    5.2  在原终端上的实时Log可以看到test.log文件被flume读取到，然后保存到了hdfs，生成了tmp文件，同时test.log被添加了后缀 .complete ；</p>

<p>    5.3  访问hadoop集群web管理界面 ，查看hadoop文件系统，根据之前配置的hdfs保存目录 ，查看是否生成了文件。</p>

<p><img alt="" class="has" height="576" src="https://img-blog.csdn.net/20180918183005480?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9sb25nZ3Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<p><img alt="" class="has" height="559" src="https://img-blog.csdn.net/20180918183049434?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9sb25nZ3Vp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="1200"></p>

<p>利用 hdfs dfs -cat  /flume/18-09-18/0410/00/XXX 查看文件内容，与test.log中的一样，呵呵。成功。</p>

<p> </p>

<p> </p>

<p><br>
 </p>

<p> </p>

<p> </p>

<p><br>
 </p>

<p> </p>

<p> </p>            </div>
                </div>