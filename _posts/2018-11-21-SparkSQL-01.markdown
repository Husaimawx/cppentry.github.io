---
layout:     post
title:      SparkSQL-01
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><span style="background-color:rgb(204,204,204);">DataFrames可以由一个存在的RDD、Hive table、Spark data sources转化而来</span></p><h1><strong>Spark SQL愿景：</strong></h1><h2><strong>    1.Less Code</strong></h2><p style="text-align:center;"><strong><img src="https://img-blog.csdn.net/20180519105931564?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE1MzAwNjgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" border="1" alt=""><br></strong></p><h2>    <strong>2.Read</strong> <strong>Less Data</strong></h2><p>       压缩，过滤等<br></p><h2>    <strong>3.交给Catalyst Optimizer来处理优化</strong></h2><div><strong></strong><p>        用Scala、Python、Java、R写出来的代码逻辑效率可能不一样，甚至差距很大</p><p>        用Scala、Python、Java、R写出来的代码逻辑效率、性能一样，因为有Catalyst Opimizer()</p></div><h1><strong>Spark SQL对比图：</strong></h1><p>Spark SQL             =&gt;Spark</p><p>Hive on Spark       =&gt;Hive    (用)</p><p><img src="https://img-blog.csdn.net/20180519075646893?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE1MzAwNjgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p><strong></strong></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);"><strong>共性 </strong></span></span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">4、三者都有partition的概念。</span></p><p><strong>RDD</strong></p><p><span style="color:rgb(0,0,0);font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);">优点： </span><br></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">1、相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">2、面向对象的编程风格</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">3、编译时类型安全，编译时就能检查出类型错误</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);">缺点： </span></span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">1、序列化和反序列化的性能开销</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">2、GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</span></p><p><strong>DataFrame</strong></p><p></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值。如</span></p><pre style="padding-top:8px;padding-bottom:4px;padding-left:56px;max-width:100%;color:rgb(51,51,51);letter-spacing:.544px;font-size:14px;line-height:22px;border:none;background-color:rgb(246,248,250);"><code style="margin:0px;max-width:100%;"><span style="margin:0px;padding:0px;max-width:100%;">df.<span style="margin:0px;padding:0px;max-width:100%;">foreach</span>{
  x =&gt;
    val v1=x.getAs[<span style="margin:0px;padding:0px;max-width:100%;">String</span>](<span style="margin:0px;padding:0px;max-width:100%;">"v1"</span>)
    val v2=x.getAs[<span style="margin:0px;padding:0px;max-width:100%;">String</span>](<span style="margin:0px;padding:0px;max-width:100%;">"v2"</span>)
}</span></code></pre><ul class="list-paddingleft-2" style="margin-bottom:0px;padding-left:2.2em;max-width:100%;list-style-type:none;"><li style="margin-top:0px;margin-left:0px;padding:0px;max-width:100%;"><p style="margin-bottom:0px;max-width:100%;clear:both;min-height:1em;"><br style="margin:0px;padding:0px;max-width:100%;"></p></li></ul><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">2、DataFrame引入了schema和off-heap</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">schema : RDD每一行的数据, 结构都是一样的. 这个结构就存储在schema中. Spark通过schame就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了.</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">off-heap : 意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存. 由于Spark理解schema, 所以知道该如何操作.</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了.</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">3、结构化数据处理非常方便，支持Avro, CSV, Elasticsearch数据等，也支持Hive, MySQL等传统数据表 <br style="margin:0px;padding:0px;max-width:100%;">4、兼容Hive，支持Hql、UDF</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">有schema和off-heap概念，DataFrame解决了RDD的缺点, 但是却丢了RDD的优点. DataFrame不是类型安全的（只有编译后才能知道类型错误）, API也不是面向对象风格的.</span></p><p><strong>DataSet</strong></p><p><strong></strong></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">1、DataSet是分布式的数据集合。DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型 和可以用强大lambda函数）以及Spark SQL优化的执行引擎。DataSet可以通过JVM的对象进行构建，可以用函数式的转换（map/flatmap/filter）进行多种操作。</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">2、DataSet结合了RDD和DataFrame的优点, 并带来的一个新的概念Encoder。DataSet 通过Encoder实现了自定义的序列化格式，使得某些操作可以在无需序列化情况下进行。另外Dataset还进行了包括Tungsten优化在内的很多性能方面的优化。</span></p><p style="max-width:100%;clear:both;min-height:1em;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;letter-spacing:.544px;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">3、Dataset</span><code style="margin:0px;padding:4px 2px 0px;max-width:100%;font-size:14px;line-height:22px;"><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);">&lt;Row&gt;</span></code><span style="margin:0px;padding:0px;max-width:100%;color:rgb(0,0,0);font-size:14px;">等同于DataFrame（Spark 2.X）</span></p><br><p><br></p><p></p><h1>Spark SQL架构图：</h1><div style="text-align:center;"><img src="https://img-blog.csdn.net/20180519110855619?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE1MzAwNjgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></div><div style="text-align:left;">HIVE AST:抽象语法树</div><p>首先不管是Hive还是spark应用程序，都先生成未解析的逻辑执行计划，然后将Scheme Catalog（元数据信息）与未解析的逻辑执行计划拼起来，之后生成逻辑执行计划，再进行优化成优化的逻辑执行计划，解析为物理执行计划，生成新的模型。</p><p>案例：</p><p>cp hive-site.xml到spark conf文件夹</p><p>启动spark-sql</p><p>bin/spark-sql --master local[2] --hiveconf "hive.metastore.warehouse.dir=hdfs://hadoop:9000/user/hive/warehouse"<br></p><p><span style="color:rgb(255,0,0);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;text-align:left;background-color:rgb(255,255,255);"></span></p><p style="margin:10px auto;color:rgb(0,0,0);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;text-align:left;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;color:rgb(255,0,0);">注意：</span></p><p style="margin:10px auto;color:rgb(0,0,0);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;text-align:left;background-color:rgb(255,255,255);"><span style="margin:0px;padding:0px;color:rgb(255,0,0);">在spark 2.0.1 中，--hiveconf "hive.metastore.warehouse" 参数已经不再生效，用户应该使用 </span></p><p class="p1" style="margin:10px auto;color:rgb(0,0,0);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;text-align:left;background-color:rgb(255,255,255);"><span class="s1" style="margin:0px;padding:0px;color:rgb(255,0,0);">--conf spark.sql.warehouse.dir=hdfs://hadoop:9000/user/hive/warehouse 命令进行代替</span></p><br>            </div>
                </div>