---
layout:     post
title:      Flume学习笔记及配置参数详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>转载自：<a href="https://blog.csdn.net/Wei_HHH/article/details/77838999" rel="nofollow">https://blog.csdn.net/Wei_HHH/article/details/77838999</a></p>

<h1 id="一-什么是flume">一、什么是flume</h1>

<p>Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p>

<h1 id="二-flume特点">二、flume特点</h1>

<p>flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。</p>

<p><strong>flume的可靠性</strong></p>

<p>当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。</p>

<p><strong>flume的可恢复性</strong></p>

<p>还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。</p>

<p><strong>flume的一些核心概念</strong></p>

<ul><li>
	<p>Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。</p>
	</li>
	<li>
	<p>Client生产数据，运行在一个独立的线程。</p>
	</li>
	<li>
	<p>Source从Client收集数据，传递给Channel。</p>
	</li>
	<li>
	<p>Sink从Channel收集数据，运行在一个独立线程。</p>
	</li>
	<li>
	<p>Channel连接 sources 和 sinks ，这个有点像一个队列。</p>
	</li>
	<li>
	<p>Events可以是日志记录、 avro 对象等。</p>
	</li>
</ul><p>值得注意的是，Flume提供了大量内置的Source、Channel和Sink类型。不同类型的Source,Channel和Sink可以自由组合。组合方式基于用户设置的配置文件，非常灵活。比如：Channel可以把事件暂存在内存里，也可以持久化到本地硬盘上。Sink可以把日志写入HDFS, HBase，甚至是另外一个Source等等。Flume支持用户建立多级流，也就是说，多个agent可以协同工作，并且支持Fan-in、Fan-out、Contextual Routing、Backup Routes</p>

<h1 id="三-flume架构">三、flume架构</h1>

<p><strong>Agent</strong></p>

<p>Flume以agent为最小的独立运行单位,一个Agent包含多个source、channel、sink和其他组件</p>

<p><strong>source</strong></p>

<p>flume提供多种source供用户进行选择，尽可能多的满足大部分日志采集的需求，常用的source的类型包括avro、exec、netcat、spooling-directory和syslog等。具体的使用范围和配置方法详见source.</p>

<p><strong>channel</strong></p>

<p>flume中的channel不如source和sink那么重要，但却是不可忽视的组成部分。常用的channel为memory-channel，同时也有其他类型的channel，如JDBC、file-channel、custom-channel等，详情见channel.</p>

<p><strong>sink</strong></p>

<p>flume的sink也有很多种，常用的包括avro、logger、HDFS、Hbase以及file-roll等，除此之外还有其他类型的sink，如thrift、IRC、custom等。具体的使用范围和使用方法详见sink.</p>

<h1 id="四-flume使用">四、flume使用</h1>

<p>创建配置文件（随意位置)</p>

<p>vi agent.conf</p>

<h1 id="五-配置">五、配置</h1>

<h2 id="常见的source">常见的source</h2>

<pre class="has">
<code>avro source:avro可以监听和收集指定端口的日志，使用avro的source需要说明被监听的主机ip和端口号

例子：

agent1.sources = r1

#描述source

agent1.sources.r1.type = avro  (类型为avro source)

agent1.sources.r1.bind = 0.0.0.0 （指定监听的主机ip.本机是0.0.0.0.）

agent1.sources.r1.port = 16667 (指定监听的端口号)

 

exec source:可以通过指定的操作对日志进行读取，使用exec时需要指定shell命令，对日志进行读取

例子：

agent1.source = r2

#描述source

agent1.sources.r2.type = exec 

agent1.sources.r2.command =tail -F /root/flume/log/event.txt (监听的文件的路径)

 

Spooling-directory source:可以读取文件夹里的日志，使用时指定一个文件夹，可以读取该文件夹中的所有文件，当出现新文件时会读取该文件并获取数据.需要注意的是该文件夹中的文件在读取过程中不能修改，同时文件名也不能修改。

1、spoolDirectory是监控目录，不能为空，没有默认值。这个source不具有监控子目录的功能，也就是不能递归监控。如果需要，这需要自己去实现，http://blog.csdn.net/yangbutao/article/details/8835563 这里有递归检测的实现；

2、completedSuffix是文件读取完毕后给完成文件添加的标记后缀，默认是".COMPLETED"；

3、deletePolicy这是是否删除读取完毕的文件，默认是"never"，就是不删除，目前只支持"never"和“IMMEDIATE”；

4、fileHeader是否在event的Header中添加文件名，boolean类型, 默认false

5、fileHeaderKey这是event的Header中的key,value是文件名

6、batchSize这个是一次处理的记录数，默认是100；

7、inputCharset编码方式，默认是"UTF-8"；

8、ignorePattern忽略符合条件的文件名

9、trackerDirPath被处理文件元数据的存储目录，默认".flumespool"

10、deserializerType将文件中的数据序列化成event的方式，默认是“LINE”---org.apache.flume.serialization.LineDeserializer

11、deserializerContext这个主要用在Deserializer中设置编码方式outputCharset和文件每行最大长度maxLineLength。

 

例子：

agent1.sources = r3

#描述source

agent1.sources.r3.type = spooldir

agent1.sources.r3.spoolDir = /root/flume/log  (监听的文件目录)

agent1.sources.r3.fileHeader = true

    

NetCat source:用来监听一个指定端口，并将接收到的数据的每一行转换为一个事件。

例子：

agent1.sources = r4 

#描述source

agent1.sources.r4.type = netcat  (source类型)

agent1.sources.r4.bind = 0.0.0.0 (指定绑定的ip或主机名)

agent1.sources.r4.port = 16668   (指定绑定的端口号)

 

HTTP source:接受HTTP的GET和POST请求作为Flume的事件,其中GET方式应该只用于试验。

！type    类型，必须为"HTTP"

！port–   监听的端口

bind   0.0.0.0    监听的主机名或ip

handler      org.apache.flume.source.http.JSONHandler处理器类，需要实现HTTPSourceHandler接口

handler.*  –   处理器的配置参数

Selector.type

selector.*   

interceptors  –  

interceptors.*        

enableSSL  false  是否开启SSL,如果需要设置为true。注意，HTTP不支持SSLv3。

excludeProtocols  SSLv3  空格分隔的要排除的SSL/TLS协议。SSLv3总是被排除的。

keystore      密钥库文件所在位置

keystorePassword Keystore 密钥库密码

例子：

agent1.sources.r1.type  = http

agent1.sources.r1.port  = 66666

</code></pre>

<h2 id="常见的channel">常见的channel</h2>

<pre class="has">
<code>memory channel:内存

例子：

agent1.channels = c1

agent1.channels.c1.type = memory

agent1.channels.c1.capacity = 100000

agent1.channels.c1.transactionCapacity = 100000
</code></pre>

<h2 id="常见的sink">常见的sink</h2>

<pre class="has">
<code>logger sink:将收集到的日志写到flume的log中

例子：

agent1.sinks = k1

agent2.sinks.k1.type = logger

 

avro sink:可以将接受到的日志发送到指定端口，供级联agent的下一跳收集和接受日志，使用时需要指定目的ip和端口

例子：

agent1.sinks=k1

agent1.sinks.k2.type = avro

agent1.sinks.k2.channel = c2

agent1.sinks.k2.hostname = hadoop03 (指定的主机名或ip)

agent1.sinks.k2.port = 16666  (指定的端口号)

 

file_roll sink:可以将一定时间内收集到的日志写到一个指定的文件中，具体过程为用户指定一个文件夹和一个周期，然后启动agent，这时该文件夹会产生一个文件将该周期内收集到的日志全部写进该文件内，直到下一个周期再次产生一个新文件继续写入，以此类推，周而复始。

例子：

agent1.sinks=k1

agent1.sinks.k1.type = file_roll

agent1.sinks.k1.channel = c1

agent1.sinks.k1.sink.directory=/root/flumelog (指定文件所在目录的路径)

 

hdfs sink:将收集到的日志写入到新创建的文件中保存起来，存储路径为分布式的文件系统hdfs的路径，同时hdfs创建新文件的周期可以是时间，也可以是文件的大小，还可以是采集日志的条数

配置参数说明：

channel：数据来源管道

type:写入类型，如：hdfs

path:写入hdfs的路径，需要包含文件系统标识，比如：hdfs://namenode/flume/webdata/

可以使用flume提供的日期及%{host}表达式。

filePrefix： 默认值：FlumeData 写入hdfs的文件名前缀，可以使用flume提供的日期及%{host}表达式。

fileSuffix：写入hdfs的文件名后缀，比如：.lzo .log等。

inUsePrefix：临时文件的文件名前缀，hdfs sink会先往目标目录中写临时文件，再根据相关规则重命名成最终目标文件；

inUseSuffi: 默认值：.tmp  临时文件的文件名后缀。

rollInterval:  默认值：30  hdfs sink间隔多长将临时文件滚动成最终目标文件，单位：秒；

如果设置成0，则表示不根据时间来滚动文件；

注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据；

rollSize  默认值：1024 当临时文件达到该大小（单位：bytes）时，滚动成目标文件；

如果设置成0，则表示不根据临时文件大小来滚动文件；

rollCount  默认值：10  当events数据达到该数量时候，将临时文件滚动成目标文件；

如果设置成0，则表示不根据events数据来滚动文件；

idleTimeout  默认值：0

当目前被打开的临时文件在该参数指定的时间（秒）内，没有任何数据写入，则将该临时文件关闭并重命名成目标文件；

batchSize   默认值：100  每个批次刷新到HDFS上的events数量；

codeC  文件压缩格式，包括：gzip, bzip2, lzo, lzop, snappy

fileType  默认值：SequenceFile

文件格式，包括：SequenceFile, DataStream,CompressedStream 当使用DataStream时候，文件不会被压缩，不需要设置hdfs.codeC;当使用CompressedStream时候，必须设置一个正确的hdfs.codeC值；

maxOpenFiles  默认值：5000 最大允许打开的HDFS文件数，当打开的文件数达到该值，最早打开的文件将会被关闭；

minBlockReplicas  默认值：HDFS副本数   写入HDFS文件块的最小副本数。该参数会影响文件的滚动配置，一般将该参数配置成1，才可以按照配置正确滚动文件。

writeFormat  写sequence文件的格式。包含：Text, Writable（默认）

callTimeout  默认值：10000 执行HDFS操作的超时时间（单位：毫秒）；

threadsPoolSize  默认值：10  hdfs sink启动的操作HDFS的线程数。

rollTimerPoolSize  默认值：1  hdfs sink启动的根据时间滚动文件的线程数。

kerberosPrincipal： HDFS安全认证kerberos配置；

kerberosKeytab

HDFS安全认证kerberos配置；

proxyUser  代理用户

round 默认值：false  是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”，后面再介绍。如果启用，则会影响除了%t的其他所有时间表达式；

roundValue  默认值：1   时间上进行“舍弃”的值；

roundUnit  默认值：seconds 时间上进行”舍弃”的单位，包含：second,minute,hour

示例：

a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S

a1.sinks.k1.hdfs.round = true

a1.sinks.k1.hdfs.roundValue = 10

a1.sinks.k1.hdfs.roundUnit = minute

当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：

/flume/events/20151016/17:30/00

因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。

timeZone

默认值：Local Time  时区。

useLocalTimeStamp 默认值：flase  是否使用当地时间。

closeTries：默认值：0

hdfs sink关闭文件的尝试次数；

如果设置为1，当一次关闭文件失败后，hdfs sink将不会再次尝试关闭文件，这个未关闭的文件将会一直留在那，并且是打开状态。

设置为0，当一次关闭失败后，hdfs sink会继续尝试下一次关闭，直到成功。

retryInterval

默认值：180（秒）

hdfs sink尝试关闭文件的时间间隔，如果设置为0，表示不尝试，相当于于将hdfs.closeTries设置成1.

Serializer  默认值：TEXT

序列化类型。其他还有：avro_event或者是实现了EventSerializer.Builder的类名。

Hbase sink:hbase是一种数据库，可以储存日志，使用时需要指定存储日志的表名和列族名，然后agent就可以将收集到的日志逐条插入到数据库中。

tableName：要写入的HBase数据表名，不能为空；

columnFamily：数据表对应的列簇名，这个sink目前只支持一个列簇，不能为空；

batchSize：每次事务可以处理的最大Event数量，默认是100；

eventSerializerType：用来将event写入HBase，即将event转化为put。默认是org.apache.flume.sink.hbase.SimpleHbaseEventSerializer，还有一个是RegexHbaseEventSerializer，即适合HBaseSink的Serializer只有这俩，否则自己定制；

serializerContext：是eventSerializerType的配置信息，就是配置文件中包含“serializer.”的项；

例子：

agent1.sinks = k1

agent1.sinks.k1.type = hbase

agent1.sinks.k1.table = flume

agent1.sinks.k1.columnFamily=fl_conf

agent1.sinks.k1.serializer=org.apache.flume.sink.hbase.RegexHbaseEventSerializer</code></pre>            </div>
                </div>