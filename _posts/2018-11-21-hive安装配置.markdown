---
layout:     post
title:      hive安装配置
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
hive安装配置---coco<br><br><br>
# by coco<br>
# 2014-07-25<br>
5. hive的安装配置<br>
下载：apache-hive-0.13.1-bin.tar.gz<br>
[root@db96 local]# wget http://apache.fayea.com/apache-mirror/hive/hive-0.13.1/<br>
apache-hive-0.13.1-bin.tar.gz<br>
[root@db96 local]# tar -zxvf apache-hive-0.13.1-bin.tar.gz -C /usr/local/<br>
[root@db96 local]# cd /usr/local/<br>
[root@db96 local]# ln -s /usr/local/apache-hive-0.13.1-bin/ hive<br>
进入解压后的hive目录，进入conf<br>
[root@db96 conf]# cp hive-env.sh.template hive-env.sh<br>
[root@db96 conf]# cp hive-default.xml.template hive-site.xml<br><br><br>
Hive的配置：<br>
配置hive-env.sh  添加hadoop_home路径：<br>
将export HADOOP_HOME前面的‘#’号去掉，<br>
并让它指向您所安装hadoop的目录 （就是切换到这个目录下有hadoop的conf,lib,bin 等文件夹的目录），<br>
（mine：HADOOP_HOME=/usr/local/hadoop）<br>
其实在安装hive时需要指定HADOOP_HOME的原理基本上与<br>
在安装Hadoop时需要指定JAVA_HOME的原理是相类似的。<br>
Hadoop需要java作支撑，而hive需要hadoop作为支撑。<br>
将export HIVE_CONF_DIR=/usr/hive/conf,并且把‘#’号去掉<br>
将export HIVE_AUX_JARS_PATH=/usr/hive/lib<br>
esc(键)<br>
:wq<br>
source /hive-env.sh(生效文件)<br>
[root@db96 conf]# vim hive-env.sh<br>
HADOOP_HOME=/usr/local/hadoop/<br>
export HIVE_CONF_DIR=/usr/local/hive/conf                                                                                    <br>
export HIVE_AUX_JARS_PATH=/usr/local/hive/lib<br>
----------------------------------------------------------finish hive-env.sh<br>
在修改之前，要相应的创建目录，以便与配置文件中的<br>
路径相对应，否则在运行hive时会报错的。<br>
mkdir -p /usr/hive/warehouse<br>
mkdir -p /usr/hive/tmp<br>
mkdir -p /usr/hive/log<br>
进入hive安装目录下的conf文件夹下：<br>
cp  hive-default.xml.template<br>
这个文件中的配置项很多，篇幅也很长，所以要有耐心看。<br>
当然也可以使用搜索匹配字符串的方式进行查找：<br>
键入‘/hive.metastore.warehouse.dir’(回车)<br>
就会锁定到所需要的字符串上。<br>
其中有三处需要修改：<br>
&lt;property&gt;<br>
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;<br>
&lt;value&gt;/usr/hive/warehouse&lt;/value&gt;<br>
&lt;/property&gt;<br>
这个是设定数据目录<br>
-------------------------------------<br>
&lt;property&gt;<br>
&lt;name&gt;hive.exec.scratdir&lt;/name&gt;<br>
&lt;value&gt;/usr/hive/tmp&lt;/value&gt;<br>
&lt;/property&gt;<br>
这个是设定临时文件目录<br>
--------------------------------------<br>
//这个在笔者的文件中没有可以自己添加<br>
&lt;property&gt;<br>
&lt;name&gt;hive.querylog.location&lt;/name&gt;<br>
&lt;value&gt;/usr/hive/log&lt;/value&gt;<br>
&lt;/property&gt;<br>
这个是用于存放hive相关日志的目录<br>
其余的不用修改。<br>
--------------------------------------finish hive-site.xml<br><br><br>
cp hive-log4j.properties.template  hive-log4j.proprties<br>
vi hive-log4j.properties<br>
hive.log.dir=<br>
这个是当hive运行时，相应的日志文档存储到什么地方<br>
（mine：hive.log.dir=/usr/hive/log/${user.name}）<br>
hive.log.file=hive.log<br>
这个是hive日志文件的名字是什么<br>
默认的就可以，只要您能认出是日志就好，<br>
只有一个比较重要的需要修改一下，否则会报错。<br>
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter<br>
如果没有修改的话会出现：<br>
WARNING: org.apache.hadoop.metrics.EventCounter is deprecated.<br>
please use org.apache.hadoop.log.metrics.EventCounter  in all the  log4j.properties files.<br>
（只要按照警告提示修改即可）。<br>
-------------------------------------------------------finish all<br>
接下来要配置的是以mysql作为存储元数据l数据库的hive的安装<br>
要使用Hadoop来创建相应的文件路径，<br>
并且要为它们设定权限：<br>
hdfs dfs -mkdir -p  /usr/hive/warehouse<br>
hdfs dfs -mkdir -p /usr/hive/tmp<br>
hdfs dfs -mkdir -p /usr/hive/log<br>
hdfs dfs -chmod g+w /usr/hive/warehouse<br>
hdfs dfs -chmod g+w /usr/hive/tmp<br>
hdfs dfs -chmod g+w /usr/hive/log<br>
此种模式下是将hive的metadata存储在Mysql中，<br>
Mysql的运行环境支撑双向同步或是集群工作环境，<br>
这样的话，至少两台数据库服务器上会备份hive的元数据。<br>
既然在derby模式下的hive已经能够成功运行，<br>
这就说明在系统中关于hive配置文件中的参数是正确的，<br>
即hive-env.sh 和 hive-env.sh这两个文件中的配置是对的。<br>
继续配置hive-site.xml文件：<br>
&lt;property&gt;<br>
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;<br>
&lt;value&gt;hdfs://node0:9000/usr/hive/warehouse&lt;/value&gt;<br>
(这里就与前面的hdfs dfs -mkdir -p /usr/hive/warehouse相对应)<br>
&lt;/property&gt;<br>
其中node0指的是笔者的NameNode的hostname；<br>
&lt;property&gt;<br>
&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;<br>
&lt;value&gt;hdfs://node0:9000/usr/hive/warehouse&lt;/value&gt;<br>
&lt;/property&gt;<br>
//这个没有变化与derby配置时相同<br>
&lt;property&gt;<br>
&lt;name&gt;hive.querylog.location&lt;/name&gt;<br>
&lt;value&gt;/usr/hive/log&lt;/value&gt;<br>
&lt;/property&gt;<br>
-------------------------------------<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>
&lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNoExist=true&lt;/value&gt;<br>
&lt;/property&gt;<br>
javax.jdo.option.ConnectionURL<br>
这个参数使用来设置元数据连接字串<br>
-------------------------------------<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br>
&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br>
&lt;/property&gt;<br>
javax.jdo.option.ConnectionDriverName<br>
关于在hive中用java来开发与mysql进行交互时，<br>
需要用到一个关于mysql的connector，<br>
这个可以将java语言描述的对database进行的操作，<br>
转化为mysql可以理解的语句。<br>
connector是一个用java语言描述的jar文件。<br>
而这个connector可在官方网站上下载得到。<br>
经证实即便connector与mysql的版本号不一致，<br>
也可以实现相应的功能。<br>
connector是要copy到/usr/hive/lib目录下面的。<br>
cp  mysql-connector-java-5.1.1.18-bin  /usr/hive/lib<br>
-------------------------------------<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectorUserName&lt;/name&gt;<br>
&lt;value&gt;hive&lt;/value&gt;<br>
&lt;/property&gt;<br>
这个javax.jdo.option.ConnectionUserName<br>
是用来设置hive存放的元数据的数据库(这里是mysql数据库)的用户名称的。<br>
而这个‘hive‘可以根据用户自己的需要来自行设置<br>
--------------------------------------<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br>
&lt;value&gt;hive&lt;/value&gt;<br>
&lt;/property&gt;<br>
这个javax.jdo.option.ConnetionPassword是用来设置，<br>
用户登录数据库的时候需要输入的密码的。<br>
而这个‘hive’同样可以根据用户自己的需要来进行设置。<br>
---------------------------------------- 到此原理介绍完毕，下面是我的具体操作：<br><br><br>
[root@db96 conf]# vim hive-site.xml <br>
&lt;configuration&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;<br>
&lt;value&gt;hdfs://db96:9000/hive/warehousedir&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;<br>
&lt;value&gt;hdfs://db96:9000/hive/scratchdir&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.querylog.location&lt;/name&gt;<br>
&lt;value&gt;/usr/local/hive/logs&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>
&lt;value&gt;jdbc:mysql://192.168.8.96:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br>
&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br>
&lt;value&gt;root&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br>
&lt;value&gt;123456&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;hive.aux.jars.path&lt;/name&gt;<br>
&lt;value&gt;file:///usr/local/hive/lib/hive-hbase-handler-0.13.1.jar,file:///usr/local/hive/lib/protobuf-java-2.5.0.jar,file:///us<br>
r/local/hive/lib/hbase-client-0.96.0-hadoop2.jar,file:///usr/local/hive/lib/hbase-common-0.96.0-hadoop2.jar,file:///usr/local<br>
/hive/lib/zookeeper-3.4.5.jar,file:///usr/local/hive/lib/guava-11.0.2.jar&lt;/value&gt;<br>
&lt;/property&gt;<br>
&lt;property&gt;  <br>
&lt;name&gt;hive.metastore.uris&lt;/name&gt;  <br>
&lt;value&gt;thrift://192.168.8.96:9083&lt;/value&gt;  <br>
&lt;/property&gt;  <br>
&lt;/configuration&gt;<br>
上面的配置中用到的jar包均在hbase/lib下，<br>
[root@db96 lib]# pwd    //发现hive的lib目录下没有该文件。去hbase的lib下找到下面4个文件拷贝<br>
                        //到hive的lib目录下。<br>
/usr/local/hive/lib<br>
[root@db96 lib]# ll protobuf-java-2.5.0.jar hbase-client-0.96.2-hadoop2.jar hbase-common-0.96.2-hadoop2.jar  guava-12.0.1.jar<br>
ls: 无法访问protobuf-java-2.5.0.jar: 没有那个文件或目录<br>
ls: 无法访问hbase-client-0.96.2-hadoop2.jar: 没有那个文件或目录<br>
ls: 无法访问hbase-common-0.96.2-hadoop2.jar: 没有那个文件或目录<br>
ls: 无法访问guava-12.0.1.jar: 没有那个文件或目录<br>
[root@db96 lib]# pwd<br>
/usr/local/hbase/lib<br>
[root@db96 lib]# ll protobuf-java-2.5.0.jar hbase-client-0.96.2-hadoop2.jar hbase-common-0.96.2-hadoop2.jar  guava-12.0.1.jar<br>
-rw-r--r-- 1 root root 1795932 12月 11 2013 guava-12.0.1.jar<br>
-rw-r--r-- 1 root root  826678 3月  25 07:03 hbase-client-0.96.2-hadoop2.jar<br>
-rw-r--r-- 1 root root  371546 3月  25 07:03 hbase-common-0.96.2-hadoop2.jar<br>
-rw-r--r-- 1 root root  533455 12月 11 2013 protobuf-java-2.5.0.jar<br>
[root@db96 lib]# cp protobuf-java-2.5.0.jar hbase-client-0.96.2-hadoop2.jar hbase-common-0.96.2-hadoop2.jar  guava-12.0.1.jar /usr/local/hive/lib<br><br><br>
说明，其中的部分是配置与mysql连接的，因为hive需要用到关系型数据库来存储元信息，<br>
目前只支持mysql，所以我们需要提前装好一个mysql，创建mysql时记得设置字符集为latin1,<br>
否则后期建表会出问题。具体hive关联mysql的配置如下：<br>
https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin<br>
另外如果我们将来需要通过jdbc/odbc的方式来连接hive，需要启动metastore shfift，<br>
因此必须配置hive.metastore.uris。而hive.aux.jars.path是与hbase整合的时候需要用到的jar包，必须加上。<br><br><br>
启动之前先要创建hive元数据存放的路径文件：<br>
[root@db96 ~]# hadoop fs -mkdir -p /hive/scratchdir<br>
[root@db96 ~]# hadoop fs -mkdir /tmp<br>
[root@db96 ~]# hadoop fs -ls /hive<br>
[root@db96 ~]# hadoop fs -chmod -R g+w /hive/<br>
[root@db96 ~]# hadoop fs -chmod -R g+w /tmp<br><br><br>
基本配置完成后，接下来启动hive<br>
hive 启动：<br>
这个又可以分为启动metastore和hiveserver，其中metastore用于和mysql之间的表结构创建或更新时通讯，<br>
hiveserver用于客户端连接，这两个都要启动，<br><br><br>
启动MySQL:<br>
创建hive连接的用户，和hive存放元数据的数据库，如下：<br>
mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY '123456' ;         <br>
Query OK, 0 rows affected (0.09 sec)<br>
mysql&gt; create database hive default character set latin1;     //字符集必须是latian1,如果是utf8则很多hive元数据表无法创建。<br>
Query OK, 1 row affected (0.01 sec)<br><br><br>
具体的启动命令：<br>
启动metastore：（远程mysql需要启动）<br>
hive --service metastore -hiveconf hbase.zookeeper.quorum=db96,db98,db99 <br>
-hiveconf hbase.zookeeper.property.clientPort=2181 <br>
启动hiveservice：<br>
hive --service hiveserver -hiveconf hbase.zookeeper.quorum=db96,db98,db99 <br>
-hiveconf hbase.zookeeper.property.clientPort=2181 //(启动服务，这样jdbc:hive就能连上，默认10000端口，后面的部分一定要带上，否则用eclipse连接不上的)<br><br><br>
hive运行测试：<br>
进入hive客户端，运行show table;查看表的情况。<br>
运行：<br>
create table test3(time string,amount string) row format delimited fields <br>
terminated by '\t' lines terminated by '\n' stored as textfile;   创建本地表,非hbase对应表<br>
运行：<br>
CREATE TABLE hivetest(key int, val string) STORED BY <br>
'org.apache.hadoop.hive.hbase.HBaseStorageHandler' <br>
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val") <br>
TBLPROPERTIES ("hbase.table.name" = "hivetest");来创建于hbase关联的表，<br>
这样在hbase shell下也能看到，两笔新增数据都能实时看到。<br><br><br>
hive&gt; show tables;<br>
OK<br>
Time taken: 0.696 seconds<br>
hive&gt; create table test(time string,amount string) row format delimited <br>
fields terminated by '\t' lines terminated by '\n' stored as textfile;<br>
OK<br>
Time taken: 0.672 seconds<br>
hive&gt; show tables;<br>
OK<br>
test3<br>
Time taken: 0.019 seconds, Fetched: 1 row(s)<br>
hive&gt; CREATE TABLE hivetest(key int, val string) <br>
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' <br>
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val") <br>
TBLPROPERTIES ("hbase.table.name" = "hivetest");<br>
OK<br>
Time taken: 2.264 seconds<br>
hive&gt; show tables;<br>
OK<br>
hivetest<br>
test3<br>
Time taken: 0.034 seconds, Fetched: 2 row(s)<br><br><br>
此时去hbase中我们已经能看到，我们刚才在hive中创建的表hivetest在hbase中能看到：<br>
hbase(main):001:0&gt; list<br>
TABLE                                                                                                                        <br>
SLF4J: Class path contains multiple SLF4J bindings.<br>
SLF4J: Found binding in [jar:file:/usr/local/hbase-0.96.2-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>
SLF4J: Found binding in [jar:file:/usr/local/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.<br>
hivetest                                                                                                                     <br>
student                                                                                                                      <br>
test                                                                                                                         <br>
3 row(s) in 1.7040 seconds<br><br><br>
=&gt; ["hivetest", "student", "test"]<br>
hbase(main):002:0&gt; <br><br><br>
到此，整个整合过程都完成了，你可以在linux用各种shell来测试，也可以通过eclipse连接到hive来测试，<br>
和通过jdbc连接普通数据库一致，不过有一些注意分享给大家：<br>
1.如果要insert与hbase整合的表，不能像本地表一样load，需要利用已有的表进行，<br>
如insert overwrite table hivetest select * from test;  <br>
其中test的语句(create table test4(key int,val string) row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile; ),<br>
注意类型要一致，否则用insert overwrite table hivetest select * from test; 导不进去数据<br>
2.在hive中的 修改能同步反应到hbase中，但是如果在hbase中对新的列做了数据修改，<br>
则不会反应到hive中，因为hive的原数据文件并没有更新。
            </div>
                </div>