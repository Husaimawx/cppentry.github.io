---
layout:     post
title:      hive学习笔记第一篇
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/ping550/article/details/78292881				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1 align="center">Hive学习篇</h1>
<h2>安装</h2>
<p>安装hadoo，本实例中hadoop在/usr/local/hadoop下</p>
<p>Hive安装在/home/Hadoop/app/hive下</p>
<p>参考文档：<a href="http://blog.csdn.net/xiaoshunzi111/article/details/51889486" rel="nofollow">http://blog.csdn.net/xiaoshunzi111/article/details/51889486</a></p>
<p><strong> </strong></p>
<h2>测试练习以及遇到的问题：</h2>
<p>开启hadoop</p>
<p>cd /usr/local/hadoop/sbin</p>
<p>./start-all.sh</p>
<p> </p>
<p>1.      开启hive：</p>
<p>1）Hive</p>
<p>报错Exception in thread "main" java.lang.NoClassDefFoundError:org/apache/hadoop/hive/ql/CommandNeedRetryException</p>
<p>解决方法:</p>
<p>修改<span style="color:rgb(85,85,85);">hadoop/conf/ hadoop-env.sh</span><span style="color:rgb(85,85,85);">；</span><span style="color:#555555;">将</span><span style="color:#555555;">HADOOP_CLASSPATH</span><span style="color:#555555;">原来的值改为如下，重启</span><span style="color:#555555;">hadoop</span><span style="color:#555555;">集群，然后启动</span><span style="color:#555555;">hive</span><span style="color:#555555;">，启动成功。</span></p>
<p><span style="color:#555555;">exportHADOOP_CLASSPATH=.:$CLASSPATH:$HADOOP_CLASSPATH:$HADOOP_HOME/bin</span></p>
<p> </p>
<p>2）依然报错：[ERROR]Terminal initialization failed; falling back to unsupported</p>
<p>解决方法：命令行输入</p>
<p><span style="color:#333333;">export HADOOP_USER_CLASSPATH_FIRST=true</span></p>
<p><span style="color:#333333;">3</span><span style="color:#333333;">）</span><span style="color:#333333;">hive</span><span style="color:#333333;">启动异常</span><span style="color:#333333;">Causedby: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relativepath
 in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span></p>
<p><span style="color:#333333;">解决方法：</span></p>
<p><span style="color:#444444;background:#F1F1F1;">1.</span><span style="color:#444444;background:#F1F1F1;">查看hive-site.xml</span>配置，会看到配置值含有"system:java.io.tmpdir"的配置项</p>
<p><span style="color:#444444;background:#F1F1F1;">2.</span><span style="color:#444444;background:#F1F1F1;">新建文件夹在hive</span>目录下新建一个tmp目录<br>
3.将含有"system:java.io.tmpdir"的配置项的值修改为如上地址<span style="color:#444444;"><br></span><span style="color:rgb(68,68,68);">把</span><span style="color:rgb(68,68,68);">${</span><span style="color:#444444;background:#F1F1F1;">system:java.io.tmpdir</span><span style="color:rgb(68,68,68);">}</span><span style="color:rgb(68,68,68);">改成：</span><span style="color:rgb(68,68,68);">/home/hadoop/hive/tmp
</span><span style="color:#444444;background:#F1F1F1;">有2</span>个地方</p>
<p><span style="color:#333333;">4</span><span style="color:#333333;">）</span><span style="color:#333333;">登录到</span><span style="color:#333333;">hive</span><span style="color:#333333;">数据仓库后，输入一些命令，例如（</span><span style="color:#333333;">show databases
</span><span style="color:#333333;">，</span><span style="color:#333333;">show tables)</span><span style="color:#333333;">，会报出如下错误：</span></p>
<p><span style="color:#333333;">Failed with exception Java</span><span style="color:#444444;">.io.IOException</span><span style="color:#333333;">:java</span><span style="color:#444444;">.lang.IllegalArgumentException</span><span style="color:#333333;">: java</span><span style="color:#444444;">.NET.URISyntaxException</span><span style="color:#333333;">:
</span><span style="color:#FF0000;">Relative path in</span>absolute URI: ${system:user.name}</p>
<p>解决方法：</p>
<p>打开hive-site.xml找到如下属性</p>
<p align="left"><span style="color:#444444;background:#F1F1F1;">&lt;property&gt;</span></p>
<p align="left"><span style="color:#444444;background:#F1F1F1;">   &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span></p>
<p align="left"><span style="color:#444444;background:#F1F1F1;">   &lt;value&gt;</span><span style="color:rgb(68,68,68);">/home/hadoop/hive/tmp</span><span style="color:#444444;background:#F1F1F1;"> /${system:user.name}&lt;/value&gt;</span></p>
<p align="left"><span style="color:#444444;background:#F1F1F1;">   &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;</span></p>
<p><span style="color:#444444;background:#F1F1F1;">  &lt;/property&gt;</span></p>
<p>修改成：</p>
<p><span style="color:#444444;background:#F1F1F1;">&lt;property&gt;</span></p>
<p><span style="color:#444444;background:#F1F1F1;">   &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span></p>
<p><span style="color:#444444;background:#F1F1F1;">    </span><span style="color:rgb(68,68,68);">/home/hadoop/hive/tmp</span><span style="color:#444444;background:#F1F1F1;">${user.name}&lt;/value&gt;</span></p>
<p><span style="color:#444444;background:#F1F1F1;">   &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;</span></p>
<p><span style="color:#444444;background:#F1F1F1;">  &lt;/property&gt;</span></p>
<p>2.      Hive 创建数据库，</p>
<p>Hive&gt;create table testtb(id INT,name STRING) ROW FORMATDELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;</p>
<p>3.      将本地文件导入数据库：</p>
<p>Hive&gt;load data local inpath ‘/home/Hadoop/app/hive/sz.data’overwrite into table testtb;</p>
<p>4.      中文乱码</p>
<p>解决：</p>
<p>5.      查看hdfs目录下的文件</p>
<p>hadoop dfs  -ls/user/Hadoop/</p>
<p>6.      查看数据库的目录</p>
<p><img src="https://img-blog.csdn.net/20171020104338534?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGluZzU1MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p></p>
<p>7.      通过脚本创建表hive-mapper.q文件内容如下：</p>
<p>（第一行text_bp指定在某个数据库下面创建表；最后一行location指定该表在hdfs上存放的位置）</p>
<p>CREATETABLE<span style="color:#FF0000;"> text_bp</span>.u_data_new (</p>
<p>  userid INT,</p>
<p>  movieid INT,</p>
<p>  rating INT,</p>
<p>  weekday INT)</p>
<p>ROWFORMAT DELIMITED</p>
<p>FIELDSTERMINATED BY '\t'</p>
<p><span style="color:#FF0000;">LOCATION</span>'/user/hive/warehouse/text_bp.db/u_data_new';</p>
<p><img src="https://img-blog.csdn.net/20171020104425109?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcGluZzU1MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p></p>
<p>Weekday_mapper.py脚本的内容如下：</p>
<p>importsys</p>
<p>importdatetime</p>
<p> </p>
<p>for linein sys.stdin:</p>
<p>  line = line.strip()</p>
<p>  userid, movieid, rating, unixtime =line.split('\t')</p>
<p>  weekday =datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()</p>
<p>  print'\t'.join([userid, movieid, rating, str(weekday)])</p>
<p> </p>
<h2>大数据处理步骤</h2>
<p>1.      导入大数据到hive</p>
<p>导入方式：从本地文件系统中导入数据到Hive表；</p>
<p>1.      Hive&gt; create table testtb(idINT,name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATEDBY '\n' STORED AS TEXTFILE;</p>
<p>2.      将本地文件导入数据库：</p>
<p>Hive&gt;load data local inpath ‘/home/Hadoop/app/hive/sz.data’overwrite into table testtb;</p>
<p>导入方式二：通过mysql同步到hive</p>
<p>前提：安装mysql；安装hadoop以及hive</p>
<p> </p>
<p align="left"><span style="color:rgb(69,69,69);">导入mysql</span>的test数据库的所有表到hive：</p>
<div>
<p align="left"><span style="color:#333333;">./sqoop import --connect jdbc:mysql://localhost:3306/test--username hadoop_test --password 123456 --table mytest--hive-import</span></p>
</div>
<p> </p>
<p> </p>
            </div>
                </div>