---
layout:     post
title:      大数据学习笔记-------------------(1)
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/henni_719/article/details/52606565				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1 align="center">第一部分 Spark学习</h1>
<p> 该部分，主要对Spark学习笔记进行记录，学习资料翻译自《apache_spark_tutorial.pdf》 ，学习网站：<a>www.tutorialspoint.com</a></p>
<p> 该部分为5个章节来学习Spark：</p>
<p> Ø  第1章 Spark介绍</p>
<p>Ø  第2章 Spark弹性分布数据集</p>
<p>Ø  第3章 Spark安装</p>
<p>Ø  第4章 Spark核心编程</p>
<p>Ø  第5章 Spark调度与高级编程</p>
<br clear="all"><p align="left"> </p>
<h2 align="center">第1章 Spark介绍</h2>
<h3>1.1Spark简介与发展</h3>
<p>       大部分企业习惯用Hadoop去分析数据集，选择Hadoop框架的原因是由于它的编程基于MapReduce编程模型，于此同时，它提供了一个scalable(扩展性)、flexible(兼容性)、fault-tolerant(容错性)、cost effective(经济有效)的计算解决方案。就查询和运行之间的等待时间而言，当前考虑的主要问题是去维护处理大数据集的速度。</p>
<p>       Spark是由Apache软件基金会推出，Spark出现时为了提高Hadoop处理大数据的速度。但与传统概念相反的是，Spark不是Hadoop的改进版，它不依赖Hadoop，因为spark有自己的群管理器。Hadoop仅仅是Spark实现的一种方法。</p>
<p>       Spark使用Hadoop有以下两种方式：存储（storage）和处理（processing）。由于Spark有自己的集群计算管理器，因此它用Hadoop的目的仅仅是为了存储。</p>
<p>       Spark是一个轻量级的集群计算基数，设计它的目的是为了更快的计算。它基于Hadoop的MapReduce而且扩展了MapReduce的模型，为了将MapReduce更有效地用于多种类型的计算，其包括交互式查询和流处理。</p>
<p>       Spark设计的目的是为了覆盖更大的工作负载范围，例如批处理应用程序、迭代算法、交互查询和流媒体。除了支持所在各自的系统中的工作负载，它还减轻了维护单独的工具的管理负担。</p>
<p>       Spark是在2009年在加州大学伯克利分校的AMPLab由马太·扎哈里亚开发Hadoop子项目之一。2010年在BSD许可下，它是开源的。在2013年，它被捐赠给ApacheSoftware Foundation，现在的ApacheSpark已经成为一个顶级Apache项目。</p>
<h3>1.2 Spark特点</h3>
<p>Ø  <strong>处理速度</strong></p>
<p>Spark有助于提高应用程序在Hadoop集群运行速度，在内存中的运行速度是之前的100倍，在硬盘上的运行速度是之前的10倍。通过减少对硬盘读写操作的次数，来提升处理速度。Spark把中间数据存储在内存中。</p>
<p>Ø  <strong>支持多种开发语言</strong></p>
<p>Spark提供了Java、Scala、Python的API。因此，可以用不同的开发语言来开发应用程序。Spark为交互查询配备了80个high-level操作符。</p>
<p>Ø  <strong>超强的分析能力</strong></p>
<p>Spark不但支持Map和Reduce，而且支持SQL查询、流数据、机器学习（ML）和图形算法。</p>
<h3>1.3 Spark与Hadoop集成</h3>
<p align="left">       下面流程图显示了Spark与Hadoop的三种集成方式：</p>
<p align="left">         <img src="https://img-blog.csdn.net/20160921124529506?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p align="left"> Spark部署的三种方式详细介绍如下：</p>
<p align="left">l <strong>独立安装(standalone)</strong></p>
<p align="left">    Spark独立部署意味着Spark占据HDFS的顶部（Hadoop分布式文件系统）与此同时为HDFS分配空间。在这里，Spark和MapReduce将并排运行来覆盖Spark集群中的所有工作。</p>
<p align="left">l <strong>Spark与Hadoop资源管理器(YARN)</strong></p>
<p align="left">    Hadoop Yarn部署意味着，Spark仅仅运行在Yarn且不带任何pre-installation或者root权限请求。它有助于把Spark集成进Hadoop生态系统或Hadoop堆栈中。它运行其他组件在堆栈顶部运行。</p>
<p align="left">l <strong>Spark与MapReduce</strong></p>
<p align="left">    SIMR习惯用来启动单独部署的sparkjob。用SIMR,用户启动并使用spark的shell，不需要任何管理员权限。</p>
<h3>1.4 Spark组件</h3>
<p align="left">下图说明了Spark的不同组件：     </p>
<p align="left"> <img src="https://img-blog.csdn.net/20160921124624154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p align="left"><strong>Spark Core</strong></p>
<p align="left">         Spark Core是Spark平台（包含所有其他功能函数）的底层一般执行引擎。它提供了内存计算和外部存储系统引用的数据集。</p>
<p align="left"><strong>Spark SQL</strong></p>
<p align="left">          Spark SQL是SparkCore顶部的一个组件，它引入了一种名为SchemaRDD新的 数据抽象，它提供了结构化和半结构化数据的支持。</p>
<p align="left"><strong>Spark Streaming</strong></p>
<p align="left">           Spark Streaming利用SparkCore快速的调度能力来执行流的分析。它获取小批量数，然后在这些小批量数据上进行RDD转换。</p>
<p align="left"><strong>MLlib (Machine Learning Library)</strong></p>
<p align="left">            MLlib是Spark上的一个分布式的机器学习框架。Spark是基于内存的分布式架构。根据基准，它是由对交替最小二乘（ALS）实现的MLlib开发完成的。Spark MLlib是ApacheMahout中的基于磁盘的Hadoop版本的9倍快。</p>
<p align="left"><strong>GraphX</strong></p>
<p align="left">            GraphX是一个在Spark顶部的布式图形处理框架。它提供了用于描述图形计算的API，它用Pregel 抽象的API建立用户定义的图形模型。为该抽象提供了一个优化 的运行时间。</p>
            </div>
                </div>