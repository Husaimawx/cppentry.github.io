---
layout:     post
title:      Hadoop在虚拟机简单的集群
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/WithLin/article/details/51175863				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<div style="text-align:center;"><span style="font-size:24px;">hadoop简单的集群</span></div>
<div style="text-align:center;"><span style="font-size:24px;">1.vi /etc/hosts （3节点都修改）<br><br>
192.168.8.77 DB77<br>
192.168.8.78 DB78<br>
192.168.8.79 DB79<br><br><br><br>
2.3台机器 创建hadoop 用户<br>
useradd hadoop<br>
passwd hadoop<br>
hadoop 密码:123<br><br>
3.安装JDK (3台都安装)<br>
[root@DB77 ~]# mkdir /usr/java<br>
[root@h201 ~]# mount //192.168.8.11/ISO /ff -o username=vfast,password=123<br>
[root@h201 hadoop]# cp jdk-8u66-linux-x64.tar.gz /usr/java<br>
[root@DB77 ~]# tar zxvf /home/hadoop/jdk-8u66-linux-x64.tar.gz -C /usr/java<br><br>
[root@h201 ~]# vi /etc/profile<br>
export JAVA_HOME=/usr/java/jdk1.8.0_66<br>
export JAVA_BIN=/usr/java/jdk1.8.0_66<br>
export PATH=$PATH:$JAVA_HOME/bin<br>
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>
export JAVA_HOME JAVA_BIN PATH CLASSPATH<br>
（<br>
export HADOOP_HOME_WARN_SUPPRESS=10<br>
export HADOOP_HOME=/home/hadoop/java/hadoop-1.1.2<br>
export JAVA_HOME=/home/hadoop/java/jdk1.7.0_65<br>
export PATH=.:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH<br>
）<br><br>
[root@h201 ~]# reboot<br><br>
4.安装ssh 证书<br>
su - hadoop<br>
[hadoop@h201 ~]$ ssh-keygen -t rsa<br>
[hadoop@h202 ~]$ ssh-keygen -t rsa<br>
[hadoop@h203 ~]$ ssh-keygen -t rsa<br><br>
[hadoop@h201 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db77<br>
[hadoop@h201 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db78<br>
[hadoop@h201 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db79<br><br>
[hadoop@h202 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db77<br>
[hadoop@h202 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db78<br>
[hadoop@h202 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db79<br><br>
[hadoop@h203 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db77<br>
[hadoop@h203 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db78<br>
[hadoop@h203 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub db79<br><br><br>
5.<br>
[hadoop@h201 hadoop]$ cp hadoop-2.6.0.tar.gz /home/hadoop<br>
[hadoop@h201 ~]$ tar -zxvf /home/hadoop/hadoop-2.6.0.tar.gz <br><br>
[hadoop@h201 ~]$ vi .bash_profile <br><br>
export JAVA_HOME=/usr/java/jdk1.8.0_66<br>
export JAVA_BIN=/usr/java/jdk1.8.0_66<br>
export PATH=$PATH:$JAVA_HOME/bin<br>
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>
export JAVA_HOME JAVA_BIN PATH CLASSPATH<br><br>
HADOOP_HOME=/home/hadoop/hadoop-2.6.0<br>
HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop<br>
PATH=$HADOOP_HOME/bin:$PATH<br>
export HADOOP_HOME HADOOP_CONF_DIR PATH<br><br>
[hadoop@h201 ~]$ source .bash_profile <br><br>
6.<br>
修改core-site.xml<br>
[hadoop@h201 ~]$ cd hadoop-2.6.0/etc/hadoop/<br>
[hadoop@h201 hadoop]$ vi core-site.xml <br><br>
&lt;configuration&gt;<br>
&lt;property&gt;<br>
&lt;name&gt;fs.default.name&lt;/name&gt;<br>
&lt;value&gt;hdfs://db77:9000&lt;/value&gt;<br>
&lt;description&gt;NameNode URI.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;io.file.buffer.size&lt;/name&gt;<br>
&lt;value&gt;131072&lt;/value&gt;<br>
&lt;description&gt;Size of read/write buffer used inSequenceFiles.&lt;/description&gt;<br>
&lt;/property&gt;<br>
&lt;/configuration&gt;<br><br>
7.<br>
编辑hdfs-site.xml<br>
[hadoop@h201 hadoop-2.6.0]$ mkdir -p dfs/name<br>
[hadoop@h201 hadoop-2.6.0]$ mkdir -p dfs/data<br>
[hadoop@h201 hadoop-2.6.0]$ mkdir -p dfs/namesecondary<br><br>
[hadoop@h201 hadoop]$ vi hdfs-site.xml<br><br>
&lt;property&gt;<br>
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;<br>
&lt;value&gt;db77:50090&lt;/value&gt;<br>
&lt;description&gt;The secondary namenode http server address andport.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br>
&lt;value&gt;file:///home/hadoop/hadoop-2.6.0/dfs/name&lt;/value&gt;<br>
&lt;description&gt;Path on the local filesystem where the NameNodestores the namespace and transactions logs persistently.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br>
&lt;value&gt;file:///home/hadoop/hadoop-2.6.0/dfs/data&lt;/value&gt;<br>
&lt;description&gt;Comma separated list of paths on the local filesystemof a DataNode where it should store its blocks.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;<br>
&lt;value&gt;file:///home/hadoop/hadoop-2.6.0/dfs/namesecondary&lt;/value&gt;<br>
&lt;description&gt;Determines where on the local filesystem the DFSsecondary name node should store the temporary images to merge. If this is acomma-delimited list of directories then the image is replicated in all of thedirectories for redundancy.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;dfs.replication&lt;/name&gt;<br>
&lt;value&gt;2&lt;/value&gt;<br>
&lt;/property&gt;<br><br><br>
8.<br>
编辑mapred-site.xml<br><br>
[hadoop@h201 hadoop]$ cp mapred-site.xml.template mapred-site.xml<br>
[hadoop@h201 hadoop]$ vim mapred-site.xml<br>
&lt;property&gt;<br>
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>
&lt;value&gt;yarn&lt;/value&gt;<br>
&lt;description&gt;Theruntime framework for executing MapReduce jobs. Can be one of local, classic oryarn.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br>
&lt;value&gt;db77:10020&lt;/value&gt;<br>
&lt;description&gt;MapReduce JobHistoryServer IPC host:port&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;<br>
&lt;value&gt;db77:19888&lt;/value&gt;<br>
&lt;description&gt;MapReduce JobHistoryServer Web UI host:port&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
*****<br>
属性”mapreduce.framework.name“表示执行mapreduce任务所使用的运行框架，默认为local，需要将其改为”yarn”<br>
*****<br><br>
9.<br>
编辑yarn-site.xml<br>
[hadoop@h201 hadoop]$ vi yarn-site.xml<br><br>
&lt;property&gt;<br>
&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>
&lt;value&gt;db77&lt;/value&gt;<br>
&lt;description&gt;The hostname of theRM.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
&lt;property&gt;<br>
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>
&lt;description&gt;Shuffle service that needs to be set for Map Reduceapplications.&lt;/description&gt;<br>
&lt;/property&gt;<br><br>
10.<br>
[hadoop@h201 hadoop]$ vi hadoop-env.sh <br>
export JAVA_HOME=/usr/java/jdk1.8.0_66<br><br>
11.<br>
[hadoop@h201 hadoop]$ vi slaves <br>
db78<br>
db79<br><br>
12.<br>
[hadoop@h201 ~]$ scp -r ./hadoop-2.6.0/ hadoop@db78:/home/hadoop/<br>
[hadoop@h201 ~]$ scp -r ./hadoop-2.6.0/ hadoop@db79:/home/hadoop/<br><br>
验证：<br><br>
[hadoop@h201 hadoop-2.6.0]$ bin/hdfs namenode -format<br>
[hadoop@h201 hadoop-2.6.0]$ sbin/start-all.sh <br><br>
[hadoop@h201 hadoop-2.6.0]$ jps<br>
7054 SecondaryNameNode<br>
7844 Jps<br>
7318 NameNode<br>
7598 ResourceManager<br><br>
[hadoop@h201 hadoop-2.6.0]$ bin/hadoop fs -ls /<br>
[hadoop@h201 hadoop-2.6.0]$ bin/hadoop fs -mkdir /aaa<br></span></div>
<div style="text-align:center;"><br></div>
            </div>
                </div>