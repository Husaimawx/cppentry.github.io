---
layout:     post
title:      Spark Streaming基础与实践
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h1>（一）Spark Streaming简介</h1><p>参考文章：<a href="http://www.cnblogs.com/haozhengfei/p/e353daff460b01a5be13688fe1f8c952.html" rel="nofollow">点击打开链接</a></p><h2>1、Spark Streaming概念</h2><p>Spark Streaming是Spark核心API的一个扩展，可以实现<span style="color:#ff0000;">高吞吐量</span>的、具备<span style="color:#ff0000;">容错机制</span>的<span style="color:#ff0000;"><strong>实时流数据</strong></span>的处理。类似于ApacheStorm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p>多种数据源获取数据：<br><p><img src="https://img-blog.csdn.net/20180331095057676" alt=""><br></p><p>Spark Streaming接收Kafka、Flume、HDFS等各种来源的实时输入数据，进行处理后，处理结构保存在HDFS、DataBase等各种地方<br></p><h2>2、为什么要学习Spark Streaming</h2><div><p>1.易用</p></div><p><img src="https://img-blog.csdn.net/2018033109524044?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><p>2.容错</p><img src="https://img-blog.csdn.net/2018033109531274?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><p></p><p>3.易整合到Spark体系</p><img src="https://img-blog.csdn.net/20180331095332811?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><h2>3、Spark Core 与 Spark Streaming</h2><div>两者关系如图：<br></div><p><img src="https://img-blog.csdn.net/20180331095423942?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• 第一步：针对小数据块的RDD DAG的构建<br>• 第二步：连续Data的切片处理<br></p><p><img src="https://img-blog.csdn.net/20180331095714540" alt=""><br></p><p>• Spark Streaming将接收到的实时流数据，按照一定时间间隔，对数据进行拆分，交给SparkEngine引擎处理，最终得到一批批的结果<br></p><p><img src="https://img-blog.csdn.net/20180331095740176?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• 每一批数据，在Spark内核对应一个RDD实例<br>• Dstream可以看做一组RDDs，即RDD的一个序列<br></p><p><img src="https://img-blog.csdn.net/20180331095804936?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><h2><span style="font-size:18px;"><span style="font-weight:normal;">4</span>.  Spark与Storm的对比</span></h2><div><span style="font-size:18px;">spark streamming需要设置batch interval，严格说也是批处理框架，时间设置较小，可以理解为微型实时流处理框架</span></div><table border="1" cellspacing="0" cellpadding="0" width="568"><tbody><tr><td valign="top"><p align="center"><strong>Spark</strong></p></td>  <td valign="top"><p align="center"><strong>Storm</strong></p></td> </tr><tr><td valign="top"><p><img src="https://img-blog.csdn.net/20180331095957481?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></td>  <td valign="top"><p><img src="https://img-blog.csdn.net/20180331100021367?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></td> </tr><tr><td valign="top"><p align="center">开发语言：Scala</p></td>  <td valign="top"><p align="center">开发语言：Clojure</p></td> </tr><tr><td valign="top"><p align="center">编程模型：DStream</p></td>  <td valign="top"><p align="center">编程模型：Spout/Bolt</p></td> </tr><tr><td valign="top"><p align="center"><img src="https://img-blog.csdn.net/20180331100033780?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></td>  <td valign="top"><p align="center"><img src="https://img-blog.csdn.net/20180331100040615?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p></td> </tr></tbody></table><p></p><h1>（二）<span style="font-size:28px;"><strong> DStream</strong></span></h1><h2>1.  什么是DStream</h2><p>Discretized Stream是Spark Streaming提供了表示<span style="color:#ff0000;"><strong>连续数据流</strong></span>的、高度抽象的被称为离散流的DStream。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据，如下图：</p><p><img src="https://img-blog.csdn.net/20180331100238717?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><p>任何对DStream的操作都会转变为对底层RDD的操作</p><img src="https://img-blog.csdn.net/20180331100254228?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>• Spark Streaming程序中一般会有若干个对DStream的操作。DStreamGraph就是由这些操作的依赖关系构成<br><p></p><p>计算过程由Spark engine来完成</p><p><img src="https://img-blog.csdn.net/20180331100308120?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• 将连续的数据持久化、离散化，然后进行批量处理<br>• 为什么？<br>    – 数据持久化：接收到的数据暂存<br>    – 离散化：按时间分片，形成处理单元<br>    – 分片处理：分批处理<br></p><p><img src="https://img-blog.csdn.net/20180331100840919?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• 作用Dstream上的Operation分成两类：<br>    – Transformation：转换<br>        • Spark支持RDD进行各种转换，因为DStream是由RDD组成的Spark Streaming提供了一个可以在DStream上使用的转换集合，这些集合和RDD上可用的转换类似<br>        • 转换应用到DStream的每个RDD<br>        • Spark Streaming提供了reduce和count这样的算子，但不会直接触发DStream计算<br>        • Map、flatMap、join、reduceByKey<br>    – Output：执行算子、或输出算子<br>        • Print<br>        • saveAsObjectFile、saveAsTextFile、saveAsHadoopFiles：将一批数据输出到Hadoop文件系统中，用批量数据的开始时间戳来命名<br>        • forEachRDD：允许用户对DStream的每一批量数据对应的RDD本身做任意操作<br></p><p>• 一系列transformation操作的抽象<br>• 例如：<br>    – c = a.join(b), d = c.filter() 时， 它们的 DAG 逻辑关系是a/b → c，c → d，但在 Spark Streaming 在进行物理记录时却是反向的 a/b ← c, c ← d<br></p><p><img src="https://img-blog.csdn.net/20180331101009585?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p><img src="https://img-blog.csdn.net/20180331101018117?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• Dstream之间的转换所形成的的依赖关系全部保存在DStreamGraph中， DStreamGraph对于后期生成RDD Graph至关重要<br>• DStreamGraph有点像简洁版的DAG scheduler，负责根据某个时间间隔生成一序列JobSet，以及按照依赖关系序列化<br></p><p><img src="https://img-blog.csdn.net/20180331101130374?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p><img src="https://img-blog.csdn.net/20180331101150664?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><h2><span style="font-size:18px;color:#333333;">2.  DStream相关操作</span></h2><p></p><p>DStream上的原语与RDD的类似，分为Transformations（转换）和OutputOperations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p><h3>2.1.  Transformations on DStreams</h3><table border="1" cellspacing="0" cellpadding="0" width="568"><tbody><tr><td valign="top"><p align="center"><strong>Transformation</strong></p></td>  <td valign="top"><p align="center"><strong>Meaning</strong></p></td> </tr><tr><td valign="top"><p>map(func)</p></td>  <td valign="top"><p>Return a new DStream by passing each  element of the source DStream through a function func.</p></td> </tr><tr><td valign="top"><p>flatMap(func)</p></td>  <td valign="top"><p>Similar to map, but each input item can  be mapped to 0 or more output items.</p></td> </tr><tr><td valign="top"><p>filter(func)</p></td>  <td valign="top"><p>Return a new DStream by selecting only  the records of the source DStream on which func returns true.</p></td> </tr><tr><td valign="top"><p>repartition(numPartitions)</p></td>  <td valign="top"><p>Changes the level of parallelism in this  DStream by creating more or fewer partitions.</p></td> </tr><tr><td valign="top"><p>union(otherStream)</p></td>  <td valign="top"><p>Return a new DStream that contains the  union of the elements in the source DStream and otherDStream.</p></td> </tr><tr><td valign="top"><p>count()</p></td>  <td valign="top"><p>Return a new DStream of single-element  RDDs by counting the number of elements in each RDD of the source DStream.</p></td> </tr><tr><td valign="top"><p>reduce(func)</p></td>  <td valign="top"><p>Return a new DStream of single-element  RDDs by aggregating the elements in each RDD of the source DStream using a  function func (which takes two arguments and returns one). The function  should be associative so that it can be computed in parallel.</p></td> </tr><tr><td valign="top"><p>countByValue()</p></td>  <td valign="top"><p>When called on a DStream of elements of  type K, return a new DStream of (K, Long) pairs where the value of each key  is its frequency in each RDD of the source DStream.</p></td> </tr><tr><td valign="top"><p>reduceByKey(func, [numTasks])       </p></td>  <td valign="top"><p>When called on a DStream of (K, V) pairs,  return a new DStream of (K, V) pairs where the values for each key are  aggregated using the given reduce function. Note: By default, this uses Spark's  default number of parallel tasks (2 for local mode, and in cluster mode the  number is determined by the config property spark.default.parallelism) to do  the grouping. You can pass an optional numTasks argument to set a different  number of tasks.</p></td> </tr><tr><td valign="top"><p>join(otherStream, [numTasks])</p></td>  <td valign="top"><p>When called on two DStreams of (K, V) and  (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of  elements for each key.</p></td> </tr><tr><td valign="top"><p>cogroup(otherStream, [numTasks])</p></td>  <td valign="top"><p>When called on a DStream of (K, V) and  (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</p></td> </tr><tr><td valign="top"><p>transform(func)        </p></td>  <td valign="top"><p>Return a new DStream by applying a  RDD-to-RDD function to every RDD of the source DStream. This can be used to  do arbitrary RDD operations on the DStream.</p></td> </tr><tr><td valign="top"><p>updateStateByKey(func)</p></td>  <td valign="top"><p>Return a new "state" DStream  where the state for each key is updated by applying the given function on the  previous state of the key and the new values for the key. This can be used to  maintain arbitrary state data for each key.</p></td></tr></tbody></table><h4><strong><span style="font-size:16px;color:#333333;">特殊的Transformations</span></strong></h4><strong>1.UpdateStateByKeyOperation</strong><br>UpdateStateByKey原语用于记录历史记录，上文中Word Count示例中就用到了该特性。若不用UpdateStateByKey来更新状态，那么每次数据进来后分析完成后，结果输出后将不在保存<br><strong>2.TransformOperation</strong><br>Transform原语允许DStream上执行任意的RDD-to-RDD函数。通过该函数可以方便的扩展Spark API。此外，MLlib（机器学习）以及Graphx也是通过本函数来进行结合的。<br><strong>3.WindowOperations</strong><br><p></p><p>Window Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态</p><p><img src="https://img-blog.csdn.net/20180331101720905?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><h3><span style="color:#333333;">2.2.  Output Operations on DStreams</span></h3><p>Output Operations可以将DStream的数据输出到外部的数据库或文件系统，当某个Output Operations原语被调用时（与RDD的Action相同），streaming程序才会开始真正的计算过程。</p><table border="1" cellspacing="0" cellpadding="0" width="568"><tbody><tr><td valign="top"><p align="center">Output  Operation</p></td>  <td valign="top"><p align="center">Meaning</p></td> </tr><tr><td valign="top"><p>print()</p></td>  <td valign="top"><p>Prints the first ten elements of every  batch of data in a DStream on the driver node running the streaming  application. This is useful for development and debugging. </p></td> </tr><tr><td valign="top"><p>saveAsTextFiles(prefix, [suffix])</p></td>  <td valign="top"><p>Save this DStream's contents as text  files. The file name at each batch interval is generated based on prefix and  suffix: "prefix-TIME_IN_MS[.suffix]".</p></td> </tr><tr><td valign="top"><p>saveAsObjectFiles(prefix, [suffix])</p></td>  <td valign="top"><p>Save this DStream's contents as  SequenceFiles of serialized Java objects. The file name at each batch  interval is generated based on prefix and suffix:  "prefix-TIME_IN_MS[.suffix]". </p></td> </tr><tr><td valign="top"><p>saveAsHadoopFiles(prefix, [suffix])</p></td>  <td valign="top"><p>Save this DStream's contents as Hadoop  files. The file name at each batch interval is generated based on prefix and  suffix: "prefix-TIME_IN_MS[.suffix]". </p></td> </tr><tr><td valign="top"><p>foreachRDD(func)</p></td>  <td valign="top"><p>The most generic output operator that  applies a function, func, to each RDD generated from the stream. This  function should push the data in each RDD to an external system, such as  saving the RDD to files, or writing it over the network to a database. Note  that the function func is executed in the driver process running the  streaming application, and will usually have RDD actions in it that will  force the computation of the streaming RDDs.</p></td> </tr></tbody></table><h1>（三）Spark Streaming架构</h1><p></p><h2>整个架构由3个模块组成：</h2>    – Master：记录Dstream之间的依赖关系或者血缘关系，并负责任务调度以生成新的RDD<br>    – Worker：从网络接收数据，存储并执行RDD计算<br>    – Client：负责向Spark Streaming中灌入数据<br><p><img src="https://img-blog.csdn.net/20180331101907536?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><h2>Spark Streaming作业提交</h2>• Network Input Tracker：跟踪每一个网络received数据，并且将其映射到相应的input Dstream上<br>• Job Scheduler：周期性的访问DStream Graph并生成Spark Job，将其交给Job Manager执行<br>• Job Manager：获取任务队列，并执行Spark任务<br><p><img src="https://img-blog.csdn.net/20180331102024939?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><h2>Streaming 窗口操作</h2>• Spark提供了一组窗口操作，通过滑动窗口技术对大规模数据的增量更新进行统计分析<br>• Window Operation：定时进行一定时间段内的数据处理<br><p><img src="https://img-blog.csdn.net/20180331102217336?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• 任何基于窗口操作需要指定两个参数：<br>    – 窗口总长度（window length）<br></p><p>    – 滑动时间间隔（slide interval）</p><p><img src="https://img-blog.csdn.net/20180331102240157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><h2>Streaming 容错性分析</h2>• 实时的流式处理系统必须是7*24运行的，同时可以从各种各样的系统错误中恢复，在设计之初，Spark Streaing就支持driver和worker节点的错误恢复。<br>• Worker容错：spark和rdd的保证worker节点的容错性。spark streaming构建在spark之上，所以它的worker节点也是同样的容错机制<br>• Driver容错：依赖WAL持久化日志<br>    – 启动WAL需要做如下的配置<br>    – 1：给streamingContext设置checkpoint的目录，该目录必须是HADOOP支持的文件系统，用来保存WAL和做Streaming的checkpoint<br>    – 2：spark.streaming.receiver.writeAheadLog.enable 设置为true<br><p></p><h2>Streaming 中 WAL工作原理</h2>• 流程梳理：<br><p><img src="https://img-blog.csdn.net/201803311024244?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p>• 当一个Driver失败重启后，恢复流程：<br></p><p><img src="https://img-blog.csdn.net/20180331102452946?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><h1>（四）Spark Streaming实战</h1><p></p><h2>1.  用Spark Streaming实现实时WordCount</h2><p>架构图：</p><img src="https://img-blog.csdn.net/20180331102605541?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>1.安装并启动生成者<br>首先在一台Linux（ip：192.168.10.101）上用YUM安装nc工具<br><span style="background-color:#FFFFFF;color:rgb(51,51,51);">    yum install -y nc</span><br>启动一个服务端并监听9999端口<br><span style="background-color:#FFFFFF;color:rgb(51,51,51);">    nc -lk 9999</span><br><p></p><p></p><p>2.编写Spark Streaming程序</p><p></p><pre><code class="language-python">package cn.itcast.spark.streaming

import cn.itcast.spark.util.LoggerLevel
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

object NetworkWordCount {
  def main(args: Array[String]) {
    //设置日志级别
    LoggerLevel.setStreamingLogLevels()
    //创建SparkConf并设置为本地模式运行
    //注意local[2]代表开两个线程
    val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
    //设置DStream批次时间间隔为2秒
    val ssc = new StreamingContext(conf, Seconds(2))
    //通过网络读取数据
    val lines = ssc.socketTextStream("192.168.10.101", 9999)
    //将读到的数据用空格切成单词
    val words = lines.flatMap(_.split(" "))
    //将单词和1组成一个pair
    val pairs = words.map(word =&gt; (word, 1))
    //按单词进行分组求相同单词出现的次数
    val wordCounts = pairs.reduceByKey(_ + _)
    //打印结果到控制台
    wordCounts.print()
    //开始计算
    ssc.start()
    //等待停止
    ssc.awaitTermination()
  }
}
</code></pre>3.启动Spark Streaming程序：由于使用的是本地模式<strong><span style="color:#008000;background:#FFFFFF;">"local[2]"</span></strong><span style="background:#FFFFFF;">所以可以直接在本地运行该程序</span><br><p></p><p><span style="color:#FF0000;">注意：</span>要指定并行度，如在本地运行设置setMaster("local[2]")，相当于启动两个线程，一个给receiver，一个给computer。如果是在集群中运行，必须要求集群中可用core数大于1</p><p><img src="https://img-blog.csdn.net/20180331102739280?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br></p><p></p><p>4.在Linux端命令行中输入单词</p><img src="https://img-blog.csdn.net/20180331102818291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><p></p><p>5.在IDEA控制台中查看结果</p><p><img src="https://img-blog.csdn.net/2018033110283850?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p></p><p><span style="color:#FF0000;">问题：</span>结果每次在Linux段输入的单词次数都被正确的统计出来，但是结果不能累加！如果需要累加需要使用updateStateByKey(func)来更新状态，下面给出一个例子：</p><pre><code class="language-python">package cn.itcast.spark.streaming

import cn.itcast.spark.util.LoggerLevel
import org.apache.spark.{HashPartitioner, SparkConf}
import org.apache.spark.streaming.{StreamingContext, Seconds}

object NetworkUpdateStateWordCount {
  /**
    * String : 单词 hello
    * Seq[Int] ：单词在当前批次出现的次数
    * Option[Int] ： 历史结果
    */
  val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =&gt; {
    //iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x)))
    iter.flatMap{case(x,y,z)=&gt;Some(y.sum + z.getOrElse(0)).map(m=&gt;(x, m))}
  }

  def main(args: Array[String]) {
    LoggerLevel.setStreamingLogLevels()
    val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkUpdateStateWordCount")
    val ssc = new StreamingContext(conf, Seconds(5))
    //做checkpoint 写入共享存储中
    ssc.checkpoint("c://aaa")
    val lines = ssc.socketTextStream("192.168.10.100", 9999)
    //reduceByKey 结果不累加
    //val result = lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
    //updateStateByKey结果可以累加但是需要传入一个自定义的累加函数：updateFunc
    val results = lines.flatMap(_.split(" ")).map((_,1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)
    results.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
</code></pre><h2>2.  Spark Streaming整合Kafka完成网站点击流实时统计</h2><img src="https://img-blog.csdn.net/20180331103007537?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmdldFRoYXROaWdodA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><blockquote style="border:none;"></blockquote>1.安装并配置zk<br><blockquote style="border:none;"></blockquote>2.安装并配置Kafka<br><blockquote style="border:none;"></blockquote>3.启动zk<br>4.启动Kafka<br><p>5.创建topic</p><p></p><p></p><p></p><pre><code class="language-python">bin/kafka-topics.sh--create --zookeeper node1.itcast.cn:2181,node2.itcast.cn:2181 \
--replication-factor3 --partitions 3 --topic urlcount</code></pre><p>6.编写Spark Streaming应用程序</p><pre><code class="language-python">package cn.itcast.spark.streaming

package cn.itcast.spark

import org.apache.spark.{HashPartitioner, SparkConf}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}

object UrlCount {
  val updateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])]) =&gt; {
    iterator.flatMap{case(x,y,z)=&gt; Some(y.sum + z.getOrElse(0)).map(n=&gt;(x, n))}
  }

  def main(args: Array[String]) {
    //接收命令行中的参数
    val Array(zkQuorum, groupId, topics, numThreads, hdfs) = args
    //创建SparkConf并设置AppName
    val conf = new SparkConf().setAppName("UrlCount")
    //创建StreamingContext
    val ssc = new StreamingContext(conf, Seconds(2))
    //设置检查点
    ssc.checkpoint(hdfs)
    //设置topic信息
    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    //重Kafka中拉取数据创建DStream
    val lines = KafkaUtils.createStream(ssc, zkQuorum ,groupId, topicMap, StorageLevel.MEMORY_AND_DISK).map(_._2)
    //切分数据，截取用户点击的url
    val urls = lines.map(x=&gt;(x.split(" ")(6), 1))
    //统计URL点击量
    val result = urls.updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)
    //将结果打印到控制台
    result.print()
    ssc.start()
    ssc.awaitTermination()
  }
}</code></pre>            </div>
                </div>