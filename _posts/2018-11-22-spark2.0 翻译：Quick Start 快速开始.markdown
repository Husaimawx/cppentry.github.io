---
layout:     post
title:      spark2.0 翻译：Quick Start 快速开始
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>实话实说，建议你们直接看这个网址：<a href="http://www.apache.wiki/display/Spark" rel="nofollow">http://www.apache.wiki/display/Spark</a> <br>
他们团队翻译的很好，以后我就不在自己博客献丑了哦。</p>

<ol>
<li>spark编译包下载解压及JDK环境配置 <br>
-首先先去下载spark编译好的包，<a href="http://spark.apache.org/downloads.html" rel="nofollow">http://spark.apache.org/downloads.html</a> <br>
由于spark不依赖hadoop而运行，所以我们此处可以下载任意hadoop编译版本的spark，之后解压 <br>
-接下来下载jdk并配置JAVA_HOME</li>
<li><p>使用spark shell进行交互式操作 <br>
1).基本用法  <br>
spark shell提供了简单方式来学习api，以及非常好用的工具交互式地去分析数据。可以使用scala（运行在java虚拟机上并且可以很好地调用java的各种库）或者python。<strong>本文只使用scala哦</strong> <br>
在spark解压目录下执行：</p>

<blockquote>
  <p>./bin/spark-shell</p>
</blockquote>

<p>spark的主要抽象是一个叫做弹性数据集（RDD）的分布式的事物集合，RDDs从hadoop的格式化输入（hdfs的文件）或者由其他RDDs转化而来。接下来就使用spark目录的README文件创建一个RDD</p>

<blockquote>
  <p>scala&gt; val textFile = sc.textFile(“README.md”) <br>
  textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at :24</p>
</blockquote>

<p>这样我们由一个文件生成了一个spark的RDD。 <br>
RDDs包含action行为操作（统计等操作的）和 transformation转换操作（生成新的RDDs）。接下来我们使用一些action操作。</p>

<blockquote>
  <p>scala&gt; textFile.count() <br>
  res0: Long = 99     </p>
</blockquote>

<p>值得一提的是我们在spark-shell的命令行窗口里可以使用tab进行命令补全提示，包括actions的提示，这里我们统计了一下文件的行数,共99行。我们还可以输出文件的第一行内容，格式如下：</p>

<blockquote>
  <p>scala&gt; textFile.first() <br>
  res1: String = # Apache Spark</p>
</blockquote>

<p>接下来，我们使用transformation转换来返回这个文件内容子集的一个新的RDD：</p>

<blockquote>
  <p>scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(“Spark”)) <br>
  linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at :26</p>
</blockquote>

<p>我们对textFile的RDD执行过滤的转换，获取其中包含Spark关键字的行的RDD集合。我们还可以将transformation与action一块用</p>

<blockquote>
  <p>scala&gt; textFile.filter(line =&gt; line.contains(“Spark”)).count() <br>
  res2: Long = 19</p>
</blockquote>

<p>2)RDD的更多操作 <br>
RDDs的action行为和transformations转换可以被应用到更加复杂的计算中。比如我们现在要获取一行最多有多少单词：</p>

<blockquote>
  <p>scala&gt; textFile.map(line =&gt; line.split(” “).size).reduce((a, b) =&gt; if (a &gt; b) a else b) <br>
  res4: Long = 15</p>
</blockquote>

<p>第一步将每行映射成一个整数，并创建成一个新的RDD。reduce被调用，查找出最大的数值。</p></li>
</ol>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>