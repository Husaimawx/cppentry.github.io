---
layout:     post
title:      HDFS 总结
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p><strong><span style="color:#366091;"><strong>目录</strong></span></strong></p>

<p style="margin-left:0pt;"><a href="#_Toc439077207" rel="nofollow"><u><span style="color:#0000ff;"><u>课程大纲（</u></span></u><u><span style="color:#0000ff;"><u>HDFS</u></span></u><u><span style="color:#0000ff;"><u>详解）</u></span></u> 2</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077208" rel="nofollow"><u><span style="color:#0000ff;"><u>1. HDFS</u></span></u><u><span style="color:#0000ff;"><u>前言</u></span></u> 3</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077209" rel="nofollow"><u><span style="color:#0000ff;"><u>2. HDFS</u></span></u><u><span style="color:#0000ff;"><u>的概念和特性</u></span></u> 3</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077210" rel="nofollow"><u><span style="color:#0000ff;"><u>3. HDFS</u></span></u><u><span style="color:#0000ff;"><u>的</u></span></u><u><span style="color:#0000ff;"><u>shell(</u></span></u><u><span style="color:#0000ff;"><u>命令行客户端</u></span></u><u><span style="color:#0000ff;"><u>)</u></span></u><u><span style="color:#0000ff;"><u>操作</u></span></u> 4</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077211" rel="nofollow"><u><span style="color:#0000ff;"><u>3.1 HDFS</u></span></u><u><span style="color:#0000ff;"><u>命令行客户端使用</u></span></u> 4</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077212" rel="nofollow"><u><span style="color:#0000ff;"><u>3.2</u></span></u><u><span style="color:#0000ff;"><u>命令行客户端支持的命令参数</u></span></u> 4</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077213" rel="nofollow"><u><span style="color:#0000ff;"><u>3.2 </u></span></u><u><span style="color:#0000ff;"><u>常用命令参数介绍</u></span></u> 5</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077214" rel="nofollow"><u><span style="color:#0000ff;"><u>4. hdfs</u></span></u><u><span style="color:#0000ff;"><u>的工作机制</u></span></u> 8</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077215" rel="nofollow"><u><span style="color:#0000ff;"><u>4.1 </u></span></u><u><span style="color:#0000ff;"><u>概述：</u></span></u> 8</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077216" rel="nofollow"><u><span style="color:#0000ff;"><u>4.2 HDFS</u></span></u><u><span style="color:#0000ff;"><u>写数据流程</u></span></u> 9</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077217" rel="nofollow"><u><span style="color:#0000ff;"><u>4.2.1 </u></span></u><u><span style="color:#0000ff;"><u>概述</u></span></u> 9</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077218" rel="nofollow"><u><span style="color:#0000ff;"><u>4.2.2 </u></span></u><u><span style="color:#0000ff;"><u>详细步骤图</u></span></u> 9</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077219" rel="nofollow"><u><span style="color:#0000ff;"><u>4.2.3 </u></span></u><u><span style="color:#0000ff;"><u>详细步骤解析</u></span></u> 9</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077220" rel="nofollow"><u><span style="color:#0000ff;"><u>4.3. HDFS</u></span></u><u><span style="color:#0000ff;"><u>读数据流程</u></span></u> 10</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077221" rel="nofollow"><u><span style="color:#0000ff;"><u>4.3.1 </u></span></u><u><span style="color:#0000ff;"><u>概述</u></span></u> 10</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077222" rel="nofollow"><u><span style="color:#0000ff;"><u>4.3.2 </u></span></u><u><span style="color:#0000ff;"><u>详细步骤图：</u></span></u> 10</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077223" rel="nofollow"><u><span style="color:#0000ff;"><u>4.3.3 </u></span></u><u><span style="color:#0000ff;"><u>详细步骤解析</u></span></u> 10</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077224" rel="nofollow"><u><span style="color:#0000ff;"><u>5. NAMENODE</u></span></u><u><span style="color:#0000ff;"><u>工作机制</u></span></u> 11</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077225" rel="nofollow"><u><span style="color:#0000ff;"><u>5.1 </u></span></u><u><span style="color:#0000ff;"><u>概述</u></span></u> 11</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077226" rel="nofollow"><u><span style="color:#0000ff;"><u>5.2</u></span></u><u><span style="color:#0000ff;"><u>元数据管理</u></span></u> 11</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077227" rel="nofollow"><u><span style="color:#0000ff;"><u>5.2.1 </u></span></u><u><span style="color:#0000ff;"><u>元数据存储机制</u></span></u> 11</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077228" rel="nofollow"><u><span style="color:#0000ff;"><u>5.2.2 </u></span></u><u><span style="color:#0000ff;"><u>元数据手动查看</u></span></u> 11</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077229" rel="nofollow"><u><span style="color:#0000ff;"><u>5.2.3 </u></span></u><u><span style="color:#0000ff;"><u>元数据的</u></span></u><u><span style="color:#0000ff;"><u>checkpoint</u></span></u> 12</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077230" rel="nofollow"><u><span style="color:#0000ff;"><u>6. DATANODE</u></span></u><u><span style="color:#0000ff;"><u>的工作机制</u></span></u> 13</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077231" rel="nofollow"><u><span style="color:#0000ff;"><u>6.1 </u></span></u><u><span style="color:#0000ff;"><u>概述</u></span></u> 13</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077232" rel="nofollow"><u><span style="color:#0000ff;"><u>6.2 </u></span></u><u><span style="color:#0000ff;"><u>观察验证</u></span></u><u><span style="color:#0000ff;"><u>DATANODE</u></span></u><u><span style="color:#0000ff;"><u>功能</u></span></u> 13</a></p>

<p style="margin-left:0pt;"><a href="#_Toc439077233" rel="nofollow"><u><span style="color:#0000ff;"><u>7. HDFS</u></span></u><u><span style="color:#0000ff;"><u>的</u></span></u><u><span style="color:#0000ff;"><u>java</u></span></u><u><span style="color:#0000ff;"><u>操作</u></span></u> 13</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077234" rel="nofollow"><u><span style="color:#0000ff;"><u>7.1 </u></span></u><u><span style="color:#0000ff;"><u>搭建开发环境</u></span></u> 13</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077235" rel="nofollow"><u><span style="color:#0000ff;"><u>7.2 </u></span></u><u><span style="color:#0000ff;"><u>获取</u></span></u><u><span style="color:#0000ff;"><u>api</u></span></u><u><span style="color:#0000ff;"><u>中的客户端对象</u></span></u> 14</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077236" rel="nofollow"><u><span style="color:#0000ff;"><u>7.3 DistributedFileSystem</u></span></u><u><span style="color:#0000ff;"><u>实例对象所具备的方法</u></span></u> 14</a></p>

<p style="margin-left:21pt;"><a href="#_Toc439077237" rel="nofollow"><u><span style="color:#0000ff;"><u>7.4 HDFS</u></span></u><u><span style="color:#0000ff;"><u>客户端操作数据代码示例：</u></span></u> 15</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077238" rel="nofollow"><u><span style="color:#0000ff;"><u>7.4.1 </u></span></u><u><span style="color:#0000ff;"><u>文件的增删改查</u></span></u> 15</a></p>

<p style="margin-left:42pt;"><a href="#_Toc439077239" rel="nofollow"><u><span style="color:#0000ff;"><u>7.4.2 </u></span></u><u><span style="color:#0000ff;"><u>通过流的方式访问</u></span></u><u><span style="color:#0000ff;"><u>hdfs</u></span></u> 18</a></p>

<h1><strong><a name="_Toc439077207"></a><strong><strong>课程大纲（HDFS详解）</strong></strong></strong></h1>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td rowspan="7" style="vertical-align:top;width:119.7pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">Hadoop HDFS</span></p>
			</td>
			<td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">分布式文件系统DFS简介</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">HDFS的系统组成介绍</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">HDFS的组成部分详解</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">副本存放策略及路由规则</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">命令行接口</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">Java接口</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:306.4pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">客户端与HDFS的数据流讲解</span></p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;">目标：</p>

<p style="margin-left:0pt;"><span style="color:#000000;">掌握hdfs的shell操作</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">掌握hdfs的java api操作</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">理解hdfs的工作原理</span></p>

<h1><strong><a name="_Toc29774"></a><strong><span style="color:#000000;"><strong><a name="_Toc7159"></a><a name="_Toc6590"></a><a name="_Toc8039"></a><a name="_Toc421731819"></a><a name="_Toc10331"></a><a name="_Toc439077208">******HDFS基本概念篇******</a></strong></span></strong></strong></h1>

<h1><strong><strong><span style="color:#000000;"><strong>1. HDFS前言</strong></span></strong></strong></h1>

<ol><li><span style="color:#000000;">设计思想</span></li>
</ol><p style="margin-left:0pt;"><span style="color:#000000;">分而治之：将大文件、大批量文件，分布式存放在大量服务器上，</span><strong><span style="color:#000000;"><strong>以便于采取分而治之的方式对海量数据进行运算分析；</strong></span></strong></p>

<ol><li><span style="color:#000000;">在大数据系统中作用：</span></li>
</ol><p style="margin-left:21pt;"><span style="color:#000000;">为各类分布式运算框架（如：mapreduce，spark，tez，</span><span style="color:#000000;">……</span><span style="color:#000000;">）提供数据存储服务</span></p>

<ol><li><span style="color:#000000;">重点概念：文件切块，副本存放，元数据</span></li>
</ol><h1><strong><a name="_Toc22308"></a><strong><strong><a name="_Toc10548"></a><a name="_Toc439077209"></a><a name="_Toc17151"></a><a name="_Toc421731821"></a><a name="_Toc24942"></a><a name="_Toc20495">2. HDFS的概念和特性</a></strong></strong></strong></h1>

<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>首先，它是一个文件系统</strong></span></strong><span style="color:#000000;">，用于存储文件，通过统一的命名空间——目录树来定位文件</span></p>

<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>其次，它是分布式的</strong></span></strong><span style="color:#000000;">，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色；</span></p>

<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>重要特性如下：</strong></span></strong></p>

<ol><li><span style="color:#000000;">HDFS中的文件在物理上是</span><strong><span style="color:#000000;"><strong>分块存储（block）</strong></span></strong><span style="color:#000000;">，块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M</span></li>
	<li><span style="color:#000000;">HDFS文件系统会给客户端提供一个</span><strong><span style="color:#000000;"><strong>统一的抽象目录树</strong></span></strong><span style="color:#000000;">，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data</span></li>
	<li><strong><span style="color:#000000;"><strong>目录结构及文件分块信息</strong></span></strong><strong><span style="color:#000000;"><strong>(元数据)</strong></span></strong><span style="color:#000000;">的管理由namenode节点承担</span></li>
</ol><p style="margin-left:0pt;"><span style="color:#000000;">——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器）</span></p>

<ol><li><span style="color:#000000;">文件的各个block的存储管理由datanode节点承担</span></li>
</ol><p style="margin-left:0pt;"><span style="color:#000000;">---- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication）</span></p>

<ol><li><span style="color:#000000;">HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改</span></li>
</ol><p style="margin-left:0pt;"><em><span style="color:#000000;"><em>(注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高)</em></span></em></p>

<h1><strong><a name="_Toc32138"></a><strong><strong><a name="_Toc15850"></a><a name="_Toc1367"></a><a name="_Toc439077210"></a><a name="_Toc421731820"></a><a name="_Toc25955"></a><a name="_Toc31321">******HDFS基本操作篇******</a></strong></strong></strong></h1>

<h1><strong><strong><strong>3. HDFS的shell(命令行客户端)操作</strong></strong></strong></h1>

<h2><strong><a name="_Toc439077211"></a><strong><strong>3.1 HDFS命令行客户端使用</strong></strong></strong></h2>

<p style="margin-left:0pt;">HDFS提供shell命令行客户端，使用方法如下</p>

<p style="margin-left:0pt;"><img alt="" class="has" height="120" src="https://img-blog.csdn.net/20180908172236164?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="693"></p>

<h2><strong><a name="_Toc439077212"></a><strong><strong>3.2 命令行客户端支持的命令参数</strong></strong></strong></h2>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">        [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-cat [-ignoreCrc] &lt;src&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-checksum &lt;src&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-chgrp [-R] GROUP PATH...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-count [-q] &lt;path&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-df [-h] [&lt;path&gt; ...]]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-du [-s] [-h] &lt;path&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-expunge]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-getfacl [-R] &lt;path&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-help [cmd ...]]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-mkdir [-p] &lt;path&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-mv &lt;src&gt; ... &lt;dst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-stat [format] &lt;path&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-tail [-f] &lt;file&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-test -[defsz] &lt;path&gt;]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-text [-ignoreCrc] &lt;src&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-touchz &lt;path&gt; ...]</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        [-usage [cmd ...]]</span></p>
			</td>
		</tr></tbody></table><h2><strong><a name="_Toc439077213"></a><strong><strong>3.2 常用命令参数介绍</strong></strong></strong></h2>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">-help             </span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">功能：输出这个命令参数手册</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-ls                  </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：显示目录信息</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例： </em></span></em><em><span style="color:#000000;"><em>hadoop fs -ls hdfs://hadoop-server01:9000/</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>备注：这些参数中，所有的hdfs路径都可以简写</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>--&gt;</em></span></em><em><span style="color:#000000;"><em>hadoop fs -ls /   等同于上一条命令的效果</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-mkdir              </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：在hdfs上创建目录</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop fs  -mkdir  -p  /aaa/bbb/cc/dd</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-moveFromLocal            </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：从本地剪切粘贴到hdfs</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：hadoop  fs  - moveFromLocal  /home/hadoop/a.txt  /aaa/bbb/cc/dd</em></span></em></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-moveToLocal              </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：从hdfs剪切粘贴到本地</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：hadoop  fs  - moveToLocal   /aaa/bbb/cc/dd  /home/hadoop/a.txt </em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>--appendToFile  </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：追加一个文件到已经存在的文件末尾</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop  fs  -appendToFile  ./hello.txt  hdfs://hadoop-server01:9000/hello.txt</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>可以简写为：</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>H</em></span></em><em><span style="color:#000000;"><em>adoop  fs  -appendToFile  ./hello.txt  /hello.txt</em></span></em></p>

			<p style="margin-left:0pt;"> </p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-cat  </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：显示文件内容  </strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop fs -cat  /hello.txt</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-tail                 </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：显示一个文件的末尾</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：hadoop  fs  -tail  /weblog/access_log.1</em></span></em></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-text                  </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：以字符形式打印一个文件的内容</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：hadoop  fs  -text  /weblog/access_log.1</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-chgrp </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-chmod</strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-chown</strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：linux文件系统中的用法一样，对文件所属权限</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>hadoop  fs  -chmod  666  /hello.txt</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>hadoop</em></span></em><em><span style="color:#000000;"><em>  fs  -chown  someuser:somegrp   /hello.txt</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-copyFromLocal    </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：从本地文件系统中拷贝文件到hdfs路径去</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/</em></span></em></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-copyToLocal      </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：从hdfs拷贝到本地</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-cp              </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：从hdfs的一个路径拷贝hdfs的另一个路径</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例： </em></span></em><em><span style="color:#000000;"><em>hadoop</em></span></em><em> </em><em><span style="color:#000000;"><em> fs</em></span></em><em> </em><em><span style="color:#000000;"><em> -cp  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-mv                     </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：在hdfs目录中移动文件</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例： </em></span></em><em><span style="color:#000000;"><em>hadoop</em></span></em><em> </em><em><span style="color:#000000;"><em> fs</em></span></em><em> </em><em><span style="color:#000000;"><em> -</em></span></em><em><span style="color:#000000;"><em>mv</em></span></em><em><span style="color:#000000;"><em>  /aaa/jdk.tar.gz  /</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-get              </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：等同于copyToLocal，就是从hdfs下载文件到本地</strong></span></strong></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">示例：hadoop fs -get  /aaa/jdk.tar.gz</span></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-getmerge             </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：合并下载多个文件</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,...</em></span></em></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">hadoop fs -getmerge /aaa/log.* ./log.sum</span></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-put                </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：等同于copyFromLocal</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop</em></span></em><em> </em><em><span style="color:#000000;"><em> fs</em></span></em><em> </em><em><span style="color:#000000;"><em> -</em></span></em><em><span style="color:#000000;"><em>put</em></span></em><em><span style="color:#000000;"><em>  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</em></span></em></p>

			<p style="margin-left:0pt;"> </p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-rm                </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：删除文件或文件夹</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop fs -rm -r /aaa/bbb/</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-rmdir                 </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：删除空目录</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：hadoop  fs  -rmdir   /aaa/bbb/ccc</em></span></em></p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-df               </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：统计文件系统的可用空间信息</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：h</em></span></em><em><span style="color:#000000;"><em>adoop</em></span></em><em> </em><em><span style="color:#000000;"><em> fs</em></span></em><em> </em><em><span style="color:#000000;"><em> -df</em></span></em><em><span style="color:#000000;"><em>  -h </em></span></em><em><span style="color:#000000;"><em> /</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-du </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：统计文件夹的大小信息</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>hadoop  fs  -du  -s  -h /aaa/*</em></span></em></p>

			<p style="margin-left:0pt;"> </p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-count         </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：统计一个指定目录下的文件节点数量</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop fs -count /aaa/</em></span></em></p>

			<p style="margin-left:0pt;"> </p>
			</td>
		</tr><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>-setrep                </strong></span></strong></p>

			<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>功能：设置hdfs中文件的副本数量</strong></span></strong></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>示例：</em></span></em><em><span style="color:#000000;"><em>hadoop fs -setrep 3 /aaa/jdk.tar.gz</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>&lt;这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量&gt;</em></span></em></p>

			<p style="margin-left:0pt;"> </p>
			</td>
		</tr></tbody></table><h1><strong><a name="_Toc421731823"></a><strong><strong><a name="_Toc439077214"></a><a name="_Toc27953"></a><a name="_Toc21978"></a><a name="_Toc23129"></a><a name="_Toc16603">******HDFS原理篇******</a></strong></strong></strong></h1>

<h1><strong><strong><strong>4. hdfs的工作机制</strong></strong></strong></h1>

<p style="margin-left:0pt;"><em><em>（工作机制的学习主要是为加深对分布式系统的理解，以及增强遇到各种问题时的分析解决能力，形成一定的集群运维能力）</em></em></p>

<p style="margin-left:0pt;"><em><em>注：很多不是真正理解hadoop技术体系的人会常常觉得HDFS可用于网盘类应用，但实际并非如此。要想将技术准确用在恰当的地方，必须对技术有深刻的理解</em></em></p>

<h2><strong><a name="_Toc439077215"></a><strong><strong>4.1 概述</strong></strong></strong></h2>

<ol><li><span style="color:#000000;">HDFS集群分为两大角色：NameNode、DataNode</span></li>
	<li><span style="color:#000000;">NameNode负责管理整个文件系统的元数据</span></li>
	<li><span style="color:#000000;">DataNode 负责管理用户的文件数据块</span></li>
	<li><span style="color:#000000;">文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上</span></li>
	<li><span style="color:#000000;">每一个文件块可以有多个副本，并存放在不同的datanode上</span></li>
	<li><span style="color:#000000;">D</span><span style="color:#000000;">atanode会定期向</span><span style="color:#000000;">N</span><span style="color:#000000;">amenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量</span></li>
	<li><span style="color:#000000;">HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行</span></li>
</ol><h2><strong><a name="_Toc439077216"></a><strong><strong>4.2 HDFS写数据流程</strong></strong></strong></h2>

<h3><strong><a name="_Toc439077217"></a><strong><strong>4.2.1 概述</strong></strong></strong></h3>

<p style="margin-left:0pt;">客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本</p>

<h3><strong><a name="_Toc439077218"></a><strong><strong>4.2.2 详细步骤图</strong></strong></strong></h3>

<p style="margin-left:0pt;"><img alt="" class="has" height="443" src="https://img-blog.csdn.net/20180908172312714?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="693"></p>

<h3><strong><a name="_Toc439077219"></a><strong><strong>4.2.3 详细步骤解析</strong></strong></strong></h3>

<p style="margin-left:0pt;"><span style="color:#000000;">1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">2、namenode返回是否可以上传</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">3、client请求第一个 block该传输到哪些datanode服务器上</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">4、namenode返回3个datanode服务器ABC</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。</span></p>

<h2><strong><a name="_Toc439077220"></a><strong><strong>4.3. HDFS读数据流程</strong></strong></strong></h2>

<h3><strong><a name="_Toc439077221"></a><strong><strong>4.3.1 概述</strong></strong></strong></h3>

<p style="margin-left:0pt;">客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件</p>

<h3><strong><a name="_Toc439077222"></a><strong><strong>4.3.2 详细步骤图</strong></strong></strong></h3>

<p style="margin-left:0pt;"><img alt="" class="has" height="338" src="https://img-blog.csdn.net/20180908172314885?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="693"></p>

<p style="margin-left:0pt;"><strong style="font-size:22px;"><strong><strong>4.3.3 详细步骤解析</strong></strong></strong></p>

<p style="margin-left:0pt;"><span style="color:#000000;">1、跟namenode通信查询元数据，找到文件块所在的datanode服务器</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件</span></p>

<h1><strong><a name="_Toc421731824"></a><strong><strong><a name="_Toc439077224">5. NAMENODE工作机制</a></strong></strong></strong></h1>

<p style="margin-left:0pt;">学习目标：理解namenode的工作机制尤其是<strong><strong>元数据管理</strong></strong>机制，以增强对HDFS工作原理的理解，及培养hadoop集群运营中“性能调优”、“namenode”故障问题的分析解决能力</p>

<p style="margin-left:0pt;"><em><em>问题场景：</em></em></p>

<p style="margin-left:0pt;"><em><em>1、集群启动后，可以查看文件，但是上传文件时报错，打开web页面可看到namenode正处于safemode状态，怎么处理？</em></em></p>

<p style="margin-left:0pt;"><em><em>2、</em></em><em><em>Namenode服务器的磁盘故障导致namenode宕机</em></em><em><em>，</em></em><em><em>如何挽救集群及数据</em></em><em><em>？</em></em></p>

<p style="margin-left:0pt;"><em><em>3、</em></em><em><em>Namenode是否可以有多个</em></em><em><em>？namenode内存要配置多大？namenode跟集群数据存储能力有关系吗？</em></em></p>

<p style="margin-left:0pt;"><em><em>4、文件的blocksize究竟调大好还是调小好？</em></em></p>

<p style="margin-left:0pt;"><em><em>……</em></em></p>

<p style="margin-left:0pt;"><em><em>诸如此类问题的回答，都需要基于对namenode自身的工作原理的深刻理解</em></em></p>

<p style="margin-left:0pt;"> </p>

<h2><strong><a name="_Toc439077225"></a><strong><strong><a name="_Toc505">5.1 </a></strong></strong><strong><span style="color:#000000;"><strong>NAMENODE</strong></span></strong><strong><strong>职责</strong></strong></strong></h2>

<p style="margin-left:0pt;"><span style="color:#000000;">NAMENODE职责：</span></p>

<p style="margin-left:0pt;"><a name="_Toc4721"></a><span style="color:#000000;">负责客户端请求的响应</span></p>

<p style="margin-left:0pt;"><a name="_Toc29702"></a><span style="color:#000000;">元数据的管理（查询，修改）</span></p>

<h2><strong><a name="_Toc439077226"></a><strong><strong>5.2 元数据管理</strong></strong></strong></h2>

<p style="margin-left:0pt;">namenode对数据的管理采用了三种存储形式：</p>

<p style="margin-left:0pt;">内存元数据(NameSystem)</p>

<p style="margin-left:0pt;">磁盘元数据镜像文件</p>

<p style="margin-left:0pt;">数据操作日志文件（可通过日志运算出元数据）</p>

<h3><strong><a name="_Toc439077227"></a><strong><strong><a name="_Toc11005">5.2.1 元数据存储机制</a></strong></strong></strong></h3>

<p style="margin-left:0pt;"><a name="_Toc29922"></a><span style="color:#000000;">A、内存中有一份完整的元数据(</span><strong><span style="color:#ff0000;"><strong>内存meta data</strong></span></strong><span style="color:#000000;">)</span></p>

<p style="margin-left:0pt;"><a name="_Toc1289"></a><span style="color:#000000;">B、磁盘有一个“准完整”的元数据镜像（</span><strong><span style="color:#ff0000;"><strong>fsimage</strong></span></strong><span style="color:#000000;">）</span><span style="color:#000000;">文件</span><span style="color:#000000;">(在namenode的工作目录中)</span></p>

<p style="margin-left:0pt;"><a name="_Toc7299"></a><span style="color:#000000;">C、用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（</span><strong><span style="color:#ff0000;"><strong>edits文件</strong></span></strong><span style="color:#000000;">）</span><em><span style="color:#000000;"><em>注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中</em></span></em></p>

<h3><strong><a name="_Toc439077228"></a><strong><strong>5.2.2 元数据手动查看</strong></strong></strong></h3>

<p style="margin-left:0pt;"><a name="_Toc8756"></a><span style="color:#000000;">可以通过hdfs的一个工具来查看edits中的信息</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">bin/hdfs oev -i edits -o edits.xml<a name="_Toc439077229"></a></span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">bin/</span><span style="color:#000000;">hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml</span></p>

<h3><strong><strong><strong>5.2.3 元数据的checkpoint</strong></strong></strong></h3>

<p style="margin-left:0pt;"><a name="_Toc2551"></a><span style="color:#000000;">每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</span></p>

<p><strong><strong><strong>checkpoint</strong></strong><strong><strong>的详细过程</strong></strong></strong></p>

<p><img alt="" class="has" height="378" src="https://img-blog.csdn.net/20180908172348736?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="695"></p>

<p style="margin-left:0pt;"><a name="_Toc17124"></a></p>

<p><strong><strong><strong>checkpoint操作的触发条件配置参数</strong></strong></strong></p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">dfs.namenode.checkpoint.check.period=60  </span><span style="color:#000000;">#</span><span style="color:#000000;">检查触发条件是否满足的频率，60秒</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">#</span><span style="color:#000000;">以上两个参数做</span><span style="color:#000000;">checkpoint</span><span style="color:#000000;">操作时，</span><span style="color:#000000;">secondary namenode</span><span style="color:#000000;">的本地工作目录</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">dfs.namenode.checkpoint.max-retries=3</span><span style="color:#000000;">  #</span><span style="color:#000000;">最大重试次数</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">dfs.namenode.checkpoint.period=3600</span><span style="color:#000000;">  #</span><span style="color:#000000;">两次</span><span style="color:#000000;">checkpoint</span><span style="color:#000000;">之间的时间间隔</span><span style="color:#000000;">3600</span><span style="color:#000000;">秒</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">dfs.namenode.checkpoint.txns=1000000</span><span style="color:#000000;"> #</span><span style="color:#000000;">两次</span><span style="color:#000000;">checkpoint</span><span style="color:#000000;">之间最大的操作记录</span></p>
			</td>
		</tr></tbody></table><p><strong><a name="_Toc32115"></a><strong><strong>checkpoint</strong></strong><strong><strong>的附带作用</strong></strong></strong></p>

<p style="margin-left:0pt;"><span style="color:#000000;">namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</span></p>

<h1><strong><a name="_Toc11613"></a><strong><span style="color:#000000;"><strong><a name="_Toc10777"></a><a name="_Toc439077230"></a><a name="_Toc421731825"></a><a name="_Toc26969"></a><a name="_Toc13739">6. DATANODE的工作机制</a></strong></span></strong></strong></h1>

<p style="margin-left:0pt;"><em><em>问题场景：</em></em></p>

<p style="margin-left:0pt;"><em><em>1、集群容量不够，怎么扩容？</em></em></p>

<p style="margin-left:0pt;"><em><em>2、如果有一些datanode宕机，该怎么办？</em></em></p>

<p style="margin-left:0pt;"><em><em>3、datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？</em></em></p>

<p style="margin-left:0pt;"><em><em>以上这类问题的解答，有赖于对datanode工作机制的深刻理解</em></em></p>

<h2><strong><a name="_Toc439077231"></a><strong><strong>6.1 概述</strong></strong></strong></h2>

<p style="margin-left:0pt;"><span style="color:#000000;">1、</span><span style="color:#000000;">D</span><span style="color:#000000;">atanode工作职责：</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">存储管理用户的文件块数据</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">定期向namenode汇报自身所持有的block信息（通过心跳信息上报）</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">（这点很重要，因为，当集群中发生某些</span><span style="color:#000000;">block副本失效时，集群如何恢复block初始副本数量的问题</span><span style="color:#000000;">）</span></p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;property&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;value&gt;3600000&lt;/value&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;/property&gt;</span></p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;"><span style="color:#000000;">2、Datanode掉线判断时限参数</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。</span></p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;property&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        &lt;value&gt;2000&lt;/value&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;/property&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;property&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">        &lt;value&gt;1&lt;/value&gt;</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">&lt;/property&gt;</span></p>
			</td>
		</tr></tbody></table><h2><strong><a name="_Toc439077232"></a><strong><strong>6.2 观察验证DATANODE功能</strong></strong></strong></h2>

<p style="margin-left:0pt;"><span style="color:#000000;">上传一个文件，观察文件的block具体的物理存放情况：</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">在每一台datanode机器上的这个目录中能找到文件的切块：</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">/home/hadoop/app/hadoop-2.4.1/tmp/dfs/data/current/BP-193442119-192.168.2.120-1432457733977/current/finalized</span></p>

<h1><strong><a name="_Toc30634"></a><strong><span style="color:#000000;"><strong><a name="_Toc30142"></a><a name="_Toc3826"></a><a name="_Toc421731822"></a><a name="_Toc439077233"></a><a name="_Toc19633"></a><a name="_Toc27866">******HDFS应用开发篇******</a></strong></span></strong></strong></h1>

<h1><strong><strong><span style="color:#000000;"><strong>7. HDFS的java操作</strong></span></strong></strong></h1>

<p style="margin-left:0pt;"><em><em>hdfs在生产应用中主要是客户端的开发，其核心步骤是从hdfs提供的api中构造一个HDFS的访问客户端对象，然后通过该客户端对象操作（增删改查）HDFS上的文件</em></em></p>

<h2><strong><a name="_Toc439077234"></a><strong><strong>7.1 搭建开发环境</strong></strong></strong></h2>

<p style="margin-left:0pt;">1、引入依赖</p>

<table cellspacing="0" style="margin-left:5.4pt;width:248.25pt;"><tbody><tr><td style="vertical-align:top;width:248.25pt;">
			<table border="1" cellspacing="0" style="margin-left:5.65pt;width:247.5pt;"><tbody><tr><td style="vertical-align:top;width:247.5pt;">
						<p style="margin-left:0pt;"><span style="color:#000000;">&lt;dependency&gt;</span></p>

						<p style="margin-left:0pt;"><span style="color:#000000;">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span></p>

						<p style="margin-left:0pt;">    <span style="color:#000000;">&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span></p>

						<p style="margin-left:0pt;"><span style="color:#000000;">    &lt;version&gt;2.</span><span style="color:#000000;">6</span><span style="color:#000000;">.1&lt;/version&gt;</span></p>

						<p style="margin-left:0pt;"><span style="color:#000000;">&lt;/dependency&gt;</span></p>
						</td>
					</tr></tbody></table><p style="margin-left:0pt;"> </p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;"><em><span style="color:#000000;"><em>注：如需手动引入jar包，hdfs的jar包----hadoop的安装目录的share下</em></span></em></p>

<p style="margin-left:0pt;"><span style="color:#000000;">2、window下开发的说明</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：</span></p>

<ol><li><span style="color:#000000;">在windows的某个目录下解压一个hadoop的安装包</span></li>
	<li><span style="color:#000000;">将安装包下的lib和bin目录用对应windows版本平台编译的本地库替换</span></li>
	<li><span style="color:#000000;">在window系统中配置HADOOP_HOME指向你解压的安装包</span></li>
	<li><span style="color:#000000;">在windows系统的path变量中加入hadoop的bin目录</span></li>
</ol><h2><strong><a name="_Toc439077235"></a><strong><strong>7.2 获取api中的客户端对象</strong></strong></strong></h2>

<p style="margin-left:0pt;"><span style="color:#000000;">在java中操作hdfs，首先要获得一个客户端实例</span></p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">Configuration conf = new Configuration()</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">FileSystem fs = FileSystem.get(conf)</span></p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;"><span style="color:#000000;">而我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例；</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">get方法是从何处判断具体实例化那种客户端类呢？</span></p>

<p style="margin-left:0pt;"><strong><span style="color:#000000;"><strong>——从conf中的一个参数 fs.defaultFS的配置值判断；</strong></span></strong></p>

<p style="margin-left:0pt;"><span style="color:#000000;">如果我们的代码中没有指定</span><span style="color:#000000;">fs.defaultFS</span><span style="color:#000000;">，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： </span><a><u><span style="color:#000000;"><u>file:///</u></span></u></a><span style="color:#000000;">，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象</span></p>

<h2><strong><a name="_Toc439077236"></a><strong><span style="color:#000000;"><strong>7.3 DistributedFileSystem实例</strong></span></strong><strong><strong>对象所具备的方法</strong></strong></strong></h2>

<p><img alt="" class="has" height="432" src="https://img-blog.csdn.net/20180908172440311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="439"></p>

<h2><strong><a name="_Toc439077237"></a><strong><strong>7.4 HDFS客户端操作数据代码示例：</strong></strong></strong></h2>

<h3><strong><a name="_Toc439077238"></a><strong><strong>7.4.1 文件的增删改查</strong></strong></strong></h3>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><span style="color:#000000;">public class HdfsClient {</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">FileSystem fs = null;</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">@Before</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">public void init() throws Exception {</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 然后再加载classpath下的hdfs-site.xml</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">Configuration conf = new Configuration();</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">conf.set("fs.defaultFS", "hdfs://hdp-node01:9000");</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">/**</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> */</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">conf.set("dfs.replication", "3");</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// fs = FileSystem.get(conf);</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 如果这样去获取，那conf里面就可以不要配"fs.defaultFS"参数，而且，这个客户端的身份标识已经是hadoop用户</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs = FileSystem.get(new URI("hdfs://hdp-node01:9000"), conf, "hadoop");</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">/**</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * 往hdfs上传文件</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * </span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws Exception</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> */</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">@Test</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">public void testAddFileToHdfs() throws Exception {</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 要上传的文件所在的本地路径</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">Path src = new Path("g:/redis-recommend.zip");</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 要上传到hdfs的目标路径</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">Path dst = new Path("/aaa");</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.copyFromLocalFile(src, dst);</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.close();</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">/**</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * 从hdfs中复制文件到本地文件系统</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * </span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws IOException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws IllegalArgumentException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> */</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">@Test</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">public void testDownloadFileToLocal() throws IllegalArgumentException, IOException {</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.copyToLocalFile(new Path("/jdk-7u65-linux-i586.tar.gz"), new Path("d:/"));</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.close();</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">@Test</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">public void testMkdirAndDeleteAndRename() throws IllegalArgumentException, IOException {</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 创建目录</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.mkdirs(new Path("/a1/b1/c1"));</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 删除文件夹 ，如果是非空文件夹，参数2必须给值true</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.delete(new Path("/aaa"), true);</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 重命名文件或文件夹</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">fs.rename(new Path("/a1"), new Path("/a2"));</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">/**</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * 查看目录信息，只显示文件</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * </span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws IOException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws IllegalArgumentException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws FileNotFoundException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> */</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">@Test</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">public void testListFiles() throws FileNotFoundException, IllegalArgumentException, IOException {</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">// 思考：为什么返回迭代器，而不是List之类的容器</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true);</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">while (listFiles.hasNext()) {</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">LocatedFileStatus fileStatus = listFiles.next();</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println(fileStatus.getPath().getName());</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println(fileStatus.getBlockSize());</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println(fileStatus.getPermission());</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println(fileStatus.getLen());</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">for (BlockLocation bl : blockLocations) {</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println("block-length:" + bl.getLength() + "--" + "block-offset:" + bl.getOffset());</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">String[] hosts = bl.getHosts();</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">for (String host : hosts) {</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println(host);</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println("--------------为angelababy打印的分割线--------------");</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">/**</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * 查看文件及文件夹信息</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * </span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws IOException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws IllegalArgumentException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> * @throws FileNotFoundException</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;"> */</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">@Test</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">public void testListAll() throws FileNotFoundException, IllegalArgumentException, IOException {</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">FileStatus[] listStatus = fs.listStatus(new Path("/"));</span></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><span style="color:#000000;">String flag = "d--             ";</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">for (FileStatus fstatus : listStatus) {</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">if (fstatus.isFile())  flag = "f--         ";</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">System.out.println(flag + fstatus.getPath().getName());</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>

			<p style="margin-left:0pt;"><span style="color:#000000;">}</span></p>
			</td>
		</tr></tbody></table><h3><strong><a name="_Toc439077239"></a><strong><strong>7.4.2 通过流的方式访问hdfs</strong></strong></strong></h3>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>/**</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * 相对那些封装好的方法而言的更底层一些的操作方式</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * 上层那些mapreduce   spark等运算框架，去hdfs中获取数据的时候，就是调的这种底层的api</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * @author</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> *</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> */</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>public class StreamAccess {</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FileSystem fs = null;</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>@Before</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>public void init() throws Exception {</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>Configuration conf = new Configuration();</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>fs = FileSystem.get(new URI("hdfs://hdp-node01:9000"), conf, "hadoop");</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>}</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>@Test</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>public void testDownLoadFileToLocal() throws IllegalArgumentException, IOException{</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//先获取一个文件的输入流----针对hdfs上的</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FSDataInputStream in = fs.open(new Path("/jdk-7u65-linux-i586.tar.gz"));</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//再构造一个文件的输出流----针对本地的</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FileOutputStream out = new FileOutputStream(new File("c:/jdk.tar.gz"));</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//再将输入流中数据传输到输出流</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>IOUtils.copyBytes(in, out, 4096);</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>}</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>/**</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * 用于上层分布式运算框架并发处理数据</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * @throws IllegalArgumentException</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * @throws IOException</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> */</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>@Test</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>public void testRandomAccess() throws IllegalArgumentException, IOException{</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//先获取一个文件的输入流----针对hdfs上的</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FSDataInputStream in = fs.open(new Path("/iloveyou.txt"));</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//可以将流的起始偏移量进行自定义</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>in.seek(22);</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//再构造一个文件的输出流----针对本地的</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FileOutputStream out = new FileOutputStream(new File("c:/iloveyou.line.2.txt"));</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>IOUtils.copyBytes(in,out,19L,true);</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>}</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>/**</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * 显示hdfs上文件的内容</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * @throws IOException </em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> * @throws IllegalArgumentException </em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em> */</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>@Test</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>public void testCat() throws IllegalArgumentException, IOException{</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FSDataInputStream in = fs.open(new Path("/iloveyou.txt"));</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>IOUtils.copyBytes(in, System.out, 1024);</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>}</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>}</em></span></em></p>
			</td>
		</tr></tbody></table><h3><strong><strong><strong>7.4.3 场景编程</strong></strong></strong></h3>

<p style="margin-left:0pt;">在mapreduce 、spark等运算框架中，有一个核心思想就是将运算移往数据，或者说，就是要在并发计算中尽可能让运算本地化，这就需要获取数据所在位置的信息并进行相应范围读取</p>

<p style="margin-left:0pt;">以下模拟实现：获取一个文件的所有block位置信息，然后读取指定block中的内容</p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>@Test</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>public void testCat() throws IllegalArgumentException, IOException{</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FSDataInputStream in = fs.open(new Path("/weblog/input/access.log.10"));</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//拿到文件信息</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FileStatus[] listStatus = fs.listStatus(new Path("/weblog/input/access.log.10"));</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//获取这个文件的所有block的信息</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(listStatus[0], 0L, listStatus[0].getLen());</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//第一个block的长度</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>long length = fileBlockLocations[0].getLength();</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//第一个block的起始偏移量</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>long offset = fileBlockLocations[0].getOffset();</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>System.out.println(length);</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>System.out.println(offset);</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//获取第一个block写入输出流</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>//</em></span></em><em> </em><em> </em><em><span style="color:#000000;"><em>IOUtils.copyBytes(in, System.out, (int)length);</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>byte[] b = new byte[4096];</em></span></em></p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>FileOutputStream os = new FileOutputStream(new File("d:/block0"));</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>while(in.read(offset, b, 0, 4096)!=-1){</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>os.write(b);</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>offset += 4096;</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>if(offset&gt;</em></span></em><em><span style="color:#000000;"><em>=</em></span></em><em><span style="color:#000000;"><em>length) return;</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>};</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>os.flush();</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>os.close();</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>in.close();</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#000000;"><em>}</em></span></em></p>
			</td>
		</tr></tbody></table><h1><strong><strong><strong>8. </strong></strong><strong><strong>案例</strong></strong><strong><strong>1</strong></strong><strong><strong>：</strong></strong><strong><strong>开发</strong></strong><strong><strong>shell</strong></strong><strong><strong>采集脚本</strong></strong></strong></h1>

<h2><strong><strong><strong>8</strong></strong><strong><strong>.1需求说明</strong></strong></strong></h2>

<p style="margin-left:0pt;">点击流日志每天都10T，在业务应用服务器上，需要准实时上传至数据仓库（Hadoop HDFS）上</p>

<h2><strong><strong><strong>8</strong></strong><strong><strong>.2需求分析</strong></strong></strong></h2>

<p style="margin-left:0pt;">一般上传文件都是在凌晨24点操作，由于很多种类的业务数据都要在晚上进行传输，为了减轻服务器的压力<strong><strong>，避开高峰期</strong></strong>。</p>

<p style="margin-left:0pt;">如果需要伪实时的上传，则采用定时上传的方式</p>

<h2><strong><strong><strong>8</strong></strong><strong><strong>.3技术分析</strong></strong></strong></h2>

<p style="margin-left:0pt;"> <strong><strong>HDFS SHELL</strong></strong>:  hadoop fs  –put   xxxx.tar  /data    还可以使用 Java Api</p>

<p style="margin-left:0pt;">  满足上传一个文件，不能满足定时、周期性传入。</p>

<p style="margin-left:0pt;"> <strong><strong>定时调度器</strong></strong>：</p>

<p style="margin-left:0pt;"><strong><strong>L</strong></strong><strong><strong>inux crontab</strong></strong></p>

<p style="margin-left:0pt;">crontab -e</p>

<p style="margin-left:0pt;">*/5 * * * * $home/bin/command.sh   //五分钟执行一次</p>

<p style="margin-left:0pt;">系统会自动执行脚本，每5分钟一次，执行时判断文件是否符合上传规则，符合则上传</p>

<h2><strong><strong><strong>8</strong></strong><strong><strong>.4实现流程</strong></strong></strong></h2>

<h3><strong><strong><strong>8</strong></strong><strong><strong>.4.1日志产生程序</strong></strong></strong></h3>

<p style="margin-left:0pt;">日志产生程序将日志生成后，产生一个一个的文件，使用滚动模式创建文件名。</p>

<p style="margin-left:0pt;"><img alt="" class="has" height="185" src="https://img-blog.csdn.net/20180908172522658?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="333"></p>

<p style="margin-left:18pt;">日志生成的逻辑由业务系统决定，比如在log4j配置文件中配置生成规则，如：当xxxx.log 等于10G时，滚动生成新日志</p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.logger.msg=info,msg</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg=cn.maoxiangyi.MyRollingFileAppender</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.layout=org.apache.log4j.PatternLayout</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.layout.ConversionPattern=%m%n</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.datePattern='.'yyyy-MM-dd</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.Threshold=info</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.append=true</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.encoding=UTF-8</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.MaxBackupIndex=100</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.MaxFileSize=10GB</em></span></em></p>

			<p style="margin-left:0pt;"><em><span style="color:#333333;"><em>log4j.appender.msg.File=/</em></span></em><em><span style="color:#333333;"><em>home/hadoop/logs</em></span></em><em><span style="color:#333333;"><em>/log/</em></span></em><em><span style="color:#333333;"><em>access</em></span></em><em><span style="color:#333333;"><em>.log</em></span></em></p>
			</td>
		</tr></tbody></table><p style="margin-left:18pt;">细节：</p>

<ol><li>如果日志文件后缀是1\2\3等数字，该文件满足需求可以上传的话。把该文件移动到准备上传的工作区间。</li>
	<li>工作区间有文件之后，可以使用hadoop put命令将文件上传。</li>
</ol><p style="margin-left:21pt;">阶段问题：</p>

<ol><li>待上传文件的工作区间的文件，在上传完成之后，是否需要删除掉。</li>
</ol><h3><strong><strong><strong>8</strong></strong><strong><strong>.4.2伪代码</strong></strong></strong></h3>

<p style="margin-left:0pt;">使用ls命令读取指定路径下的所有文件信息，</p>

<p style="margin-left:0pt;">ls  | while read  line</p>

<p style="margin-left:0pt;"> //判断line这个文件名称是否符合规则</p>

<p style="margin-left:0pt;">if  line=access.log.* (</p>

<p style="margin-left:0pt;">将文件移动到待上传的工作区间</p>

<p style="margin-left:0pt;">)</p>

<p style="margin-left:0pt;"> </p>

<p style="margin-left:0pt;">//批量上传工作区间的文件</p>

<p style="margin-left:0pt;">hadoop fs  –put   xxx</p>

<p style="margin-left:0pt;"><strong><span style="color:#ff0000;"><strong>脚本写完之后</strong></span></strong>，配置linux定时任务，每5分钟运行一次。</p>

<h2><strong><strong><strong>8</strong></strong><strong><strong>.5代码实现</strong></strong></strong></h2>

<p style="margin-left:0pt;">代码第一版本，实现基本的上传功能和定时调度功能</p>

<p style="margin-left:0pt;"><img alt="" class="has" height="291" src="https://img-blog.csdn.net/20180908172617212?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="693"></p>

<p style="margin-left:0pt;">代码第二版本：增强版V2(基本能用，还是不够健全)</p>

<p style="margin-left:0pt;"><img alt="" class="has" height="116" src="https://img-blog.csdn.net/20180908172548430?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="693"></p>

<h2><strong><strong><strong>8</strong></strong><strong><strong>.6效果展示及操作步骤</strong></strong></strong></h2>

<p style="margin-left:0pt;">1、日志收集文件收集数据，并将数据保存起来，效果如下：</p>

<p style="margin-left:0pt;">2、上传程序通过crontab定时调度</p>

<p style="margin-left:0pt;"> </p>

<p style="margin-left:0pt;">3、程序运行时产生的临时文件</p>

<p style="margin-left:0pt;">4、Hadoo hdfs上的效果</p>

<p style="margin-left:0pt;"> </p>

<h1><strong><strong><strong>9. 案例2：开发JAVA采集程序</strong></strong></strong></h1>

<h2><strong><strong><strong>9.1 需求</strong></strong></strong></h2>

<p style="margin-left:0pt;">从外部购买数据，数据提供方会实时将数据推送到6台FTP服务器上，我方部署6台接口采集机来对接采集数据，并上传到HDFS中</p>

<p style="margin-left:0pt;">提供商在FTP上生成数据的规则是以小时为单位建立文件夹(2016-03-11-10)，每分钟生成一个文件（00.dat,01.data,02.dat,........）</p>

<p style="margin-left:0pt;">提供方不提供数据备份，推送到FTP服务器的数据如果丢失，不再重新提供，且FTP服务器磁盘空间有限，最多存储最近10小时内的数据</p>

<p style="margin-left:0pt;">由于每一个文件比较小，只有150M左右，因此，我方在上传到HDFS过程中，需要将15分钟时段的数据合并成一个文件上传到HDFS</p>

<p style="margin-left:0pt;">为了区分数据丢失的责任，我方在下载数据时最好进行校验</p>

<h2><strong><strong><strong>9.2 设计分析</strong></strong></strong></h2>

<p><img alt="" class="has" height="380" src="https://img-blog.csdn.net/20180908172749247?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjAyNzU2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="675"></p>

<p style="margin-left:0pt;"> </p>            </div>
                </div>