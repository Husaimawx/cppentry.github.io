---
layout:     post
title:      spark 简单入门
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <h2>运行 Spark 示例</h2>

<p>注意，必须安装 Hadoop 才能使用 Spark，但如果使用 Spark 过程中没用到 HDFS，不启动 Hadoop 也是可以的。此外，接下来教程中出现的命令、目录，若无说明，则一般以 Spark 的安装目录（/usr/local/spark）为当前路径，请注意区分。</p>

<p>在 ./examples/src/main 目录下有一些 Spark 的示例程序，有 Scala、Java、Python、R 等语言的版本。我们可以先运行一个示例程序 SparkPi（即计算 π 的近似值），执行如下命令：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">cd /usr/local/spark</span></li>
	<li><span style="color:#33cc00;">./bin/run-example SparkPi</span></li>
</ol><p>Shell 命令</p>

<p>执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 <code>grep</code> 命令进行过滤（命令中的 <code>2&gt;&amp;1</code> 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">./bin/run-example SparkPi 2&gt;&amp;1 | grep "Pi is roughly"</span></li>
</ol><p>Shell 命令</p>

<p>过滤后的运行结果如下图所示，可以得到 π 的 5 位小数近似值 ：</p>

<p><img alt="从官网下载 Spark" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-02-sparkpi.png" style="margin-left:auto;"><span style="color:#8d8d8d;">从官网下载 Spark</span></p>

<p>Python 版本的 SparkPi 则需要通过 spark-submit 运行：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">./bin/spark-submit examples/src/main/python/pi.py</span></li>
</ol><p>Shell 命令</p>

<h2>通过 Spark Shell 进行交互分析</h2>

<p>Spark shell 提供了简单的方式来学习 API，也提供了交互的方式来分析数据。Spark Shell 支持 Scala 和 Python，本教程选择使用 Scala 来进行介绍。</p>

<p><strong>Scala</strong></p>

<p>Scala 是一门现代的多范式编程语言，志在以简练、优雅及类型安全的方式来表达常用编程模式。它平滑地集成了面向对象和函数语言的特性。Scala 运行于 Java 平台（JVM，Java 虚拟机），并兼容现有的 Java 程序。</p>

<p>Scala 是 Spark 的主要编程语言，如果仅仅是写 Spark 应用，并非一定要用 Scala，用 Java、Python 都是可以的。使用 Scala 的优势是开发效率更高，代码更精简，并且可以通过 Spark Shell 进行交互式实时查询，方便排查问题。</p>

<p>执行如下命令启动 Spark Shell：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">./bin/spark-shell</span></li>
</ol><p>Shell 命令</p>

<p>启动成功后如图所示，会有 “scala &gt;” 的命令提示符。</p>

<p><img alt="成功启动Spark Shell" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-03-spark-shell.png" style="margin-left:auto;"><span style="color:#8d8d8d;">成功启动Spark Shell</span></p>

<h3>基础操作</h3>

<p>Spark 的主要抽象是分布式的元素集合（distributed collection of items），称为RDD（Resilient Distributed Dataset，弹性分布式数据集），它可被分发到集群各个节点上，进行并行操作。RDDs 可以通过 Hadoop InputFormats 创建（如 HDFS），或者从其他 RDDs 转化而来。</p>

<p>我们从 ./README 文件新建一个 RDD，代码如下（本文出现的 Spark 交互式命令代码中，与位于同一行的注释内容为该命令的说明，命令之后的注释内容表示交互式输出结果）：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#000000;">val textFile = sc.textFile("file:///usr/local/spark/README.md")</span></li>
	<li><em><span style="color:#000000;">// textFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at &lt;console&gt;:27</span></em></li>
</ol><p>scala</p>

<p>代码中通过 “file://” 前缀指定读取本地文件。Spark shell 默认是读取 HDFS 中的文件，需要先上传文件到 HDFS 中，否则会有“org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://localhost:9000/user/hadoop/README.md”的错误。</p>

<p>上述命令的输出结果如下图所示：</p>

<p><img alt="新建RDD" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-04-rdd.png" style="margin-left:auto;"><span style="color:#8d8d8d;">新建RDD</span></p>

<p>RDDs 支持两种类型的操作</p>

<ul style="margin-left:20px;"><li style="margin-left:0px;"><a href="http://spark.apache.org/docs/latest/programming-guide.html#actions" rel="nofollow">actions</a>: 在数据集上运行计算后返回值</li>
	<li style="margin-left:0px;"><a href="http://spark.apache.org/docs/latest/programming-guide.html#transformations" rel="nofollow">transformations</a>: 转换, 从现有数据集创建一个新的数据集</li>
</ul><p>下面我们就来演示 count() 和 first() 操作：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">textFile.count() <em>// RDD 中的 item 数量，对于文本文件，就是总行数</em></span></li>
	<li><em><span style="color:#33cc00;">// res0: Long = 95</span></em></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">textFile.first() <em>// RDD 中的第一个 item，对于文本文件，就是第一行内容</em></span></li>
	<li><em><span style="color:#33cc00;">// res1: String = # Apache Spark</span></em></li>
</ol><p>scala</p>

<p>接着演示 transformation，通过 filter transformation 来返回一个新的 RDD，代码如下：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">val linesWithSpark = textFile.filter(line =&gt; line.contains("Spark")) <em>// 筛选出包含 Spark 的行</em></span></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">linesWithSpark.count() <em>// 统计行数</em></span></li>
	<li><em><span style="color:#33cc00;">// res4: Long = 17</span></em></li>
</ol><p>scala</p>

<p>可以看到一共有 17 行内容包含 Spark，这与通过 Linux 命令 <code>cat ./README.md | grep "Spark" -c</code> 得到的结果一致，说明是正确的。action 和 transformation 可以用链式操作的方式结合使用，使代码更为简洁：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">textFile.filter(line =&gt; line.contains("Spark")).count() <em>// 统计包含 Spark 的行数</em></span></li>
	<li><em><span style="color:#33cc00;">// res4: Long = 17</span></em></li>
</ol><p>scala</p>

<h3>RDD的更多操作</h3>

<p>RDD 的 actions 和 transformations 可用在更复杂的计算中，例如通过如下代码可以找到包含单词最多的那一行内容共有几个单词：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">textFile.map(line =&gt; line.split(" ").size).reduce((a, b) =&gt; if (a &gt; b) a else b)</span></li>
	<li><em><span style="color:#33cc00;">// res5: Int = 14</span></em></li>
</ol><p>scala</p>

<p>代码首先将每一行内容 map 为一个整数，这将创建一个新的 RDD，并在这个 RDD 中执行 reduce 操作，找到最大的数。map()、reduce() 中的参数是 Scala 的函数字面量（function literals，也称为闭包 closures），并且可以使用语言特征或 Scala/Java 的库。例如，通过使用 Math.max() 函数（需要导入 Java 的 Math 库），可以使上述代码更容易理解：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">import java.lang.Math</span></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">textFile.map(line =&gt; line.split(" ").size).reduce((a, b) =&gt; Math.max(a, b))</span></li>
	<li><em><span style="color:#33cc00;">// res6: Int = 14</span></em></li>
</ol><p>scala</p>

<p>Hadoop MapReduce 是常见的数据流模式，在 Spark 中同样可以实现（下面这个例子也就是 WordCount）：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">val wordCounts = textFile.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b) <em>// 实现单词统计</em></span></li>
	<li><em><span style="color:#33cc00;">// wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:29</span></em></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">wordCounts.collect() <em>// 输出单词统计结果</em></span></li>
	<li><em><span style="color:#33cc00;">// res7: Array[(String, Int)] = Array((package,1), (For,2), (Programs,1), (processing.,1), (Because,1), (The,1)...)</span></em></li>
</ol><p>scala</p>

<h3>缓存</h3>

<p>Spark 支持在集群范围内将数据集缓存至每一个节点的内存中，可避免数据传输，当数据需要重复访问时这个特征非常有用，例如查询体积小的“热”数据集，或是运行如 PageRank 的迭代算法。调用 cache()，就可以将数据集进行缓存：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">linesWithSpark.cache()</span><span style="color:#ffffff;"> </span></li>
</ol><p>scala</p>

<h2>Spark SQL 和 DataFrames</h2>

<p>Spark SQL 是 Spark 内嵌的模块，用于结构化数据。在 Spark 程序中可以使用 SQL 查询语句或 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="nofollow">DataFrame API</a>。DataFrames 和 SQL 提供了通用的方式来连接多种数据源，支持 Hive、Avro、Parquet、ORC、JSON、和 JDBC，并且可以在多种数据源之间执行 join 操作。</p>

<p>下面仍在 Spark shell 中演示一下 Spark SQL 的基本操作，该部分内容主要参考了 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="nofollow">Spark SQL、DataFrames 和 Datasets 指南</a>。</p>

<p>Spark SQL 的功能是通过 SQLContext 类来使用的，而创建 SQLContext 是通过 SparkContext 创建的。在 Spark shell 启动时，输出日志的最后有这么几条信息</p>

<pre class="has">
<code>16/01/16 13:25:41 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
16/01/16 13:25:41 INFO repl.SparkILoop: Created sql context..
SQL context available as sqlContext.
</code></pre>

<p>这些信息表明 SparkContent 和 SQLContext 都已经初始化好了，可通过对应的 sc、sqlContext 变量直接进行访问。</p>

<p>使用 SQLContext 可以从现有的 RDD 或数据源创建 DataFrames。作为示例，我们通过 Spark 提供的 JSON 格式的数据源文件 ./examples/src/main/resources/people.json 来进行演示，该数据源内容如下：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">{"name":"Michael"}</span></li>
	<li><span style="color:#33cc00;">{"name":"Andy", "age":30}</span></li>
	<li><span style="color:#33cc00;">{"name":"Justin", "age":19}</span></li>
</ol><p>json</p>

<p>执行如下命令导入数据源，并输出内容：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">val df = sqlContext.read.json("file:///usr/local/spark/examples/src/main/resources/people.json")</span></li>
	<li><em><span style="color:#33cc00;">// df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span></em></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">df.show() <em>// 输出数据源内容</em></span></li>
	<li><em><span style="color:#33cc00;">// +----+-------+</span></em></li>
	<li><em><span style="color:#33cc00;">// | age| name|</span></em></li>
	<li><em><span style="color:#33cc00;">// +----+-------+</span></em></li>
	<li><em><span style="color:#33cc00;">// |null|Michael|</span></em></li>
	<li><em><span style="color:#33cc00;">// | 30| Andy|</span></em></li>
	<li><em><span style="color:#33cc00;">// | 19| Justin|</span></em></li>
	<li><em><span style="color:#33cc00;">// +----+-------+</span></em></li>
</ol><p>scala</p>

<p>接着，我们来演示 DataFrames 处理结构化数据的一些基本操作：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">df.select("name").show() <em>// 只显示 "name" 列</em></span></li>
	<li><em><span style="color:#33cc00;">// +-------+</span></em></li>
	<li><em><span style="color:#33cc00;">// | name|</span></em></li>
	<li><em><span style="color:#33cc00;">// +-------+</span></em></li>
	<li><em><span style="color:#33cc00;">// |Michael|</span></em></li>
	<li><em><span style="color:#33cc00;">// | Andy|</span></em></li>
	<li><em><span style="color:#33cc00;">// | Justin|</span></em></li>
	<li><em><span style="color:#33cc00;">// +-------+</span></em></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">df.select(df("name"), df("age") + 1).show() <em>// 将 "age" 加 1</em></span></li>
	<li><em><span style="color:#33cc00;">// +-------+---------+</span></em></li>
	<li><em><span style="color:#33cc00;">// | name|(age + 1)|</span></em></li>
	<li><em><span style="color:#33cc00;">// +-------+---------+</span></em></li>
	<li><em><span style="color:#33cc00;">// |Michael| null|</span></em></li>
	<li><em><span style="color:#33cc00;">// | Andy| 31|</span></em></li>
	<li><em><span style="color:#33cc00;">// | Justin| 20|</span></em></li>
	<li><em><span style="color:#33cc00;">// +-------+---------+</span></em></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">df.filter(df("age") &gt; 21).show() <em># 条件语句</em></span></li>
	<li><em><span style="color:#33cc00;">// +---+----+</span></em></li>
	<li><em><span style="color:#33cc00;">// |age|name|</span></em></li>
	<li><em><span style="color:#33cc00;">// +---+----+</span></em></li>
	<li><em><span style="color:#33cc00;">// | 30|Andy|</span></em></li>
	<li><em><span style="color:#33cc00;">// +---+----+</span></em></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">df.groupBy("age").count().show() <em>// groupBy 操作</em></span></li>
	<li><em><span style="color:#33cc00;">// +----+-----+</span></em></li>
	<li><em><span style="color:#33cc00;">// | age|count|</span></em></li>
	<li><em><span style="color:#33cc00;">// +----+-----+</span></em></li>
	<li><em><span style="color:#33cc00;">// |null| 1|</span></em></li>
	<li><em><span style="color:#33cc00;">// | 19| 1|</span></em></li>
	<li><em><span style="color:#33cc00;">// | 30| 1|</span></em></li>
	<li><em><span style="color:#33cc00;">// +----+-----+</span></em></li>
</ol><p>scala</p>

<p>当然，我们也可以使用 SQL 语句来进行操作：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">df.registerTempTable("people") <em>// 将 DataFrame 注册为临时表 people</em></span></li>
	<li><span style="color:#33cc00;">val result = sqlContext.sql("SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19") <em>// 执行 SQL 查询</em></span></li>
	<li><span style="color:#33cc00;">result.show() <em>// 输出结果</em></span></li>
	<li><em><span style="color:#33cc00;">// +------+---+</span></em></li>
	<li><em><span style="color:#33cc00;">// | name|age|</span></em></li>
	<li><em><span style="color:#33cc00;">// +------+---+</span></em></li>
	<li><em><span style="color:#33cc00;">// |Justin| 19|</span></em></li>
	<li><em><span style="color:#33cc00;">// +------+---+</span></em></li>
</ol><p>scala</p>

<p>更多的功能可以查看完整的 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" rel="nofollow">DataFrames API</a> ，此外 DataFrames 也包含了丰富的 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions%24" rel="nofollow">DataFrames Function</a> 可用于字符串处理、日期计算、数学计算等。</p>

<h2>Spark Streaming</h2>

<p>流计算除了使用 <a href="http://dblab.xmu.edu.cn/blog/install-storm/" rel="nofollow">Storm</a> 框架，使用 Spark Streaming 也是一个很好的选择。基于 Spark Streaming，可以方便地构建可拓展、高容错的流计算应用程序。Spark Streaming 使用 Spark API 进行流计算，这意味着在 Spark 上进行流处理与批处理的方式一样。因此，你可以复用批处理的代码，使用 Spark Streaming 构建强大的交互式应用程序，而不仅仅是用于分析数据。</p>

<p>下面以一个简单的 Spark Streaming 示例（基于流的单词统计）来演示一下 Spark Streaming：本地服务器通过 TCP 接收文本数据，实时输出单词统计结果。该部分内容主要参考了 <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="nofollow">Spark Streaming 编程指南</a>。</p>

<p>运行该示例需要 Netcat（在网络上通过 TCP 或 UDP 读写数据），CentOS 6.x 系统中默认没有安装，经过测试，如果通过 yum 直接安装，运行时会有 “nc: Protocol not available” 的错误，需要下载<a href="http://sourceforge.net/projects/netcat/files/netcat/0.6.1/" rel="nofollow">较低版本的 nc</a> 才能正常使用。我们选择 Netcat 0.6.1 版本，在终端中运行如下命令进行安装：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">wget http://downloads.sourceforge.net/project/netcat/netcat/0.6.1/netcat-0.6.1-1.i386.rpm -O ~/netcat-0.6.1-1.i386.rpm <em># 下载</em></span></li>
	<li><span style="color:#33cc00;">sudo rpm -iUv ~/netcat-0.6.1-1.i386.rpm <em># 安装</em></span></li>
</ol><p>Shell 命令</p>

<p>安装好 NetCat 之后，使用如下命令建立本地数据服务，监听 TCP 端口 9999：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><em><span style="color:#33cc00;"># 记为终端 1</span></em></li>
	<li><span style="color:#33cc00;">nc -l -p 9999</span></li>
</ol><p>Shell 命令</p>

<p>启动后，该端口就被占用了，需要开启另一个终端运行示例程序，执行如下命令：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><em><span style="color:#33cc00;"># 需要另外开启一个终端，记为终端 2，然后运行如下命令</span></em></li>
	<li><span style="color:#33cc00;">/usr/local/spark/bin/run-example streaming.NetworkWordCount localhost 9999</span></li>
</ol><p>Shell 命令</p>

<p>接着在终端 1 中输入文本，在终端 2 中就可以实时看到单词统计结果了。</p>

<p>Spark Streaming 的内容较多，本教程就简单介绍到这，更详细的内容可查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="nofollow">官网教程</a>。最后需要关掉终端 2，并按 ctrl+c 退出 终端 1 的Netcat。</p>

<h2>独立应用程序（Self-Contained Applications）</h2>

<p>接着我们通过一个简单的应用程序 SimpleApp 来演示如何通过 Spark API 编写一个独立应用程序。使用 Scala 编写的程序需要使用 sbt 进行编译打包，相应的，Java 程序使用 Maven 编译打包，而 Python 程序通过 spark-submit 直接提交。</p>

<h3>应用程序代码</h3>

<p>在终端中执行如下命令创建一个文件夹 sparkapp 作为应用程序根目录：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">cd ~ <em># 进入用户主文件夹</em></span></li>
	<li><span style="color:#33cc00;">mkdir ./sparkapp <em># 创建应用程序根目录</em></span></li>
	<li><span style="color:#33cc00;">mkdir -p ./sparkapp/src/main/scala <em># 创建所需的文件夹结构</em></span></li>
</ol><p>Shell 命令</p>

<p>在 ./sparkapp/src/main/scala 下建立一个名为 SimpleApp.scala 的文件（<code>vim ./sparkapp/src/main/scala/SimpleApp.scala</code>），添加代码如下：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><em><span style="color:#33cc00;">/* SimpleApp.scala */</span></em></li>
	<li><span style="color:#33cc00;">import org.apache.spark.SparkContext</span></li>
	<li><span style="color:#33cc00;">import org.apache.spark.SparkContext._</span></li>
	<li><span style="color:#33cc00;">import org.apache.spark.SparkConf</span></li>
	<li><span style="color:#33cc00;"> </span></li>
	<li><span style="color:#33cc00;">object SimpleApp {</span></li>
	<li><span style="color:#33cc00;">def main(args: Array[String]) {</span></li>
	<li><span style="color:#33cc00;">val logFile = "file:///usr/local/spark/README.md" <em>// Should be some file on your system</em></span></li>
	<li><span style="color:#33cc00;">val conf = new SparkConf().setAppName("Simple Application")</span></li>
	<li><span style="color:#33cc00;">val sc = new SparkContext(conf)</span></li>
	<li><span style="color:#33cc00;">val logData = sc.textFile(logFile, 2).cache()</span></li>
	<li><span style="color:#33cc00;">val numAs = logData.filter(line =&gt; line.contains("a")).count()</span></li>
	<li><span style="color:#33cc00;">val numBs = logData.filter(line =&gt; line.contains("b")).count()</span></li>
	<li><span style="color:#33cc00;">println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))</span></li>
	<li><span style="color:#33cc00;">}</span></li>
	<li><span style="color:#33cc00;">}</span></li>
</ol><p>scala</p>

<p>该程序计算 /usr/local/spark/README 文件中包含 “a” 的行数 和包含 “b” 的行数。代码第8行的 /usr/local/spark 为 Spark 的安装目录，如果不是该目录请自行修改。不同于 Spark shell，独立应用程序需要通过 <code>val sc = new SparkContext(conf)</code> 初始化 SparkContext，SparkContext 的参数 SparkConf 包含了应用程序的信息。</p>

<p>该程序依赖 Spark API，因此我们需要通过 sbt 进行编译打包。在 ./sparkapp 中新建文件 simple.sbt（<code>vim ./sparkapp/simple.sbt</code>），添加内容如下，声明该独立应用程序的信息以及与 Spark 的依赖关系：</p>

<pre class="has">
<code>name := "Simple Project"

version := "1.0"

scalaVersion := "2.10.5"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.0"
</code></pre>

<p>文件 simple.sbt 需要指明 Spark 和 Scala 的版本。启动 Spark shell 的过程中，当输出到 Spark 的符号图形时，可以看到相关的版本信息。</p>

<p><img alt="查看 Spark 和 Scala 的版本信息" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-05-version.png" style="margin-left:auto;"><span style="color:#8d8d8d;">查看 Spark 和 Scala 的版本信息</span></p>

<h3>安装 sbt</h3>

<p>Spark 中没有自带 sbt，需要<a href="http://www.scala-sbt.org/0.13/docs/zh-cn/Manual-Installation.html" rel="nofollow">手动安装 sbt</a>，我们选择安装在 /usr/local/sbt 中：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">sudo mkdir /usr/local/sbt</span></li>
	<li><span style="color:#33cc00;">sudo chown -R hadoop /usr/local/sbt <em># 此处的 hadoop 为你的用户名</em></span></li>
	<li><span style="color:#33cc00;">cd /usr/local/sbt</span></li>
</ol><p>Shell 命令</p>

<p>经笔者测试，按官网教程安装 sbt 0.13.9 后，使用时可能存在网络问题，无法下载依赖包，导致 sbt 无法正常使用，需要进行一定的修改。为方便，请使用笔者修改后的版本，下载地址：<a href="http://pan.baidu.com/s/1eRyFddw" rel="nofollow">http://pan.baidu.com/s/1eRyFddw</a>。</p>

<p>下载后，执行如下命令拷贝至 /usr/local/sbt 中：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">cp ~/下载/sbt-launch.jar .</span></li>
</ol><p>Shell 命令</p>

<p>接着在 /usr/local/sbt 中创建 sbt 脚本（<code>vim ./sbt</code>），添加如下内容：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><em><span style="color:#33cc00;">#!/bin/bash</span></em></li>
	<li><span style="color:#33cc00;">SBT_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"</span></li>
	<li><span style="color:#33cc00;">java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar "$@"</span></li>
</ol><p>Shell 命令</p>

<p>保存后，为 ./sbt 脚本增加可执行权限：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">chmod u+x ./sbt</span></li>
</ol><p>Shell 命令</p>

<p>最后检验 sbt 是否可用（首次运行会处于 “Getting org.scala-sbt sbt 0.13.9 …” 的下载状态，请耐心等待。笔者等待了 7 分钟才出现第一条下载提示）：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">./sbt sbt-version</span></li>
</ol><p>Shell 命令</p>

<p>下载过程中可能会类似 “Server access Error: java.security.ProviderException: java.security.KeyException url=https://jcenter.bintray.com/org/scala-sbt/precompiled-2_9_3/0.13.9/precompiled-2_9_3-0.13.9.jar” 的错误，可以忽略。可再执行一次 <code>./sbt sbt-version</code>，只要能得到如下图的版本信息就没问题：</p>

<p><img alt="验证 sbt 是否可用" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-06-sbt-version.png" style="margin-left:auto;"><span style="color:#8d8d8d;">验证 sbt 是否可用</span></p>

<p>如果由于网络问题无法下载依赖，导致 sbt 无法正确运行的话，可以下载笔者提供的离线依赖包 sbt-0.13.9-repo.tar.gz 到本地中（依赖包的本地位置为 ~/.sbt 和 ~/.ivy2，检查依赖关系时，首先检查本地，本地未找到，再从网络中下载），下载地址：<a href="http://pan.baidu.com/s/1sjTQ8yD" rel="nofollow">http://pan.baidu.com/s/1sjTQ8yD</a>。下载后，执行如下命令解压依赖包：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">tar -zxf ~/下载/sbt-0.13.9-local-repo.tar.gz ~</span></li>
</ol><p>Shell 命令</p>

<p>通过这个方式，一般可以解决依赖包缺失的问题（读者提供的依赖包仅适合于 Spark 1.6 版本，不同版本依赖关系不一样）。</p>

<p>如果对 sbt 存在的网络问题以及如何解决感兴趣，请点击下方查看。</p>

<p>点击查看：解决 sbt 无法下载依赖包的问题</p>

<h3>使用 sbt 打包 Scala 程序</h3>

<p>为保证 sbt 能正常运行，先执行如下命令检查整个应用程序的文件结构：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">cd ~/sparkapp</span></li>
	<li><span style="color:#33cc00;">find .</span></li>
</ol><p>Shell 命令</p>

<p>文件结构应如下图所示：</p>

<p><img alt="SimpleApp的文件结构" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-07-directory-structure.png" style="margin-left:auto;"><span style="color:#8d8d8d;">SimpleApp的文件结构</span></p>

<p>接着，我们就可以通过如下代码将整个应用程序打包成 JAR（首次运行同样需要下载依赖包，如果这边遇到网络问题无法成功，也请下载上述安装 sbt 提到的离线依赖包 sbt-0.13.9-repo.tar.gz ）：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">/usr/local/sbt/sbt package</span></li>
</ol><p>Shell 命令</p>

<p>打包成功的话，会输出如下图内容：</p>

<p><img alt="SimpleApp的文件结构" class="has" src="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/01/spark-quick-start-guide-08-sbt-package.png" style="margin-left:auto;"><span style="color:#8d8d8d;">SimpleApp的文件结构</span></p>

<p>生成的 jar 包的位置为 ~/sparkapp/target/scala-2.10/simple-project_2.10-1.0.jar。</p>

<h3>通过 spark-submit 运行程序</h3>

<p>最后，我们就可以将生成的 jar 包通过 spark-submit 提交到 Spark 中运行了，命令如下：</p>

<pre style="margin-left:1em;">

<code class="language-html hljs"> </code></pre>

<ol><li><span style="color:#33cc00;">/usr/local/spark/bin/spark-submit --class "SimpleApp" ~/sparkapp/target/scala-2.10/simple-project_2.10-1.0.jar</span></li>
	<li><em><span style="color:#33cc00;"># 输出信息太多，可以通过如下命令过滤直接查看结果</span></em></li>
	<li><span style="color:#33cc00;">/usr/local/spark/bin/spark-submit --class "SimpleApp" ~/sparkapp/target/scala-2.10/simple-project_2.10-1.0.jar 2&gt;&amp;1 | grep "Lines with a:"</span></li>
</ol><p>Shell 命令</p>

<p>最终得到的结果如下：</p>

<pre class="has">
<code>Lines with a: 58, Lines with b: 26
</code></pre>

<p>自此，你就完成了你的第一个 Spark 应用程序了。</p>

<h2>进阶学习</h2>

<p>Spark 官网提供了完善的学习文档（许多技术文档都只有英文版本，因此学会查看英文文档也是学习大数据技术的必备技能）：</p>

<ul style="margin-left:20px;"><li style="margin-left:0px;">如果想对 Spark 的 API 有更深入的了解，可查看的 <a href="http://spark.apache.org/docs/latest/programming-guide.html" rel="nofollow">Spark 编程指南（Spark Programming Guide）</a>；</li>
	<li style="margin-left:0px;">如果你想对 Spark SQL 的使用有更多的了解，可以查看 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="nofollow">Spark SQL、DataFrames 和 Datasets 指南</a>；</li>
	<li style="margin-left:0px;">如果你想对 Spark Streaming 的使用有更多的了解，可以查看 <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="nofollow">Spark Streaming 编程指南</a>；</li>
	<li style="margin-left:0px;">如果需要在集群环境中运行 Spark 程序，可查看官网的 <a href="http://spark.apache.org/docs/latest/cluster-overview.html" rel="nofollow">Spark 集群部署</a></li>
</ul>            </div>
                </div>