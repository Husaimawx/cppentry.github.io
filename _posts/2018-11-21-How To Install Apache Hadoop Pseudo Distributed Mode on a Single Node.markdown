---
layout:     post
title:      How To Install Apache Hadoop Pseudo Distributed Mode on a Single Node
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                http://www.thegeekstuff.com/2012/02/hadoop-pseudo-distributed-installation/<br><br>1. Create a Hadoop User<br><br># adduser hadoop<br># passwd hadoop<br><br>2. Download Hadoop Common<br><br># su - hadoop<br>$ wget http://mirror.nyi.net/apache//hadoop/common/stable/hadoop-0.20.203.0rc1.tar.gz<br><br>Make sure Java 1.6 is installed on your system.<br><br>$ java -version<br>java version "1.6.0_20"<br>OpenJDK Runtime Environment (IcedTea6 1.9.7) (rhel-1.39.1.9.7.el6-x86_64)<br>OpenJDK 64-Bit Server VM (build 19.0-b09, mixed mode)<br><br>3. Unpack under hadoop User<br><br>$ tar xvfz hadoop-0.20.203.0rc1.tar.gz<br><br>This will create the “hadoop-0.20.204.0″ directory.<br>$ ls -l hadoop-0.20.204.0<br>total 6780<br>drwxr-xr-x.  2 hadoop hadoop    4096 Oct 12 08:50 bin<br>-rw-rw-r--.  1 hadoop hadoop  110797 Aug 25 16:28 build.xml<br>drwxr-xr-x.  4 hadoop hadoop    4096 Aug 25 16:38 c++<br>-rw-rw-r--.  1 hadoop hadoop  419532 Aug 25 16:28 CHANGES.txt<br>drwxr-xr-x.  2 hadoop hadoop    4096 Nov  2 05:29 conf<br>drwxr-xr-x. 14 hadoop hadoop    4096 Aug 25 16:28 contrib<br>drwxr-xr-x.  7 hadoop hadoop    4096 Oct 12 08:49 docs<br>drwxr-xr-x.  3 hadoop hadoop    4096 Aug 25 16:29 etc<br><br>Modify the hadoop-0.20.204.0/conf/hadoop-env.sh file and make sure JAVA_HOME environment variable is pointing to the correct location of the java that is installed on your system.<br><br>$ grep JAVA ~/hadoop-0.20.204.0/conf/hadoop-env.sh<br>export JAVA_HOME=/usr/java/jdk1.6.0_27.<br><br>4. Modify Hadoop Configuration Files<br>Add the &lt;configuration&gt; section shown below to the core-site.xml file. This indicates the HDFS default location and the port.<br><br>$ cat ~/hadoop-0.20.204.0/conf/core-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;fs.default.name&lt;/name&gt;<br>               &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;<br><br>Add the &lt;configuration&gt; section shown below to the hdfs-site.xml file.<br>$ cat ~/hadoop-0.20.204.0/conf/hdfs-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;dfs.replication&lt;/name&gt;<br>              &lt;value&gt;1&lt;/value&gt;<br>     &lt;/property&gt;<br>     &lt;property&gt;<br>     &lt;name&gt;dfs.permissions&lt;/name&gt;<br>     &lt;value&gt;false&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;<br>Add the &lt;configuration&gt; section shown below to the mapred-site.xml file. This indicates that the job tracker uses 9001 as the port.<br>$ cat ~/hadoop-0.20.204.0/conf/mapred-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br>              &lt;value&gt;localhost:9001&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;<br>5. Setup passwordless ssh to localhost<br>In a typical Hadoop production environment you’ll be setting up this passwordless ssh access between the different servers. Since we are simulating a distributed environment on a single server, we need to setup the passwordless ssh access to the localhost itself.<br><br>Use ssh-keygen to generate the private and public key value pair.<br>$ ssh-keygen<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/hadoop/.ssh/id_rsa.<br>Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>02:5a:19:ab:1e:g2:1a:11:bb:22:30:6d:12:38:a9:b1 hadoop@hadoop<br>The key's randomart image is:<br>+--[ RSA 2048]----+<br>|oo               |<br>|o + .    .       |<br>| + +  o o        |<br>|o   .o = .       |<br>| .   += S        |<br>|.   o.o+.        |<br>|.    ..o.        |<br>| . E  ..         |<br>|  .   ..         |<br>+-----------------+<br>Add the public key to the authorized_keys. Just use the ssh-copy-id command, which will take care of this step automatically and assign appropriate permissions to these files.<br><br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub localhost<br>hadoop@localhost's password:<br>Now try logging into the machine, with "ssh 'localhost'", and check in:<br><br>  .ssh/authorized_keys<br><br>to make sure we haven't added extra keys that you weren't expecting.<br><br>Test the passwordless login to the localhost as shown below.<br>$ ssh localhost<br>Last login: Sat Jan 14 23:01:59 2012 from localhost<br><br>6. Format Hadoop NameNode<br>Format the namenode using the hadoop command as shown below. You’ll see the message “Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted” if this command works properly.<br>.  How To Install Apache Hadoop Pseudo Distributed Mode on a Single Node<br>by Ramesh Natarajan on February 8, 2012<br><br>Tweet <br>Apache Hadoop Pseudo-distributed mode installation helps you to simulate a multi node installation on a single node. Instead of installing hadoop on different servers, you can simulate it on a single server.<br><br>Before you continue, make sure you understand the hadoop fundamentals, and have tested the standalone hadoop installation.<br><br>If you’ve already completed the 1st three steps mentioned below as part of the standlone hadoop installation, jump to step 4.<br><br>1. Create a Hadoop User<br>You can download and install hadoop on root. But, it is recommended to install it as a separate user. So, login to root and create a user called hadoop.<br><br># adduser hadoop<br># passwd hadoop2. Download Hadoop Common<br>Download the Apache Hadoop Common and move it to the server where you want to install it.<br><br>You can also use wget to download it directly to your server using wget.<br><br># su - hadoop<br>$ wget http://mirror.nyi.net/apache//hadoop/common/stable/hadoop-0.20.203.0rc1.tar.gzMake sure Java 1.6 is installed on your system.<br><br>$ java -version<br>java version "1.6.0_20"<br>OpenJDK Runtime Environment (IcedTea6 1.9.7) (rhel-1.39.1.9.7.el6-x86_64)<br>OpenJDK 64-Bit Server VM (build 19.0-b09, mixed mode)3. Unpack under hadoop User<br>As hadoop user, unpack this package.<br><br>$ tar xvfz hadoop-0.20.203.0rc1.tar.gzThis will create the “hadoop-0.20.204.0″ directory.<br><br>$ ls -l hadoop-0.20.204.0<br>total 6780<br>drwxr-xr-x.  2 hadoop hadoop    4096 Oct 12 08:50 bin<br>-rw-rw-r--.  1 hadoop hadoop  110797 Aug 25 16:28 build.xml<br>drwxr-xr-x.  4 hadoop hadoop    4096 Aug 25 16:38 c++<br>-rw-rw-r--.  1 hadoop hadoop  419532 Aug 25 16:28 CHANGES.txt<br>drwxr-xr-x.  2 hadoop hadoop    4096 Nov  2 05:29 conf<br>drwxr-xr-x. 14 hadoop hadoop    4096 Aug 25 16:28 contrib<br>drwxr-xr-x.  7 hadoop hadoop    4096 Oct 12 08:49 docs<br>drwxr-xr-x.  3 hadoop hadoop    4096 Aug 25 16:29 etcModify the hadoop-0.20.204.0/conf/hadoop-env.sh file and make sure JAVA_HOME environment variable is pointing to the correct location of the java that is installed on your system.<br><br>$ grep JAVA ~/hadoop-0.20.204.0/conf/hadoop-env.sh<br>export JAVA_HOME=/usr/java/jdk1.6.0_27After this step, hadoop will be installed under /home/hadoop/hadoop-0.20.204.0 directory.<br><br>4. Modify Hadoop Configuration Files<br>Add the &lt;configuration&gt; section shown below to the core-site.xml file. This indicates the HDFS default location and the port.<br><br>$ cat ~/hadoop-0.20.204.0/conf/core-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;fs.default.name&lt;/name&gt;<br>               &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;Add the &lt;configuration&gt; section shown below to the hdfs-site.xml file.<br><br>$ cat ~/hadoop-0.20.204.0/conf/hdfs-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;dfs.replication&lt;/name&gt;<br>              &lt;value&gt;1&lt;/value&gt;<br>     &lt;/property&gt;<br>     &lt;property&gt;<br>     &lt;name&gt;dfs.permissions&lt;/name&gt;<br>     &lt;value&gt;false&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;Add the &lt;configuration&gt; section shown below to the mapred-site.xml file. This indicates that the job tracker uses 9001 as the port.<br><br>$ cat ~/hadoop-0.20.204.0/conf/mapred-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br>              &lt;value&gt;localhost:9001&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;5. Setup passwordless ssh to localhost<br>In a typical Hadoop production environment you’ll be setting up this passwordless ssh access between the different servers. Since we are simulating a distributed environment on a single server, we need to setup the passwordless ssh access to the localhost itself.<br><br>Use ssh-keygen to generate the private and public key value pair.<br><br>$ ssh-keygen<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/hadoop/.ssh/id_rsa.<br>Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>02:5a:19:ab:1e:g2:1a:11:bb:22:30:6d:12:38:a9:b1 hadoop@hadoop<br>The key's randomart image is:<br>+--[ RSA 2048]----+<br>|oo               |<br>|o + .    .       |<br>| + +  o o        |<br>|o   .o = .       |<br>| .   += S        |<br>|.   o.o+.        |<br>|.    ..o.        |<br>| . E  ..         |<br>|  .   ..         |<br>+-----------------+Add the public key to the authorized_keys. Just use the ssh-copy-id command, which will take care of this step automatically and assign appropriate permissions to these files.<br><br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub localhost<br>hadoop@localhost's password:<br>Now try logging into the machine, with "ssh 'localhost'", and check in:<br><br>  .ssh/authorized_keys<br><br>to make sure we haven't added extra keys that you weren't expecting.Test the passwordless login to the localhost as shown below.<br><br>$ ssh localhost<br>Last login: Sat Jan 14 23:01:59 2012 from localhostFor more details on this, read 3 Steps to Perform SSH Login Without Password Using ssh-keygen &amp; ssh-copy-id<br><br>6. Format Hadoop NameNode<br>Format the namenode using the hadoop command as shown below. You’ll see the message “Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted” if this command works properly.<br><br>$ cd ~/hadoop-0.20.204.0<br><br>$ bin/hadoop namenode -format<br>12/01/14 23:02:27 INFO namenode.NameNode: STARTUP_MSG:<br>/************************************************************<br>STARTUP_MSG: Starting NameNode<br>STARTUP_MSG:   host = hadoop/127.0.0.1<br>STARTUP_MSG:   args = [-format]<br>STARTUP_MSG:   version = 0.20.204.0<br>STARTUP_MSG:   build = git://hrt8n35.cc1.ygridcore.net/ on branch branch-0.20-security-204 -r 65e258bf0813ac2b15bb4c954660eaf9e8fba141; compiled by 'hortonow' on Thu Aug 25 23:35:31 UTC 2011<br>************************************************************/<br>12/01/14 23:02:27 INFO util.GSet: VM type       = 64-bit<br>12/01/14 23:02:27 INFO util.GSet: 2% max memory = 17.77875 MB<br>12/01/14 23:02:27 INFO util.GSet: capacity      = 2^21 = 2097152 entries<br>12/01/14 23:02:27 INFO util.GSet: recommended=2097152, actual=2097152<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: fsOwner=hadoop<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: supergroup=supergroup<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: isPermissionEnabled=true<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)<br>12/01/14 23:02:27 INFO namenode.NameNode: Caching file names occuring more than 10 times<br>12/01/14 23:02:27 INFO common.Storage: Image file of size 112 saved in 0 seconds.<br>12/01/14 23:02:27 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted.<br>12/01/14 23:02:27 INFO namenode.NameNode: SHUTDOWN_MSG:<br>/************************************************************<br>SHUTDOWN_MSG: Shutting down NameNode at hadoop/127.0.0.1<br>************************************************************/<br>7. Start All Hadoop Related Services<br>.  How To Install Apache Hadoop Pseudo Distributed Mode on a Single Node<br>by Ramesh Natarajan on February 8, 2012<br><br>Tweet <br>Apache Hadoop Pseudo-distributed mode installation helps you to simulate a multi node installation on a single node. Instead of installing hadoop on different servers, you can simulate it on a single server.<br><br>Before you continue, make sure you understand the hadoop fundamentals, and have tested the standalone hadoop installation.<br><br>If you’ve already completed the 1st three steps mentioned below as part of the standlone hadoop installation, jump to step 4.<br><br>1. Create a Hadoop User<br>You can download and install hadoop on root. But, it is recommended to install it as a separate user. So, login to root and create a user called hadoop.<br><br># adduser hadoop<br># passwd hadoop2. Download Hadoop Common<br>Download the Apache Hadoop Common and move it to the server where you want to install it.<br><br>You can also use wget to download it directly to your server using wget.<br><br># su - hadoop<br>$ wget http://mirror.nyi.net/apache//hadoop/common/stable/hadoop-0.20.203.0rc1.tar.gzMake sure Java 1.6 is installed on your system.<br><br>$ java -version<br>java version "1.6.0_20"<br>OpenJDK Runtime Environment (IcedTea6 1.9.7) (rhel-1.39.1.9.7.el6-x86_64)<br>OpenJDK 64-Bit Server VM (build 19.0-b09, mixed mode)3. Unpack under hadoop User<br>As hadoop user, unpack this package.<br><br>$ tar xvfz hadoop-0.20.203.0rc1.tar.gzThis will create the “hadoop-0.20.204.0″ directory.<br><br>$ ls -l hadoop-0.20.204.0<br>total 6780<br>drwxr-xr-x.  2 hadoop hadoop    4096 Oct 12 08:50 bin<br>-rw-rw-r--.  1 hadoop hadoop  110797 Aug 25 16:28 build.xml<br>drwxr-xr-x.  4 hadoop hadoop    4096 Aug 25 16:38 c++<br>-rw-rw-r--.  1 hadoop hadoop  419532 Aug 25 16:28 CHANGES.txt<br>drwxr-xr-x.  2 hadoop hadoop    4096 Nov  2 05:29 conf<br>drwxr-xr-x. 14 hadoop hadoop    4096 Aug 25 16:28 contrib<br>drwxr-xr-x.  7 hadoop hadoop    4096 Oct 12 08:49 docs<br>drwxr-xr-x.  3 hadoop hadoop    4096 Aug 25 16:29 etcModify the hadoop-0.20.204.0/conf/hadoop-env.sh file and make sure JAVA_HOME environment variable is pointing to the correct location of the java that is installed on your system.<br><br>$ grep JAVA ~/hadoop-0.20.204.0/conf/hadoop-env.sh<br>export JAVA_HOME=/usr/java/jdk1.6.0_27After this step, hadoop will be installed under /home/hadoop/hadoop-0.20.204.0 directory.<br><br>4. Modify Hadoop Configuration Files<br>Add the &lt;configuration&gt; section shown below to the core-site.xml file. This indicates the HDFS default location and the port.<br><br>$ cat ~/hadoop-0.20.204.0/conf/core-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;fs.default.name&lt;/name&gt;<br>               &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;Add the &lt;configuration&gt; section shown below to the hdfs-site.xml file.<br><br>$ cat ~/hadoop-0.20.204.0/conf/hdfs-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;dfs.replication&lt;/name&gt;<br>              &lt;value&gt;1&lt;/value&gt;<br>     &lt;/property&gt;<br>     &lt;property&gt;<br>     &lt;name&gt;dfs.permissions&lt;/name&gt;<br>     &lt;value&gt;false&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;Add the &lt;configuration&gt; section shown below to the mapred-site.xml file. This indicates that the job tracker uses 9001 as the port.<br><br>$ cat ~/hadoop-0.20.204.0/conf/mapred-site.xml<br>&lt;?xml version="1.0"?&gt;<br>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br><br>&lt;!-- Put site-specific property overrides in this file. --&gt;<br><br>&lt;configuration&gt;<br>     &lt;property&gt;<br>              &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br>              &lt;value&gt;localhost:9001&lt;/value&gt;<br>     &lt;/property&gt;<br>&lt;/configuration&gt;5. Setup passwordless ssh to localhost<br>In a typical Hadoop production environment you’ll be setting up this passwordless ssh access between the different servers. Since we are simulating a distributed environment on a single server, we need to setup the passwordless ssh access to the localhost itself.<br><br>Use ssh-keygen to generate the private and public key value pair.<br><br>$ ssh-keygen<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/hadoop/.ssh/id_rsa.<br>Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>02:5a:19:ab:1e:g2:1a:11:bb:22:30:6d:12:38:a9:b1 hadoop@hadoop<br>The key's randomart image is:<br>+--[ RSA 2048]----+<br>|oo               |<br>|o + .    .       |<br>| + +  o o        |<br>|o   .o = .       |<br>| .   += S        |<br>|.   o.o+.        |<br>|.    ..o.        |<br>| . E  ..         |<br>|  .   ..         |<br>+-----------------+Add the public key to the authorized_keys. Just use the ssh-copy-id command, which will take care of this step automatically and assign appropriate permissions to these files.<br><br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub localhost<br>hadoop@localhost's password:<br>Now try logging into the machine, with "ssh 'localhost'", and check in:<br><br>  .ssh/authorized_keys<br><br>to make sure we haven't added extra keys that you weren't expecting.Test the passwordless login to the localhost as shown below.<br><br>$ ssh localhost<br>Last login: Sat Jan 14 23:01:59 2012 from localhostFor more details on this, read 3 Steps to Perform SSH Login Without Password Using ssh-keygen &amp; ssh-copy-id<br><br>6. Format Hadoop NameNode<br>Format the namenode using the hadoop command as shown below. You’ll see the message “Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted” if this command works properly.<br><br>$ cd ~/hadoop-0.20.204.0<br><br>$ bin/hadoop namenode -format<br>12/01/14 23:02:27 INFO namenode.NameNode: STARTUP_MSG:<br>/************************************************************<br>STARTUP_MSG: Starting NameNode<br>STARTUP_MSG:   host = hadoop/127.0.0.1<br>STARTUP_MSG:   args = [-format]<br>STARTUP_MSG:   version = 0.20.204.0<br>STARTUP_MSG:   build = git://hrt8n35.cc1.ygridcore.net/ on branch branch-0.20-security-204 -r 65e258bf0813ac2b15bb4c954660eaf9e8fba141; compiled by 'hortonow' on Thu Aug 25 23:35:31 UTC 2011<br>************************************************************/<br>12/01/14 23:02:27 INFO util.GSet: VM type       = 64-bit<br>12/01/14 23:02:27 INFO util.GSet: 2% max memory = 17.77875 MB<br>12/01/14 23:02:27 INFO util.GSet: capacity      = 2^21 = 2097152 entries<br>12/01/14 23:02:27 INFO util.GSet: recommended=2097152, actual=2097152<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: fsOwner=hadoop<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: supergroup=supergroup<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: isPermissionEnabled=true<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100<br>12/01/14 23:02:27 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)<br>12/01/14 23:02:27 INFO namenode.NameNode: Caching file names occuring more than 10 times<br>12/01/14 23:02:27 INFO common.Storage: Image file of size 112 saved in 0 seconds.<br>12/01/14 23:02:27 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted.<br>12/01/14 23:02:27 INFO namenode.NameNode: SHUTDOWN_MSG:<br>/************************************************************<br>SHUTDOWN_MSG: Shutting down NameNode at hadoop/127.0.0.1<br>************************************************************/7. Start All Hadoop Related Services<br>Use the ~/hadoop-0.20.204.0/bin/start-all.sh script to start all hadoop related services. This will start the namenode, datanode, secondary namenode, jobtracker, tasktracker, etc.<br>$ bin/start-all.sh<br>starting namenode, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-namenode-hadoop.out<br>localhost: starting datanode, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-datanode-hadoop.out<br>localhost: starting secondarynamenode, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-secondarynamenode-hadoop.out<br>starting jobtracker, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-jobtracker-hadoop.out<br>localhost: starting tasktracker, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../<br><br>8. Browse NameNode and JobTracker Web GUI<br>Once all the Hadoop processes are started, you can view the health and status of the HDFS from a web interface. Use http://{your-hadoop-server-ip}:50070/dfshealth.jsp<br><br>For example, if you’ve installed hadoop on a server with ip-address 192.168.1.10, then use http://192.168.1.10:50070/dfshealth.jsp to view the NameNode GUI<br><br>This will display the following information:<br><br>Basic NameNode information:<br><br>■This will show when the Namenode was started, the hadoop version number, whether any upgrades are currently in progress or not.<br>■This also has link “Browse the filesystem”, which will let browse the content of HDFS filesystem from browser<br>■Click on “Namenode Logs” to view the logs<br>Cluster Summary displays the following information:<br><br>■Total number of files and directories managed by the HDFS<br>■Any warning message (for example: missing blocks as shown in the image below)<br>■Total HDFS file system size<br>■Both HDFS %age-used and size-used<br>■Total number of nodes in this distributed system<br>NameNode storage information: This displays the storage directory of the HDFS file system, the filesystem type, and the state (Active or not)<br>To access the JobTracker web interface, use http://{your-hadoop-server-ip}:50090<br><br>For example, if you’ve installed hadoop on a server with ip-address 192.168.1.10, then use http://192.168.102.20:50090/ to view the JobTracker GUI<br><br>As shown by the netstat command below, you can see both these ports are getting used.<br><br><br>$ netstat -a | grep 500<br>tcp        0      0 *:50090                     *:*                         LISTEN<br>tcp        0      0 *:50070                     *:*                         LISTEN<br>tcp        0      0 hadoop.thegeekstuff.com:50090    ::ffff:192.168.1.98:55923 ESTABL<br><br>9. Test Sample Hadoop Program<br>This example program is provided as part of the hadoop, and it is shown in the hadoop document as an simple example to see whether this setup work.<br><br>For testing purpose, add some sample data files to the input directory. Let us just copy all the xml file from the conf directory to the input directory. So, these xml file will be considered as the data file for the example program. In the standalone version, you used the standard cp command to copy it to the input directory.<br><br>However in a distributed Hadoop setup, you’ll be using -put option of the hadoop command to add files to the HDFS file system. Keep in mind that you are not adding the files to a Linux filesystem, you are adding the input files to the Hadoop Distributed file system. So, you use use the hadoop command to do this.<br><br>$ cd ~/hadoop-0.20.204.0<br><br>$ bin/hadoop fs -put conf inputExecute the sample hadoop test program. This is a simple hadoop program that simulates a grep. This searches for the reg-ex pattern “dfs[a-z.]+” in all the input/*.xml files (that is stored in the HDFS) and stores the output in the output directory that will be stored in the HDFS.<br><br>$ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+'When everything is setup properly, the above sample hadoop test program will display the following messages on the screen when it is executing it.<br><br>$ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+'<br>12/01/14 23:45:02 INFO mapred.FileInputFormat: Total input paths to process : 18<br>12/01/14 23:45:02 INFO mapred.JobClient: Running job: job_201111020543_0001<br>12/01/14 23:45:03 INFO mapred.JobClient:  map 0% reduce 0%<br>12/01/14 23:45:18 INFO mapred.JobClient:  map 11% reduce 0%<br>12/01/14 23:45:24 INFO mapred.JobClient:  map 22% reduce 0%<br>12/01/14 23:45:27 INFO mapred.JobClient:  map 22% reduce 3%<br>12/01/14 23:45:30 INFO mapred.JobClient:  map 33% reduce 3%<br>12/01/14 23:45:36 INFO mapred.JobClient:  map 44% reduce 7%<br>12/01/14 23:45:42 INFO mapred.JobClient:  map 55% reduce 14%<br>12/01/14 23:45:48 INFO mapred.JobClient:  map 66% reduce 14%<br>12/01/14 23:45:51 INFO mapred.JobClient:  map 66% reduce 18%<br>12/01/14 23:45:54 INFO mapred.JobClient:  map 77% reduce 18%<br>12/01/14 23:45:57 INFO mapred.JobClient:  map 77% reduce 22%<br>12/01/14 23:46:00 INFO mapred.JobClient:  map 88% reduce 22%<br>12/01/14 23:46:06 INFO mapred.JobClient:  map 100% reduce 25%<br>12/01/14 23:46:15 INFO mapred.JobClient:  map 100% reduce 100%<br>12/01/14 23:46:20 INFO mapred.JobClient: Job complete: job_201111020543_0001<br>...The above command will create the output directory (in HDFS) with the results as shown below. To view this output directory, you should use “-get” option in the hadoop command as shown below.<br><br>$  bin/hadoop fs -get output output<br><br>$ ls -l output<br>total 4<br>-rwxrwxrwx. 1 root root 11 Aug 23 08:39 part-00000<br>-rwxrwxrwx. 1 root root  0 Aug 23 08:39 _SUCCESS<br><br>$ cat output/*<br>1       dfsadmin10. Troubleshooting Hadoop Issues<br>Issue 1: “Temporary failure in name resolution”<br><br>While executing the sample hadoop program, you might get the following error message.<br><br>12/01/14 07:34:57 INFO mapred.JobClient: Cleaning up the staging area file:/tmp/hadoop-root/mapred/staging/root-1040516815/.staging/job_local_0001<br>java.net.UnknownHostException: hadoop: hadoop: Temporary failure in name resolution<br>        at java.net.InetAddress.getLocalHost(InetAddress.java:1438)<br>        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:815)<br>        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:791)<br>        at java.security.AccessController.doPrivileged(Native Method)Solution 1: Add the following entry to the /etc/hosts file that contains the ip-address, FQDN fully qualified domain name, and host name.<br><br>192.168.1.10 hadoop.thegeekstuff.com hadoopIssue 2: “localhost: Error: JAVA_HOME is not set”<br><br>While executing hadoop start-all.sh, you might get this error as shown below.<br><br>$ bin/start-all.sh<br>starting namenode, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-namenode-hadoop.out<br>localhost: starting datanode, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-datanode-hadoop.out<br>localhost: Error: JAVA_HOME is not set.<br>localhost: starting secondarynamenode, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-secondarynamenode-hadoop.out<br>localhost: Error: JAVA_HOME is not set.<br>starting jobtracker, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-jobtracker-hadoop.out<br>localhost: starting tasktracker, logging to /home/hadoop/hadoop-0.20.204.0/libexec/../logs/hadoop-hadoop-tasktracker-hadoop.out<br>localhost: Error: JAVA_HOME is not set.Solution 2: Make sure JAVA_HOME is setup properly in the conf/hadoop-env.sh as shown below.<br><br>$ grep JAVA_HOME conf/hadoop-env.sh<br>export JAVA_HOME=/usr/java/jdk1.6.0_27Issue 3: Error while executing “bin/hadoop fs -put conf input”<br><br>You might get one of the following error messages (including put: org.apache.hadoop.security.AccessControlException: Permission denied:) while executing the hadoop fs put command as shown below.<br><br>$ bin/hadoop fs -put conf input<br>12/01/14 23:21:53 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s).<br>12/01/14 23:21:54 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s).<br>12/01/14 23:21:55 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s).<br>Bad connection to FS. command aborted. exception: Call to localhost/127.0.0.1:9000 failed on connection exception: java.net.ConnectException: Connection refused<br><br>$  bin/hadoop fs -put conf input<br>put: org.apache.hadoop.security.AccessControlException: Permission denied: user=hadoop, access=WRITE, inode="":root:supergroup:rwxr-xr-x<br>$ ls -l inputSolution 3: Make sure /etc/hosts file is setup properly. Also, if your HDFS filesystem is not created properly, you might have issues during “hadoop fs -put”. Format your HDFS using “bin/hadoop namenode -format” and confirm that this displays “successfully formatted” message.<br><br>Issue 4: While executing start-all.sh (or start-dfs.sh), you might get this error message: “localhost: Unrecognized option: -jvm localhost: Could not create the Java virtual machine.”<br><br>Solution 4: This might happens if you’ve installed hadoop as root and trying to start the process. This is know bug, that is fixed according to this bug report. But, if you hit this bug, try installing hadoop as a non-root account (just like how we’ve explained in this article), which should fix this issue.            </div>
                </div>