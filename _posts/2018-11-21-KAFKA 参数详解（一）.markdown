---
layout:     post
title:      KAFKA 参数详解（一）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/mrczr/article/details/80242036				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>KAFKA采用zookeeper保存集群的元数据和消费者信息，所以安装kafka之前必须现有zookeeper</p><p><img src="https://img-blog.csdn.net/20180508164124392?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21yY3py/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p>zookeeper采用一致性协议，所以部署的集群最好是奇数个，少数服从多数，假设3个节点中有一个失效并不影响zookeeper处理外部请求，否则失效。以上主要是kafka基于zookeeper的一些注意事项。</p><p>接着，主要是kafka的的配置参数</p><p>（一）公共配置<br></p><p>1，broker id： kafka集群的<u><em>唯一</em></u>表示符，服务器之间的交换均使用该ID</p><p>2，port：kafka监听端口，一般默认是9092，使用1024以前端口的话就需要root权限。E.g:开启线程消费时用到的就是这个端口</p><p>3，zookeeper.connect：用于保存broker元数据的zookeeper地址是通过该参数指定的，当producer向kafka topic发布消息事，指定的就是zookeeper.connect这个路劲。</p><p>    （1）hostName：是Zookeeper IP地址或者机器名</p><p>    （2）zookeeper客户端连接端口</p><p>    （3）/path 可选路径，没有则默认根目录，该参数作为kafka集群的chroot环境，一旦指定之后一旦路径不存在，则会自动创建它，但是生成者或者穿件kafka topic的时候记得把这个路劲带上，如：</p><div>hadoop--1:2181,hadoop-02:2181,hadoop-03:2181/kafka_2 （/kafka_2就是kafka集群环境的chroot参数指定的路径）否则就会包broker找不到的错误，估计是因为broker的信息和初始化的信息存在chroot下，但是使用的时候没指定路径就会去根目录下找，结果找不到，就报了找不到broker的错误，下面有一个错误例子的把报错信息：<img src="https://img-blog.csdn.net/20180508165540613?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21yY3py/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>4，log.dir：Kafka是把所有消息都保存在在磁盘上，存放目录即通过该参数指定的。可以配置多个，逗号分隔开，同个分区的数据会保存在同个路径下，分配原则是最少使用原则，即哪个路径分区数目少，broker有优先往这个路径新增分区<br>5，num.recovery.threads.per.data.dir：对于以下三种情况，kafka会使用用可配置的线程来处理日志片段<br>    （1）服务器正常启动，用户打开每个分区的日志片段<br>    （2）服务器奔溃后，用户检测和窃取每个分区的日志片段，进行合并<br>    （3）服务器正常关闭，正常关闭日志片段<br>每个片段目录只使用一个线程，改线程只有在服务启动关闭时才会用到，该参数设置和上诉的log.dir有关，比如：该参数设置为3个线程，如果log.dir也是为3，则实际上总线程数是3*3=9<br>6，auto.create.topic.enable：是否自动创建线程<br>（二）主题的配置<br>1，num.partitions：分区数，已经存在的主题可以新增分区，但是不可以减少分区。为了能使分区分布在broker上，分区数必须大于broker数量，分区数的确定，dt：预计主题吞吐量，num=dt/max(pt,ct) 其中pt，ct分别为生成者，消费者吞吐量<br>2，log.retention.ms，log.retention.min，log.rentention.hours：kafka数据保留时间（两者选其一，都设置时选最小，默认168小时）<br>3，log.rentention.byte：每个分区的数据保留大小，分区为单位。所以，当主图中的分区数增加时，可以连带增加主题数据的保留大小；保留时间和保留大小谁先到达，谁先触发。<br>4，log.segment.byte：日志切片大小，当日志大小达到该设定值时才会关闭文件，重新打开新的日志文件记录<br>5，log.segment.ms：日志切片时间，当日志打开时间达到该设定值时才会关闭文件，重新打开新的日志文件记录<br>6，message.max.byte：限制消息大小，超过的时候，生产者发送的消息会收到broker返回来的错误    7，fetch.message.max.byte：消费者客户端设置，这个值必须比message.max.byte大，才能确保消费端不会出错，阻塞。<br>8，replica.fetch.max.byte：副本最大大小，和7同理<br>（三）系统参数<br>1，vm.swappiness：内存交换大小<br>2，vm.dirty_background_ratio：系统内存的百分比（后天刷新进程将脏页数据写入磁盘），即超过该值时，就会刷新页面，将缓存数据入磁盘<br>（四）java调优<br>这个用到了java JVM的知识储备。由于kafka运行过程中，对堆内存的使用GI，容易产生大量的垃圾对象，垃圾回收机制就显得比较重要kafka默认是使用parallel New 和CMS（并发标记和清除）来回收。建议使用GI 回收<br><img src="https://img-blog.csdn.net/20180509105430810?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21yY3py/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>MaxGCPauseMillis：垃圾回收停顿时间，停顿期间，所有进程会被挂起，默认200ms，但是会根据回收频率和回收区域大小来计算停顿时间<br>InitiatingHeapoccupancyPercent：堆内存百分比（年轻代和年老代），默认45%，超过该设定值则自动启动回收机制。<br><br></div>            </div>
                </div>