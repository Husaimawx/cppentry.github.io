---
layout:     post
title:      Hadoop生态圈开源项目总结
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
一.Hadoop总结<br>
1.HDFS shell<br><span></span>1.0查看帮助<br><span></span>hadoop fs -help &lt;cmd&gt;<br><span></span>1.1上传<br><span></span>hadoop fs -put &lt;linux上文件&gt; &lt;hdfs上的路径&gt;<br><span></span>1.2查看文件内容<br><span></span>hadoop fs -cat &lt;hdfs上的路径&gt;<br><span></span>1.3查看文件列表<br><span></span>hadoop fs -ls /<br><span></span>1.4下载文件<br><span></span>hadoop fs -get &lt;hdfs上的路径&gt; &lt;linux上文件&gt;<br><span></span>1.5删除文件<br><span></span>hadoop fs -rmr &lt;hdfs上的路径&gt;<br><span></span>1.6创建文件夹<br><span></span>hadoop fs -mkdir /dirname<br><br><br>
2.HDFS源码分析<br><span></span>FileSystem.get --&gt; 通过反射实例化了一个DistributedFileSystem --&gt; new DFSCilent()把他作为自己的成员变量<br><span></span>在DFSClient构造方法里面，调用了createNamenode，使用了RPC机制，得到了一个NameNode的代理对象，就可以和NameNode进行通信了<br><span></span>FileSystem --&gt; DistributedFileSystem --&gt; DFSClient --&gt; NameNode的代理<br>
3.MR执行流程<br><span></span>(1).客户端提交一个mr的jar包给JobClient<br><span></span>(2).JobClient通过RPC和JobTracker进行通信，返回一个存放jar包的地址（HDFS）和jobId<br><span></span>(3).client将jar包写入到HDFS当中<br><span></span>(4).开始提交任务(任务的描述信息，不是jar)<br><span></span>(5).JobTracker进程初始化任务<br><span></span>(6).读取HDFS上的要处理的文件，开始计算输入分片，每一个分片对应一个MapperTask<br><span></span>(7).TaskTracker通过心跳机制领取任务<br><span></span>(8).下载所需的jar，配置文件等<br><span></span>(9).TaskTracker启动一个java child进程，用来执行具体的任务（MapperTask或ReducerTask）<br><span></span>(10).将结果写入到HDFS当中<br>
4.maptask分片<br><span></span>片大小：splitSize = max(minsize,min(maxsize,blockSize))；<br><span></span>minsize：mapred.min.split.size(配置文件中设置默认没设置)<br><span></span>maxsize：mapred.max.split.size(配置文件中设置默认long类型的最大值263-1)<br><span></span>默认一般一个block块对应一个map的分片（有些文件进行压缩后不支持分块，上传到hdfs也是一块不会被拆分）<br><span></span>如果想多个map分片对应一个分块就要设置mapred.max.split.size<br>
5.hadoop集群的6个配置文件<br><span></span>第一个：hadoop-env.sh <br><span></span>export JAVA_HOME=/usr/java/jdk1.6.0_45<br><span></span><br><span></span>第二个：core-site.xml<br><span></span>&lt;!-- 指定HDFS的namenode的通信地址 --&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;fs.default.name&lt;/name&gt;<br><span></span>&lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span>&lt;!-- 指定hadoop运行时产生文件的存放目录 --&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br><span></span>&lt;value&gt;/cloud/hadoop-1.1.2/tmp&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span><br><span></span>第三个：hdfs-site.xml<br><span></span>&lt;!-- 配置HDFS副本的数量 --&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;dfs.replication&lt;/name&gt;<br><span></span>&lt;value&gt;3&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span><br><span></span>第四个：mapred-site.xml<br><span></span>&lt;!-- 指定jobtracker地址 --&gt;<br><span></span>&lt;property&gt;<br><span></span>&lt;name&gt;mapred.job.tracker&lt;/name&gt;<br><span></span>&lt;value&gt;hadoop01:9001&lt;/value&gt;<br><span></span>&lt;/property&gt;<br><span></span><br><span></span>第五个：masters(指定secondarynamenode地址)<br><span></span>hadoop01<br><span></span><br><span></span>第六个：slaves<br><span></span>hadoop02<br><span></span>hadoop03 <br>
6.动态添加一台节点<br><span></span>6.1修改主机名 vim /etc/sysconfig/network<br><span></span>6.2使用root用户登录，右键点击右上角网络，选择Edit connections，修改Auto eth1，设置为manual方式，添加IP，NETMASK，GAYEWAY,再点apply<br><span></span>6.3关闭防火墙 service iptables stop ;chkconfig iptables off 重启机器：reboot<br><span></span>6.4配置主节点和新添加节点的hosts文件<br><span></span>6.5分别启动datanode和tasktracker<br><span></span>hadoop-daemon.sh start datanode<br><span></span>hadoop-daemon.sh start tasktracker<br><span></span>6.6在主节点上即namenode所在节点上运行命令刷新<br><span></span>hadoop dfsadmin -refreshNodes<br>
7.hadoop集群管理相关命令<br><span></span>hadoop安全模式：Safe mode is ON（可以读取文件，但是不可以向HDFS写入文件）<br><span></span>hadoop dfsadmin -safemode enter(进入安装器模式) / leave(离开安全模式) / get(获取当前状态) / wait(竟然等待状态)<br><span></span>hadoop mradmin -safemode enter / leave / get / wait <br><span></span><br>
二.配置mysql远程连接<br><span></span>GRANT ALL PRIVILEGES ON database.* TO 'root'@'192.168.1.201' IDENTIFIED BY '123' WITH GRANT OPTION;<br><span></span>FLUSH PRIVILEGES; <br>
三.Sqoop总结<br><span></span>注：将数据库连接驱动拷贝到$SQOOP_HOME/lib里<br><span></span>第一类：sqoop和hdfs交互<br><span></span>数据库中的数据导入到HDFS上<br><span></span>sqoop import --connect jdbc:mysql://192.168.1.10:3306/database --username root --password 123  --table trade_detail --columns 'id, account, income, expenses'<br><span></span>指定输出路径、指定数据分隔符<br><span></span>sqoop import --connect jdbc:mysql://192.168.35.100:3306/database --username root --password 123  --table trade_detail --target-dir '/sqoop/td' --fields-terminated-by '\t'<br><span></span>指定Map数据 -m <br><span></span>sqoop import --connect jdbc:mysql://192.168.1.10:3306/database --username root --password 123  --table trade_detail --target-dir '/sqoop/td1' --fields-terminated-by '\t' -m 2<br><span></span>增加where条件, 注意：条件必须用引号引起来<br><span></span>sqoop import --connect jdbc:mysql://192.168.1.10:3306/database --username root --password 123  --table trade_detail --where 'id&gt;3' --target-dir '/sqoop/td2' <br><span></span>增加query语句(使用 \ 将语句换行)<br><span></span>sqoop import --connect jdbc:mysql://192.168.1.10:3306/database --username root --password 123 \<br><span></span>--query 'SELECT * FROM trade_detail where id &gt; 2 AND $CONDITIONS' --split-by trade_detail.id --target-dir '/sqoop/td3'<br><span></span><br><span></span>注意：如果使用--query这个命令的时候，需要注意的是where后面的参数，AND $CONDITIONS这个参数必须加上<br><span></span>而且存在单引号与双引号的区别，如果--query后面使用的是双引号，那么需要在$CONDITIONS前加上\即\$CONDITIONS<br><span></span>如果设置map数量为1个时即-m 1，不用加上--split-by ${tablename.column}，否则需要加上<br><span></span>将HDFS上的数据导出到数据库中（结尾的-m 1设置执行的map个数为1）<br><span></span>sqoop export --connect jdbc:mysql://192.168.8.120:3306/database --username root --password 123 --export-dir '/td3' --table td_bak -m 1<br><span></span>第二类：sqoop和hive交互<br><span></span>sqoop import --connect jdbc:mysql://192.168.35.100:3306/database --username root --password 123 --table user_info --hive-import --hive-overwrite --hive-table user_info --fields-terminated-by '\t'<br>
四.ZooKeeper总结<br><span></span>1.应用场景（简单介绍7个场景）<br><span></span>数据发布与订阅（配置中心）、负载均衡、命名服务、分布式通知/协调、集群管理与Master选举、分布式锁、分布式队列<br><span></span>2.zk要部署奇数个<br><span></span>3.注意事项（特别要注意，否则启动报错）<br><span></span>在（dataDir=/cloud/zookeeper-3.4.5/tmp）创建一个myid文件，里面内容是server.N中的N（server.2里面内容为2）<br><span></span>echo "1" &gt; myid<br><span></span>将配置好的zk拷贝到其他节点在其他节点上一定要修改myid的内容<br><span></span>在hadoop02应该讲myid的内容改为2 （echo "2" &gt; myid）<br><span></span>在hadoop03应该讲myid的内容改为3 （echo "3" &gt; myid）<br>
五.HBase总结<br><span></span>简介：<br><span></span>建立的hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。它介于nosql和RDBMS之间，仅能通过主键(row key)<br><span></span>和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。主要用来存储非结构化和半结构化的松散数据。<br><span></span>与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。<br><span></span>HBase中的表一般有这样的特点：<br><span></span>1) 大：一个表可以有上亿行，上百万列<br><span></span>2) 面向列:面向列(族)的存储和权限控制，列(族)独立检索。<br><span></span>3) 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。<br><span></span>1.HBase启动顺序<br><span></span>启动hadfs start-dfs.sh --&gt;启动zk zkServer.sh start --&gt; 启动hbase start-hbase.sh --&gt;启动第二个hbase master: hbase-daemon.sh start master<br><span></span>2.不要忘记修改hbase-env.sh<br><span></span>export HBASE_MANAGES_ZK=false<span>
</span>//告诉hbase使用外部的zk<br>
六.Flume总结<br><span></span>配置HDFS Sink时首先需要将$HADOOP_HOME/hadoop-core-1.0.4.jar和$HADOOP_HOME/lib/commons-configuration-1.6.jar拷贝到$FLUME_HOME/lib/下<br><span></span>启动（切换到flume的安装路径下启动）<br><span></span>#将日志信息打印到控制台<br><span></span>bin/flume-ng agent -n agent -c conf -f conf/flume-conf.properties -Dflume.root.logger=INFO,console<br><span></span>#将产生的日志信息保存到日志文件中,并后台运行<br><span></span>bin/flume-ng agent -n agent -c conf -f conf/flume-conf.properties &gt;logs/flume_log.log &amp;<br>
七.hive的总结就不赘述了，专门写了博文进行介绍<br><span></span><br><span></span><br><span></span><br><span></span><br><span></span><br><br><br><br><br><span></span>
            </div>
                </div>