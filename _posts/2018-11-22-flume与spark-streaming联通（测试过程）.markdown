---
layout:     post
title:      flume与spark-streaming联通（测试过程）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/superce/article/details/80921150				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>安装好flume</p><p>       配置flume配置文件，确定flume数据源以及要将数据发送给谁</p><p>安装telnet</p><p>       apt-getinstall xinetd telnetd</p><p>       </p><p>       安装后使用显示</p><p>       root@master:/usr/local/hadoop-2.7.5/sbin#telnet  </p><p>       bash:telnet: command not found</p><p>       因为telnet依赖xinetd启动，所以xinetd得先启动</p><p>       root@master:/etc/xinetd.d#service xinetd status</p><p>        *  isnot running</p><p>       root@master:/etc/xinetd.d#service xinetd staart</p><p>       Usage:/etc/init.d/xinetd {start|stop|reload|force-reload|restart|status}</p><p>       root@master:/etc/xinetd.d#service xinetd start </p><p>        * Starting internet superserver xinetd                                                                    [ OK ] </p><p>       root@master:/etc/xinetd.d#</p><p>       出现问题</p><p>              root@master:/etc/xinetd.d#apt-get install telnetd</p><p>              Readingpackage lists... Done</p><p>              Buildingdependency tree       </p><p>              Readingstate information... Done</p><p>              telnetdis already the newest version (0.17-40).</p><p>              0upgraded, 0 newly installed, 0 to remove and 4 not upgraded.</p><p>              root@master:/etc/xinetd.d#telnetd</p><p>              bash:telnetd: command not found</p><p>              root@master:/etc/xinetd.d#</p><p>       然后百度，竟然百度到自己之前写的博客。。。。。。</p><p>       问题解决</p><p>              不是apt-get install xinetd telnetd,是apt-getinstall telnet</p><p>              使用apt-get install telnet安装后就能用了，同样的错误两次。。。。。。</p><p>配置flume链接spark-streaming需要的jar包</p><p>       首先看本地的scala版本，spark版本</p><p>              root@master:/usr/local/spark/bin#spark-shell </p><p>              Settingdefault log level to "WARN".</p><p>              Toadjust logging level use sc.setLogLevel(newLevel).</p><p>              18/04/2601:28:22 WARN spark.SparkContext: Use an existing SparkContext, someconfiguration may not take effect.</p><p>              Sparkcontext Web UI available at http://172.17.0.2:4040</p><p>              Sparkcontext available as 'sc' (master = local[*], app id = local-1524706101693).</p><p>              Sparksession available as 'spark'.</p><p>              Welcometo</p><p>                    ____              __</p><p>                   / __/__ ___ _____/ /__</p><p>                  _\ \/ _ \/ _ `/ __/  '_/</p><p>                 /___/ .__/\_,_/_/ /_/\_\   version 2.0.2</p><p>                    /_/</p><p>                       </p><p>              UsingScala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_162)</p><p>              Typein expressions to have them evaluated.</p><p>              Type:help for more information.</p><p>              </p><p>              scala&gt;</p><p>       然后到官网下载相应的包</p><p>              http://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume_2.11/2.0.2</p><p>       然后复制到docker中</p><p>              sudodocker cp spark-streaming-flume_2.11-2.0.2.jar master:/root/build</p><p>       放入spark的jars目录下</p><p>              root@master:/usr/local/spark/jars#mkdir flume</p><p>              root@master:/usr/local/spark/jars#cd flume/</p><p>              root@master:/usr/local/spark/jars/flume#ll</p><p>              total8</p><p>              drwxr-xr-x2 root root 4096 Apr 26 01:33 ./</p><p>              drwxr-xr-x3  500 500 4096 Apr 26 01:33 ../</p><p>              root@master:/usr/local/spark/jars/flume#cp /root/build/spark-streaming-flume_2.11-2.0.2.jar .</p><p>              root@master:/usr/local/spark/jars/flume#ll</p><p>              total112</p><p>              drwxr-xr-x2 root root   4096 Apr 26 01:34 ./</p><p>              drwxr-xr-x3  500 500   4096 Apr 26 01:33 ../</p><p>              -rw-------1 root root 105087 Apr 26 01:34 spark-streaming-flume_2.11-2.0.2.jar</p><p>              root@master:/usr/local/spark/jars/flume#</p><p>       修改spark-env.sh文件中的SPARK_DIST_CLASSPATH变量</p><p>              原来是export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-2.7.5/bin/hadoopclasspath)</p><p>              添加为export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-2.7.5/bin/hadoopclasspath):/usr/local/spark/jars/flume/*:/usr/local/flume/lib/*</p><p>编写spark程序测试</p><p>       网上找到一段代码，来源http://dblab.xmu.edu.cn/blog/1745-2/</p><p>       from__future__ import print_function</p><p>        </p><p>       importsys</p><p>        </p><p>       frompyspark import SparkContext</p><p>       frompyspark.streaming import StreamingContext</p><p>       frompyspark.streaming.flume import FlumeUtils</p><p>       importpyspark</p><p>       if__name__ == "__main__":</p><p>           if len(sys.argv) != 3:</p><p>               print("Usage: flume_wordcount.py&lt;hostname&gt; &lt;port&gt;", file=sys.stderr)</p><p>               exit(-1)</p><p>        </p><p>           sc =SparkContext(appName="FlumeEventCount")</p><p>           ssc = StreamingContext(sc, 2)</p><p>        </p><p>           hostname= sys.argv[1]</p><p>           port = int(sys.argv[2])</p><p>           stream = FlumeUtils.createStream(ssc,hostname, port,pyspark.StorageLevel.MEMORY_AND_DISK_SER_2)</p><p>           stream.count().map(lambda cnt :"Recieve " + str(cnt) +" Flume events!!!!").pprint()</p><p>        </p><p>           ssc.start()</p><p>           ssc.awaitTermination()</p><p>使用spark-submit运行spark应用程序</p><p>       root@master:~/pyworkspace#spark-submit --driver-class-path/usr/local/spark/jars/*:/usr/local/spark/jars/flume/* flumetest.py localhost44444</p><p>       这一步相当于打开了服务器，在本地端口等待flume接收的消息发送过来</p><p>启动flume</p><p>       进入flume文件夹下，输入</p><p>              root@master:/usr/local/flume/conf#bin/flume-ng agent --conf ./conf --conf-file ./conf/flume-to-spark.conf --namea1 -Dflume.root.logger=INFO,console</p><p>              </p><p>              需要输入的有两个参数，一个是配置文件的路径和文件名字，一个是配置文件中这个flume的名字</p><p>使用telnet向flume发送数据</p><p>       root@master:/#telnet localhost 33333</p><p>       Trying127.0.0.1...</p><p>       Connectedto localhost.</p><p>       Escapecharacter is '^]'.</p><p>       vvvvvvvvvvvvvvv  gggggggg     ttttttt</p><p>       OK</p><p>       uuuuuuuuuuu</p><p>       OK</p><p>出现问题</p><p>       在spark输出终端上，没有输出接收的数据，而是输出WARN BlockManager:Block input-0-1524707416800 replicated to only 0 peer(s) instead of 1 peers</p><p>              此时的状态是只是启动了hadoop集群，没有启动spark集群，启动spark集群后，还是出现这个错误</p><p>       解决方法：</p><p>              百度到的解释是：Do not run Spark Streaming programs locally with master configuredas local or local[1]. This allocates only one CPU for tasks and if a receiveris running on it, there is no resource left to process the received data. Useat least local[2] to have more cores.</p><p>              </p><p>       测试：使用root@master:~/pyworkspace# spark-submit --master yarn--driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/flume/*flumetest.py localhost 44444在yarn上管理</p><p>              此时flume和spark都出现了错误</p><p>              flume的错误</p><p>                     org.apache.flume.EventDeliveryException:Failed to send events</p><p>              spark的错误</p><p>                     WARNClient: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back touploading libraries under SPARK_HOME.</p><p>                     WARNDFSClient: Caught exception </p><p>                     java.lang.InterruptedException</p><p>                     WARNTransportChannelHandler: Exception in connection from /172.17.0.3:60354</p><p>                     java.io.IOException:Connection reset by peer</p><p>                     </p><p>                     ERRORSparkContext: Error initializing SparkContext.</p><p>                     org.apache.spark.SparkException:Yarn application has already ended! It might have been killed or unable tolaunch application master.</p><p>                     </p><p>                     WARNYarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executorsbefore the AM has registered!</p><p>                     WARN MetricsSystem: Stopping aMetricsSystem that is not running</p><p>                     Traceback(most recent call last):</p><p>                     </p><p>                     py4j.protocol.Py4JJavaError:An error occurred while callingNone.org.apache.spark.api.java.JavaSparkContext.</p><p>                     </p><p>                     </p><p>                     </p><p>                     </p><p>                     </p><p>       测试：使用root@master:~/pyworkspace# spark-submit --master local[4]--driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/flume/*flumetest.py localhost 44444</p><p>       </p><p>              这回的确显示出东西来了，不过还是有WARN BlockManager: Block input-0-1524813488200 replicated to only 0peer(s) instead of 1 peers这个问题,这个问题好像并不影响</p><p>       测试：spark-submit在yarn 上运行</p><p>              要想在HADOOP YARN 上运行程序，必须先设置HADOOP_CONF_DIR环境变量</p><p>                     exportHADOOP_CONF_DIR=/usr/local/hadoop-2.7.5/etc/hadoop</p><p>              添加后spark-submit运行没有问题</p><p>                     spark-submit--master yarn --driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/flume/*flumetest.py localhost 44444</p><p>              但是启动flume出现问题</p><p>                      (SinkRunner-PollingRunner-DefaultSinkProcessor)[INFO -org.apache.flume.sink.AbstractRpcSink.createConnection(AbstractRpcSink.java:205)]Rpc sink k1: Building RpcClient with hostname: localhost, port: 44444</p><p>                     </p><p>                     (SinkRunner-PollingRunner-DefaultSinkProcessor)[INFO - org.apache.flume.sink.AvroSink.initializeRpcClient(AvroSink.java:126)]Attempting to create Avro Rpc client.</p><p>                     </p><p>                      (SinkRunner-PollingRunner-DefaultSinkProcessor)[WARN -org.apache.flume.api.NettyAvroRpcClient.configure(NettyAvroRpcClient.java:634)]Using default maxIOWorkers</p><p>                     </p><p>                      (SinkRunner-PollingRunner-DefaultSinkProcessor)[ERROR - org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:158)]Unable to deliver event. Exception follows.</p><p>                     </p><p>                     org.apache.flume.EventDeliveryException:Failed to send events</p><p>                     </p><p>                     Causedby: org.apache.flume.FlumeException: NettyAvroRpcClient { host: localhost,port: 44444 }: RPC connection error</p><p>                     </p><p>                     Causedby: java.io.IOException: Error connecting to localhost/127.0.0.1:44444</p><p>                     </p><p>                     Causedby: java.net.ConnectException: Connection refused: localhost/127.0.0.1:44444</p><p>              修改flume配置文件</p><p>                     a1.sources=r1</p><p>                     a1.sinks=k1</p><p>                     a1.channels=c1</p><p>                     </p><p>                     #Describe/configure the source</p><p>                     a1.sources.r1.type=netcat</p><p>                     a1.sources.r1.bind=localhost</p><p>                     a1.sources.r1.port=33333</p><p>                     </p><p>                     #Describe the sink</p><p>                     a1.sinks.k1.type=logger</p><p>                     #a1.sinks.k1.hostname=localhost</p><p>                     #a1.sinks.k1.port=44444</p><p>                     </p><p>                     #Use a channel which buffers events in memory</p><p>                     a1.channels.c1.type=memory</p><p>                     a1.channels.c1.capacity=1000000</p><p>                     a1.channels.c1.transactionCapacity=1000000</p><p>                     </p><p>                     #Bind the source and sink to the channel</p><p>                     a1.sources.r1.channels=c1</p><p>                     a1.sinks.k1.channel=c1</p><p>              即flume不再向spark发送消息，则flume正常工作，没有错误</p><p>              </p><p>              </p><p>       问题：WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set,falling back to uploading libraries under SPARK_HOME</p><p>       </p><p>       方法一：</p><p>       </p><p>              root@master:~/pyworkspace#hadoop fs -mkdir spark_jars</p><p>              root@master:~/pyworkspace#hadoop fs -ls</p><p>              Found2 items</p><p>              drwxr-xr-x   - root supergroup          0 2018-04-27 13:42 .sparkStaging</p><p>              drwxr-xr-x   - root supergroup          0 2018-04-27 13:44 spark_jars</p><p>              root@master:~/pyworkspace#</p><p>              root@master:~/pyworkspace#hadoop fs -copyFromLocal /usr/local/spark/jars/* spark_jars</p><p>              在spark的conf的spark-default.conf添加</p><p>                     spark.yarn.jars       hdfs://master:9000/spark_jars/*</p><p>              上述错误WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set,falling back to uploading libraries under SPARK_HOME消失</p><p>              但是出现新错误</p><p>                     WARNDFSClient: Caught exception </p><p>                     java.lang.InterruptedException</p><p>                            atjava.lang.Object.wait(Native Method)</p><p>                     </p><p>                     ERRORYarnClientSchedulerBackend: Yarn application has already exited with stateFINISHED!</p><p>                     </p><p>                     ERRORTransportClient: Failed to send RPC 9028375459380775738 to /172.17.0.3:55780:java.nio.channels.ClosedChannelException</p><p>                     </p><p>                     java.io.IOException:Failed to send RPC 9028375459380775738 to /172.17.0.3:55780:java.nio.channels.ClosedChannelException</p><p>                     </p><p>                     Causedby: java.io.IOException: Failed to send RPC 9028375459380775738 to/172.17.0.3:55780: java.nio.channels.ClosedChannelException</p><p>                     </p><p>                     解决方法一：</p><p>                            在yarn-site.xml中配置</p><p>                                   &lt;property&gt;</p><p>                                   &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</p><p>                                   &lt;value&gt;false&lt;/value&gt;</p><p>                                   &lt;/property&gt;</p><p>                                   </p><p>                                   &lt;property&gt;</p><p>                                   &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</p><p>                                   &lt;value&gt;false&lt;/value&gt;</p><p>                                   &lt;/property&gt;</p><p>                            运行出现错误</p><p>                                   ERRORSparkContext: Error initializing SparkContext.</p><p>                                   org.apache.spark.SparkException:Yarn application has already ended! It might have been killed or unable tolaunch application master.</p><p>                                   ERRORYarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(0,0,Map())to AM was unsuccessful</p><p>                                   java.io.IOException:Failed to send RPC 8340030922793312011 to /172.17.0.4:52390:java.nio.channels.ClosedChannelException</p><p>                                   ERRORTransportClient: Failed to send RPC 8340030922793312011 to /172.17.0.4:52390:java.nio.channels.ClosedChannelException</p><p>                                   java.nio.channels.ClosedChannelException</p><p>                            仔细查了查，spark-env.sh中masterip配置错了</p><p>                            </p><p>                            将配置文件复制到slave</p><p>                                   root@master:~/pyworkspace#scp /usr/local/spark/conf/spark-defaults.conf slave01:/usr/local/spark/conf/</p><p>                                   spark-defaults.conf                                                            100% 1429     1.4KB/s   00:00   </p><p>                                   root@master:~/pyworkspace#scp /usr/local/spark/conf/spark-defaults.conf slave02:/usr/local/spark/conf/</p><p>                                   spark-defaults.conf                                                            100% 1429     1.4KB/s   00:00   </p><p>                                   root@master:~/pyworkspace#</p><p>                            进入spark日志配置文件，修改配置</p><p>                                   log4j.rootCategory=DEBUG,console</p><p>                            重启hadoop与spark集群</p><p>                     </p><p>                     解决方法二：</p><p>                            配置yarn-site.xml,配置队列权限</p><p>                                   &lt;property&gt;</p><p>                                   &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;  </p><p>                                   &lt;value&gt;default&lt;/value&gt;</p><p>                                   &lt;/property&gt;  </p><p>                                   &lt;property&gt;</p><p>                                   &lt;name&gt;yarn.scheduler.capacity.root.capacity&lt;/name&gt; </p><p>                                   &lt;value&gt;100&lt;/value&gt;</p><p>                                   &lt;/property&gt;</p><p>                                   &lt;property&gt;</p><p>                                   &lt;name&gt;yarn.scheduler.capacity.root.acl_submit_applications&lt;/name&gt;  </p><p>                                   &lt;value&gt;root&lt;/value&gt;</p><p>                                   &lt;/property&gt;</p><p>                                   &lt;property&gt;</p><p>                                   &lt;name&gt;yarn.scheduler.capacity.root.acl_administer_queue&lt;/name&gt;  </p><p>                                   &lt;value&gt;root&lt;/value&gt;</p><p>                                   &lt;/property&gt;</p><p>                            运行spark-submit，上面的问题没有了，又出现这个问题</p><p>                                   ERRORSparkContext: Error initializing SparkContext.</p><p>                             org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException):Cannot create directory/user/root/.sparkStaging/application_1524905120483_0001. Name node is in safemode.</p><p>                                   Thereported blocks 0 needs additional 101 blocks to reach the threshold 0.9990 oftotal blocks 101.</p><p>                                   Thenumber of live datanodes 0 has reached the minimum number 0. Safe mode will beturned off automatically once the thresholds have been reached.</p><p>                                   </p><p>                                   错误显示namenode处于安全模式下，好像在这种模式不能操作文件什么的，然后退出安全模式</p><p>                                          root@master:~/pyworkspace#hadoop dfsadmin -safemode leave</p><p>                                          DEPRECATED:Use of this script to execute hdfs command is deprecated.</p><p>                                          Insteaduse the hdfs command for it.</p><p>                                          </p><p>                                          Safemode is OFF</p><p>                                   重新运行spark-submit。又出现</p><p>                                   WARNDFSClient: DataStreamer Exception</p><p>                                   org.apache.hadoop.ipc.RemoteException(java.io.IOException):File /user/root/.sparkStaging/application_1524905120483_0002/pyspark.zip couldonly be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and nonode(s) are excluded in this operation.</p><p>                                   </p><p>                                   查看slave，jps命令显示什么都没有，看来slave没有启动服务</p><p>                                          关闭集群的时候显示没有datanode可关闭</p><p>                                                 root@master:~/pyworkspace#. /usr/local/hadoop-2.7.5/sbin/stop-all.sh </p><p>                                                 Thisscript is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh</p><p>                                                 Stoppingnamenodes on [master]</p><p>                                                 master:stopping namenode</p><p>                                                 slave02:no datanode to stop</p><p>                                                 slave01:no datanode to stop</p><p>                                                 Stoppingsecondary namenodes [0.0.0.0]</p><p>                                                 0.0.0.0:stopping secondarynamenode</p><p>                                                 stoppingyarn daemons</p><p>                                                 stoppingresourcemanager</p><p>                                                 slave02:no nodemanager to stop</p><p>                                                 slave01:no nodemanager to stop</p><p>                                                 noproxyserver to stop</p><p>                                   修改好后出现新错误</p><p>                                          WARNYarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed:</p><p>                                          ERRORmaster.Master: RECEIVED SIGNAL TERM</p><p>                            </p><p>                                   </p><p>              </p><p>       方法二：不可用</p><p>              修改yarn-site.xml文件</p><p>              添加</p><p>                     &lt;property&gt;</p><p>                     &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</p><p>                     &lt;value&gt;false&lt;/value&gt;</p><p>                     &lt;/property&gt;</p><p>                     </p><p>                     &lt;property&gt;</p><p>                     &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</p><p>                     &lt;value&gt;false&lt;/value&gt;</p><p>                     &lt;/property&gt;</p><p>              没有用</p><p>       </p><p>              如果不使用--master yarn</p><p>                     spark-submit  --driver-class-path/usr/local/spark/jars/*:/usr/local/spark/jars/flume/* flumetest.py --confspark.yarn.jars="hdfs://master:9000/usr/local/spark/jars/* "</p><p>              可以出现正确显示，但是不能运行</p><p>       </p><p>              </p><p>测试</p><p>       在spark-streaming的输出中显示使用telnet输入的信息</p><p>              在spark应用程序中添加了一句</p><p>                     stream.map(lambdacn : "Recieve " + str(cn)).pprint()</p><p>              在终端的显示是</p><p>                     Recieve({}, 'gggggg\r')</p><p>定期清理spark中已停止的应用文件</p><p>       在spark-env.sh添加</p><p>              exportSPARK_WORKER_OPTS="</p><p>              -Dspark.worker.cleanup.enabled=true</p><p>              -Dspark.worker.cleanup.interval=1800</p><p>              -Dspark.worker.cleanup.appDataTtl=604800"</p><p>              </p><p>       </p><p>linux查看端口占用</p><p>       netstat-tunlp</p><br>            </div>
                </div>