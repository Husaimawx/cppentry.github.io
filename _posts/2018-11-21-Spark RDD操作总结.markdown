---
layout:     post
title:      Spark RDD操作总结
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p><strong>前言</strong> <br>
Spark的核心抽象是RDD，Spark程序中RDD对象无处不在，因此在基于Spark进行编程开发之前，需要对RDD的特征和基本操作有所了解，以便能顺利进行Spark程序的编程开发。</p>

<ol>
<li><p><strong>Spark程序依赖的运行环境</strong></p>

<p>1）已安装好Spark集群环境（单机版或集群版均可，正式环境都是集群版） <br>
2）已安装好Hadoop集群环境以及相关组件（如hive）</p></li>
<li><p><strong>RDD创建方式</strong> <br>
sc =sparkContext()  #sc代表Spark的上下文环境 <br>
hc = HiveContext(sc)  #hc代表Spark中的Hive对象 <br>
1）读取外部数据源创建sc =sparkContext() <br>
 （1）sc.textFile(filePath)  #filePath为文件路径 <br>
 （2）hc.sql(sqltext)  #hive查询的结果数据，返回类型为dataFrame    （dataFrame为RDD的封装类型，在Spark SQL中将进行详细介绍）。 <br>
2）自定义数据对象 <br>
 sc.parallelize(A,size)  变量A为列表或元组类型，size为RDD分区值,  <br>
3）RDD之间的相互转换</p></li>
<li><p><strong>RDD类型划分</strong> <br>
（Spark中“算子”表示RDD的操作） <br>
1）Transformation RDD（转换类RDD）  <br>
（1）映射型算子（map）#变换操作，func表示具体的操作过程 <br>
    例如：map(func),flatmap(func),filter(func) <br>
（2）集合型算子（union）#合并操作 <br>
    例如：join(RDD),union(RDD)等 <br>
（3）聚合型算子（group）#统计分类操作 <br>
    例如：groupByKey(), reduceByKey(func) <br>
2）Action RDD （执行类RDD） <br>
（1）输出型算子 <br>
    例如：saveAsTextFile(path) <br>
（2）统计型算子 <br>
     例如：count() <br>
（3）显示型算子 <br>
     例如：taken(n),collect(),first(),reduce(func)</p></li>
<li><p><strong>RDD算子分类</strong> <br>
1）value 类transformation 如：map() <br>
2）key-value 类transformation  如：groupByKey() <br>
3）action算子</p></li>
</ol>

<p>因此，在Spark编程过程中需基于实际需求明确每个阶段要选择什么类型的算子，然后结合算子的输入输出数据类型将各种算子组合在一起，最终指定结果的输出方式即可。</p>

<p><strong>编程实例</strong> <br>
Spark目前提供了四种编程语言（Scala、Java、Python和R）的API文档，我们可以选择自己擅长的语言进行Spark程序的编程开发（个人看法：Spark虽然提供了Java的API接口，但Java不支持函数式编程范式，使得Java编写的Spark程序代码十分冗长，可读性差）。 本文实例采用Python语言，具体如下所示：</p>

<p>–功能说明：读取外部数据源，进行分类统计汇总操作，并将结果写入到指定路径的文件中 <br>
from pyspark import SparkContext,SparkConf <br>
from pyspark.sql import HiveContext <br>
conf = SparkConf() <br>
sc = SparkContext(conf=conf) <br>
hc = HiveContext(sc) <br>
path =”/home/spark/router” <br>
logtext =”select dnum, mac from router  where date=’2017-09-28’ ” <br>
sqlRDD = hc.sql(logtext) <br>
mapRDD = sqlRDD.map(lambda x: (x[0],x[1].split(‘:’)[0])) <br>
groupRDD = mapRDD.group ByKey() <br>
–结果输出 <br>
groupRDD.saveAsTextFile(path)</p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>