---
layout:     post
title:      Hadoop伪分布式环境搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
Hadoop伪分布式环境搭建 <br>
简单记录hadoop伪分布式环境搭建的步骤<br>
软件环境配置<br>
VM：VMware-workstation REDHAT linux, ip 192.168.2.6<br>
Hadoop：hadoop-0.20.2<br><br>
1, 创建一个hadoop用户<br>
groupadd hadoop<br>
useradd hadoop -g hadoop<br>
passwd hadoop<br>
以后的步骤都以创建好的hadoop用户进行操作<br>
2, setup ssh<br>
基于空口令创建一个新的SSH密钥，以启动无密码登录。<br>
ssh-keygen -t rsa<br>
会在~/.ssh目录下生成一个私钥文件和一个公钥文件<br>
将公钥文件的内容添加到authorized_keys，这样ssh的时候就不需要密码了。<br>
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>
需要chmod 700 .ssh<br>
chmod 600 authorized_keys <br>
ssh localhost 或者 ssh ip 不需要密码则ok<br><br>
3,配置hadoop<br>
tar -xzvf hadoop-0.20.2.tar.gz<br>
修改配置，在目录/home/hadoop/hadoop-0.20.2/conf<br>
修改hadoop-env.sh<br>
export JAVA_HOME=/usr/local/jdk1.6.0_33<br><p>修改core-site.xml 如下</p>
<p></p>
<pre><code class="language-html">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;fs.default.name&lt;/name&gt;
		&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
	&lt;/property&gt;

	&lt;property&gt;
		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
		&lt;value&gt;/home/hadoop/tmp&lt;/value&gt;
		&lt;description&gt;A base for other temporary directories.&lt;/description&gt;
	&lt;/property&gt;
&lt;/configuration&gt;</code></pre>修改mapred-site.xml:
<p></p>
<p></p>
<pre><code class="language-html">&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;localhost:9001&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;</code></pre>修改hdfs-site.xml:<br><pre><code class="language-html">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.name.dir&lt;/name&gt;
		&lt;value&gt;/home/hadoop/dfs/name&lt;/value&gt;
		&lt;description&gt;Determines where on the local filesystem the DFS name node should store &lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.data.dir&lt;/name&gt;
		&lt;value&gt;/home/hadoop/dfs/data&lt;/value&gt;
		&lt;description&gt;Determin. If this is a comma-delimited &lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;1&lt;/value&gt;
		&lt;description&gt;Default block replicied when the file is created. The default &lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;false&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;</code></pre>4，运行<br>
cd /home/hadoop/hadoop-0.20.2/bin<br>
格式化HDFS：<br>
./hadoop namenode -format<br>
启动hadoop守护进程：<br>
./start-all.sh<br>
通过浏览器查看hadoop运行状态：<br>
NameNode - http://localhost:50070/<br>
JobTracker - http://localhost:50030/<br><br>
5，测试<br>
在/home/hadoop/hadoop-0.20.2 目录中进行如下的操作<br><br>
复制本地文件到HDFS的input目录：<br>
[hadoop@srv6 hadoop-0.20.2]$ bin/hadoop fs -put conf input<br><br>
运行hadoop提供的例子：<br>
[hadoop@srv6 hadoop-0.20.2]$ bin/hadoop jar hadoop-0.20.2-examples.jar grep input output 'dfs[a-z.]+'<br><br>
查看DFS文件：<br>
[hadoop@srv6 hadoop-0.20.2]$ bin/hadoop fs -ls output<br><br>
复制DFS文件到本地，并在本地查看：<br>
[hadoop@srv6 hadoop-0.20.2]$ bin/hadoop fs -get output output<br>
[hadoop@srv6 hadoop-0.20.2]$ cat output/*<br><br>
或者直接查看DFS文件:<br>
[hadoop@srv6 hadoop-0.20.2]$ bin/hadoop fs -cat output/*<br><br>
关闭hadoop守护进程：<br>
[hadoop@srv6 hadoop-0.20.2]$ bin/stop-all.sh<br><br>
在windows下的eclipse利用插件连接linux的hdfs的时候遇到<br>
java.net.ConnectException: Call to /192.168.2.6:9001 failed on connection exception:<br>
原因是linux的hadoop配置文件中写的是localhost，改为ip 192.168.2.6,连接成功。<br><p></p>
            </div>
                </div>