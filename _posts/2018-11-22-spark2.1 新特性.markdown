---
layout:     post
title:      spark2.1 新特性
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。博客地址:http://www.fanlegefan.com/					https://blog.csdn.net/woloqun/article/details/80635324				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-light">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p>Apache Spark 2.0是基于spark branch-2.x 开发的，相比于branch-1.0，它在功能和性能等方面均有巨大改进。在性能方面，Spark 2.x 有2~10倍的提升；在功能方面，Spark SQL中的Dataset变得成熟，Spark 2.x通过Dataset重构了Spark Streaming和MLlib的API，进而使得这两个系统在易用性和性能方面有重大提升，在不久的将来，Dataframe/Dataset API（high-level API）将取代RDD API（low-level API），成为主流的Spark编程接口。</p>

<p>Apache Spark 2.x在性能和功能方面的改进主要包括：</p>



<h3 id="1-性能方面">1. 性能方面</h3>

<p>相比于Spark 1.0，Spark 2.0在引擎性能方面有重大优化，其优化主要体现在Spark Core和Spark SQL两个系统上，其优化主要得益于Tungsten计划（“钨丝计划”），其主要动机是优化Spark内存和CPU的使用，使其能够逼近物理机器的性能极限。</p>

<p>利用“整阶段代码生成”（“whole stage code generation”），使得SQL和DataFrame中算子性能优化2-10倍</p>

<p>通过“向量化计算”提升Parquet格式文件的扫描吞吐率</p>

<p>提升ORC格式文件的读写性能</p>

<p>提升Catalyst查询优化器性能</p>



<h3 id="2-功能方面">2. 功能方面</h3>



<h4 id="1spark-coresqltungsten-phase-2优化cpu与memory方面">（1）Spark Core/SQL:Tungsten Phase 2，优化CPU与Memory方面</h4>

<p>“钨丝计划”完成第二阶段任务，在内存和CPU使用方面进一步优化Spark引擎性能，重构了大量数据结构和算法的实现，使得Dataframe/Dataset性能得到显著提升，这使得Dataframe/Dataset有能力成为其他几个系统（比如Spark Streaming和MLlib）的基础API。</p>

<p>注：“钨丝计划”包括三个方面的优化：</p>

<p>Memory Management and Binary Processing： Java GC严重，且java对象内存开销大，可采用类似C语言机制，直接操纵binary data（sun.misc.Unsafe）</p>

<p>Cache-aware Computation：合理使用CPU的L1/L2/L3 cache，设计对cache友好的算法</p>

<p>Code Generation：可去除条件检查，减少虚函数调度等</p>



<h4 id="2spark-sql-统一dataframe与dataset-api">（2）Spark SQL: 统一DataFrame与Dataset API</h4>

<p>众所周知，在Spark 1.x中，DataFrame API存在很多问题，包括不是类型安全的(not type-safe)，缺乏函数式编程能力（not object-oriented）等，为了克服这些问题，社区引入了Dataset，相比于DataFrame，它具有以下几个特点：类型安全，面向对象编程方式；支持非结构化数据（json）；java与scala统一接口和性能极好的序列化框架等，她将成为Spark未来主流的编程接口（RDD API是low-level API，而Dataset则是high-level API）。</p>



<h4 id="3spark-sql支持sql-2003">（3）Spark SQL：支持SQL 2003</h4>

<p>Spark SQL在通用性方面有重大突破，它跑通了所有（99个）TPC-DS查询 ，并有以下几个改进：</p>

<p>解析器可同时支持ANSI-SQL 和Hive QL</p>

<p>实现了DDL</p>

<p>支持大部分子查询</p>

<p>支持View</p>



<h4 id="4spark-streaming引入structured-streaming">（4）Spark Streaming：引入Structured Streaming</h4>

<p>Spark Streaming基于Spark SQL（DataFrame / Dataset ）构建了high-level API，使得Spark Streaming充分受益Spark SQL的易用性和性能提升。Spark Streaming重构的API主要是面向结构化数据的，被称为“Structured Streaming”，其主要特性包括：</p>

<p>DataFrame / Dataset API的支持</p>

<p>提供了Event time, windowing, sessions, sources &amp; sink等API</p>

<p>连接流式数据与静态数据集</p>

<p>交互式查询结果：通过JDBC server将RDD结果暴露出去，以便于交互式查询</p>



<h4 id="5spark-mllib-mllib-20诞生">（5）Spark MLlib: MLlib 2.0诞生</h4>

<p>Spark MLlib朝着2.0进化，主要体现在机器学习模型的多样化，持久化和定制化上，具体包括：</p>

<p>广义线性模型的全面实现</p>

<p>Python &amp; R API的支持</p>

<p>增强模型持久化能力</p>

<p>Pipieline定制化</p>

<p>Apache Spark 2.0在功能和性能的重大改进，使得它在分布式计算领域进一步巩固了自己的地位，随着Spark应用越来越广泛，它将变成数据工程师的一项基本技能。</p>

<p>相关链接：</p>

<p><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315420&amp;version=12336857" rel="nofollow">https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315420&amp;version=12336857</a> <br>
<a href="http://spark.apache.org/downloads.html" rel="nofollow">http://spark.apache.org/downloads.html</a> <br>
<a href="http://www.chinahadoop.cn/course/697" rel="nofollow">http://www.chinahadoop.cn/course/697</a></p>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>