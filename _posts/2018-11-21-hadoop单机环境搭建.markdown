---
layout:     post
title:      hadoop单机环境搭建
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>环境：<br><strong>宿主机系统：</strong>windows7</p>
<p><strong>虚拟机：</strong>Oracle VMVirtualBox</p>
<p><strong>Linux:</strong> ubuntukylin-14.04.1-amd64.iso</p>
<p><strong>jdk</strong>:1.7.0_101</p>
<p><strong>hadoop:</strong>2.7.2</p>
<p align="left"><br>
虚拟主机1台<br>
网络模式：桥接</p>
<p align="left">hadoop位置<br>
/usr/local/hadoop</p>
<p align="left"> </p>
<p align="left">hadoop下载地址：</p>
<p align="left">http://apache.fayea.com/hadoop/common/stable/</p>
<p align="left"> </p>
<p align="left"><strong><span style="color:#4C33E5;">Step1</span><span style="color:#4C33E5;">：安装JDK</span></strong><br>
安装过程可参考：http://blog.csdn.net/lanonola/article/details/51479127</p>
<p align="left"><strong><span style="color:#4C33E5;">Step2</span><span style="color:#4C33E5;">：安装SSH</span></strong></p>
<p align="left">安装过程可参考：http://blog.csdn.net/lanonola/article/details/51384914<br><strong><span style="color:#4C33E5;">Step3</span><span style="color:#4C33E5;">：配置SSH</span>免密码登录</strong></p>
<p>1.输入命令：ssh-keygen  -t  dsa  -P  ''";</p>
<p>执行完该指令后，在/root/.ssh目录下会出现两个文件：id_dsa和id_dsa.pub文件；</p>
<p>2.输入命令： cat ./id_dsa.pub &gt;&gt; authorized_keys;</p>
<p>3.输入命令：sshlocalhost</p>
<p>查看是否可以无密码登录，</p>
<p>出现错误：The authenticity of host 'localhost (127.0.0.1)' can't beestablished.</p>
<p>4.更改权限</p>
<p>输入命令：</p>
<p>chmod700 /root/.ssh<br>
chmod 644 /root/.ssh/authorized_keys </p>
<p>执行完上述命令之后，/root/.ssh文件中多了一个文件known_hosts,</p>
<p>再次运行ssh localhost,可以无密码登录；</p>
<p align="left"><strong><span style="color:#4C33E5;">Step4</span><span style="color:#4C33E5;">：源码安装hadoop</span></strong>    </p>
<p align="left">打开网址：http://apache.fayea.com/hadoop/common/stable/</p>
<p align="left">下载：hadoop-2.7.2.tar.gz</p>
<p align="left">1.将文件解压:</p>
<p align="left">tar zxvf hadoop-2.7.2.tar.gz  /usr/local</p>
<p align="left">2.进入hadoop的目录  cd  /usr/local/hadoop</p>
<p align="left">主要目录结构为：</p>
<p align="left"><strong>bin</strong>：Hadoop最基本的管理脚本和使用脚本所在目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop。</p>
<p align="left"><strong>etc</strong>：Hadoop配置文件所在的目录，包括core-site.xml、hdfs-site.xml、mapred-site.xml等从Hadoop 1.0继承而来的配置文件和yarn-site.xml等Hadoop 2.0新增的配置文件。</p>
<p align="left"><strong>include</strong>：对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</p>
<p align="left"><strong>lib</strong>：该目录包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用。</p>
<p align="left"><strong>libexec</strong>：各个服务对应的shell配置文件所在目录，可用于配置日志输出目录、启动参数（比如JVM参数）等基本信息。</p>
<p align="left"><strong>sbin</strong>：Hadoop管理脚本所在目录，主要包含HDFS和YARN中各类服务的启动/关闭脚本。</p>
<p align="left"><strong>share</strong>：Hadoop各个模块编译后的jar包所在目录</p>
<p align="left">3.修改配置文件</p>
<p align="left">1）/usr/local/hadoop/etc/hadoop/下</p>
<p align="left">NO1:hadoop-env.sh修改如下配置</p>
<p align="left">export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</p>
<p align="left">NO2:slaves</p>
<p align="left">默认为localhost，可修改为其他名称，这里修改为YARN001</p>
<p align="left">NO3：mapred-site.xml  在&lt;configuration&gt;&lt;/ configuration &gt;之间添加</p>
<p align="left">&lt;property&gt;</p>
<p align="left">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</p>
<p align="left">&lt;value&gt;yarn&lt;/value&gt;</p>
<p align="left">&lt;/property&gt;</p>
<p align="left">NO4:core-site.xml 在&lt;configuration&gt;&lt;/configuration &gt;之间添加</p>
<p align="left">&lt;property&gt;</p>
<p align="left">&lt;name&gt;fs.default.name&lt;/name&gt;</p>
<p align="left">&lt;value&gt;hdfs://YARN001:8020&lt;/value&gt;</p>
<p align="left">&lt;/property&gt;</p>
<p align="left">NO5:yarn-site.xml 在&lt;configuration&gt;&lt;/configuration &gt;之间添加</p>
<p align="left">&lt;property&gt;</p>
<p align="left">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</p>
<p align="left">&lt;value&gt;mapreduce_shuffle&lt;/ value&gt;</p>
<p align="left">&lt;/property&gt;</p>
<p align="left">NO6:core-site.xml</p>
<p align="left">&lt;property:</p>
<p align="left">&lt;name&gt;dfs.replication&lt;/name&gt;</p>
<p align="left">&lt;value&gt;1&lt;/value&gt;</p>
<p align="left">&lt;/property&gt;</p>
<p align="left"><strong><span style="color:#4C33E5;">Step5</span><span style="color:#4C33E5;">：启动服务</span></strong></p>
<p align="left">1.格式化HDFS</p>
<p align="left">命令：cd  /usr/local/hadoop/bin</p>
<p align="left">命令：./hadoop  namenode  -format</p>
<p align="left">2.启动HDFS</p>
<p align="left">命令：cd    /usr/local/hadoop/sbin</p>
<p align="left">命令：./start-dfs.sh</p>
<p align="left">3.启动YARN</p>
<p align="left">命令：cd  /usr/local/hadoop/sbin  <br></p>
<p align="left">命令：./start-yarn.sh</p>
<p> </p>
<p></p>
<p align="left"><strong><span style="color:#4C33E5;">Step6</span><span style="color:#4C33E5;">：验证部署是否成功</span></strong></p>
<p align="left"><strong><span style="color:#4C33E5;"></span></strong></p>
http://yarn001:8088/cluster
<p>http://yarn001:50070</p>
<p><br></p>
<p align="left"><strong><span style="color:#4C33E5;"><br></span></strong></p>
<br>            </div>
                </div>