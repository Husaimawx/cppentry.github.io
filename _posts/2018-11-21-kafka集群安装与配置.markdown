---
layout:     post
title:      kafka集群安装与配置
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/bluejoe2000/article/details/50885179				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <strong>一、集群安装</strong><p>1. Kafka下载：</p><p>可以从kafka官方网站（http://kafka.apache.org）上找到下载地址，再wget<br>wget http://mirrors.cnnic.cn/apache/kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz</p><p>解压该文件：</p><p> tar zxvf kafka_2.10-0.8.2.2.tgz</p><p>注意kafka依赖于zookeeper和scala，以上tgz文件名中的2.10即为scala的版本号</p><p>zk和scala的安装在此不再赘述</p><p><span style="line-height:1.5;">2. 启动zk集群</span></p><p>假设zk集群分别为bluejoe1:2181,bluejoe2:2181,bluejoe3:2181</p><p><span style="line-height:1.5;">3. 配置config/server.properties</span></p><p><span style="line-height:1.5;">server.properties各项参数的含义如下：</span></p><ul><li>broker.id代理节点的id，集群中唯一id</li><li>log.dirs设置到硬盘路径下</li><li>num.network.threads</li><li>num.partitions 默认分区数</li><li>num.io.threads 建议值为机器的核数；</li><li>zookeeper.connect 设置为zookeeper Servers 列表，各节点以逗号分开；</li></ul><table style="border:1px solid #000000;background-color:#fcf1d9;border-color:#c91e02;border-width:2px;" border="2"><tbody><tr><td><div class="line number1 index0 alt2"><code class="java plain">server.properties中所有配置参数说明(解释)如下列表：</code></div><div class="line number2 index1 alt1"><code class="java plain">参数</code></div><div class="line number3 index2 alt2"><code class="java plain">说明(解释)</code></div><div class="line number4 index3 alt1"><code class="java plain">broker.id =</code><code class="java value">0</code></div><div class="line number5 index4 alt2"><code class="java plain">每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况</code></div><div class="line number6 index5 alt1"><code class="java plain">log.dirs=/data/kafka-logs</code></div><div class="line number7 index6 alt2"><code class="java plain">kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-</code><code class="java value">1</code><code class="java plain">，/data/kafka-logs-</code><code class="java value">2</code></div><div class="line number8 index7 alt1"><code class="java plain">port =</code><code class="java value">9092</code></div><div class="line number9 index8 alt2"><code class="java plain">broker server服务端口</code></div><div class="line number10 index9 alt1"><code class="java plain">message.max.bytes =</code><code class="java value">6525000</code></div><div class="line number11 index10 alt2"><code class="java plain">表示消息体的最大大小，单位是字节</code></div><div class="line number12 index11 alt1"><code class="java plain">num.network.threads =</code><code class="java value">4</code></div><div class="line number13 index12 alt2"><code class="java plain">broker处理消息的最大线程数，一般情况下不需要去修改</code></div><div class="line number14 index13 alt1"><code class="java plain">num.io.threads =</code><code class="java value">8</code></div><div class="line number15 index14 alt2"><code class="java plain">broker处理磁盘IO的线程数，数值应该大于你的硬盘数</code></div><div class="line number16 index15 alt1"><code class="java plain">background.threads =</code><code class="java value">4</code></div><div class="line number17 index16 alt2"><code class="java plain">一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改</code></div><div class="line number18 index17 alt1"><code class="java plain">queued.max.requests =</code><code class="java value">500</code></div><div class="line number19 index18 alt2"><code class="java plain">等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。</code></div><div class="line number20 index19 alt1"><code class="java plain">host.name</code></div><div class="line number21 index20 alt2"><code class="java plain">broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置</code></div><div class="line number22 index21 alt1"><code class="java plain">socket.send.buffer.bytes=</code><code class="java value">100</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number23 index22 alt2"><code class="java plain">socket的发送缓冲区，socket的调优参数SO_SNDBUFF</code></div><div class="line number24 index23 alt1"><code class="java plain">socket.receive.buffer.bytes =</code><code class="java value">100</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number25 index24 alt2"><code class="java plain">socket的接受缓冲区，socket的调优参数SO_RCVBUFF</code></div><div class="line number26 index25 alt1"><code class="java plain">socket.request.max.bytes =</code><code class="java value">100</code><code class="java plain">*</code><code class="java value">1024</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number27 index26 alt2"><code class="java plain">socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖</code></div><div class="line number28 index27 alt1"><code class="java plain">log.segment.bytes =</code><code class="java value">1024</code><code class="java plain">*</code><code class="java value">1024</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number29 index28 alt2"><code class="java plain">topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖</code></div><div class="line number30 index29 alt1"><code class="java plain">log.roll.hours =</code><code class="java value">24</code><code class="java plain">*</code><code class="java value">7</code></div><div class="line number31 index30 alt2"><code class="java plain">这个参数会在日志segment没有达到log.segment.bytes设置的大小，也会强制新建一个segment会被 topic创建时的指定参数覆盖</code></div><div class="line number32 index31 alt1"><code class="java plain">log.cleanup.policy = delete</code></div><div class="line number33 index32 alt2"><code class="java plain">日志清理策略选择有：delete和compact主要针对过期数据的处理，或是日志文件达到限制的额度，会被 topic创建时的指定参数覆盖</code></div><div class="line number34 index33 alt1"><code class="java plain">log.retention.minutes=3days</code></div><div class="line number35 index34 alt2"><code class="java plain">数据存储的最大时间超过这个时间会根据log.cleanup.policy设置的策略处理数据，也就是消费端能够多久去消费数据</code></div><div class="line number36 index35 alt1"><code class="java plain">log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖</code></div><div class="line number37 index36 alt2"><code class="java plain">log.retention.bytes=-</code><code class="java value">1</code></div><div class="line number38 index37 alt1"><code class="java plain">topic每个分区的最大文件大小，一个topic的大小限制 = 分区数*log.retention.bytes。-</code><code class="java value">1</code><code class="java plain">没有大小限log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖</code></div><div class="line number39 index38 alt2"><code class="java plain">log.retention.check.interval.ms=5minutes</code></div><div class="line number40 index39 alt1"><code class="java plain">文件大小检查的周期时间，是否处罚 log.cleanup.policy中设置的策略</code></div><div class="line number41 index40 alt2"><code class="java plain">log.cleaner.enable=</code><code class="java keyword">false</code></div><div class="line number42 index41 alt1"><code class="java plain">是否开启日志压缩</code></div><div class="line number43 index42 alt2"><code class="java plain">log.cleaner.threads = </code><code class="java value">2</code></div><div class="line number44 index43 alt1"><code class="java plain">日志压缩运行的线程数</code></div><div class="line number45 index44 alt2"><code class="java plain">log.cleaner.io.max.bytes.per.second=None</code></div><div class="line number46 index45 alt1"><code class="java plain">日志压缩时候处理的最大大小</code></div><div class="line number47 index46 alt2"><code class="java plain">log.cleaner.dedupe.buffer.size=</code><code class="java value">500</code><code class="java plain">*</code><code class="java value">1024</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number48 index47 alt1"><code class="java plain">日志压缩去重时候的缓存空间，在空间允许的情况下，越大越好</code></div><div class="line number49 index48 alt2"><code class="java plain">log.cleaner.io.buffer.size=</code><code class="java value">512</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number50 index49 alt1"><code class="java plain">日志清理时候用到的IO块大小一般不需要修改</code></div><div class="line number51 index50 alt2"><code class="java plain">log.cleaner.io.buffer.load.factor =</code><code class="java value">0.9</code></div><div class="line number52 index51 alt1"><code class="java plain">日志清理中hash表的扩大因子一般不需要修改</code></div><div class="line number53 index52 alt2"><code class="java plain">log.cleaner.backoff.ms =</code><code class="java value">15000</code></div><div class="line number54 index53 alt1"><code class="java plain">检查是否处罚日志清理的间隔</code></div><div class="line number55 index54 alt2"><code class="java plain">log.cleaner.min.cleanable.ratio=</code><code class="java value">0.5</code></div><div class="line number56 index55 alt1"><code class="java plain">日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被topic创建时的指定参数覆盖</code></div><div class="line number57 index56 alt2"><code class="java plain">log.cleaner.delete.retention.ms =1day</code></div><div class="line number58 index57 alt1"><code class="java plain">对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被topic创建时的指定参数覆盖</code></div><div class="line number59 index58 alt2"><code class="java plain">log.index.size.max.bytes =</code><code class="java value">10</code><code class="java plain">*</code><code class="java value">1024</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number60 index59 alt1"><code class="java plain">对于segment日志的索引文件大小限制，会被topic创建时的指定参数覆盖</code></div><div class="line number61 index60 alt2"><code class="java plain">log.index.interval.bytes =</code><code class="java value">4096</code></div><div class="line number62 index61 alt1"><code class="java plain">当执行一个fetch操作后，需要一定的空间来扫描最近的offset大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数</code></div><div class="line number63 index62 alt2"><code class="java plain">log.flush.interval.messages=None</code></div><div class="line number64 index63 alt1"><code class="java plain">log文件”sync”到磁盘之前累积的消息条数,因为磁盘IO操作是一个慢操作,但又是一个”数据可靠性</code><code class="java string">"的必要手段,所以此参数的设置,需要在"</code><code class="java plain">数据可靠性</code><code class="java string">"与"</code><code class="java plain">性能</code><code class="java string">"之间做必要的权衡.如果此值过大,将会导致每次"</code><code class="java plain">fsync</code><code class="java string">"的时间较长(IO阻塞),如果此值过小,将会导致"</code><code class="java plain">fsync"的次数较多,这也意味着整体的client请求有一定的延迟.物理server故障,将会导致没有fsync的消息丢失.</code></div><div class="line number65 index64 alt2"><code class="java plain">log.flush.scheduler.interval.ms =</code><code class="java value">3000</code></div><div class="line number66 index65 alt1"><code class="java plain">检查是否需要固化到硬盘的时间间隔</code></div><div class="line number67 index66 alt2"><code class="java plain">log.flush.interval.ms = None</code></div><div class="line number68 index67 alt1"><code class="java plain">仅仅通过interval来控制消息的磁盘写入时机,是不足的.此参数用于控制</code><code class="java string">"fsync"</code><code class="java plain">的时间间隔,如果消息量始终没有达到阀值,但是离上一次磁盘同步的时间间隔达到阀值,也将触发.</code></div><div class="line number69 index68 alt2"><code class="java plain">log.delete.delay.ms =</code><code class="java value">60000</code></div><div class="line number70 index69 alt1"><code class="java plain">文件在索引中清除后保留的时间一般不需要去修改</code></div><div class="line number71 index70 alt2"><code class="java plain">log.flush.offset.checkpoint.interval.ms =</code><code class="java value">60000</code></div><div class="line number72 index71 alt1"><code class="java plain">控制上次固化硬盘的时间点，以便于数据恢复一般不需要去修改</code></div><div class="line number73 index72 alt2"><code class="java plain">auto.create.topics.enable =</code><code class="java keyword">true</code></div><div class="line number74 index73 alt1"><code class="java plain">是否允许自动创建topic，若是</code><code class="java keyword">false</code><code class="java plain">，就需要通过命令创建topic</code></div><div class="line number75 index74 alt2"><code class="java keyword">default</code><code class="java plain">.replication.factor =</code><code class="java value">1</code></div><div class="line number76 index75 alt1"><code class="java plain">是否允许自动创建topic，若是</code><code class="java keyword">false</code><code class="java plain">，就需要通过命令创建topic</code></div><div class="line number77 index76 alt2"><code class="java plain">num.partitions =</code><code class="java value">1</code></div><div class="line number78 index77 alt1"><code class="java plain">每个topic的分区个数，若是在topic创建时候没有指定的话会被topic创建时的指定参数覆盖</code></div><div class="line number79 index78 alt2"> </div><div class="line number80 index79 alt1"> </div><div class="line number81 index80 alt2"><code class="java plain">以下是kafka中Leader,replicas配置参数</code></div><div class="line number82 index81 alt1"> </div><div class="line number83 index82 alt2"><code class="java plain">controller.socket.timeout.ms =</code><code class="java value">30000</code></div><div class="line number84 index83 alt1"><code class="java plain">partition leader与replicas之间通讯时,socket的超时时间</code></div><div class="line number85 index84 alt2"><code class="java plain">controller.message.queue.size=</code><code class="java value">10</code></div><div class="line number86 index85 alt1"><code class="java plain">partition leader与replicas数据同步时,消息的队列尺寸</code></div><div class="line number87 index86 alt2"><code class="java plain">replica.lag.time.max.ms =</code><code class="java value">10000</code></div><div class="line number88 index87 alt1"><code class="java plain">replicas响应partition leader的最长等待时间，若是超过这个时间，就将replicas列入ISR(in-sync replicas)，并认为它是死的，不会再加入管理中</code></div><div class="line number89 index88 alt2"><code class="java plain">replica.lag.max.messages =</code><code class="java value">4000</code></div><div class="line number90 index89 alt1"><code class="java plain">如果follower落后与leader太多,将会认为此follower[或者说partition relicas]已经失效</code></div><div class="line number91 index90 alt2"><code class="java plain">##通常,在follower与leader通讯时,因为网络延迟或者链接断开,总会导致replicas中消息同步滞后</code></div><div class="line number92 index91 alt1"><code class="java plain">##如果消息之后太多,leader将认为此follower网络延迟较大或者消息吞吐能力有限,将会把此replicas迁移</code></div><div class="line number93 index92 alt2"><code class="java plain">##到其他follower中.</code></div><div class="line number94 index93 alt1"><code class="java plain">##在broker数量较少,或者网络不足的环境中,建议提高此值.</code></div><div class="line number95 index94 alt2"><code class="java plain">replica.socket.timeout.ms=</code><code class="java value">30</code><code class="java plain">*</code><code class="java value">1000</code></div><div class="line number96 index95 alt1"><code class="java plain">follower与leader之间的socket超时时间</code></div><div class="line number97 index96 alt2"><code class="java plain">replica.socket.receive.buffer.bytes=</code><code class="java value">64</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number98 index97 alt1"><code class="java plain">leader复制时候的socket缓存大小</code></div><div class="line number99 index98 alt2"><code class="java plain">replica.fetch.max.bytes =</code><code class="java value">1024</code><code class="java plain">*</code><code class="java value">1024</code></div><div class="line number100 index99 alt1"><code class="java plain">replicas每次获取数据的最大大小</code></div><div class="line number101 index100 alt2"><code class="java plain">replica.fetch.wait.max.ms =</code><code class="java value">500</code></div><div class="line number102 index101 alt1"><code class="java plain">replicas同leader之间通信的最大等待时间，失败了会重试</code></div><div class="line number103 index102 alt2"><code class="java plain">replica.fetch.min.bytes =</code><code class="java value">1</code></div><div class="line number104 index103 alt1"><code class="java plain">fetch的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会阻塞,直到满足条件</code></div><div class="line number105 index104 alt2"><code class="java plain">num.replica.fetchers=</code><code class="java value">1</code></div><div class="line number106 index105 alt1"><code class="java plain">leader进行复制的线程数，增大这个数值会增加follower的IO</code></div><div class="line number107 index106 alt2"><code class="java plain">replica.high.watermark.checkpoint.interval.ms =</code><code class="java value">5000</code></div><div class="line number108 index107 alt1"><code class="java plain">每个replica检查是否将最高水位进行固化的频率</code></div><div class="line number109 index108 alt2"><code class="java plain">controlled.shutdown.enable =</code><code class="java keyword">false</code></div><div class="line number110 index109 alt1"><code class="java plain">是否允许控制器关闭broker ,若是设置为</code><code class="java keyword">true</code><code class="java plain">,会关闭所有在这个broker上的leader，并转移到其他broker</code></div><div class="line number111 index110 alt2"><code class="java plain">controlled.shutdown.max.retries =</code><code class="java value">3</code></div><div class="line number112 index111 alt1"><code class="java plain">控制器关闭的尝试次数</code></div><div class="line number113 index112 alt2"><code class="java plain">controlled.shutdown.retry.backoff.ms =</code><code class="java value">5000</code></div><div class="line number114 index113 alt1"><code class="java plain">每次关闭尝试的时间间隔</code></div><div class="line number115 index114 alt2"><code class="java plain">leader.imbalance.per.broker.percentage =</code><code class="java value">10</code></div><div class="line number116 index115 alt1"><code class="java plain">leader的不平衡比例，若是超过这个数值，会对分区进行重新的平衡</code></div><div class="line number117 index116 alt2"><code class="java plain">leader.imbalance.check.interval.seconds =</code><code class="java value">300</code></div><div class="line number118 index117 alt1"><code class="java plain">检查leader是否不平衡的时间间隔</code></div><div class="line number119 index118 alt2"><code class="java plain">offset.metadata.max.bytes</code></div><div class="line number120 index119 alt1"><code class="java plain">客户端保留offset信息的最大空间大小</code></div><div class="line number121 index120 alt2"><code class="java plain">kafka中zookeeper参数配置</code></div><div class="line number122 index121 alt1"> </div><div class="line number123 index122 alt2"><code class="java plain">zookeeper.connect = localhost:</code><code class="java value">2181</code></div><div class="line number124 index123 alt1"><code class="java plain">zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3</code></div><div class="line number125 index124 alt2"><code class="java plain">zookeeper.session.timeout.ms=</code><code class="java value">6000</code></div><div class="line number126 index125 alt1"><code class="java plain">ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大</code></div><div class="line number127 index126 alt2"><code class="java plain">zookeeper.connection.timeout.ms =</code><code class="java value">6000</code></div><div class="line number128 index127 alt1"><code class="java plain">ZooKeeper的连接超时时间</code></div><div class="line number129 index128 alt2"><code class="java plain">zookeeper.sync.time.ms =</code><code class="java value">2000</code></div><div class="line number130 index129 alt1"><code class="java plain">ZooKeeper集群中leader和follower之间的同步时间</code></div></td></tr></tbody></table><p> </p><p>这里主要修改下zookeeper.connect为zk集群的服务地址以及brokerid，如：</p><p>zookeeper.connect=bluejoe1:2181,bluejoe2:2181,bluejoe3:2181</p><p><code class="java plain" style="background-color:rgb(252,241,217);">broker.id=3</code><br></p><p>4. scp至其他节点，如：bluejoe2，bluejoe3</p><p>5.在kafka的部署目录下，在各个节点上通过如下命令来启动：<br>$ bin/kafka-server-start.sh config/server.properties</p><p>6. 此时可以启动zk的客户端，查看到broker的列表</p><p><img src="http://images2015.cnblogs.com/blog/453391/201601/453391-20160110113953012-1355413137.png" alt="" width="807" height="91"></p><p><strong>二、测试消息的收发</strong></p><p>创建Topic</p><p>bin/kafka-topics.sh --zookeeper bluejoe1:2181,bluejoe2:2181,bluejoe3:2181 --topic mytopic --replication-factor 1 --partitions 1 --create</p><p>查看topic列表</p><p>bin/kafka-topics.sh --zookeeper bluejoe1:2181,bluejoe2:2181,bluejoe3:2181 --list</p><p><span style="line-height:1.5;">此时也可以从zk中查看到topic列表</span></p><p><span style="line-height:1.5;"><img src="http://images2015.cnblogs.com/blog/453391/201601/453391-20160110115249340-27481914.png" alt="" width="574" height="94"></span></p><p><span style="line-height:1.5;">生产者测试</span></p><p><span style="line-height:1.5;">bin/kafka-console-producer.sh --broker-list bluejoe1:9092,bluejoe2:9092,bluejoe3:9092 --topic mytopic</span></p><p><span style="line-height:1.5;">消费者测试</span></p><p>bin/kafka-console-consumer.sh --zookeeper bluejoe1:2181,bluejoe2:2181,bluejoe3:2181 --topic mytopic</p><p>如下示出producer的界面：</p><p><img src="http://images2015.cnblogs.com/blog/453391/201601/453391-20160110155237950-857728695.png" alt="" width="996" height="96"></p><p>如下示出consumer的界面：</p><p><img src="http://images2015.cnblogs.com/blog/453391/201601/453391-20160110155500121-1079753054.png" alt="" width="944" height="117"></p>            </div>
                </div>