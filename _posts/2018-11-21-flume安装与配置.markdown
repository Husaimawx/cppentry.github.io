---
layout:     post
title:      flume安装与配置
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/Vinsuan1993/article/details/71406917				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>flume-1.5.0安装包下载（flume NG）：http://download.csdn.net/detail/vinsuan1993/9836334</p>
<p>安装环境：centOS-6.5-64位</p>
<p>1、需求：在一台机器部署flume，让其收集数据并将数据写到hdfs中。<br>
2、安装flume（flume并不依赖于hadoop框架，只依赖JDK和一些hadoop的jar包）<br>
2.1、将flume压缩包上传到centOS<br>
2.2、解压flume<br>
tar -zxvf apache-flume... /heres/<br>
2.3、配置flume<br>
2.3.1、进入conf目录<br>
mv flume-env.sh.template flume-env.sh<br>
2.3.2、把java_home导进来<br><span></span>JAVA_HOME=/usr/java/jdk...<br>
2.3.3、配置sources、channels、sinks 、组装信息（<span style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;text-indent:5px;">SpoolSource-source，对于source类型可以参看博客：http://blog.csdn.net/vinsuan1993/article/details/71374383</span>）<br></p><pre><code class="language-html">#定义agent名， source、channel、sink的名称
a4.sources = r1
a4.channels = c1
a4.sinks = k1


#具体定义source，有一个spoolidr实现类，通过读这个配置文件，利用反射将这个类实例化
a4.sources.r1.type = spooldir
#监听这个目录
a4.sources.r1.spoolDir = /root/logs


#具体定义channel
a4.channels.c1.type = memory
#容量
a4.channels.c1.capacity = 10000
a4.channels.c1.transactionCapacity = 100


#定义拦截器，为消息添加时间戳
a4.sources.r1.interceptors = i1
a4.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder




#具体定义sink
a4.sinks.k1.type = hdfs
a4.sinks.k1.hdfs.path = hdfs://ns1/flume/%Y%m%d
#产生日志的前缀
a4.sinks.k1.hdfs.filePrefix = events-
#纯文本
a4.sinks.k1.hdfs.fileType = DataStream
#不按照条数生成文件
a4.sinks.k1.hdfs.rollCount = 0
#HDFS上的文件达到128M时生成一个文件
a4.sinks.k1.hdfs.rollSize = 134217728
#HDFS上的文件达到60秒生成一个文件，两个条件只要满足一个，就可以写入HDFS
a4.sinks.k1.hdfs.rollInterval = 60


#组装source、channel、sink
a4.sources.r1.channels = c1
a4.sinks.k1.channel = c1</code></pre><br><br>
2.3.4、将上面的配置文件拷贝到flume的conf目录<br>
cp /root/a4.conf /heres/flume/conf<br>
2.3.5、拷贝hadoop的包<br>
scp hadoop-common-2.2.0.jar commons-configration-1.6.jar hadoop-auth-2.2.0.jar hadoop-hdfs-2.2.0.jar 192.168.2.113:/heres/apache-flume.../lib<br>
2.3.6、把关于hdfs的配置文件拷贝到flume<br>
scp /heres/hadoop-2.2.0/etc/hadoop/{core-site.xml,hdfs-site.xml} 192.168.2.113:/heres/apache-flume.../conf<br>
2.3.7、配置ip地址映射<br>
vim /etc/hosts<br>
2.3.7、启动flume -n agent的名称 -C 读配置信息 -f 决定三大组件类型 把日志打印出来<br>
bin/flume-ng agent -n a4 -c conf -f conf/a4.conf -Dflume.root.logger=INFO,console<br><br><br><br><br>
3、向目录中丢数据<br>
cp access... /root/logs<br><p>4、<span style="font-family:Verdana, '宋体';color:#444444;"><span style="font-size:16px;line-height:24px;">附（exec-source）：</span></span></p>
<p><span style="font-family:Verdana, '宋体';color:#444444;"><span style="font-size:16px;line-height:24px;"></span></span></p><pre><code class="language-html">#bin/flume-ng agent -n a2 -f /home/hadoop/a2.conf -c conf -Dflume.root.logger=INFO,console
#定义agent名， source、channel、sink的名称
a2.sources = r1
a2.channels = c1
a2.sinks = k1


#具体定义source
a2.sources.r1.type = exec
a2.sources.r1.command = tail -F /home/hadoop/a.log


#具体定义channel
a2.channels.c1.type = memory
a2.channels.c1.capacity = 1000
a2.channels.c1.transactionCapacity = 100


#具体定义sink
a2.sinks.k1.type = logger


#组装source、channel、sink
a2.sources.r1.channels = c1
a2.sinks.k1.channel = c1

</code></pre><br><br><br><br><br><br><br><br><br>            </div>
                </div>