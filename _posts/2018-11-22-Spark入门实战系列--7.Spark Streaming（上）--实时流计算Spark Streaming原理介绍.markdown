---
layout:     post
title:      Spark入门实战系列--7.Spark Streaming（上）--实时流计算Spark Streaming原理介绍
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1 style="color:rgb(70,70,70)"><a target="_blank" id="cb_post_title_url" href="http://www.cnblogs.com/shishanyuan/p/4747735.html" rel="nofollow" style="color:rgb(100,102,179)">Spark入门实战系列--7.Spark Streaming--实时流计算Spark Streaming原理介绍</a></h1>
<div style="color:rgb(57,57,57)">
<div id="cnblogs_post_body">
<h1 align="justify"><span style="color:rgb(0,128,0)">【注】该系列文章以及使用到安装包/测试数据 可以在《<span style="color:rgb(0,0,255)"><a target="_blank" href="http://www.cnblogs.com/shishanyuan/p/4699644.html" rel="nofollow" style="color:rgb(100,102,179)"><span style="color:rgb(0,0,255)">倾情大奉送--Spark入门实战系列</span></a></span>》获取</span></h1>
<h1 align="justify"><a target="_blank" name="_Toc427067851"></a><span style="color:rgb(0,0,0)">1</span>、Spark Streaming简介</h1>
<h2 align="justify"><a target="_blank" name="_Toc427067852"></a><span style="color:rgb(0,0,0)">1.1</span> 概述</h2>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming </span>是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。在“One
 Stack rule them all”的基础上，还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。</p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>处理的数据流图：</p>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408290197622.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image002" src="http://images0.cnblogs.com/blog/107289/201508/211408297537778.jpg" alt="clip_image002" width="553" height="179" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark</span>的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">对应的批数据，在Spark</span>内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率。</p>
<h2 align="justify"><a target="_blank" name="_Toc427067853"></a><span style="color:rgb(0,0,0)">1.2</span> 术语定义</h2>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>离散流（discretized stream）或DStream</strong>：这是Spark Streaming对内部持续的实时数据流的抽象描述，即我们处理的一个实时数据流，在Spark Streaming中对应于一个DStream 实例。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>批数据（batch data）</strong>：这是化整为零的第一步，将实时流数据以时间片为单位进行分批，将流处理转化为时间片数据的批处理。随着持续时间的推移，这些处理结果就形成了对应的结果数据流了。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>时间片或批处理时间间隔（ batch interval）</strong>：这是人为地对流数据进行定量的标准，以时间片作为我们拆分流数据的依据。一个时间片的数据对应一个RDD实例。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>窗口长度（window length）</strong>：一个窗口覆盖的流数据的时间长度。必须是批处理时间间隔的倍数，</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>滑动时间间隔</strong>：前一个窗口到后一个窗口所经过的时间长度。必须是批处理时间间隔的倍数</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>Input DStream</strong> :一个input DStream是一个特殊的DStream，将Spark Streaming连接到一个外部数据源来读取数据。</p>
<h2 align="justify"><a target="_blank" name="_Toc427067854"></a><span style="color:rgb(0,0,0)">1.3</span> Storm与Spark Streming比较</h2>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>处理模型以及延迟</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">虽然两框架都提供了可扩展性(scalability)</span>和可容错性(fault tolerance)，但是它们的处理模型从根本上说是不一样的。Storm可以实现亚秒级时延的处理，而每次只处理一条event，而Spark Streaming可以在一个短暂的时间窗口里面处理多条(batches)Event。所以说Storm可以实现亚秒级时延的处理，而Spark Streaming则有一定的时延。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>容错和数据保证</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">然而两者的代价都是容错时候的数据保证，Spark Streaming</span>的容错为有状态的计算提供了更好的支持。在Storm中，每条记录在系统的移动过程中都需要被标记跟踪，所以Storm只能保证每条记录最少被处理一次，但是允许从错误状态恢复时被处理多次。这就意味着可变更的状态可能被更新两次从而导致结果不正确。</p>
<p align="justify"><span style="color:rgb(0,0,0)">任一方面，Spark Streaming</span>仅仅需要在批处理级别对记录进行追踪，所以他能保证每个批处理记录仅仅被处理一次，即使是node节点挂掉。虽然说Storm的Trident library可以保证一条记录被处理一次，但是它依赖于事务更新状态，而这个过程是很慢的，并且需要由用户去实现。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>实现和编程API</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">Storm</span>主要是由Clojure语言实现，Spark Streaming是由Scala实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm是由BackType和 Twitter开发，而Spark Streaming是在UC Berkeley开发的。</p>
<p align="justify"><span style="color:rgb(0,0,0)">Storm</span>提供了Java API，同时也支持其他语言的API。 Spark Streaming支持Scala和Java语言(其实也支持Python)。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>批处理框架集成</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写Spark Streaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>生产支持</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">Storm</span>已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目，并且在2013年仅仅被Sharethrough使用(据作者了解)。</p>
<p align="justify"><span style="color:rgb(0,0,0)">Storm</span>是 Hortonworks Hadoop数据平台中流处理的解决方案，而Spark Streaming出现在 MapR的分布式平台和Cloudera的企业数据平台中。除此之外，Databricks是为Spark提供技术支持的公司，包括了Spark Streaming。</p>
<p align="justify"><span style="color:rgb(0,0,0)">虽然说两者都可以在各自的集群框架中运行，但是Storm</span>可以在Mesos上运行, 而Spark Streaming可以在YARN和Mesos上运行。</p>
<h1 align="justify"><a target="_blank" name="_Toc427067855"></a><span style="color:rgb(0,0,0)">2</span>、运行原理</h1>
<h2 align="justify"><a target="_blank" name="_Toc427067856"></a><span style="color:rgb(0,0,0)">2.1</span> Streaming架构</h2>
<p align="justify"><span style="color:rgb(0,0,0)">SparkStreaming</span>是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，并将结果保存到外部文件系统、数据库或应用到实时仪表盘。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>计算流程</strong>：Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark
 Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。</p>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408317691596.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image004" src="http://images0.cnblogs.com/blog/107289/201508/211408323637709.jpg" alt="clip_image004" width="373" height="293" border="0"></a></p>
<p align="justify"><span style="color:rgb(51,51,51)">图Spark Streaming</span>构架</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>容错性</strong>：对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。  </p>
<p align="justify"><span style="color:rgb(0,0,0)">对于Spark Streaming</span>来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark
 Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。</p>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408329883067.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image006" src="http://images0.cnblogs.com/blog/107289/201508/211408335663411.jpg" alt="clip_image006" width="388" height="235" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>中RDD的lineage关系图</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>实时性</strong>：对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右），所以Spark
 Streaming能够满足除对实时性要求非常高（如高频实时交易）之外的所有流式准实时计算场景。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>扩展性与吞吐量</strong>：Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍，图4是Berkeley利用WordCount和Grep两个用例所做的测试，在Grep这个测试中，Spark Streaming中的每个节点的吞吐量是670k records/s，而Storm是115k
 records/s。</p>
<p align="justify"><a target="_blank" href="http://cms.csdnimg.cn/article/201401/27/52e5fc3d07287.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image008" src="http://images0.cnblogs.com/blog/107289/201508/211408342691026.jpg" alt="clip_image008" width="489" height="214" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>与Storm吞吐量比较图</p>
<h2 align="justify"><a target="_blank" name="_Toc427067857"></a><span style="color:rgb(0,0,0)">2.2</span> 编程模型</h2>
<p align="justify"><span style="color:rgb(0,0,0)">DStream</span>（Discretized Stream）作为Spark Streaming的基础抽象，它代表持续性的数据流。这些数据流既可以通过外部输入源赖获取，也可以通过现有的Dstream的transformation操作来获得。在内部实现上，DStream由一组时间序列上连续的RDD来表示。每个RDD都包含了自己特定时间间隔内的数据流。如图7-3所示。</p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408350358425.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image010" src="http://images0.cnblogs.com/blog/107289/201508/211408354109240.jpg" alt="clip_image010" width="553" height="107" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">图7-3   </span>DStream中在时间轴下生成离散的RDD序列</p>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408358168298.gif" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image012" src="http://images0.cnblogs.com/blog/107289/201508/211408366443185.gif" alt="clip_image012" width="553" height="150" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">对DStream</span>中数据的各种操作也是映射到内部的RDD上来进行的，如图7-4所示，对Dtream的操作可以通过RDD的transformation生成新的DStream。这里的执行引擎是Spark。</p>
<h3 align="justify"><a target="_blank" name="_Toc427067858"></a><span style="color:rgb(0,0,0)">2.2.1 </span>如何使用Spark Streaming</h3>
<p align="justify"><span style="color:rgb(0,0,0)">作为构建于Spark</span>之上的应用框架，Spark Streaming承袭了Spark的编程风格，对于已经了解Spark的用户来说能够快速地上手。接下来以Spark Streaming官方提供的WordCount代码为例来介绍Spark Streaming的使用方式。</p>
<p align="left"><em><span style="color:rgb(4,150,21)">import org.apache.spark._</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming._</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left"><em><span style="color:rgb(192,0,0)">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></em></p>
<p align="left"><em><span style="color:rgb(192,0,0)">// The master requires 2 cores to prevent from a starvation scenario.</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">val ssc = new StreamingContext(conf, Seconds(1))</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left"><em><span style="color:rgb(192,0,0)">// Create a DStream that will connect to hostname:port, like localhost:9999</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">val lines = ssc.socketTextStream("localhost", 9999)</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left"><em><span style="color:rgb(192,0,0)">// Split each line into words</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">val words = lines.flatMap(_.split(" "))</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left"><em><span style="color:rgb(192,0,0)">// Count each word in each batch</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">val pairs = words.map(word =&gt; (word, 1))</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">val wordCounts = pairs.reduceByKey(_ + _)</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left"><em><span style="color:rgb(192,0,0)">// Print the first ten elements of each RDD generated in this DStream to the console</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">wordCounts.print()</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">ssc.start()             </span> <span style="color:rgb(192,0,0)">// Start the computation</span></em></p>
<p align="left"><em><span style="color:rgb(4,150,21)">ssc.awaitTermination()  </span><span style="color:rgb(192,0,0)">// Wait for the computation to terminate</span></em></p>
<p align="justify"><span style="color:rgb(0,0,0)"><strong>1.</strong></span><strong>创建StreamingContext对象</strong> 同Spark初始化需要创建SparkContext对象一样，使用Spark Streaming就需要创建StreamingContext对象。创建StreamingContext对象所需的参数与SparkContext基本一致，包括指明Master，设定名称(如NetworkWordCount)。需要注意的是参数Seconds(1)，Spark
 Streaming需要指定处理数据的时间间隔，如上例所示的1s，那么Spark Streaming会以1s为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置；</p>
<p align="justify"><span style="color:rgb(0,0,0)"><strong>2.</strong></span><strong>创建InputDStream</strong>如同Storm的Spout，Spark Streaming需要指明数据源。如上例所示的socketTextStream，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括Kafka、 Flume、HDFS/S3、Kinesis和Twitter等数据源；</p>
<p align="justify"><span style="color:rgb(0,0,0)"><strong>3.</strong></span><strong>操作DStream</strong>对于从数据源得到的DStream，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的WordCount执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用Map和ReduceByKey方法进行计算，当然最后还有使用print()方法输出结果；</p>
<p align="justify"><span style="color:rgb(0,0,0)"><strong>4.</strong></span><strong>启动Spark Streaming</strong>之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当ssc.start()启动后程序才真正进行所有预期的操作。</p>
<p align="justify"><span style="color:rgb(0,0,0)">至此对于Spark Streaming</span>的如何使用有了一个大概的印象，在后面的章节我们会通过源代码深入探究一下Spark Streaming的执行流程。</p>
<h3 align="justify"><a target="_blank" name="_Toc427067859"></a><span style="color:rgb(0,0,0)">2.2.2 </span>DStream的输入源</h3>
<p align="justify"><span style="color:rgb(0,0,0)">在Spark Streaming</span>中所有的操作都是基于流的，而输入源是这一系列操作的起点。输入 DStreams 和 DStreams 接收的流都代表输入数据流的来源，在Spark Streaming 提供两种内置数据流来源：</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>  <strong>基础来源</strong> 在 StreamingContext API 中直接可用的来源。例如：文件系统、Socket（套接字）连接和 Akka actors；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>  <strong>高级来源</strong> 如 Kafka、Flume、Kinesis、Twitter 等，可以通过额外的实用工具类创建。</p>
<h4 align="justify" style="color:rgb(51,51,51)"><span style="color:rgb(0,0,0)"><em>2.2.2.1 </em></span><em>基础来源</em></h4>
<p align="justify"><span style="color:rgb(0,0,0)">在前面分析怎样使用Spark Streaming</span>的例子中我们已看到ssc.socketTextStream()方法，可以通过 TCP 套接字连接，从从文本数据中创建了一个 DStream。除了套接字，StreamingContext 的API还提供了方法从文件和 Akka actors 中创建 DStreams作为输入源。</p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>提供了streamingContext.fileStream(dataDirectory)方法可以从任何文件系统(如：HDFS、S3、NFS 等）的文件中读取数据，然后创建一个DStream。Spark Streaming 监控 dataDirectory 目录和在该目录下任何文件被创建处理(不支持在嵌套目录下写文件)。需要注意的是：读取的必须是具有相同的数据格式的文件；创建的文件必须在 dataDirectory 目录下，并通过自动移动或重命名成数据目录；文件一旦移动就不能被改变，如果文件被不断追加,新的数据将不会被阅读。对于简单的文本文，可以使用一个简单的方法streamingContext.textFileStream(dataDirectory)来读取数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>也可以基于自定义 Actors 的流创建DStream ，通过 Akka actors 接受数据流，使用方法streamingContext.actorStream(actorProps, actor-name)。Spark Streaming使用 streamingContext.queueStream(queueOfRDDs)方法可以创建基于 RDD 队列的DStream，每个RDD 队列将被视为 DStream 中一块数据流进行加工处理。</p>
<h4 align="justify" style="color:rgb(51,51,51)"><span style="color:rgb(0,0,0)"><em>2.2.2.2 </em></span><em>高级来源</em></h4>
<p align="justify"><span style="color:rgb(0,0,0)">这一类的来源需要外部 non-Spark </span>库的接口，其中一些有复杂的依赖关系(如 Kafka、Flume)。因此通过这些来源创建 DStreams 需要明确其依赖。例如，如果想创建一个使用 Twitter tweets 的数据的DStream 流，必须按以下步骤来做：</p>
<p align="justify"><span style="color:rgb(0,0,0)">1</span>）在 SBT 或 Maven工程里添加 spark-streaming-twitter_2.10 依赖。</p>
<p align="justify"><span style="color:rgb(0,0,0)">2</span>）开发：导入 TwitterUtils 包，通过 TwitterUtils.createStream 方法创建一个DStream。</p>
<p align="justify"><span style="color:rgb(0,0,0)">3</span>）部署：添加所有依赖的 jar 包(包括依赖的spark-streaming-twitter_2.10 及其依赖)，然后部署应用程序。</p>
<p align="justify"><span style="color:rgb(0,0,0)">需要注意的是，这些高级的来源一般在Spark Shell</span>中不可用，因此基于这些高级来源的应用不能在Spark Shell中进行测试。如果你必须在Spark shell中使用它们，你需要下载相应的Maven工程的Jar依赖并添加到类路径中。</p>
<p align="justify"><span style="color:rgb(0,0,0)">其中一些高级来源如下：</span></p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>Twitter</strong> Spark Streaming的TwitterUtils工具类使用Twitter4j，Twitter4J 库支持通过任何方法提供身份验证信息，你可以得到公众的流，或得到基于关键词过滤流。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>Flume</strong> Spark Streaming可以从Flume中接受数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>Kafka</strong> Spark Streaming可以从Kafka中接受数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>Kinesis</strong> Spark Streaming可以从Kinesis中接受数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">需要重申的一点是在开始编写自己的 SparkStreaming </span>程序之前，一定要将高级来源依赖的Jar添加到SBT 或 Maven 项目相应的artifact中。常见的输入源和其对应的Jar包如下图所示。</p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408388948072.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image014" src="http://images0.cnblogs.com/blog/107289/201508/211408407384902.jpg" alt="clip_image014" width="553" height="280" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">另外，输入DStream</span>也可以创建自定义的数据源，需要做的就是实现一个用户定义的接收器。</p>
<h3 align="justify"><a target="_blank" name="_Toc427067860"></a><span style="color:rgb(0,0,0)">2.2.3 </span>DStream的操作</h3>
<p align="justify"><span style="color:rgb(0,0,0)">与RDD</span>类似，DStream也提供了自己的一系列操作方法，这些操作可以分成三类：普通的转换操作、窗口转换操作和输出操作。</p>
<h4 align="justify" style="color:rgb(51,51,51)"><span style="color:rgb(0,0,0)"><em>2.2.3.1 </em></span><em>普通的转换操作</em></h4>
<p align="justify"><span style="color:rgb(0,0,0)">普通的转换操作如下表所示：</span></p>
<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" style="background:white">
<p align="center"><strong><span style="color:rgb(51,51,51)">转换</span></strong></p>
</td>
<td valign="top" style="background:white">
<p align="center"><strong><span style="color:rgb(51,51,51)">描述</span></strong></p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>map</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">源 DStream</span>的每个元素通过函数func返回一个新的DStream。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>flatMap</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">类似与map</span>操作，不同的是每个输入元素可以被映射出0或者更多的输出元素。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>filter</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">在源DSTREAM</span>上选择Func函数返回仅为true的元素,最终返回一个新的DSTREAM 。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>repartition</strong></span>(<em>numPartitions</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">通过输入的参数numPartitions</span>的值来改变DStream的分区大小。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>union</strong></span>(<em>otherStream</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">返回一个包含源DStream</span>与其他 DStream的元素合并后的新DSTREAM。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>count</strong></span>()</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">对源DStream</span>内部的所含有的RDD的元素数量进行计数，返回一个内部的RDD只包含一个元素的DStreaam。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>reduce</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">使用函数func</span>（有两个参数并返回一个结果）将源DStream 中每个RDD的元素进行聚 合操作,返回一个内部所包含的RDD只有一个元素的新DStream。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>countByValue</strong></span>()</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">计算DStream</span>中每个RDD内的元素出现的频次并返回新的DStream[(K,Long)]，其中K是RDD中元素的类型，Long是元素出现的频次。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>reduceByKey</strong></span>(<em>func</em>, [<em>numTasks</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">当一个类型为（K</span>，V）键值对的DStream被调用的时候,返回类型为类型为（K，V）键值对的新 DStream,其中每个键的值V都是使用聚合函数func汇总。注意：默认情况下，使用 Spark的默认并行度提交任务（本地模式下并行度为2，集群模式下位8），可以通过配置numTasks设置不同的并行任务数。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>join</strong></span>(<em>otherStream</em>, [<em>numTasks</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">当被调用类型分别为（K</span>，V）和（K，W）键值对的2个DStream 时，返回类型为（K，（V，W））键值对的一个新DSTREAM。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>cogroup</strong></span>(<em>otherStream</em>, [<em>numTasks</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">当被调用的两个DStream</span>分别含有(K, V) 和(K, W)键值对时,返回一个(K, Seq[V], Seq[W])类型的新的DStream。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>transform</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">通过对源DStream</span>的每RDD应用RDD-to-RDD函数返回一个新的DStream，这可以用来在DStream做任意RDD操作。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>updateStateByKey</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">返回一个新状态的DStream,</span>其中每个键的状态是根据键的前一个状态和键的新值应用给定函数func后的更新。这个方法可以被用来维持每个键的任何状态数据。</p>
</td>
</tr>
</tbody>
</table>
<p align="justify"><span style="color:rgb(0,0,0)">在上面列出的这些操作中，transform()</span>方法和updateStateByKey()方法值得我们深入的探讨一下：</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>  <strong>transform(func)操作</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">该transform</span>操作（转换操作）连同其其类似的 transformWith操作允许DStream 上应用任意RDD-to-RDD函数。它可以被应用于未在 DStream API 中暴露任何的RDD操作。例如，在每批次的数据流与另一数据集的连接功能不直接暴露在DStream API 中，但可以轻松地使用transform操作来做到这一点，这使得DStream的功能非常强大。例如，你可以通过连接预先计算的垃圾邮件信息的输入数据流（可能也有Spark生成的），然后基于此做实时数据清理的筛选，如下面官方提供的伪代码所示。事实上，也可以在transform方法中使用机器学习和图形计算的算法。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>  <strong>updateStateByKey操作</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">该 updateStateByKey </span>操作可以让你保持任意状态，同时不断有新的信息进行更新。要使用此功能，必须进行两个步骤 ：</p>
<p align="justify"><span style="color:rgb(0,0,0)">（1）</span>  定义状态 - 状态可以是任意的数据类型。</p>
<p align="justify"><span style="color:rgb(0,0,0)">（2）</span>  定义状态更新函数 - 用一个函数指定如何使用先前的状态和从输入流中获取的新值 更新状态。</p>
<p align="justify"><span style="color:rgb(0,0,0)">让我们用一个例子来说明，假设你要进行文本数据流中单词计数。在这里，正在运行的计数是状态而且它是一个整数。我们定义了更新功能如下：</span></p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408411135716.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image016" src="http://images0.cnblogs.com/blog/107289/201508/211408415351545.jpg" alt="clip_image016" width="553" height="78" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">此函数应用于含有键值对的DStream</span>中（如前面的示例中，在DStream中含有（word，1）键值对）。它会针对里面的每个元素（如wordCount中的word）调用一下更新函数，newValues是最新的值，runningCount是之前的值。</p>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408421605902.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image018" src="http://images0.cnblogs.com/blog/107289/201508/211408425664960.jpg" alt="clip_image018" width="553" height="41" border="0"></a></p>
<h4 align="justify" style="color:rgb(51,51,51)"><span style="color:rgb(0,0,0)"><em>2.2.3.2 </em></span><em>窗口转换操作</em></h4>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming </span>还提供了窗口的计算，它允许你通过滑动窗口对数据进行转换，窗口转换操作如下：</p>
<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" style="background:white">
<p align="center"><strong><span style="color:rgb(51,51,51)">转换</span></strong></p>
</td>
<td valign="top" style="background:white">
<p align="center"><strong><span style="color:rgb(51,51,51)">描述</span></strong></p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>window</strong></span>(<em>windowLength</em>, <em>slideInterval</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">返回一个基于源DStream</span>的窗口批次计算后得到新的DStream。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>countByWindow</strong></span>(<em>windowLength</em>,<em>slideInterval</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">返回基于滑动窗口的DStream</span>中的元素的数量。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>reduceByWindow</strong></span>(<em>func</em>, <em>windowLength</em>,<em>slideInterval</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">基于滑动窗口对源DStream</span>中的元素进行聚合操作，得到一个新的DStream。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>reduceByKeyAndWindow</strong></span>(<em>func</em>,<em>windowLength</em>,<em>slideInterval</em>, [<em>numTasks</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">基于滑动窗口对（K</span>，V）键值对类型的DStream中的值按K使用聚合函数func进行聚合操作，得到一个新的DStream。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>reduceByKeyAndWindow</strong></span>(<em>func</em>,<em>invFunc</em>,<em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">一个更高效的reduceByKkeyAndWindow()</span>的实现版本，先对滑动窗口中新的时间间隔内数据增量聚合并移去最早的与新增数据量的时间间隔内的数据统计量。例如，计算t+4秒这个时刻过去5秒窗口的WordCount，那么我们可以将t+3时刻过去5秒的统计量加上[t+3，t+4]的统计量，在减去[t-2，t-1]的统计量，这种方法可以复用中间三秒的统计量，提高统计的效率。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>countByValueAndWindow</strong></span>(<em>windowLength</em>,<em>slideInterval</em>, [<em>numTasks</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)">基于滑动窗口计算源DStream</span>中每个RDD内每个元素出现的频次并返回DStream[(K,Long)]，其中K是RDD中元素的类型，Long是元素频次。与countByValue一样，reduce任务的数量可以通过一个可选参数进行配置。</p>
</td>
</tr>
</tbody>
</table>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408429889788.gif" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image020" src="http://images0.cnblogs.com/blog/107289/201508/211408434887875.gif" alt="clip_image020" width="554" height="212" border="0"></a></p>
<p align="center"><span style="color:rgb(0,0,0)">批处理间隔示意图</span></p>
<p align="justify"><span style="color:rgb(0,0,0)">在Spark Streaming</span>中，数据处理是按批进行的，而数据采集是逐条进行的，因此在Spark Streaming中会先设置好批处理间隔（batch duration），当超过批处理间隔的时候就会把采集到的数据汇总起来成为一批数据交给系统去处理。</p>
<p align="justify"><span style="color:rgb(0,0,0)">对于窗口操作而言，在其窗口内部会有N</span>个批处理数据，批处理数据的大小由窗口间隔（window duration）决定，而窗口间隔指的就是窗口的持续时间，在窗口操作中，只有窗口的长度满足了才会触发批数据的处理。除了窗口的长度，窗口操作还有另一个重要的参数就是滑动间隔（slide duration），它指的是经过多长时间窗口滑动一次形成新的窗口，滑动窗口默认情况下和批次间隔的相同，而窗口间隔一般设置的要比它们两个大。在这里必须注意的一点是滑动间隔和窗口间隔的大小一定得设置为批处理间隔的整数倍。</p>
<p align="justify"><span style="color:rgb(0,0,0)">如批处理间隔示意图所示，批处理间隔是1</span>个时间单位，窗口间隔是3个时间单位，滑动间隔是2个时间单位。对于初始的窗口time 1-time 3，只有窗口间隔满足了才触发数据的处理。这里需要注意的一点是，初始的窗口有可能流入的数据没有撑满，但是随着时间的推进，窗口最终会被撑满。当每个2个时间单位，窗口滑动一次后，会有新的数据流入窗口，这时窗口会移去最早的两个时间单位的数据，而与最新的两个时间单位的数据进行汇总形成新的窗口（time3-time5）。</p>
<p align="justify"><span style="color:rgb(0,0,0)">对于窗口操作，批处理间隔、窗口间隔和滑动间隔是非常重要的三个时间概念，是理解窗口操作的关键所在。</span></p>
<h4 align="justify" style="color:rgb(51,51,51)"><span style="color:rgb(0,0,0)"><em>2.2.3.3 </em></span><em>输出操作</em></h4>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>允许DStream的数据被输出到外部系统，如数据库或文件系统。由于输出操作实际上使transformation操作后的数据可以通过外部系统被使用，同时输出操作触发所有DStream的transformation操作的实际执行（类似于RDD操作）。以下表列出了目前主要的输出操作：</p>
<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" style="background:white">
<p align="center"><strong><span style="color:rgb(51,51,51)">转换</span></strong></p>
</td>
<td valign="top" style="background:white">
<p align="center"><strong><span style="color:rgb(51,51,51)">描述</span></strong></p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>print</strong></span>()</p>
</td>
<td valign="top" style="background:white">
<p align="left">在Driver中打印出DStream中数据的前10个元素。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>saveAsTextFiles</strong></span>(<em>prefix</em>, [<em>suffix</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left">将DStream中的内容以文本的形式保存为文本文件，其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>saveAsObjectFiles</strong></span>(<em>prefix</em>, [<em>suffix</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left">将DStream中的内容按对象序列化并且以SequenceFile的格式保存。其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>saveAsHadoopFiles</strong></span>(<em>prefix</em>, [<em>suffix</em>])</p>
</td>
<td valign="top" style="background:white">
<p align="left">将DStream中的内容以文本的形式保存为Hadoop文件，其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。</p>
</td>
</tr>
<tr>
<td valign="top" style="background:white">
<p align="left"><span style="color:rgb(51,51,51)"><strong>foreachRDD</strong></span>(<em>func</em>)</p>
</td>
<td valign="top" style="background:white">
<p align="left">最基本的输出操作，将func函数应用于DStream中的RDD上，这个操作会输出数据到外部系统，比如保存RDD到文件或者网络数据库等。需要注意的是func函数是在运行该streaming应用的Driver进程里执行的。</p>
</td>
</tr>
</tbody>
</table>
<p align="justify"><span style="color:rgb(0,0,0)">dstream.foreachRDD</span>是一个非常强大的输出操作，它允将许数据输出到外部系统。但是 ，如何正确高效地使用这个操作是很重要的，下面展示了如何去避免一些常见的错误。</p>
<p align="justify"><span style="color:rgb(0,0,0)">通常将数据写入到外部系统需要创建一个连接对象（如 TCP</span>连接到远程服务器），并用它来发送数据到远程系统。出于这个目的，开发者可能在不经意间在Spark driver端创建了连接对象，并尝试使用它保存RDD中的记录到Spark worker上，如下面代码：</p>
<p align="justify"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408441289003.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image022" src="http://images0.cnblogs.com/blog/107289/201508/211408445192291.jpg" alt="clip_image022" width="553" height="143" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">这是不正确的，这需要连接对象进行序列化并从Driver</span>端发送到Worker上。连接对象很少在不同机器间进行这种操作，此错误可能表现为序列化错误（连接对不可序列化），初始化错误（连接对象在需要在Worker 上进行需要初始化） 等等，正确的解决办法是在 worker上创建的连接对象。</p>
<p align="justify"><span style="color:rgb(0,0,0)">通常情况下，创建一个连接对象有时间和资源开销。因此，创建和销毁的每条记录的连接对象可能招致不必要的资源开销，并显著降低系统整体的吞吐量 。一个更好的解决方案是使用rdd.foreachPartition</span>方法创建一个单独的连接对象，然后使用该连接对象输出的所有RDD分区中的数据到外部系统。</p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span>这缓解了创建多条记录连接的开销。最后，还可以进一步通过在多个RDDs/ batches上重用连接对象进行优化。一个保持连接对象的静态池可以重用在多个批处理的RDD上将其输出到外部系统，从而进一步降低了开销。</p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span>需要注意的是，在静态池中的连接应该按需延迟创建，这样可以更有效地把数据发送到外部系统。另外需要要注意的是：DStreams延迟执行的，就像RDD的操作是由actions触发一样。默认情况下，输出操作会按照它们在Streaming应用程序中定义的顺序一个个执行。</p>
<h2 align="justify"><a target="_blank" name="_Toc427067861"></a><span style="color:rgb(0,0,0)">2.3</span>  容错、持久化和性能调优</h2>
<h3 align="justify"><a target="_blank" name="_Toc427067862"></a><span style="color:rgb(0,0,0)">2.3.1 </span>容错</h3>
<p align="justify"><span style="color:rgb(0,0,0)">DStream</span>基于RDD组成，RDD的容错性依旧有效，我们首先回忆一下SparkRDD的基本特性。</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>RDD是一个不可变的、确定性的可重复计算的分布式数据集。RDD的某些partition丢失了，可以通过血统（lineage）信息重新计算恢复；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>如果RDD任何分区因worker节点故障而丢失，那么这个分区可以从原来依赖的容错数据集中恢复；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>由于Spark中所有的数据的转换操作都是基于RDD的，即使集群出现故障，只要输入数据集存在，所有的中间结果都是可以被计算的。</p>
<p align="justify"><span style="color:rgb(0,0,0)">Spark Streaming</span>是可以从HDFS和S3这样的文件系统读取数据的，这种情况下所有的数据都可以被重新计算，不用担心数据的丢失。但是在大多数情况下，Spark Streaming是基于网络来接受数据的，此时为了实现相同的容错处理，在接受网络的数据时会在集群的多个Worker节点间进行数据的复制（默认的复制数是2），这导致产生在出现故障时被处理的两种类型的数据：</p>
<p align="justify"><span style="color:rgb(0,0,0)">1</span>）Data received and replicated ：一旦一个Worker节点失效，系统会从另一份还存在的数据中重新计算。</p>
<p align="justify"><span style="color:rgb(0,0,0)">2</span>）Data received but buffered for replication ：一旦数据丢失，可以通过RDD之间的依赖关系，从HDFS这样的外部文件系统读取数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">此外，有两种故障，我们应该关心：</span></p>
<p align="justify"><span style="color:rgb(0,0,0)">（1</span>）Worker节点失效：通过上面的讲解我们知道，这时系统会根据出现故障的数据的类型，选择是从另一个有复制过数据的工作节点上重新计算，还是直接从从外部文件系统读取数据。</p>
<p align="justify"><span style="color:rgb(0,0,0)">（2</span>）Driver（驱动节点）失效 ：如果运行 Spark Streaming应用时驱动节点出现故障，那么很明显的StreamingContext已经丢失，同时在内存中的数据全部丢失。对于这种情况，Spark Streaming应用程序在计算上有一个内在的结构——在每段micro-batch数据周期性地执行同样的Spark计算。这种结构允许把应用的状态（亦称checkpoint）周期性地保存到可靠的存储空间中，并在driver重新启动时恢复该状态。具体做法是在ssc.checkpoint(&lt;checkpoint
 directory&gt;)函数中进行设置，Spark Streaming就会定期把DStream的元信息写入到HDFS中，一旦驱动节点失效，丢失的StreamingContext会通过已经保存的检查点信息进行恢复。</p>
<p align="justify"><span style="color:rgb(0,0,0)">最后我们谈一下Spark Stream</span>的容错在Spark 1.2版本的一些改进：</p>
<p align="justify"><span style="color:rgb(0,0,0)">实时流处理系统必须要能在24/7</span>时间内工作，因此它需要具备从各种系统故障中恢复过来的能力。最开始，SparkStreaming就支持从driver和worker故障恢复的能力。然而有些数据源的输入可能在故障恢复以后丢失数据。在Spark1.2版本中，Spark已经在SparkStreaming中对预写日志（也被称为journaling）作了初步支持，改进了恢复机制，并使更多数据源的零数据丢失有了可靠。</p>
<p align="justify"><span style="color:rgb(0,0,0)">对于文件这样的源数据，driver</span>恢复机制足以做到零数据丢失，因为所有的数据都保存在了像HDFS或S3这样的容错文件系统中了。但对于像Kafka和Flume等其它数据源，有些接收到的数据还只缓存在内存中，尚未被处理，它们就有可能会丢失。这是由于Spark应用的分布操作方式引起的。当driver进程失败时，所有在standalone/yarn/mesos集群运行的executor，连同它们在内存中的所有数据，也同时被终止。对于Spark
 Streaming来说，从诸如Kafka和Flume的数据源接收到的所有数据，在它们处理完成之前，一直都缓存在executor的内存中。纵然driver重新启动，这些缓存的数据也不能被恢复。为了避免这种数据损失，在Spark1.2发布版本中引进了预写日志（WriteAheadLogs）功能。</p>
<p align="justify"><span style="color:rgb(0,0,0)">预写日志功能的流程是：1</span>）一个SparkStreaming应用开始时（也就是driver开始时），相关的StreamingContext使用SparkContext启动接收器成为长驻运行任务。这些接收器接收并保存流数据到Spark内存中以供处理。2）接收器通知driver。3）接收块中的元数据（metadata）被发送到driver的StreamingContext。这个元数据包括：（a）定位其在executor内存中数据的块referenceid，（b）块数据在日志中的偏移信息（如果启用了）。</p>
<p align="justify"><span style="color:rgb(0,0,0)">用户传送数据的生命周期如下图所示。</span></p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211408450664391.gif" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image024" src="http://images0.cnblogs.com/blog/107289/201508/211408478167364.gif" alt="clip_image024" width="553" height="237" border="0"></a></p>
<p align="justify"><span style="color:rgb(0,0,0)">类似Kafka</span>这样的系统可以通过复制数据保持可靠性。允许预写日志两次高效地复制同样的数据：一次由Kafka，而另一次由SparkStreaming。Spark未来版本将包含Kafka容错机制的原生支持，从而避免第二个日志。</p>
<h3 align="justify"><a target="_blank" name="_Toc427067863"></a><span style="color:rgb(0,0,0)">2.3.2 </span>持久化</h3>
<p align="justify"><span style="color:rgb(0,0,0)">与RDD</span>一样，DStream同样也能通过persist()方法将数据流存放在内存中，默认的持久化方式是MEMORY_ONLY_SER，也就是在内存中存放数据同时序列化的方式，这样做的好处是遇到需要多次迭代计算的程序时，速度优势十分的明显。而对于一些基于窗口的操作，如reduceByWindow、reduceByKeyAndWindow，以及基于状态的操作，如updateStateBykey，其默认的持久化策略就是保存在内存中。</p>
<p align="justify"><span style="color:rgb(0,0,0)">对于来自网络的数据源（Kafka</span>、Flume、sockets等），默认的持久化策略是将数据保存在两台机器上，这也是为了容错性而设计的。</p>
<p align="justify"><span style="color:rgb(0,0,0)">另外，对于窗口和有状态的操作必须checkpoint</span>，通过StreamingContext的checkpoint来指定目录，通过 Dtream的checkpoint指定间隔时间，间隔必须是滑动间隔（slide interval）的倍数。</p>
<h3 align="justify"><a target="_blank" name="_Toc427067864"></a><span style="color:rgb(0,0,0)">2.3.3 </span>性能调优</h3>
<p align="justify"><span style="color:rgb(0,0,0)"><strong>1.</strong></span>  <strong>优化运行时间</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span> <strong>增加并行度</strong> 确保使用整个集群的资源，而不是把任务集中在几个特定的节点上。对于包含shuffle的操作，增加其并行度以确保更为充分地使用集群资源；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span> <strong>减少数据序列化，反序列化的负担</strong> Spark Streaming默认将接受到的数据序列化后存储，以减少内存的使用。但是序列化和反序列话需要更多的CPU时间，因此更加高效的序列化方式（Kryo）和自定义的系列化接口可以更高效地使用CPU；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span> <strong>设置合理的batch duration（批处理时间间）</strong> 在Spark Streaming中，Job之间有可能存在依赖关系，后面的Job必须确保前面的作业执行结束后才能提交。若前面的Job执行的时间超出了批处理时间间隔，那么后面的Job就无法按时提交，这样就会进一步拖延接下来的Job，造成后续Job的阻塞。因此设置一个合理的批处理间隔以确保作业能够在这个批处理间隔内结束时必须的；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span>  <strong>减少因任务提交和分发所带来的负担</strong> 通常情况下，Akka框架能够高效地确保任务及时分发，但是当批处理间隔非常小（500ms）时，提交和分发任务的延迟就变得不可接受了。使用Standalone和Coarse-grained Mesos模式通常会比使用Fine-grained Mesos模式有更小的延迟。</p>
<p align="justify"><span style="color:rgb(0,0,0)"><strong>2.</strong></span>  <strong>优化内存使用</strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>控制batch size（批处理间隔内的数据量）</strong> Spark Streaming会把批处理间隔内接收到的所有数据存放在Spark内部的可用内存区域中，因此必须确保当前节点Spark的可用内存中少能容纳这个批处理时间间隔内的所有数据，否则必须增加新的资源以提高集群的处理能力；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>及时清理不再使用的数据</strong> 前面讲到Spark Streaming会将接受的数据全部存储到内部可用内存区域中，因此对于处理过的不再需要的数据应及时清理，以确保Spark Streaming有富余的可用内存空间。通过设置合理的spark.cleaner.ttl时长来及时清理超时的无用数据，这个参数需要小心设置以免后续操作中所需要的数据被超时错误处理；</p>
<p align="justify"><span style="color:rgb(0,0,0)">l</span><strong>观察及适当调整GC策略</strong> GC会影响Job的正常运行，可能延长Job的执行时间，引起一系列不可预料的问题。观察GC的运行情况，采用不同的GC策略以进一步减小内存回收对Job运行的影响。</p>
<p align="justify"><span style="color:rgb(0,0,0)"> </span></p>
<p align="justify"><strong><span style="color:rgb(0,0,0)">参考资料：</span></strong></p>
<p align="justify"><span style="color:rgb(0,0,0)">（1）</span>《Spark Streaming》 http://blog.debugo.com/spark-streaming/</p>
<p align="justify"><br>
</p>
<p align="justify"><br>
</p>
<p align="justify"><br>
</p>
<p align="justify"><br>
</p>
<p align="justify"></p>
<h1 align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">1</span>、实例演示</h1>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058815"></a><span style="color:rgb(0,0,0)">1.1</span> 流数据模拟器</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058816"></a><span style="color:rgb(0,0,0)">1.1.1 </span>流数据说明</h3>
<p></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在实例演示中模拟实际情况，需要源源不断地接入流数据，为了在演示过程中更接近真实环境将定义流数据模拟器。该模拟器主要功能：通过Socket</span>方式监听指定的端口号，当外部程序通过该端口连接并请求数据时，模拟器将定时将指定的文件数据随机获取发送给外部程序。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058817"></a><span style="color:rgb(0,0,0)">1.1.2 </span>模拟器代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import java.io.{PrintWriter}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import java.net.ServerSocket</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import scala.io.Source</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object StreamingSimulation {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>  </em></span>// 定义随机获取整数的方法</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def index(length: Int) = {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>import java.util.Random</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val rdm = new Random</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>rdm.nextInt(length)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>调用该模拟器需要三个参数，分为为文件路径、端口号和间隔时间（单位：毫秒）</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 3) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: &lt;filename&gt; &lt;port&gt; &lt;millisecond&gt;")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>获取指定文件总的行数</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val filename = args(0)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = Source.fromFile(filename).getLines.toList</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val filerow = lines.length</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>指定监听某端口，当外部程序请求时建立连接</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val listener = new ServerSocket(args(1).toInt)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>while (true) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>val socket = listener.accept()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>new Thread() {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">        </span>override def run = {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>println("Got client connected from: " + socket.getInetAddress)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>val out = new PrintWriter(socket.getOutputStream(), true)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>while (true) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>Thread.sleep(args(2).toLong)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>当该端口接受请求时，随机获取某行数据发送给对方</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>val content = lines(index(filerow))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>println(content)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>out.write(content + '\n')</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>out.flush()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>socket.close()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">        </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>}.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412113636415.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image002" src="http://images0.cnblogs.com/blog/107289/201508/211412120199016.jpg" alt="clip_image002" width="662" height="308" border="0"></a></em></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058818"></a><span style="color:rgb(0,0,0)">1.1.3 </span>生成打包文件</h3>
<p align="justify" style="color:rgb(57,57,57)"><strong><span style="color:rgb(255,0,0)">【注】</span></strong><span style="color:rgb(0,0,0)">可以参见第3</span>课《Spark编程模型（下）--IDEA搭建及实战》进行打包</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412126756915.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image004" src="http://images0.cnblogs.com/blog/107289/201508/211412133002273.jpg" alt="clip_image004" width="663" height="339" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在打包配置界面中，需要在Class Path</span>加入：/app/scala-2.10.4/lib/scala-swing.jar /app/scala-2.10.4/lib/scala-library.jar /app/scala-2.10.4/lib/scala-actors.jar，各个jar包之间用空格分开，</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">点击菜单Build-&gt;Build Artifacts</span>，弹出选择动作，选择Build或者Rebuild动作，使用如下命令复制打包文件到Spark根目录下</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">cd /home/hadoop/IdeaProjects/out/artifacts/LearnSpark_jar</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">cp LearnSpark.jar /app/hadoop/spark-1.1.0/</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">ll /app/hadoop/spark-1.1.0/</span></em></p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412137696817.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image006" src="http://images0.cnblogs.com/blog/107289/201508/211412142068417.jpg" alt="clip_image006" width="663" height="251" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058819"></a><span style="color:rgb(0,0,0)">1.2</span> 实例1：读取文件演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058820"></a><span style="color:rgb(0,0,0)">1.2.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在该实例中Spark Streaming</span>将监控某目录中的文件，获取在间隔时间段内变化的数据，然后通过Spark Streaming计算出改时间段内单词统计数。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058821"></a><span style="color:rgb(0,0,0)">1.2.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.SparkConf</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object FileWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sparkConf = new SparkConf().setAppName("FileWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em><span style="color:rgb(192,0,0)"><em>   </em></span>// 创建Streaming的上下文，包括Spark的配置和时间间隔，这里时间为间隔20秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sparkConf, Seconds(20))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 指定监控的目录，在这里为/home/hadoop/temp/</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.textFileStream("/home/hadoop/temp/")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 对指定文件夹变化的数据进行单词统计并且打印</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(" "))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span> </em><span style="color:rgb(192,0,0)"><em>// </em></span>启动Streaming</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412148313775.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image008" src="http://images0.cnblogs.com/blog/107289/201508/211412154259888.jpg" alt="clip_image008" width="663" height="265" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058822"></a><span style="color:rgb(0,0,0)">1.2.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   创建Streaming监控目录</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">创建/home/hadoop/temp</span>为Spark Streaming监控的目录，通过在该目录中定时添加文件内容，然后由Spark Streaming统计出单词个数</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412161287504.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image010" src="http://images0.cnblogs.com/blog/107289/201508/211412167694334.jpg" alt="clip_image010" width="663" height="222" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   使用如下命令启动Spark集群</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$sbin/start-all.sh</span></em></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   在IDEA中运行Streaming程序</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，由于该实例没有输入参数故不需要配置参数，在运行日志中将定时打印时间戳。如果在监控目录中加入文件内容，将输出时间戳的同时将输出单词统计个数。</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412173004962.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image012" src="http://images0.cnblogs.com/blog/107289/201508/211412178785305.jpg" alt="clip_image012" width="663" height="364" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058823"></a><span style="color:rgb(0,0,0)">1.2.4 </span>添加文本及内容</h3>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412183318377.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image014" src="http://images0.cnblogs.com/blog/107289/201508/211412192532292.jpg" alt="clip_image014" width="650" height="197" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412206132508.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image016" src="http://images0.cnblogs.com/blog/107289/201508/211412210191566.jpg" alt="clip_image016" width="505" height="204" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058824"></a><span style="color:rgb(0,0,0)">1.2.5 </span>查看结果</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   查看IDEA中运行情况</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行日志窗口中，可以观察到输出时间戳的同时将输出单词统计个数</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412214416394.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image018" src="http://images0.cnblogs.com/blog/107289/201508/211412223165295.jpg" alt="clip_image018" width="663" height="218" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   通过webUI监控运行情况</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在http://hadoop1:4040</span>监控Spark Streaming运行情况，可以观察到每20秒运行一次作业</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412228001909.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image020" src="http://images0.cnblogs.com/blog/107289/201508/211412235038523.jpg" alt="clip_image020" width="663" height="280" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">并且与其他运行作业相比在监控菜单增加了"Streaming"</span>项目，点击可以看到监控内容：</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412240501624.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image022" src="http://images0.cnblogs.com/blog/107289/201508/211412245508710.jpg" alt="clip_image022" width="663" height="272" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058825"></a><span style="color:rgb(0,0,0)">1.3</span> 实例2：网络数据演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058826"></a><span style="color:rgb(0,0,0)">1.3.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在该实例中将由4.1</span>流数据模拟以1秒的频度发送模拟数据，Spark Streaming通过Socket接收流数据并每20秒运行一次用来处理接收到数据，处理完毕后打印该时间段内数据出现的频度，即在各处理段时间之间状态并无关系。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058827"></a><span style="color:rgb(0,0,0)">1.3.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.storage.StorageLevel</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object NetworkWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(20))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>    </em></span>// 通过Socket获取数据，该处需要提供Socket的主机名和端口号，数据保存在内存和硬盘中</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>    </em></span>// 对读入的数据进行分割、计数</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(","))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412249729240.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image024" src="http://images0.cnblogs.com/blog/107289/201508/211412254884097.jpg" alt="clip_image024" width="663" height="300" border="0"></a></em></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058828"></a><span style="color:rgb(0,0,0)">1.3.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送/home/hadoop/upload/class7目录下的people.txt数据文件（该文件可以在本系列配套资源目录/data/class7中找到），其中people.txt数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412259563940.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image026" src="http://images0.cnblogs.com/blog/107289/201508/211412268166070.jpg" alt="clip_image026" width="384" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒，</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class7/people.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412289106442.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image028" src="http://images0.cnblogs.com/blog/107289/201508/211412295195029.jpg" alt="clip_image028" width="663" height="111" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在没有程序连接时，该程序处于阻塞状态</span></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在IDEA中运行Streaming程序</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1和端口号为9999</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412299102615.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image030" src="http://images0.cnblogs.com/blog/107289/201508/211412305973459.jpg" alt="clip_image030" width="663" height="240" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058829"></a><span style="color:rgb(0,0,0)">1.3.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   观察模拟器发送情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">IDEA</span>中的Spark Streaming程序运行与模拟器建立连接，当模拟器检测到外部连接时开始发送测试数据，数据是随机的在指定的文件中获取一行数据并发送，时间间隔为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412310501829.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image032" src="http://images0.cnblogs.com/blog/107289/201508/211412326448817.jpg" alt="clip_image032" width="663" height="208" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每20秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412330664646.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image034" src="http://images0.cnblogs.com/blog/107289/201508/211412334889474.jpg" alt="clip_image034" width="661" height="286" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观测到的统计结果，通过分析在Spark Streaming每段时间内单词数为20，正好是20秒内每秒发送总数。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412339413547.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image036" src="http://images0.cnblogs.com/blog/107289/201508/211412344567403.jpg" alt="clip_image036" width="663" height="198" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058830"></a><span style="color:rgb(0,0,0)">1.4</span> 实例3：销售数据统计演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058831"></a><span style="color:rgb(0,0,0)">1.4.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在该实例中将由4.1</span>流数据模拟器以1秒的频度发送模拟数据（销售数据），Spark Streaming通过Socket接收流数据并每5秒运行一次用来处理接收到数据，处理完毕后打印该时间段内销售数据总和，需要注意的是各处理段时间之间状态并无关系。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058832"></a><span style="color:rgb(0,0,0)">1.4.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.log4j.{Level, Logger}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.storage.StorageLevel</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object SaleAmount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 2) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: SaleAmount &lt;hostname&gt; &lt;port&gt; ")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("SaleAmount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(5))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>   </em></span>// 通过Socket获取数据，该处需要提供Socket的主机名和端口号，数据保存在内存和硬盘中</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.map(_.split(",")).filter(_.length == 6)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x=&gt;(1, x(5).toDouble)).reduceByKey(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412349101476.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image038" src="http://images0.cnblogs.com/blog/107289/201508/211412355819847.jpg" alt="clip_image038" width="663" height="257" border="0"></a></em></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058833"></a><span style="color:rgb(0,0,0)">1.4.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送第五课/home/hadoop/upload/class5/saledata目录下的tbStockDetail.txt数据文件（参见第五课《5.Hive（下）--Hive实战》中2.1.2数据描述，该文件可以在本系列配套资源目录/data/class5/saledata中找到），其中表tbStockDetail字段分别为订单号、行号、货品、数量、金额，数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><span><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412360979405.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image040" src="http://images0.cnblogs.com/blog/107289/201508/211412365033762.jpg" alt="clip_image040" width="500" height="152" border="0"></a></span></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class5/saledata/tbStockDetail.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412369102820.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image042" src="http://images0.cnblogs.com/blog/107289/201508/211412372853635.jpg" alt="clip_image042" width="663" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1和端口号为9999</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412393318992.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image044" src="http://images0.cnblogs.com/blog/107289/201508/211412399256107.jpg" alt="clip_image044" width="663" height="218" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058834"></a><span style="color:rgb(0,0,0)">1.4.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   观察模拟器发送情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">IDEA</span>中的Spark Streaming程序运行与模拟器建立连接，当模拟器检测到外部连接时开始发送销售数据，时间间隔为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412404415666.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image046" src="http://images0.cnblogs.com/blog/107289/201508/211412410034537.jpg" alt="clip_image046" width="663" height="189" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)"> </span></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观察到每5秒运行一次作业（两次运行间隔为5000毫秒），运行完毕后打印该时间段内销售数据总和。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412415194096.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image048" src="http://images0.cnblogs.com/blog/107289/201508/211412420357952.jpg" alt="clip_image048" width="663" height="222" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每5秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412427385567.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image050" src="http://images0.cnblogs.com/blog/107289/201508/211412438472241.jpg" alt="clip_image050" width="661" height="286" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058835"></a><span style="color:rgb(0,0,0)">1.5</span> 实例4：Stateful演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058836"></a><span style="color:rgb(0,0,0)">1.5.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">该实例为Spark Streaming</span>状态操作，模拟数据由4.1流数据模拟以1秒的频度发送，Spark Streaming通过Socket接收流数据并每5秒运行一次用来处理接收到数据，处理完毕后打印程序启动后单词出现的频度，相比较前面4.3实例在该实例中各时间段之间状态是相关的。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058837"></a><span style="color:rgb(0,0,0)">1.5.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.log4j.{Level, Logger}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object StatefulWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 2) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: StatefulWordCount &lt;filename&gt; &lt;port&gt; ")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val updateFunc = (values: Seq[Int], state: Option[Int]) =&gt; {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>val currentCount = values.foldLeft(0)(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>val previousCount = state.getOrElse(0)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>Some(currentCount + previousCount)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("StatefulWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 创建StreamingContext，Spark Steaming运行时间间隔为5秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(5))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 定义checkpoint目录为当前目录</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.checkpoint(".")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 获取从Socket发送过来数据</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(","))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x, 1))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>    </em></span>// 使用updateStateByKey来更新状态，统计从运行开始以来单词总的次数</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val stateDstream = wordCounts.updateStateByKey[Int](updateFunc)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>stateDstream.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412448169169.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image052" src="http://images0.cnblogs.com/blog/107289/201508/211412458003869.jpg" alt="clip_image052" width="663" height="344" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058838"></a><span style="color:rgb(0,0,0)">1.5.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送/home/hadoop/upload/class7目录下的people.txt数据文件（该文件可以在本系列配套资源目录/data/class7中找到），其中people.txt数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412469882799.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image026[1]" src="http://images0.cnblogs.com/blog/107289/201508/211412475198128.jpg" alt="clip_image026[1]" width="384" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class7/people.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412481756027.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image028[1]" src="http://images0.cnblogs.com/blog/107289/201508/211412489419129.jpg" alt="clip_image028[1]" width="663" height="111" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在没有程序连接时，该程序处于阻塞状态，</span>在IDEA中运行Streaming程序</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1和端口号为9999</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412497385772.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image054" src="http://images0.cnblogs.com/blog/107289/201508/211412502539629.jpg" alt="clip_image054" width="663" height="240" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058839"></a><span style="color:rgb(0,0,0)">1.5.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观察到第一次运行统计单词总数为1，第二次为6，第N次为5(N-1)+1，即统计单词的总数为程序运行单词数总和。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412511288530.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image056" src="http://images0.cnblogs.com/blog/107289/201508/211412521758716.jpg" alt="clip_image056" width="663" height="215" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每5秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412530815860.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image058" src="http://images0.cnblogs.com/blog/107289/201508/211412540503789.jpg" alt="clip_image058" width="659" height="306" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   查看CheckPoint情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在项目根目录下可以看到checkpoint</span>文件</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412568315006.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image060" src="http://images0.cnblogs.com/blog/107289/201508/211412574252121.jpg" alt="clip_image060" width="656" height="321" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058840"></a><span style="color:rgb(0,0,0)">1.6</span> 实例5：Window演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058841"></a><span style="color:rgb(0,0,0)">1.6.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">该实例为Spark Streaming</span>窗口操作，模拟数据由4.1流数据模拟以1秒的频度发送，Spark Streaming通过Socket接收流数据并每10秒运行一次用来处理接收到数据，处理完毕后打印程序启动后单词出现的频度。相比前面的实例，Spark Streaming窗口统计是通过reduceByKeyAndWindow()方法实现的，在该方法中需要指定窗口时间长度和滑动时间间隔。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058842"></a><span style="color:rgb(0,0,0)">1.6.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.log4j.{Level, Logger}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.storage.StorageLevel</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object WindowWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 4) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: WindowWorldCount &lt;filename&gt; &lt;port&gt; &lt;windowDuration&gt; &lt;slideDuration&gt;")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("WindowWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 创建StreamingContext</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(5))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 定义checkpoint目录为当前目录</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.checkpoint(".")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>通过Socket获取数据，该处需要提供Socket的主机名和端口号，数据保存在内存和硬盘中</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_ONLY_SER)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(","))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// windows操作，第一种方式为叠加处理，第二种方式为增量处理</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x , 1)).reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b), Seconds(args(2).toInt), Seconds(args(3).toInt))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(192,0,0)">//val wordCounts = words.map(x =&gt; (x , 1)).reduceByKeyAndWindow(_+_, _-_,Seconds(args(2).toInt), Seconds(args(3).toInt))</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412582536007.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image062" src="http://images0.cnblogs.com/blog/107289/201508/211412591758921.jpg" alt="clip_image062" width="662" height="256" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058843"></a><span style="color:rgb(0,0,0)">1.6.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送/home/hadoop/upload/class7目录下的people.txt数据文件（该文件可以在本系列配套资源目录/data/class7中找到），其中people.txt数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412598312523.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image026[2]" src="http://images0.cnblogs.com/blog/107289/201508/211413003784623.jpg" alt="clip_image026[2]" width="384" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class7/people.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413023785967.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image028[2]" src="http://images0.cnblogs.com/blog/107289/201508/211413052697684.jpg" alt="clip_image028[2]" width="663" height="111" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在没有程序连接时，该程序处于阻塞状态，</span>在IDEA中运行Streaming程序</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1、端口号为9999、时间窗口为30秒和滑动时间间隔10秒</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413080504200.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image064" src="http://images0.cnblogs.com/blog/107289/201508/211413102535073.jpg" alt="clip_image064" width="663" height="238" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058844"></a><span style="color:rgb(0,0,0)">1.6.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观察到第一次运行统计单词总数为4，第二次为14，第N次为10(N-1)+4，即统计单词的总数为程序运行单词数总和。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413108168645.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image066" src="http://images0.cnblogs.com/blog/107289/201508/211413111757987.jpg" alt="clip_image066" width="663" height="233" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每10秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413117856574.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image068" src="http://images0.cnblogs.com/blog/107289/201508/211413130356289.jpg" alt="clip_image068" width="657" height="264" border="0"></a></p>
<br>
<p align="justify"></p>
<h1 align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">1</span>、实例演示</h1>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058815"></a><span style="color:rgb(0,0,0)">1.1</span> 流数据模拟器</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058816"></a><span style="color:rgb(0,0,0)">1.1.1 </span>流数据说明</h3>
<p></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在实例演示中模拟实际情况，需要源源不断地接入流数据，为了在演示过程中更接近真实环境将定义流数据模拟器。该模拟器主要功能：通过Socket</span>方式监听指定的端口号，当外部程序通过该端口连接并请求数据时，模拟器将定时将指定的文件数据随机获取发送给外部程序。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058817"></a><span style="color:rgb(0,0,0)">1.1.2 </span>模拟器代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import java.io.{PrintWriter}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import java.net.ServerSocket</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import scala.io.Source</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object StreamingSimulation {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>  </em></span>// 定义随机获取整数的方法</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def index(length: Int) = {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>import java.util.Random</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val rdm = new Random</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>rdm.nextInt(length)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>调用该模拟器需要三个参数，分为为文件路径、端口号和间隔时间（单位：毫秒）</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 3) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: &lt;filename&gt; &lt;port&gt; &lt;millisecond&gt;")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>获取指定文件总的行数</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val filename = args(0)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = Source.fromFile(filename).getLines.toList</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val filerow = lines.length</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>指定监听某端口，当外部程序请求时建立连接</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val listener = new ServerSocket(args(1).toInt)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>while (true) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>val socket = listener.accept()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>new Thread() {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">        </span>override def run = {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>println("Got client connected from: " + socket.getInetAddress)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>val out = new PrintWriter(socket.getOutputStream(), true)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>while (true) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>Thread.sleep(args(2).toLong)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>当该端口接受请求时，随机获取某行数据发送给对方</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>val content = lines(index(filerow))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>println(content)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>out.write(content + '\n')</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">            </span>out.flush()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">          </span>socket.close()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">        </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>}.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412113636415.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image002" src="http://images0.cnblogs.com/blog/107289/201508/211412120199016.jpg" alt="clip_image002" width="662" height="308" border="0"></a></em></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058818"></a><span style="color:rgb(0,0,0)">1.1.3 </span>生成打包文件</h3>
<p align="justify" style="color:rgb(57,57,57)"><strong><span style="color:rgb(255,0,0)">【注】</span></strong><span style="color:rgb(0,0,0)">可以参见第3</span>课《Spark编程模型（下）--IDEA搭建及实战》进行打包</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412126756915.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image004" src="http://images0.cnblogs.com/blog/107289/201508/211412133002273.jpg" alt="clip_image004" width="663" height="339" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在打包配置界面中，需要在Class Path</span>加入：/app/scala-2.10.4/lib/scala-swing.jar /app/scala-2.10.4/lib/scala-library.jar /app/scala-2.10.4/lib/scala-actors.jar，各个jar包之间用空格分开，</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">点击菜单Build-&gt;Build Artifacts</span>，弹出选择动作，选择Build或者Rebuild动作，使用如下命令复制打包文件到Spark根目录下</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">cd /home/hadoop/IdeaProjects/out/artifacts/LearnSpark_jar</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">cp LearnSpark.jar /app/hadoop/spark-1.1.0/</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">ll /app/hadoop/spark-1.1.0/</span></em></p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412137696817.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image006" src="http://images0.cnblogs.com/blog/107289/201508/211412142068417.jpg" alt="clip_image006" width="663" height="251" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058819"></a><span style="color:rgb(0,0,0)">1.2</span> 实例1：读取文件演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058820"></a><span style="color:rgb(0,0,0)">1.2.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在该实例中Spark Streaming</span>将监控某目录中的文件，获取在间隔时间段内变化的数据，然后通过Spark Streaming计算出改时间段内单词统计数。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058821"></a><span style="color:rgb(0,0,0)">1.2.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.SparkConf</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object FileWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sparkConf = new SparkConf().setAppName("FileWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em><span style="color:rgb(192,0,0)"><em>   </em></span>// 创建Streaming的上下文，包括Spark的配置和时间间隔，这里时间为间隔20秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sparkConf, Seconds(20))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 指定监控的目录，在这里为/home/hadoop/temp/</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.textFileStream("/home/hadoop/temp/")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 对指定文件夹变化的数据进行单词统计并且打印</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(" "))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span> </em><span style="color:rgb(192,0,0)"><em>// </em></span>启动Streaming</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412148313775.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image008" src="http://images0.cnblogs.com/blog/107289/201508/211412154259888.jpg" alt="clip_image008" width="663" height="265" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058822"></a><span style="color:rgb(0,0,0)">1.2.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   创建Streaming监控目录</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">创建/home/hadoop/temp</span>为Spark Streaming监控的目录，通过在该目录中定时添加文件内容，然后由Spark Streaming统计出单词个数</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412161287504.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image010" src="http://images0.cnblogs.com/blog/107289/201508/211412167694334.jpg" alt="clip_image010" width="663" height="222" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   使用如下命令启动Spark集群</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$sbin/start-all.sh</span></em></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   在IDEA中运行Streaming程序</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，由于该实例没有输入参数故不需要配置参数，在运行日志中将定时打印时间戳。如果在监控目录中加入文件内容，将输出时间戳的同时将输出单词统计个数。</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412173004962.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image012" src="http://images0.cnblogs.com/blog/107289/201508/211412178785305.jpg" alt="clip_image012" width="663" height="364" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058823"></a><span style="color:rgb(0,0,0)">1.2.4 </span>添加文本及内容</h3>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412183318377.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image014" src="http://images0.cnblogs.com/blog/107289/201508/211412192532292.jpg" alt="clip_image014" width="650" height="197" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412206132508.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image016" src="http://images0.cnblogs.com/blog/107289/201508/211412210191566.jpg" alt="clip_image016" width="505" height="204" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058824"></a><span style="color:rgb(0,0,0)">1.2.5 </span>查看结果</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   查看IDEA中运行情况</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行日志窗口中，可以观察到输出时间戳的同时将输出单词统计个数</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412214416394.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image018" src="http://images0.cnblogs.com/blog/107289/201508/211412223165295.jpg" alt="clip_image018" width="663" height="218" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   通过webUI监控运行情况</p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在http://hadoop1:4040</span>监控Spark Streaming运行情况，可以观察到每20秒运行一次作业</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412228001909.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image020" src="http://images0.cnblogs.com/blog/107289/201508/211412235038523.jpg" alt="clip_image020" width="663" height="280" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">并且与其他运行作业相比在监控菜单增加了"Streaming"</span>项目，点击可以看到监控内容：</p>
<p align="justify" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412240501624.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image022" src="http://images0.cnblogs.com/blog/107289/201508/211412245508710.jpg" alt="clip_image022" width="663" height="272" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058825"></a><span style="color:rgb(0,0,0)">1.3</span> 实例2：网络数据演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058826"></a><span style="color:rgb(0,0,0)">1.3.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在该实例中将由4.1</span>流数据模拟以1秒的频度发送模拟数据，Spark Streaming通过Socket接收流数据并每20秒运行一次用来处理接收到数据，处理完毕后打印该时间段内数据出现的频度，即在各处理段时间之间状态并无关系。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058827"></a><span style="color:rgb(0,0,0)">1.3.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.storage.StorageLevel</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object NetworkWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(20))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>    </em></span>// 通过Socket获取数据，该处需要提供Socket的主机名和端口号，数据保存在内存和硬盘中</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>    </em></span>// 对读入的数据进行分割、计数</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(","))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412249729240.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image024" src="http://images0.cnblogs.com/blog/107289/201508/211412254884097.jpg" alt="clip_image024" width="663" height="300" border="0"></a></em></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058828"></a><span style="color:rgb(0,0,0)">1.3.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送/home/hadoop/upload/class7目录下的people.txt数据文件（该文件可以在本系列配套资源目录/data/class7中找到），其中people.txt数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412259563940.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image026" src="http://images0.cnblogs.com/blog/107289/201508/211412268166070.jpg" alt="clip_image026" width="384" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒，</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class7/people.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412289106442.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image028" src="http://images0.cnblogs.com/blog/107289/201508/211412295195029.jpg" alt="clip_image028" width="663" height="111" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在没有程序连接时，该程序处于阻塞状态</span></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在IDEA中运行Streaming程序</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1和端口号为9999</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412299102615.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image030" src="http://images0.cnblogs.com/blog/107289/201508/211412305973459.jpg" alt="clip_image030" width="663" height="240" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058829"></a><span style="color:rgb(0,0,0)">1.3.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   观察模拟器发送情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">IDEA</span>中的Spark Streaming程序运行与模拟器建立连接，当模拟器检测到外部连接时开始发送测试数据，数据是随机的在指定的文件中获取一行数据并发送，时间间隔为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412310501829.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image032" src="http://images0.cnblogs.com/blog/107289/201508/211412326448817.jpg" alt="clip_image032" width="663" height="208" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每20秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412330664646.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image034" src="http://images0.cnblogs.com/blog/107289/201508/211412334889474.jpg" alt="clip_image034" width="661" height="286" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观测到的统计结果，通过分析在Spark Streaming每段时间内单词数为20，正好是20秒内每秒发送总数。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412339413547.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image036" src="http://images0.cnblogs.com/blog/107289/201508/211412344567403.jpg" alt="clip_image036" width="663" height="198" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058830"></a><span style="color:rgb(0,0,0)">1.4</span> 实例3：销售数据统计演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058831"></a><span style="color:rgb(0,0,0)">1.4.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在该实例中将由4.1</span>流数据模拟器以1秒的频度发送模拟数据（销售数据），Spark Streaming通过Socket接收流数据并每5秒运行一次用来处理接收到数据，处理完毕后打印该时间段内销售数据总和，需要注意的是各处理段时间之间状态并无关系。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058832"></a><span style="color:rgb(0,0,0)">1.4.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.log4j.{Level, Logger}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.storage.StorageLevel</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object SaleAmount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 2) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: SaleAmount &lt;hostname&gt; &lt;port&gt; ")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("SaleAmount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(5))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>   </em></span>// 通过Socket获取数据，该处需要提供Socket的主机名和端口号，数据保存在内存和硬盘中</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.map(_.split(",")).filter(_.length == 6)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x=&gt;(1, x(5).toDouble)).reduceByKey(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412349101476.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image038" src="http://images0.cnblogs.com/blog/107289/201508/211412355819847.jpg" alt="clip_image038" width="663" height="257" border="0"></a></em></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058833"></a><span style="color:rgb(0,0,0)">1.4.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送第五课/home/hadoop/upload/class5/saledata目录下的tbStockDetail.txt数据文件（参见第五课《5.Hive（下）--Hive实战》中2.1.2数据描述，该文件可以在本系列配套资源目录/data/class5/saledata中找到），其中表tbStockDetail字段分别为订单号、行号、货品、数量、金额，数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><span><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412360979405.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image040" src="http://images0.cnblogs.com/blog/107289/201508/211412365033762.jpg" alt="clip_image040" width="500" height="152" border="0"></a></span></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class5/saledata/tbStockDetail.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412369102820.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image042" src="http://images0.cnblogs.com/blog/107289/201508/211412372853635.jpg" alt="clip_image042" width="663" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1和端口号为9999</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412393318992.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image044" src="http://images0.cnblogs.com/blog/107289/201508/211412399256107.jpg" alt="clip_image044" width="663" height="218" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058834"></a><span style="color:rgb(0,0,0)">1.4.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   观察模拟器发送情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">IDEA</span>中的Spark Streaming程序运行与模拟器建立连接，当模拟器检测到外部连接时开始发送销售数据，时间间隔为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412404415666.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image046" src="http://images0.cnblogs.com/blog/107289/201508/211412410034537.jpg" alt="clip_image046" width="663" height="189" border="0"></a></p>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)"> </span></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观察到每5秒运行一次作业（两次运行间隔为5000毫秒），运行完毕后打印该时间段内销售数据总和。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412415194096.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image048" src="http://images0.cnblogs.com/blog/107289/201508/211412420357952.jpg" alt="clip_image048" width="663" height="222" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每5秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412427385567.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image050" src="http://images0.cnblogs.com/blog/107289/201508/211412438472241.jpg" alt="clip_image050" width="661" height="286" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058835"></a><span style="color:rgb(0,0,0)">1.5</span> 实例4：Stateful演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058836"></a><span style="color:rgb(0,0,0)">1.5.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">该实例为Spark Streaming</span>状态操作，模拟数据由4.1流数据模拟以1秒的频度发送，Spark Streaming通过Socket接收流数据并每5秒运行一次用来处理接收到数据，处理完毕后打印程序启动后单词出现的频度，相比较前面4.3实例在该实例中各时间段之间状态是相关的。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058837"></a><span style="color:rgb(0,0,0)">1.5.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.log4j.{Level, Logger}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.{Seconds, StreamingContext}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object StatefulWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 2) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: StatefulWordCount &lt;filename&gt; &lt;port&gt; ")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val updateFunc = (values: Seq[Int], state: Option[Int]) =&gt; {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>val currentCount = values.foldLeft(0)(_ + _)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>val previousCount = state.getOrElse(0)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>Some(currentCount + previousCount)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("StatefulWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 创建StreamingContext，Spark Steaming运行时间间隔为5秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(5))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 定义checkpoint目录为当前目录</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.checkpoint(".")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 获取从Socket发送过来数据</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(","))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x, 1))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(192,0,0)"><em>    </em></span>// 使用updateStateByKey来更新状态，统计从运行开始以来单词总的次数</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val stateDstream = wordCounts.updateStateByKey[Int](updateFunc)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>stateDstream.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412448169169.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image052" src="http://images0.cnblogs.com/blog/107289/201508/211412458003869.jpg" alt="clip_image052" width="663" height="344" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058838"></a><span style="color:rgb(0,0,0)">1.5.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送/home/hadoop/upload/class7目录下的people.txt数据文件（该文件可以在本系列配套资源目录/data/class7中找到），其中people.txt数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412469882799.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image026[1]" src="http://images0.cnblogs.com/blog/107289/201508/211412475198128.jpg" alt="clip_image026[1]" width="384" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class7/people.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412481756027.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image028[1]" src="http://images0.cnblogs.com/blog/107289/201508/211412489419129.jpg" alt="clip_image028[1]" width="663" height="111" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在没有程序连接时，该程序处于阻塞状态，</span>在IDEA中运行Streaming程序</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1和端口号为9999</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412497385772.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image054" src="http://images0.cnblogs.com/blog/107289/201508/211412502539629.jpg" alt="clip_image054" width="663" height="240" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058839"></a><span style="color:rgb(0,0,0)">1.5.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观察到第一次运行统计单词总数为1，第二次为6，第N次为5(N-1)+1，即统计单词的总数为程序运行单词数总和。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412511288530.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image056" src="http://images0.cnblogs.com/blog/107289/201508/211412521758716.jpg" alt="clip_image056" width="663" height="215" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每5秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412530815860.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image058" src="http://images0.cnblogs.com/blog/107289/201508/211412540503789.jpg" alt="clip_image058" width="659" height="306" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第三步</span>   查看CheckPoint情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在项目根目录下可以看到checkpoint</span>文件</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412568315006.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image060" src="http://images0.cnblogs.com/blog/107289/201508/211412574252121.jpg" alt="clip_image060" width="656" height="321" border="0"></a></p>
<h2 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058840"></a><span style="color:rgb(0,0,0)">1.6</span> 实例5：Window演示</h2>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058841"></a><span style="color:rgb(0,0,0)">1.6.1 </span>演示说明</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">该实例为Spark Streaming</span>窗口操作，模拟数据由4.1流数据模拟以1秒的频度发送，Spark Streaming通过Socket接收流数据并每10秒运行一次用来处理接收到数据，处理完毕后打印程序启动后单词出现的频度。相比前面的实例，Spark Streaming窗口统计是通过reduceByKeyAndWindow()方法实现的，在该方法中需要指定窗口时间长度和滑动时间间隔。</p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058842"></a><span style="color:rgb(0,0,0)">1.6.2 </span>演示代码</h3>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.log4j.{Level, Logger}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.{SparkContext, SparkConf}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.storage.StorageLevel</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">import org.apache.spark.streaming.StreamingContext._</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">object WindowWordCount {</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>def main(args: Array[String]) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>if (args.length != 4) {</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.err.println("Usage: WindowWorldCount &lt;filename&gt; &lt;port&gt; &lt;windowDuration&gt; &lt;slideDuration&gt;")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">      </span>System.exit(1)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val conf = new SparkConf().setAppName("WindowWordCount").setMaster("local[2]")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val sc = new SparkContext(conf)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 创建StreamingContext</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val ssc = new StreamingContext(sc, Seconds(5))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// 定义checkpoint目录为当前目录</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.checkpoint(".")</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span></em><span style="color:rgb(192,0,0)"><em>// </em></span>通过Socket获取数据，该处需要提供Socket的主机名和端口号，数据保存在内存和硬盘中</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_ONLY_SER)</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val words = lines.flatMap(_.split(","))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">   </span></em><span style="color:rgb(192,0,0)"><em> </em></span>// windows操作，第一种方式为叠加处理，第二种方式为增量处理</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>val wordCounts = words.map(x =&gt; (x , 1)).reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b), Seconds(args(2).toInt), Seconds(args(3).toInt))</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(192,0,0)">//val wordCounts = words.map(x =&gt; (x , 1)).reduceByKeyAndWindow(_+_, _-_,Seconds(args(2).toInt), Seconds(args(3).toInt))</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)"> </span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>wordCounts.print()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.start()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">    </span>ssc.awaitTermination()</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">  </span>}</em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">}</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412582536007.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image062" src="http://images0.cnblogs.com/blog/107289/201508/211412591758921.jpg" alt="clip_image062" width="662" height="256" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058843"></a><span style="color:rgb(0,0,0)">1.6.3 </span>运行代码</h3>
<p align="justify" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   启动流数据模拟器</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">启动4.1</span>打包好的流数据模拟器，在该实例中将定时发送/home/hadoop/upload/class7目录下的people.txt数据文件（该文件可以在本系列配套资源目录/data/class7中找到），其中people.txt数据内容如下：</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211412598312523.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image026[2]" src="http://images0.cnblogs.com/blog/107289/201508/211413003784623.jpg" alt="clip_image026[2]" width="384" height="74" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">模拟器Socket</span>端口号为9999，频度为1秒</p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$cd /app/hadoop/spark-1.1.0</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><em><span style="color:rgb(4,150,21)">$java -cp LearnSpark.jar class7.StreamingSimulation /home/hadoop/upload/class7/people.txt 9999 1000</span></em></p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413023785967.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image028[2]" src="http://images0.cnblogs.com/blog/107289/201508/211413052697684.jpg" alt="clip_image028[2]" width="663" height="111" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在没有程序连接时，该程序处于阻塞状态，</span>在IDEA中运行Streaming程序</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>中运行该实例，该实例需要配置连接Socket主机名和端口号，在这里配置参数机器名为hadoop1、端口号为9999、时间窗口为30秒和滑动时间间隔10秒</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413080504200.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image064" src="http://images0.cnblogs.com/blog/107289/201508/211413102535073.jpg" alt="clip_image064" width="663" height="238" border="0"></a></p>
<h3 align="justify" style="color:rgb(57,57,57)"><a target="_blank" name="_Toc427058844"></a><span style="color:rgb(0,0,0)">1.6.4 </span>查看结果</h3>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第一步</span>   IDEA运行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在IDEA</span>的运行窗口中，可以观察到第一次运行统计单词总数为4，第二次为14，第N次为10(N-1)+4，即统计单词的总数为程序运行单词数总和。</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413108168645.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image066" src="http://images0.cnblogs.com/blog/107289/201508/211413111757987.jpg" alt="clip_image066" width="663" height="233" border="0"></a></p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">第二步</span>   在监控页面观察执行情况</p>
<p align="left" style="color:rgb(57,57,57)"><span style="color:rgb(0,0,0)">在webUI</span>上监控作业运行情况，可以观察到每10秒运行一次作业</p>
<p align="left" style="color:rgb(57,57,57)"><a target="_blank" href="http://images0.cnblogs.com/blog/107289/201508/211413117856574.jpg" rel="nofollow" style="color:rgb(100,102,179)"><img title="clip_image068" src="http://images0.cnblogs.com/blog/107289/201508/211413130356289.jpg" alt="clip_image068" width="657" height="264" border="0"></a></p>
<br>
</div>
</div>
            </div>
                </div>