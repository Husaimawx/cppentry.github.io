---
layout:     post
title:      Hive入门
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p>HIve总结：</p>

<p>首先要学习Hive，第一步是了解Hive，Hive是基于Hadoop的一个数据仓库，可以将结构化的数据文件映射为一张表，并提供类sql查询功能，Hive底层将sql语句转化为mapreduce任务运行。相对于用java代码编写mapreduce来说，Hive的优势明显：快速开发，人员成本低，可扩展性（自由扩展集群规模），延展性（支持自定义函数）。</p>

<p>Hive的构架：</p>

<p><img alt="" class="has" src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205214177-782883965.png"></p>

<p> </p>

<p>Hive提供了三种用户接口：CLI、HWI和客户端。客户端是使用JDBC驱动通过thrift，远程操作Hive。HWI即提供Web界面远程访问Hive。但是最常见的使用方式还是使用CLI方式。（在linux终端操作Hive）</p>

<p>Hive有三种安装方式：</p>

<p>1、内嵌模式（元数据保村在内嵌的derby种，允许一个会话链接，尝试多个会话链接时会报错，不适合开发环境）</p>

<p> 2、本地模式（本地安装mysql 替代derby存储元数据）</p>

<p> 3、远程模式（远程安装mysql 替代derby存储元数据）</p>

<p>安装Hive：（本地模式）</p>

<p>首先Hive的安装是在Hadoop集群正常安装的基础上，并且集群启动</p>

<p>  安装Hive之前我们要先安装mysql，</p>

<p>  查看是否安装过mysql：rpm -qa|grep mysql*  </p>

<p>  查看有没有安装包：yum list mysql* </p>

<p>  安装mysql客户端：yum install -y mysql</p>

<p>  安装服务器端：yum install -y mysql-server </p>

<p>                yum install -y mysql-devel</p>

<p>  启动数据库 service mysqld start或者/etc/init.d/mysqld start</p>

<p>  创建hadoop用户并赋予权限：</p>

<p>  mysql&gt;grant all on *.* to hadoop@'%' identified by 'hadoop';</p>

<p>  mysql&gt;grant all on *.* to hadoop@'localhost' identified by 'hadoop';</p>

<p>  mysql&gt;grant all on *.* to hadoop@'master' identified by 'hadoop';</p>

<p>  mysql&gt;flush privileges;</p>

<p>  然后在Hive官网上下载需要的版本，hive.apache.org  <a href="http://archiv.apache.org/" rel="nofollow">archive.apache.org</a></p>

<p>解压：tar -zxvf apache-hive-1.2.1-bin.tar.gz</p>

<p>配置：cd /apache-hive-1.2.1-bin/conf/  vim hive-site.xml</p>

<p>    &lt;?xml version="1.0"?&gt;</p>

<p>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</p>

<p>&lt;configuration&gt;</p>

<p>    &lt;property&gt;</p>

<p>        &lt;name&gt;hive.metastore.local&lt;/name&gt;</p>

<p>        &lt;value&gt;true&lt;/value&gt;</p>

<p>    &lt;/property&gt;</p>

<p>    &lt;property&gt;</p>

<p>        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</p>

<p>         &lt;value&gt;jdbc:mysql://master:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</p>

<p>    &lt;/property&gt;</p>

<p>    &lt;property&gt;</p>

<p>        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</p>

<p>        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</p>

<p>    &lt;/property&gt;</p>

<p>    &lt;property&gt;</p>

<p>        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</p>

<p>        &lt;value&gt;hadoop&lt;/value&gt;</p>

<p>    &lt;/property&gt;</p>

<p>    &lt;property&gt;</p>

<p>        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</p>

<p>        &lt;value&gt;hadoop&lt;/value&gt;</p>

<p>    &lt;/property&gt;</p>

<p>&lt;/configuration&gt;</p>

<p>复制依赖包：cp mysql-connector-java-5.1.43-bin.jar apache-hive-1.2.1-bin/lib/</p>

<p>配置环境变量：</p>

<p>export HIVE_HOME=$PWD/apache-hive-1.2.1-bin</p>

<p>export PATH=$PATH:$HIVE_HOME/bin</p>

<p>启动hive：hive</p>

<p>hive中可以运行shell命令:! shell命令</p>

<p> <img alt="" class="has" src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205233131-1045638578.png"></p>

<p><img alt="" class="has" src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205328037-247600045.png"></p>

<p> </p>

<p> </p>

<p>hive中可以运行hadoop命令：</p>

<p><img alt="" class="has" src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017205341256-973417976.png"></p>

<p> </p>

<p>hive中的数据类型：</p>

<p>原子数据类型：TINYINT SMALLINT INT BIGINT FLOAT DOUBLE BOOLEAN STRING </p>

<p>复杂数据类型：STRUCT MAP ARRAY </p>

<p>hive的使用：</p>

<p>建表语句：</p>

<p><strong>DDL：</strong></p>

<p>创建内部表：</p>

<p>create table mytable(</p>

<p>id int, </p>

<p>name string) </p>

<p>row format delimited fields terminated by '\t' stored as textfile;</p>

<p>常见外部表：关键字 external</p>

<p>create external table mytable2(<br>
id int, <br>
name string)<br>
row format delimited fields terminated by '\t' location '/user/hive/warehouse/mytable2';</p>

<p>创建分区表：分区字段要写在partiton by（）</p>

<p>create table mytable3(<br>
id int, <br>
name string)<br>
partitioned by(sex string) row format delimited fields terminated by '\t'stored as textfile;</p>

<p>静态分区插入数据</p>

<p>load data local inpath '/root/hivedata/boy.txt' overwrite into table mytable3 partition(sex='boy');</p>

<p>增加分区：</p>

<p>alter table mytable3 add partition (sex='unknown') location '/user/hive/warehouse/mytable3/sex=unknown';</p>

<p>删除分区：alter table mytable3 drop if exists partition(sex='unknown');</p>

<p>分区表默认为静态分区，可转换为自动套分区</p>

<p>set hive.exec.dynamic.partition=true;</p>

<p>set hive.exec.dynamic.partition.mode=nonstrict;</p>

<p>给分区表灌入数据：</p>

<p>insert into table mytable3 partition (sex) select id,name,'boy' from student_mdf;</p>

<p>查询表分区：show partitions mytable3;</p>

<p>查询分区表数据：select * from mytable3;</p>

<p>查询表结构：desc mytable3;</p>

<p><strong>DML:</strong></p>

<p>重命名表：alter table student rename to student_mdf</p>

<p>增加列：alter table student_mdf add columns (sex string);</p>

<p>修改列名：alter table student_mdf change sex gender string;</p>

<p>替换列结构：alter table student_mdf replace columns (id string, name string);</p>

<p>装载数据：（本地数据）load data local inpath '/home/lym/zs.txt' overwrite into student_mdf;</p>

<p>                   （HDFS数据）load data inpath '/zs.txt' into table student_mdf;</p>

<p>插入一条数据：insert into table student_mdf values('1','zhangsan');</p>

<p>创建表接收查询结果：create table mytable5 as select id, name from mytable3;</p>

<p>导出数据：（导出到本地）insert overwrite local directory '/root/hivedata/mytable5.txt' select * from mytable5;</p>

<p>                   （导出到HDFS）</p>

<p>insert overwrite directory 'hdfs://master:9000/user/hive/warehouse/mytable5_load' select * from mytable5;</p>

<p><strong>数据查询：</strong></p>

<p> select * from mytable3;  查询全表</p>

<p>select uid,uname from student; 查询学生表中的学生姓名与学号字段</p>

<p>select  uname,count(*) from student group by uname; 统计学生表中每个名字的个数</p>

<p>常用的功能还有 having、order by、sort by、distribute by、cluster by；等等</p>

<p>关联查询中有</p>

<p>内连接：将符合两边连接条件的数据查询出来</p>

<p>select * from t_a a inner join t_b b on a.id=b.id;</p>

<p>左外连接：以左表数据为匹配标准，右边若匹配不上则数据显示null</p>

<p>select * from t_a a left join t_b b on a.id=b.id;</p>

<p>右外连接：与左外连接相反</p>

<p>select * from t_a a right join t_b b on a.id=b.id;</p>

<p>左半连接：左半连接会返回左边表的记录，前提是其记录对于右边表满足on语句中的判定条件。</p>

<p>select * from t_a a left semi join t_b b on a.id=b.id;</p>

<p>全连接(full outer join)：</p>

<p>select * from t_a a full join t_b b on a.id=b.id;</p>

<p>in/exists关键字(1.2.1之后新特性)：效果等同于left semi join</p>

<p>select * from t_a a where a.id in (select id from t_b);</p>

<p>select * from t_a a where exists (select * from t_b b where a.id = b.id);</p>

<p><strong>shell操作Hive指令：</strong></p>

<p>-e：从命令行执行指定的HQL:</p>

<p><img alt="" class="has" src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017212953162-438605194.png"></p>

<p> </p>

<p>-f：执行HQL脚本</p>

<p>-v：输出执行的HQL语句到控制台</p>

<p><strong><strong> <img alt="" class="has" src="https://images2017.cnblogs.com/blog/1233200/201710/1233200-20171017212813537-1403384921.png"></strong></strong></p>

<p><strong> 内置函数</strong></p>

<p> 查看内置函数：show functions;</p>

<p>显示函数的详细信息：DESC FUNCTION abs;</p>

<p> 重要常用内置函数：sum()--求和        count()--求数据量             avg()--求平均值</p>

<p><em id="__mceDel"><em><em><em>                                  distinct--去重       </em></em></em></em><em><em><em><em><em>min--求最小值               </em></em></em></em></em><em><em><em><em><em><em>max--求最大值</em></em></em></em></em></em></p>

<p> 自定义函数：</p>

<p>1.先开发一个简单的Java类，org.apache.hadoop.hive.ql.exec.UDF，重载evaluate方法</p>

<p><em>import org.apache.hadoop.hive.ql.exec.UDF;</em></p>

<p><em><em>public final class AddUdf extends UDF {</em></em></p>

<p><em><em>public Integer evaluate(Integer a, Integer b) {</em></em></p>

<p><em><em>if (null == a || null == b) {</em></em></p>

<p><em><em>return null;</em></em></p>

<p><em><em>} return a + b;</em></em></p>

<p><em><em>}</em></em></p>

<p><em><em>public Double evaluate(Double a, Double b) {</em></em></p>

<p><em><em>if (a == null || b == null)</em></em></p>

<p><em><em>return null;</em></em></p>

<p><em><em>return a + b;</em></em><em><em><em>}</em></em></em></p>

<p><em><em><em><em>}</em></em></em></em></p>

<p> 2.打成jar包上传到服务器</p>

<p> 3.将jar包添加到hive  add jar /home/lan/jar/addudf.jar;</p>

<p> 4.创建临时函数与开发好的class关联起来 </p>

<p>     CREATE TEMPORARY FUNCTION add_example AS 'org.day0914.AddUdf';</p>

<p> 5.使用自定义函数  SELECT add_example(scores.math, scores.art) FROM scores;</p>

<p> 销毁临时函数：DROP TEMPORARY FUNCTION add_example;</p>

<p><strong> Hive相关工具：Sqoop Azkaban Flume</strong></p>

<p><strong>Sqoop</strong></p>

<p>Sqoop是一个开源工具，它允许用户将数据从关系型数据库抽取到Hadoop中，用于进一步的处理。抽取出的数据可以被MapReduce程序使用，也可以被其他类似于Hive的工具使用。一旦形成分析结果，Sqoop便可以将这些结果导回数据库，供其他客户端使用</p>

<p>用户向 Sqoop 发起一个命令之后，这个命令会转换为一个基于 Map Task 的 MapReduce 作业。Map Task 会访问数据库的元数据信息，通过并行的 Map Task 将数据库的数据读取出来，然后导入 Hadoop 中。 将 Hadoop 中的数据，导入传统的关系型数据库中。它的核心思想就是通过基于 Map Task （只有 map）的 MapReduce 作业，实现数据的并发拷贝和传输，这样可以大大提高效率</p>

<p>数据导入：首先用户输入一个 Sqoop import 命令，Sqoop 会从关系型数据库中获取元数据信息，比如要操作数据库表的 schema是什么样子，这个表有哪些字段，这些字段都是什么数据类型等。它获取这些信息之后，会将输入命令转化为基于 Map 的 MapReduce作业。这样 MapReduce作业中有很多 Map 任务，每个 Map 任务从数据库中读取一片数据，这样多个 Map 任务实现并发的拷贝，把整个数据快速的拷贝到 HDFS 上</p>

<p>数据导出：首先用户输入一个 Sqoop export 命令，它会获取关系型数据库的 schema，建立 Hadoop 字段与数据库表字段的映射关系。 然后会将输入命令转化为基于 Map 的 MapReduce作业，这样 MapReduce作业中有很多 Map 任务，它们并行的从 HDFS 读取数据，并将整个数据拷贝到数据库中</p>

<p>sqoop查询语句</p>

<p>进入sqoop安装主目录：cd /home/lanou/sqoop-1.4.5.bin__hadoop-2.0.4-alpha</p>

<p>将HDFS上的数据导入到MySQL中：bin/sqoop export --connect jdbc:mysql://192.168.2.136:3306/test --username hadoop --password hadoop --table name_cnt --export-dir '/user/hive/warehouse/mydb.db/name_cnt' --fields-terminated-by '\t'</p>

<p>将MySQL中的数据导入到HDFS上：</p>

<p>bin/sqoop import --connect jdbc:mysql://192.168.2.136:3306/test --username hadoop --password hadoop --table name_cnt -m 1</p>

<p>sqoop：表示sqoop命令 <br>
export：表示导出 <br>
--connect jdbc:mysql://192.168.2.136:3306/test ：表示告诉jdbc，连接mysql的url。<br>
--username hadoop: 连接mysql的用户名 <br>
--password hadoop: 连接mysql的密码<br>
--table name_cnt: 从mysql要导出数据的表名称 <br>
--export-dir '/user/hive/warehouse/mydb.db/name_cnt' hive中被导出的文件 <br>
--fields-terminated-by '\t': 指定输出文件中的行的字段分隔符   </p>            </div>
                </div>