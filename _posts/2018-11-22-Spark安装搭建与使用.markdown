---
layout:     post
title:      Spark安装搭建与使用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1 id="spark安装搭建与使用">Spark安装搭建与使用</h1>

<p></p><div class="toc">
<ul>
<li><a href="#spark%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8" rel="nofollow" target="_blank">Spark安装搭建与使用</a><ul>
<li><a href="#spark%E7%AE%80%E4%BB%8B" rel="nofollow" target="_blank">Spark简介</a></li>
<li><a href="#spark%E6%A1%86%E6%9E%B6" rel="nofollow" target="_blank">Spark框架</a></li>
<li><a href="#spark%E5%B8%B8%E8%A7%81api%E5%8A%9F%E8%83%BD" rel="nofollow" target="_blank">Spark常见API功能</a></li>
<li><a href="#spark-rdd" rel="nofollow" target="_blank">Spark RDD</a></li>
<li><a href="#spark%E4%B8%8B%E8%BD%BD%E4%B8%8E%E5%AE%89%E8%A3%85" rel="nofollow" target="_blank">Spark下载与安装</a></li>
<li><a href="#spark-windows%E9%9B%86%E6%88%90%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA" rel="nofollow" target="_blank">Spark windows集成开发环境搭建</a></li>
<li><a href="#%E5%88%A9%E7%94%A8spark%E8%B0%83%E7%94%A8ansj%E8%BF%9B%E8%A1%8C%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D" rel="nofollow" target="_blank">利用Spark调用Ansj进行中文分词</a></li>
</ul>
</li>
</ul>
</div>




<h2 id="spark简介">Spark简介</h2>

<p>Apache Spark是一个开源分布式的数据处理平台，支持集群进行数据处理。类似于hadoop，却又能提供灵活的编程接口（而不是map和reduce过程）。目前Spark的API支持Java, Python, Scala和R语言。</p>



<h2 id="spark框架">Spark框架</h2>

<p><img src="https://img-blog.csdn.net/20171028153159057?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Spark框架" title=""></p>

<ul>
<li><p>Spark Core : 框架核心组件，提供内存管理、任务调度等功能，并对第三方存储系统（例如HDFS,S3等）提供接口支持。同样也提供RDD （弹性分布式数据集，Spark特有的存储单元）API。</p></li>
<li><p>Spark SQL : 提供结构化或半结构化的数据处理。支持用户在Spark内使用SQL进行数据访问，包括Hive datasets, JSON,JDBC等。</p></li>
<li><p>Spark Streaming : 提供流式处理</p></li>
<li><p>MLib : 提供机器学习算法库</p></li>
<li><p>GraphX: 提供图计算。（PageRank等算法）</p></li>
</ul>



<h2 id="spark常见api功能">Spark常见API功能</h2>

<p>两种操作类型：Transformation 和Action <br>
Transformation是一种操作RDD数据集的指令，会创建新的RDD进行数据存储，但不会反馈结果给主程序。可以理解为数据的操作。常见的Transformation接口有： <br>
<img src="https://img-blog.csdn.net/20171028155222460?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Trans" title=""> <br>
Action是会产生返回结果给主程序的。常见的Action指令有： <br>
<img src="https://img-blog.csdn.net/20171028155446528?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Action" title=""></p>

<ul>
<li>map函数功能</li>
</ul>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># map</span>
<span class="hljs-preprocessor"># sc = spark context, parallelize creates an RDD from the passed object</span>
<span class="hljs-built_in">x</span> = sc<span class="hljs-preprocessor">.parallelize</span>([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
<span class="hljs-built_in">y</span> = <span class="hljs-built_in">x</span><span class="hljs-preprocessor">.map</span>(lambda <span class="hljs-built_in">x</span>: (<span class="hljs-built_in">x</span>,<span class="hljs-built_in">x</span>**<span class="hljs-number">2</span>))

<span class="hljs-preprocessor"># collect copies RDD elements to a list on the driver</span>
print(<span class="hljs-built_in">x</span><span class="hljs-preprocessor">.collect</span>()) 
print(<span class="hljs-built_in">y</span><span class="hljs-preprocessor">.collect</span>())

[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
[(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">9</span>)]</code></pre>

<ul>
<li>flatMap功能</li>
</ul>



<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># flatMap</span>
<span class="hljs-built_in">x</span> = sc<span class="hljs-preprocessor">.parallelize</span>([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
<span class="hljs-built_in">y</span> = <span class="hljs-built_in">x</span><span class="hljs-preprocessor">.flatMap</span>(lambda <span class="hljs-built_in">x</span>: (<span class="hljs-built_in">x</span>, <span class="hljs-number">100</span>*<span class="hljs-built_in">x</span>, <span class="hljs-built_in">x</span>**<span class="hljs-number">2</span>))
print(<span class="hljs-built_in">x</span><span class="hljs-preprocessor">.collect</span>())
print(<span class="hljs-built_in">y</span><span class="hljs-preprocessor">.collect</span>())

[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
[<span class="hljs-number">1</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">200</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">300</span>, <span class="hljs-number">9</span>]</code></pre>

<p>详情可以参见：<a href="https://www.iteblog.com/archives/1395.html" rel="nofollow" target="_blank">https://www.iteblog.com/archives/1395.html</a></p>



<h2 id="spark-rdd">Spark RDD</h2>

<p>RDD: Resilient Distributed Datasets <br>
Spark在集群上的数据集。支持数据在不同机器上分片存储。具体如下所示： <br>
<img src="https://img-blog.csdn.net/20171028160152398?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="RDD" title=""> <br>
提供多种接口，支持聪其它数据源获取数据，转换为RDD <br>
<em>hadoopFile()</em> : 从Hadoop文件转换为RDD <br>
<em>objectFile()</em> :序列化数据 <br>
<em>textFile()</em> : HDFS 或本地文件</p>



<h2 id="spark下载与安装">Spark下载与安装</h2>

<p>下载地址： <a href="http://spark.apache.org/downloads.html" rel="nofollow" target="_blank">http://spark.apache.org/downloads.html</a> <br>
Spark有多种运行方式。此处强调本地模式（Standalone），可以使用Hadoop本地模式，也可以不使用Hadoop。即Spark的运行可以独立Hadoop进行。 <br>
支持的语言版本： <br>
（1）Java 7+ <br>
（2）Python 2.6+ <br>
（3）R 3.1+ <br>
（4） Scala 2.10+ <br>
下载后解压文件，cmd下进入解压文件目录，即可运行(要求安装了Python)： <br>
<img src="https://img-blog.csdn.net/20171028161253081?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Spark" title=""> <br>
<strong>说明：虽然不需要Hadoop，可以直接使用Spark，但是本地还需要配置HADOOP_HOME，添加winutils.exe文件，具体文件下载可以自行搜索。</strong></p>



<h2 id="spark-windows集成开发环境搭建">Spark windows集成开发环境搭建</h2>

<p>选择使用Intellij IDEA集成开发环境，选用Scala语言进行搭建。 <br>
下载IDEA(社区免费版本)<a href="http://www.jetbrains.com/idea/download/" rel="nofollow" target="_blank">http://www.jetbrains.com/idea/download/</a> <br>
<img src="https://img-blog.csdn.net/20171028162206223?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="IDEA" title=""> <br>
IDEA工程里library添加Spark的Jar包即可</p>



<h2 id="利用spark调用ansj进行中文分词">利用Spark调用Ansj进行中文分词</h2>

<ul>
<li><p>Ansj是一个基于java实现的n-Gram+CRF+HMM的中文分词工具，使用说明和源码参见：<a href="https://github.com/NLPchina/ansj_seg" rel="nofollow" target="_blank">https://github.com/NLPchina/ansj_seg</a></p></li>
<li><p>添加Ansj的Jar包 <br>
<img src="https://img-blog.csdn.net/20171028162645863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVzdGNoZW56ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Ansj" title=""></p></li>
</ul>

<p>主要代码如下：</p>



<pre class="prettyprint"><code class=" hljs livecodeserver">      val <span class="hljs-keyword">lines</span> = sc.textFile(doc, <span class="hljs-number">1</span>).map { x =&gt;
      val temp = ToAnalysis.parse(x).getTerms()
      val <span class="hljs-built_in">word</span> = <span class="hljs-keyword">for</span>(i&lt;-Range(<span class="hljs-number">0</span>,temp.size())) yield temp.<span class="hljs-built_in">get</span>(i).getName()
      <span class="hljs-built_in">word</span>
    }
      val pairs = <span class="hljs-keyword">lines</span>.flatMap(<span class="hljs-keyword">words</span> =&gt; <span class="hljs-keyword">words</span>.map(<span class="hljs-built_in">word</span> =&gt;(<span class="hljs-built_in">word</span>,<span class="hljs-number">1</span>)))
</code></pre>            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>