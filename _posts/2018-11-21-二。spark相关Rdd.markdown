---
layout:     post
title:      二。spark相关Rdd
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                Spark<br>--------------------<br><span>	</span>通用性。<br><br>Spark模块<br>-------------<br><span>	</span>Spark Core<span>			</span>//核心库<br><span>	</span>Spark SQL<span>			</span>//SQL<br><span>	</span>Spark Streaming<span>		</span>//准实时计算。<br><span>	</span>Spark MLlib<span>			</span>//机器学习库<br><span>	</span>Spark graph<span>			</span>//图计算<br><br>Spark集群运行<br>--------------------<br><span>	</span>1.local<span>			</span>//本地模式<br><span>	</span>2.standalone<span>	</span>//独立模式<br><span>	</span>3.yarn<span>			</span>//yarn模式<br><span>	</span>4.mesos<span>			</span>//mesos<br><br>start-all.sh<br>------------------<br><span>	</span>start-master.sh<span>	</span>//RPC端口 7077<br><span>	</span>start-slave.sh<span>	</span>spark://s201:7077<br><br>webui<br>------------------<br><span>	</span>http://s201:8080<br><br>添加针对scala文件的编译插件<br>------------------------------<br><span>	</span>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br><span>	</span>&lt;project xmlns="http://maven.apache.org/POM/4.0.0"<br><span>			</span> xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"<br><span>			</span> xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;<br><span>		</span>&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;<br><br><span>		</span>&lt;groupId&gt;com.it18zhang&lt;/groupId&gt;<br><span>		</span>&lt;artifactId&gt;SparkDemo1&lt;/artifactId&gt;<br><span>		</span>&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;<br><br><span>		</span>&lt;build&gt;<br><span>			</span>&lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt;<br><span>			</span>&lt;plugins&gt;<br><span>				</span>&lt;plugin&gt;<br><span>					</span>&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;<br><span>					</span>&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;<br><span>					</span>&lt;configuration&gt;<br><span>						</span>&lt;source&gt;1.8&lt;/source&gt;<br><span>						</span>&lt;target&gt;1.8&lt;/target&gt;<br><span>					</span>&lt;/configuration&gt;<br><span>				</span>&lt;/plugin&gt;<br><span>				</span>&lt;plugin&gt;<br><span>					</span>&lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;<br><span>					</span>&lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;<br><span>					</span>&lt;version&gt;3.2.2&lt;/version&gt;<br><span>					</span>&lt;configuration&gt;<br><span>						</span>&lt;recompileMode&gt;incremental&lt;/recompileMode&gt;<br><span>					</span>&lt;/configuration&gt;<br><span>					</span>&lt;executions&gt;<br><span>						</span>&lt;execution&gt;<br><span>							</span>&lt;goals&gt;<br><span>								</span>&lt;goal&gt;compile&lt;/goal&gt;<br><span>								</span>&lt;goal&gt;testCompile&lt;/goal&gt;<br><span>							</span>&lt;/goals&gt;<br><span>						</span>&lt;/execution&gt;<br><span>					</span>&lt;/executions&gt;<br><span>				</span>&lt;/plugin&gt;<br><span>			</span>&lt;/plugins&gt;<br><span>		</span>&lt;/build&gt;<br><br><span>		</span>&lt;dependencies&gt;<br><span>			</span>&lt;dependency&gt;<br><span>				</span>&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br><span>				</span>&lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br><span>				</span>&lt;version&gt;2.1.0&lt;/version&gt;<br><span>			</span>&lt;/dependency&gt;<br><span>		</span>&lt;/dependencies&gt;<br><span>	</span>&lt;/project&gt;<br><br><br>C:\Users\Administrator\.m2\repository\net<br>C:\Users\Administrator\.m2\repository\net\alchim31\maven\...<br><br>SparkContext:<br>------------------<br><span>	</span>Spark集群的连接。主要入口点。<br><span>	</span>SparkConf = new ();<br><span>	</span>conf.setApp("")<br><span>	</span>conf.setMaster("local") ;<br><span>	</span>sc = new SparkContext(conf);<br><span>	</span>//RDD : Resilient distributed dataset,弹性分布式数据集。<br><span>	</span>val rdd1 = sc.textFile("d:/scala/test.txt");<br><span>	</span>val rdd2 = rdd1.flatMap(line=&gt;line.split(" "));<br><span>	</span>val rdd3 = rdd2.map(word=&gt;(word,1));<br><span>	</span>val rdd4 = rdd3.reduceByKey(_ + _) ;<br><span>	</span>val list = rdd4.collect()<br><span>	</span>list.foreach(e=&gt;println(e));<br><br><br><span>	</span>//<br><span>	</span>sc.textFile("d:/scala").flatMap(_.split(" ")).map((_1)).reduceByKey(_ + _).collect().foreach(println)<br><br>spark<br>-------------<br><span>	</span>基于hadoop的mr，扩展MR模型高效使用MR模型，内存型集群计算，提高app处理速度。<br><br>spark特点<br>-------------<br><span>	</span>速度:在内存中存储中间结果。<br><span>	</span>支持多种语言.<br><span>	</span>内置了80+的算子.<br><span>	</span>高级分析:MR，SQL/ Streamming /mllib / graph<br><br><br>spark模块<br><span>	</span>core<span>		</span>//通用执行引擎，提供内存计算和对外部数据集的引用。<br><span>	</span>SQL<span>			</span>//构建在core之上，引入新的抽象SchemaRDD，提供了结构化和半结构化支持。<br><span>	</span>Streaming<span>	</span>//小批量计算，RDD.<br><span>	</span>MLlib<span>		</span>//机器学习库。core在。<br><span>	</span>Graphx<span>		</span>//图计算。<br><br>RDD:<br>----------------<br><span>	</span>是spark的基本数据结构，是不可变数据集。RDD中的数据集进行逻辑分区，每个分区可以单独在集群节点<br><span>	</span>进行计算。可以包含任何java,scala，python和自定义类型。<br><br><span>	</span>RDD是只读的记录分区集合。RDD具有容错机制。<br><br><span>	</span>创建RDD方式，一、并行化一个现有集合。<br><br><span>	</span>hadoop 花费90%时间用户rw。<br><span>	</span><br><span>	</span>内存处理计算。在job间进行数据共享。内存的IO速率高于网络和disk的10 ~ 100之间。<br><br><span>	</span>内部包含5个主要属性<br><span>	</span>-----------------------<br><span>	</span>1.分区列表<br><span>	</span>2.针对每个split的计算函数。<br><span>	</span>3.对其他rdd的依赖列表<br><span>	</span>4.可选，如果是KeyValueRDD的话，可以带分区类。<br><span>	</span>5.可选，首选块位置列表(hdfs block location);<br><br>//默认并发度<br>local.backend.defaultParallelism() = scheduler.conf.getInt("spark.default.parallelism", totalCores)<br>taskScheduler.defaultParallelism = backend.defaultParallelism()<br>sc.defaultParallelism =...; taskScheduler.defaultParallelism<br>defaultMinPartitions = math.min(defaultParallelism, 2)<br>sc.textFile(path,defaultMinPartitions)<span>			</span>//1,2<br><br><br>RDD变换<br>------------------<br><span>	</span>返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。<br><span>	</span><br><span>	</span>map()<span>									</span>//对每个元素进行变换，应用变换函数<br><span>											</span>//(T)=&gt;V<br><span>	</span>filter()<span>								</span>//过滤器,(T)=&gt;Boolean<br><span>	</span>flatMap()<span>								</span>//压扁,T =&gt; TraversableOnce[U]<br><br><br><span>	</span>mapPartitions()<span>							</span>//对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。<br><span>											</span>//Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;<br><br><span>	</span>mapPartitionsWithIndex(func)<span>			</span>//同上，(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;<br><br><span>	</span>sample(withReplacement, fraction, seed)<span>	</span>//采样返回采样的RDD子集。<br><span>											</span>//withReplacement 元素是否可以多次采样.<br><span>											</span>//fraction : 期望采样数量.[0,1]<br><br><span>	</span>union()<span>									</span>//类似于mysql union操作。<br><span>											</span>//select * from persons where id &lt; 10 <br><span>											</span>//union select * from id persons where id &gt; 29 ;<br><br><span>	</span>intersection<span>							</span>//交集,提取两个rdd中都含有的元素。<br><span>	</span>distinct([numTasks]))<span>					</span>//去重,去除重复的元素。<br><br><br><span>	</span>groupByKey()<span>							</span>//(K,V) =&gt; (K,Iterable&lt;V&gt;)<br><br><br><span>	</span>reduceByKey(*)<span>							</span>//按key聚合。 <br><br><span>	</span>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])<br><span>											</span>//按照key进行聚合<br><span>	</span>key:String U:Int = 0<br><br><br><span>	</span>sortByKey<span>								</span>//排序<br><br><span>	</span>join(otherDataset, [numTasks])<span>			</span>//连接,(K,V).join(K,W) =&gt;(K,(V,W)) <br><br><br><span>	</span>cogroup<span>									</span>//协分组<br><span>											</span>//(K,V).cogroup(K,W) =&gt;(K,(Iterable&lt;V&gt;,Iterable&lt;!-- &lt;W&gt; --&gt;)) <br><span>	</span>cartesian(otherDataset)<span>					</span>//笛卡尔积,RR[T] RDD[U] =&gt; RDD[(T,U)]<br><br><br><span>	</span>pipe<span>									</span>//将rdd的元素传递给脚本或者命令，执行结果返回形成新的RDD<br><span>	</span>coalesce(numPartitions)<span>					</span>//减少分区<br><span>	</span>repartition<span>								</span>//可增可减<br><span>	</span>repartitionAndSortWithinPartitions(partitioner)<br><span>											</span>//再分区并在分区内进行排序<br><br><br>RDD Action<br>------------------<br><span>	</span>collect()<span>								</span>//收集rdd元素形成数组.<br><span>	</span>count()<span>									</span>//统计rdd元素的个数<br><span>	</span>reduce()<span>								</span>//聚合,返回一个值。<br><span>	</span>first<span>									</span>//取出第一个元素take(1)<br><span>	</span>take<span>									</span>//<br><span>	</span>takeSample (withReplacement,num, [seed])<br><span>	</span>takeOrdered(n, [ordering])<br><span>	</span><br><span>	</span>saveAsTextFile(path)<span>					</span>//保存到文件<br><span>	</span>saveAsSequenceFile(path)<span>				</span>//保存成序列文件<br><br><span>	</span>saveAsObjectFile(path) (Java and Scala)<br><br><span>	</span>countByKey()<span>							</span>//按照key,统计每个key下value的个数.<br><span>	</span><br><p><img src="https://img-blog.csdn.net/20180711170522813?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1ZVFpbmdGZW5nNDQ1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><br></p>spark集成hadoop ha<br>-------------------------<br><span>	</span>1.复制core-site.xml + hdfs-site.xml到spark/conf目录下<br><span>	</span>2.分发文件到spark所有work节点<br><span>	</span>3.启动spark集群<br><span>	</span>4.启动spark-shell,连接spark集群上<br><span>		</span>$&gt;spark-shell --master spark://s201:7077<br><span>		</span>$scala&gt;sc.textFile("hdfs://mycluster/user/centos/test.txt").collect();<span>	</span>            </div>
                </div>