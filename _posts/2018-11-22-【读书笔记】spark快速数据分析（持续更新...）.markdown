---
layout:     post
title:      【读书笔记】spark快速数据分析（持续更新...）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<p></p><div class="toc"><h3>文章目录</h3><ul><ul><li><a href="#spark_1" rel="nofollow">spark核心概念</a></li><li><a href="#RDD_12" rel="nofollow">RDD编程</a></li><ul><li><a href="#RDD_14" rel="nofollow">RDD支持的操作</a></li><li><a href="#RDD_27" rel="nofollow">RDD持久化方式</a></li><li><a href="#RDD_31" rel="nofollow">RDD的创建</a></li><li><a href="#RDD_35" rel="nofollow">RDD伪集合操作</a></li></ul><li><a href="#spark_43" rel="nofollow">向spark传参</a></li><ul><li><a href="#RDD_49" rel="nofollow">RDD常见操作</a></li><li><a href="#RDD_63" rel="nofollow">RDD持久化</a></li><li><a href="#PairRDD__67" rel="nofollow">PairRDD 转换操作</a></li></ul><li><a href="#spark_69" rel="nofollow">spark常见问题</a></li><ul><li><a href="#_70" rel="nofollow">数据倾斜</a></li></ul></ul></ul></div><p></p>
<h2><a id="spark_1"></a>spark核心概念</h2>
<p><code>drive program 驱动器程序</code></p>
<blockquote>
<p>当我们使用spark shell时，我们已经拥有了一个实际的驱动器即spark shell本身</p>
</blockquote>
<p><code>连接对象 SparkContext</code></p>
<blockquote>
<p>一个sparkcontent代表了对计算集群的一个连接，spark shell启动时已经自动创建了一个sparkcontext对象，sc对象</p>
</blockquote>
<p><code>executor 执行器</code></p>
<blockquote>
<p>执行器是具体集群中的计算节点</p>
</blockquote>
<h2><a id="RDD_12"></a>RDD编程</h2>
<blockquote>
<p>RDD(Resilient Distributed Dataset)弹性分布式数据集</p>
</blockquote>
<h3><a id="RDD_14"></a>RDD支持的操作</h3>
<ul>
<li>转换操作（transformation）</li>
</ul>
<blockquote>
<p>RDD是惰性求值，转换操作是不会立即进行计算的，只有在行动操作时才会进行计算</p>
</blockquote>
<ul>
<li>行动操作（action）</li>
</ul>
<blockquote>
<p>action 操作会产生结果或者将数据写入到其他地方，transformation操作则返回新的RDD对象</p>
</blockquote>
<ul>
<li>
<p>注意点1<br>
<code>默认情况下，spark的RDD会在你每次对它们进行行动操作时重新计算，如果想在多个操作中重用同一个RDD，可以使用RDD.persist()让spark把这个RDD缓存下来，即持久化</code></p>
</li>
<li>
<p>注意点2<br>
<code>在类似mapreduce的系统中，常常需要考虑如果把操作组合在一起，以减少mapreduce的周期数，但是在spark中，用户使用更小的操作来组织他们的程序，或许比写出复杂的map函数要更加能提升计算性能</code></p>
</li>
</ul>
<h3><a id="RDD_27"></a>RDD持久化方式</h3>
<ul>
<li>RDD.persist</li>
<li>RDD.cache()</li>
</ul>
<h3><a id="RDD_31"></a>RDD的创建</h3>
<ul>
<li>sc.parallelize()</li>
<li>sc.textFile()</li>
</ul>
<h3><a id="RDD_35"></a>RDD伪集合操作</h3>
<ul>
<li>union  和</li>
<li>intersection  相交</li>
<li>subtract</li>
<li>cartesian</li>
</ul>
<h2><a id="spark_43"></a>向spark传参</h2>
<ul>
<li>注意点1<br>
<code>传递函数时需要注意，python会将函数所在对象也序列化传出，当你传递的对象是某个对象的成员，或者包含了对某个对象中一个字段的引用时，如self.field spark就会把整个对象发到工作节点上，会导致传递的东西很大</code></li>
<li>注意点2<br>
<code>传递函数时，如果包含不知道如何序列化的对象，也会报错</code></li>
</ul>
<h3><a id="RDD_49"></a>RDD常见操作</h3>
<ul>
<li>map()</li>
<li>flatMap()</li>
<li>filter()</li>
<li>distinct()</li>
<li>sample() 采样</li>
<li>fold()</li>
<li>reduce()</li>
<li>aggregate()</li>
</ul>
<blockquote>
<p>aggregate 具有特殊性，它不要求返回值与RDD中的元素值的类型一致，而是可以自定义的传入期待返回的类型的初始值</p>
</blockquote>
<ul>
<li>collect() 返回RDD的数据</li>
</ul>
<h3><a id="RDD_63"></a>RDD持久化</h3>
<p><img src="./1535986164112.png" alt="Alt text"></p>
<h3><a id="PairRDD__67"></a>PairRDD 转换操作</h3>
<h2><a id="spark_69"></a>spark常见问题</h2>
<h3><a id="_70"></a>数据倾斜</h3>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>