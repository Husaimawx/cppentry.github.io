---
layout:     post
title:      大数据----【HDFS】
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h1><a id="HDFS_0"></a>HDFS入门</h1>
<h2><a id="1_HDFS_2"></a>1. HDFS基本概念</h2>
<h3><a id="11_HDFS_4"></a>1.1 HDFS介绍</h3>
<p>HDFS 是 Hadoop Distribute File System 的简称，意为：<code>Hadoop 分布式文件系统</code>。</p>
<p><code>分布式文件系统解决的问题就是大数据存储</code>。</p>
<h3><a id="12_HDFS_10"></a>1.2 HDFS设计目标</h3>
<ul>
<li>硬件故障是常态 , 因此故障的检测和自动快速恢复是HDFS的黑心架构目标</li>
<li>数据访问的高吞吐量</li>
<li>支持大文件</li>
<li>对文件的要求是 write-one-read-many 访问模型 , 即一次写入多次读取</li>
<li>移动计算的代价比移动数据的代价低</li>
<li>可移植性</li>
</ul>
<h2><a id="2__HDFS_19"></a>2.  HDFS重要特性</h2>
<ul>
<li>
<p>HDFS特性</p>
<ul>
<li>主从架构 , 各司其职</li>
<li>分块存储 hadoop 2.x默认是128M/块(1.x默认是64M)</li>
</ul>
<pre><code>如下例子
1.txt  100M  blk-1:0-100M 
2.txt  150M	 blk-1:0-128M  blk-2:129-150M
</code></pre>
<ul>
<li>副本机制 默认3副本 指的是最终有几个副本</li>
</ul>
</li>
</ul>
<p>主角色有自己的职责 : 管理文件系统目录树 文件块信息</p>
<p>从角色也有自己的职责 : 保存最终的块的存储</p>
<p>两者各司其职 , 共同对外提供了分布式文件存储的能力</p>
<p>hdfs职责概述图解如下</p>
<p><img src="https://img-blog.csdnimg.cn/20181115230908967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NvZGVyQm9vbQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<blockquote>
<p>namenode作为hdfs的老大 , 管理着文件系统的全部元数据</p>
<ul>
<li>目录树</li>
<li>文件跟块对应信息</li>
<li>datanode信息(dn是否存活 , 磁盘是否已满)</li>
</ul>
<p>访问 : hdfs://namenode_ip:9000</p>
</blockquote>
<p>Q1 : 分布式是用来干什么的 , 为什么使用分布式?</p>
<p>​	分布式是用来解决大数据的存储的问题 , 由于传统的文件存储模式固有的瓶颈 : 内存不足、磁盘不足 , 我们本能的采取缺啥补啥(加内存 , 加磁盘) , 但是这样的做法是有上限的 , 因此我们想到了增加机器 , 也就是分布式来解决这个问题.</p>
<p>Q2 : 为什么单独配置一台文件位置存储的服务器?</p>
<p>​	由于文件系统变成多台的时候 , 查询文件变得不那么方便了 , 因此我们专门部署一台服务器用于记录文件所在的位置信息 , 用<code>元数据</code>描述清文件所在的位置 , 这样使得查询文件变得方便且快速.</p>
<p>Q3 : 为什么使用分块存储?</p>
<p>​	由于文件的过大 , 会导致文件上传和下载变得耗时 , 因此我们采取分块存储的技术来实现存储的高效性 , 同时分块使得文件查询也变得有条不紊 .</p>
<p>Q4 : 为什么要备份(副本)?</p>
<p>​	为了防止机器出现故障 , 导致数据块的缺失 , 同时保证数据的安全性 , 同时也有一些弊端 , 备份越大 , 表明冗余越高 , 但是数据越安全 , 这是一个抉择问题.</p>
<blockquote>
<p>总结 : 分布式文件系统特征</p>
<p>如何解决文件存不下问题----&gt;分布式 , 多台机器</p>
<p>如何解决数据查找不方便的问题----&gt; 元数据记录 , 抽象成立于人理解的目录树</p>
<p>如何解决数据上传下载耗时问题----&gt;分块存储</p>
<p>如何解决数据丢失安全问题----&gt;备份 , 副本机制</p>
</blockquote>
<p>模拟实现分布式文件系统图解</p>
<p><img src="https://img-blog.csdnimg.cn/20181115230938182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NvZGVyQm9vbQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="3_HDFS_88"></a>3. HDFS基本操作</h2>
<h3><a id="31_Shell_90"></a>3.1 Shell命令行客户端</h3>
<p>方法 : hadoop fs <code>&lt;args&gt;</code>  args代表参数</p>
<p>URI 格式为 <code>scheme://authority/path</code>。对于 HDFS，该 scheme 是 hdfs，对于本地 FS，该 scheme 是 file。scheme 和 authority 是可选的。<code>如果未指定，则使用配置中指定的默认方案。</code></p>
<p>hdfs shell客户端</p>
<pre><code class="prism language-shell">hadoop fs -ls hdfs://node-1:9000/   操作hdfs文件系统
hadoop fs -ls file:///root   操作本地文件系统
hadoop fs -ls /  如果不指定 就采用默认文件系统  至于默认文件系统是谁 看配置文件fs.defaultFS
</code></pre>
<ul>
<li>hadoop  fs -put  local mubiao  把文件从本地文件系统复制到目标文件系统</li>
<li>hadoop  fs -get  mubiao  local  把目标文件复制到本地文件系统</li>
</ul>
<p>Q: 哪个是本地文件系统？（windows  linux  hdfs）</p>
<p>本地文件系统 指的是   执行命令所在机器的文件系统就是本地</p>
<p>比如：node-1 执行   从node-1的本地linux文件系统上传到hdfs分布式文件系统</p>
<pre><code class="prism language-shell">hadoop fs -put file:///root/install.log.syslog hdfs://node-1:9000/in
等价于
hadoop fs -put install.log.syslog /in
</code></pre>
<h3><a id="32_Shell_119"></a>3.2 Shell命令选项</h3>

<table>
<thead>
<tr>
<th align="center">选项名称</th>
<th align="center">使用格式</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">-ls</td>
<td align="center">-ls &lt;路径&gt;</td>
<td align="center">查看指定路径的当前目录结构</td>
</tr>
<tr>
<td align="center">-lsr</td>
<td align="center">-lsr &lt;路径&gt;</td>
<td align="center">递归查看指定路径的目录结构</td>
</tr>
<tr>
<td align="center">-du</td>
<td align="center">-du &lt;路径&gt;</td>
<td align="center">统计目录下个文件大小</td>
</tr>
<tr>
<td align="center">-dus</td>
<td align="center">-dus &lt;路径&gt;</td>
<td align="center">汇总统计目录下文件(夹)大小</td>
</tr>
<tr>
<td align="center">-count</td>
<td align="center">-count [-q] &lt;路径&gt;</td>
<td align="center">统计文件(夹)数量</td>
</tr>
<tr>
<td align="center">-mv</td>
<td align="center">-mv &lt;源路径&gt; &lt;目的路径&gt;</td>
<td align="center">移动</td>
</tr>
<tr>
<td align="center">-cp</td>
<td align="center">-cp &lt;源路径&gt; &lt;目的路径</td>
<td align="center">复制</td>
</tr>
<tr>
<td align="center">-rm</td>
<td align="center">-rm [-skipTrash] &lt;路径&gt;</td>
<td align="center">删除文件/空白文件夹</td>
</tr>
<tr>
<td align="center">-rmr</td>
<td align="center">-rmr [-skipTrash] &lt;路径&gt;</td>
<td align="center">递归删除</td>
</tr>
<tr>
<td align="center">-put</td>
<td align="center">-put &lt;多个 linux 上的文件&gt; &lt;hdfs 路径&gt;</td>
<td align="center">上传文件</td>
</tr>
<tr>
<td align="center">-copyFromLocal</td>
<td align="center">-copyFromLocal &lt;多个 linux 上的文件&gt;&lt;hdfs 路径&gt;</td>
<td align="center">从本地复制</td>
</tr>
<tr>
<td align="center">-moveFromLocal</td>
<td align="center">-moveFromLocal &lt;多个 linux 上的文件&gt;&lt;hdfs 路径&gt;</td>
<td align="center">从本地移动</td>
</tr>
<tr>
<td align="center">-getmerge</td>
<td align="center">-getmerge &lt;源路径&gt; &lt;linux 路径&gt;</td>
<td align="center">合并到本地</td>
</tr>
<tr>
<td align="center">-cat</td>
<td align="center">-cat &lt;hdfs 路径&gt;</td>
<td align="center">查看文件内容</td>
</tr>
<tr>
<td align="center">-text</td>
<td align="center">-text &lt;hdfs 路径&gt;</td>
<td align="center">查看文件内容</td>
</tr>
<tr>
<td align="center">-copyToLocal</td>
<td align="center"><code>-copyToLocal [-ignoreCrc] [-crc] [hdfs源路径] [linux 目的路径]</code></td>
<td align="center">从本地复制</td>
</tr>
<tr>
<td align="center">-moveToLocal</td>
<td align="center"><code>-moveToLocal [-crc] &lt;hdfs 源路径&gt; &lt;linux 目的路径&gt;</code></td>
<td align="center">从本地移动</td>
</tr>
<tr>
<td align="center">-mkdir</td>
<td align="center">-mkdir &lt;hdfs 路径&gt;</td>
<td align="center">创建空白文件夹</td>
</tr>
<tr>
<td align="center">-setrep</td>
<td align="center"><code>-setrep [-R] [-w] &lt;副本数&gt; &lt;路径&gt;</code></td>
<td align="center">修改副本数量</td>
</tr>
<tr>
<td align="center">-touchz</td>
<td align="center">-touchz &lt;文件路径&gt;</td>
<td align="center">创建空白文件</td>
</tr>
<tr>
<td align="center">-stat</td>
<td align="center">-stat [format] &lt;路径&gt;</td>
<td align="center">显示文件统计信息</td>
</tr>
<tr>
<td align="center">-tail</td>
<td align="center">-tail [-f] &lt;文件&gt;</td>
<td align="center">查看文件尾部信息</td>
</tr>
<tr>
<td align="center">-chmod</td>
<td align="center"><code>-chmod [-R] &lt;权限模式&gt; [路径]</code></td>
<td align="center">修改权限</td>
</tr>
<tr>
<td align="center">-chown</td>
<td align="center"><code>-chown [-R] [属主][:[属组]] 路径</code></td>
<td align="center">修改属主</td>
</tr>
<tr>
<td align="center">-chgrp</td>
<td align="center">-chgrp [-R] 属组名称 路径</td>
<td align="center">修改属组</td>
</tr>
<tr>
<td align="center">-help</td>
<td align="center">-help [命令选项]</td>
<td align="center">帮助</td>
</tr>
</tbody>
</table><h3><a id="33_Shell__153"></a>3.3 Shell 常用命令介绍</h3>
<ul>
<li>ls<br>
使用方法：hadoop fs -ls <code>[-h][-R] &lt;args&gt;</code><br>
功能：显示文件、目录信息。<br>
示例：<code>hadoop fs -ls /user/hadoop/file1</code>==&gt; 显示user/hadoop下的file1文件信息</li>
<li>mkdir<br>
使用方法：hadoop fs -mkdir [-p] <code>&lt;paths&gt;</code><br>
功能：在 hdfs 上创建目录，-p 表示会创建路径中的各级父目录。<br>
示例：<code>hadoop fs -mkdir –p /user/hadoop/dir1</code>==&gt; 在user/hadoop下创建dir1文件夹</li>
<li>put<br>
使用方法：<code>hadoop fs -put [-f][-p] [ -|&lt;localsrc1&gt; .. ]. &lt;dst&gt;</code><br>
功能：将单个 src 或多个 srcs 从本地文件系统复制到目标文件系统。<br>
-p：保留访问和修改时间，所有权和权限。<br>
-f：覆盖目的地（如果已经存在）<br>
示例：<code>hadoop fs -put -f startZk.sh /test</code>==&gt; 将linux本地的startZk.sh上传到hadoop根目录下的test目录下</li>
<li>get<br>
使用方法：<code>hadoop fs -get [-ignorecrc][-crc] [-p][-f] &lt;src&gt; &lt;localdst&gt;</code><br>
-ignorecrc：跳过对下载文件的 CRC 检查。<br>
-crc：为下载的文件写 CRC 校验和。<br>
功能：将文件复制到本地文件系统。<br>
示例：<code>hadoop fs -get /test/startZk.sh /root/test</code>==&gt; 将hadoop的test文件夹下的startZk.sh下载到本地root目录下的test目录下</li>
<li>appendToFile<br>
使用方法：<code>hadoop fs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</code><br>
功能：追加一个文件到已经存在的文件末尾<br>
示例：<code>hadoop fs -appendToFile 1.txt /test/2.txt</code>==&gt; 将本地1.txt内容追加到hadoop的test目录下的2.txt后</li>
<li>cat<br>
使用方法：<code>hadoop fs -cat [-ignoreCrc] URI [URI ...]</code><br>
功能：显示文件内容到 stdout<br>
示例：<code>hadoop fs -cat /test/2.txt</code>==&gt;显示2.txt的内容到stdout</li>
<li>tail<br>
使用方法：<code>hadoop fs -tail [-f] URI</code><br>
功能：将文件的最后一千字节内容显示到 stdout。<br>
-f 选项将在文件增长时输出附加数据。<br>
示例：<code>hadoop fs -tail /hadoop/hadoopfile</code></li>
<li>chgrp<br>
使用方法：<code>hadoop fs -chgrp [-R] GROUP URI [URI ...]</code><br>
功能：更改文件组的关联。用户必须是文件的所有者，否则是超级用户。<br>
-R 将使改变在目录结构下递归进行。<br>
示例：<code>hadoop fs -chgrp othergroup /hadoop/hadoopfile</code></li>
<li>chmod<br>
功能：改变文件的权限。使用-R 将使改变在目录结构下递归进行。<br>
示例：<code>hadoop fs -chmod 666 /hadoop/hadoopfile</code></li>
<li>chown<br>
功能：改变文件的拥有者。使用-R 将使改变在目录结构下递归进行。<br>
示例：<code>hadoop fs -chown someuser:somegrp /hadoop/hadoopfile</code></li>
<li>copyFromLocal<br>
使用方法：<code>hadoop fs -copyFromLocal &lt;localsrc&gt; URI</code><br>
功能：从本地文件系统中拷贝文件到 hdfs 路径去<br>
示例：<code>hadoop fs -copyFromLocal /root/1.txt /</code></li>
<li>copyToLocal<br>
功能：从 hdfs 拷贝到本地<br>
示例：<code>hadoop fs -copyToLocal /aaa/jdk.tar.gz</code></li>
<li>cp<br>
功能：从 hdfs 的一个路径拷贝 hdfs 的另一个路径<br>
示例： <code>hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2</code></li>
<li>mv<br>
功能：在 hdfs 目录中移动文件<br>
示例： <code>hadoop fs -mv /aaa/jdk.tar.gz /</code></li>
<li>getmerge<br>
功能：合并下载多个文件<br>
示例：比如 hdfs 的目录 /aaa/下有多个文件:log.1, log.2,log.3,…<br>
<code>hadoop fs -getmerge /aaa/log.* ./log.sum</code></li>
<li>rm<br>
功能：删除指定的文件。只删除非空目录和文件。-r 递归删除。<br>
示例：<code>hadoop fs -rm -r /aaa/bbb/</code></li>
<li>df<br>
功能：统计文件系统的可用空间信息<br>
示例：<code>hadoop fs -df -h /</code></li>
<li>du<br>
功能：显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。<br>
示例：hadoop fs -du /user/hadoop/dir1</li>
<li>setrep<br>
功能：改变一个文件的副本系数。-R 选项用于递归改变目录下所有文件的副本<br>
系数。<br>
示例：<code>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</code></li>
</ul>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>