---
layout:     post
title:      Hbase安装及使用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/zuochang_liu/article/details/81452124				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p id="main-toc"><strong>目录</strong></p>

<p id="-toc" style="margin-left:0px;"> </p>

<p id="%E4%B8%80%20%E4%BB%80%E4%B9%88%E6%98%AFHBASE-toc" style="margin-left:0px;"><a href="#%E4%B8%80%20%E4%BB%80%E4%B9%88%E6%98%AFHBASE" rel="nofollow">一 什么是HBASE</a></p>

<p id="%E4%BA%8C%20%E5%AE%89%E8%A3%85HBASE-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%20%E5%AE%89%E8%A3%85HBASE" rel="nofollow">二 安装HBASE</a></p>

<p id="%E4%B8%89%20hbase%E5%88%9D%E4%BD%93%E9%AA%8C-toc" style="margin-left:0px;"><a href="#%E4%B8%89%20hbase%E5%88%9D%E4%BD%93%E9%AA%8C" rel="nofollow">三 hbase初体验</a></p>

<p id="%E5%9B%9B%20HBASE%E5%AE%A2%E6%88%B7%E7%AB%AFAPI%E6%93%8D%E4%BD%9C-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%20HBASE%E5%AE%A2%E6%88%B7%E7%AB%AFAPI%E6%93%8D%E4%BD%9C" rel="nofollow">四 HBASE客户端API操作</a></p>

<p id="%E4%BA%94%20HBASE%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#%E4%BA%94%20HBASE%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86" rel="nofollow">五 HBASE运行原理</a></p>

<p id="5.1%20master%E8%81%8C%E8%B4%A3-toc" style="margin-left:40px;"><a href="#5.1%20master%E8%81%8C%E8%B4%A3" rel="nofollow">5.1 master职责</a></p>

<p id="5.2%20Region%20Server%20%E8%81%8C%E8%B4%A3-toc" style="margin-left:40px;"><a href="#5.2%20Region%20Server%20%E8%81%8C%E8%B4%A3" rel="nofollow">5.2 Region Server 职责</a></p>

<p id="5.3%20zookeeper%E9%9B%86%E7%BE%A4%E6%89%80%E8%B5%B7%E4%BD%9C%E7%94%A8-toc" style="margin-left:40px;"><a href="#5.3%20zookeeper%E9%9B%86%E7%BE%A4%E6%89%80%E8%B5%B7%E4%BD%9C%E7%94%A8" rel="nofollow">5.3 zookeeper集群所起作用</a></p>

<p id="5.4%20HBASE%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-toc" style="margin-left:40px;"><a href="#5.4%20HBASE%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B" rel="nofollow">5.4 HBASE读写数据流程</a></p>

<p id="5.5%20hbase%3Ameta%E8%A1%A8-toc" style="margin-left:40px;"><a href="#5.5%20hbase%3Ameta%E8%A1%A8" rel="nofollow">5.5 hbase:meta表</a></p>

<p id="5.6%20Region%20Server%E5%86%85%E9%83%A8%E6%9C%BA%E5%88%B6-toc" style="margin-left:40px;"><a href="#5.6%20Region%20Server%E5%86%85%E9%83%A8%E6%9C%BA%E5%88%B6" rel="nofollow">5.6 Region Server内部机制</a></p>

<hr id="hr-toc"><h1 id="%E4%B8%80%20%E4%BB%80%E4%B9%88%E6%98%AFHBASE">一 什么是HBASE</h1>

<blockquote>
<p style="margin-left:0pt;">HBASE是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBASE技术可在廉价PC Server上搭建起大规模结构化存储集群。</p>

<p style="margin-left:0pt;">HBASE的目标是存储并处理大型的数据，更具体来说是仅需使用普通的硬件配置，就能够处理由成千上万的行和列所组成的大型数据。</p>

<p style="margin-left:0pt;">HBASE是Google Bigtable的开源实现，但是也有很多不同之处。比如：Google Bigtable利用GFS作为其文件存储系统，HBASE利用Hadoop HDFS作为其文件存储系统；Google运行MAPREDUCE来处理Bigtable中的海量数据，HBASE同样利用Hadoop MapReduce来处理HBASE中的海量数据；Google Bigtable利用Chubby作为协同服务，HBASE利用Zookeeper作为对应。</p>
</blockquote>

<p style="margin-left:0pt;">HBASE与mysql、oralce、db2、sqlserver等关系型数据库不同，它是一个NoSQL数据库（非关系型数据库）</p>

<ol><li>Hbase的表模型与关系型数据库的表模型不同：</li>
	<li>Hbase的表没有固定的字段定义；</li>
	<li>Hbase的表中每行存储的都是一些<span style="color:#ff0000;">key-value对</span></li>
	<li>Hbase的表中有列族的划分，用户可以指定将哪些kv插入哪个列族</li>
	<li>Hbase的表在物理存储上，是按照列族来分割的，不同列族的数据一定存储在不同的文件中</li>
	<li>Hbase的表中的每一行都固定有一个行键，而且每一行的行键在表中不能重复</li>
	<li>Hbase中的数据，包含行键，包含key，包含value，都是byte[ ]类型，hbase不负责为用户维护数据类型</li>
	<li>HBASE对事务的支持很差</li>
</ol><p style="margin-left:0pt;">HBASE相比于其他nosql数据库(mongodb、redis、cassendra、hazelcast)的特点：</p>

<p style="margin-left:0pt;">Hbase的表数据存储在HDFS文件系统中</p>

<p style="margin-left:0pt;">从而，hbase具备如下特性：存储容量可以线性扩展； 数据存储的安全性可靠性极高！</p>

<h1 id="%E4%BA%8C%20%E5%AE%89%E8%A3%85HBASE" style="margin-left:0pt;">二 安装HBASE</h1>

<p style="margin-left:0pt;">HBASE是一个分布式系统</p>

<p style="margin-left:0pt;">其中有一个管理角色：  HMaster(一般2台，一台active，一台backup)</p>

<p style="margin-left:0pt;">其他的数据节点角色：  HRegionServer(很多台，看数据容量)</p>

<p style="margin-left:0pt;"><strong>2.1 安装准备</strong></p>

<p style="margin-left:0pt;">首先，要有一个HDFS集群，并正常运行； regionserver应该跟hdfs中的datanode在一起</p>

<p style="margin-left:0pt;">其次，还需要一个zookeeper集群，并正常运行</p>

<p style="margin-left:0pt;">然后，安装HBASE</p>

<p style="margin-left:0pt;">角色分配如下：</p>

<p style="margin-left:0pt;">Hdp01:  namenode  datanode  regionserver  hmaster  zookeeper</p>

<p style="margin-left:0pt;">Hdp02:  datanode   regionserver  zookeeper</p>

<p style="margin-left:0pt;">Hdp03:  datanode   regionserver  zookeeper</p>

<p style="margin-left:0pt;"><strong>2.2 安装步骤</strong></p>

<p style="margin-left:0pt;">解压hbase安装包</p>

<p style="margin-left:0pt;">修改hbase-env.sh</p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;">export JAVA_HOME=/root/apps/jdk1.7.0_67</p>

			<p style="margin-left:0pt;">export HBASE_MANAGES_ZK=false</p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;">修改hbase-site.xml</p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;">&lt;configuration&gt;</p>

			<p style="margin-left:0pt;">&lt;!-- 指定hbase在HDFS上存储的路径 --&gt;</p>

			<p style="margin-left:0pt;">        &lt;property&gt;</p>

			<p style="margin-left:0pt;">                &lt;name&gt;hbase.rootdir&lt;/name&gt;</p>

			<p style="margin-left:0pt;">                &lt;value&gt;hdfs://hdp01:9000/hbase&lt;/value&gt;</p>

			<p style="margin-left:0pt;">        &lt;/property&gt;</p>

			<p style="margin-left:0pt;">&lt;!-- 指定hbase是分布式的 --&gt;</p>

			<p style="margin-left:0pt;">        &lt;property&gt;</p>

			<p style="margin-left:0pt;">                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</p>

			<p style="margin-left:0pt;">                &lt;value&gt;true&lt;/value&gt;</p>

			<p style="margin-left:0pt;">        &lt;/property&gt;</p>

			<p style="margin-left:0pt;">&lt;!-- 指定zk的地址，多个用“,”分割 --&gt;</p>

			<p style="margin-left:0pt;">        &lt;property&gt;</p>

			<p style="margin-left:0pt;">                &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</p>

			<p style="margin-left:0pt;">                &lt;value&gt;hdp01:2181,hdp02:2181,hdp03:2181&lt;/value&gt;</p>

			<p style="margin-left:0pt;">        &lt;/property&gt;</p>

			<p style="margin-left:0pt;">&lt;/configuration&gt;</p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;">修改 regionservers</p>

<table border="1" cellspacing="0" style="width:426.1pt;"><tbody><tr><td style="vertical-align:top;width:426.1pt;">
			<p style="margin-left:0pt;">hdp01</p>

			<p style="margin-left:0pt;">hdp02</p>

			<p style="margin-left:0pt;">hdp03</p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;"><strong>2.3 启动hbase集群</strong></p>

<blockquote>
<p style="margin-left:0pt;">bin/start-hbase.sh</p>
</blockquote>

<p style="margin-left:0pt;">启动完后，还可以在集群中找任意一台机器启动一个备用的master</p>

<blockquote>
<p style="margin-left:0pt;">bin/hbase-daemon.sh start master</p>
</blockquote>

<p style="margin-left:0pt;">新启的这个master会处于backup状态</p>

<h1 id="%E4%B8%89%20hbase%E5%88%9D%E4%BD%93%E9%AA%8C" style="margin-left:0pt;">三 hbase初体验</h1>

<p><strong>3.1 启动hbase命令行客户端</strong></p>

<blockquote>
<p style="margin-left:0pt;">bin/hbase shell</p>

<p style="margin-left:0pt;">Hbase&gt; list     // 查看表</p>

<p style="margin-left:0pt;">Hbase&gt; status   // 查看集群状态</p>

<p style="margin-left:0pt;">Hbase&gt; version  // 查看集群版本</p>
</blockquote>

<p style="margin-left:0pt;"><strong>3.2 hbase表模型的特点</strong></p>

<p style="margin-left:0pt;"><img alt="" class="has" height="219" src="https://img-blog.csdn.net/2018080612475756?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p1b2NoYW5nX2xpdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="693"></p>

<ol><li>一个表，有表名</li>
	<li>一个表可以分为多个<strong><span style="color:#ff0000;"><strong>列族</strong></span></strong>（不同列族的数据会存储在不同文件中）</li>
	<li>表中的每一行有一个“<strong><span style="color:#ff0000;"><strong>行键rowkey</strong></span></strong>”，而且行键在表中不能重复</li>
	<li>表中的每一对kv数据称作一个<span style="color:#ff0000;">cell</span></li>
	<li>hbase可以对数据存储多个历史版本（历史版本数量可配置）</li>
	<li>整张表由于数据量过大，会被横向切分成若干个<strong><span style="color:#ff0000;"><strong>region</strong></span></strong>（用rowkey范围标识），不同region的数据也存储在不同文件中<img alt="" class="has" height="138" src="https://img-blog.csdn.net/20180806122126498?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p1b2NoYW5nX2xpdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="692"></li>
	<li>hbase会对插入的数据按顺序存储：</li>
</ol><p style="margin-left:0pt;">     要点一：首先会按行键排序</p>

<p style="margin-left:0pt;">     要点二：同一行里面的kv会按列族排序，再按k排序</p>

<p style="margin-left:0pt;"><strong>3.3 hbase的表中能存储什么数据类型</strong></p>

<p style="margin-left:0pt;">hbase中只支持byte[]</p>

<p style="margin-left:0pt;">此处的byte[] 包括了： rowkey,key,value,列族名,表名</p>

<p style="margin-left:0pt;"><strong>3.4 hbase命令行客户端操作</strong></p>

<table border="1" cellspacing="0" style="margin-left:-1.8pt;width:493pt;"><tbody><tr><td style="background-color:#336666;width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>名称</strong></strong></p>
			</td>
			<td style="background-color:#336666;width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>命令表达式</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>创建表</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#ff0000;"><strong>create </strong></span></strong><strong><strong>'表名', '列族名1','列族名2','列族名N'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>查看所有表</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>list</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>描述表</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>describe  ‘表名’</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;">判断表存在</p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;">exists  '表名'</p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;">判断是否禁用启用表</p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;">is_enabled '表名'</p>

			<p style="margin-left:0pt;">is_disabled ‘表名’</p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>添加记录      </strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>put  ‘表名’, ‘rowKey’, ‘列族 : 列‘  ,  '值'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>查看记录rowkey下的所有数据</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>get  '表名' , 'rowKey'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>查看表中的记录总数</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>count  '表名'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>获取某个列族</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><span style="color:#ff0000;">get </span>'表名','rowkey','列族'</p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>获取某个列族的某个列</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><span style="color:#ff0000;">get </span>'表名','rowkey','列族：列’</p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>删除记录</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>delete  ‘表名’ ,‘行名’ , ‘列族：列'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>删除整行</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>deleteall '</strong></strong><strong><strong>表名</strong></strong><strong><strong>','</strong></strong><strong><strong>rowkey</strong></strong><strong><strong>'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>删除一张表</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>先要屏蔽该表，才能对该表进行删除</strong></strong></p>

			<p style="margin-left:0pt;"><strong><strong>第一步 disable ‘表名’ ，第二步  drop '表名'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>清空表</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#ff0000;"><strong>truncate </strong></span></strong><strong><strong>'</strong></strong><strong><strong>表名</strong></strong><strong><strong>'</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>查看所有记录</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#ff0000;"><strong>scan </strong></span></strong><strong><strong>"表名"  </strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>查看某个表某个列中所有数据</strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><span style="color:#ff0000;"><strong>scan </strong></span></strong><strong><strong>"表名" , {COLUMNS=&gt;'列族名:列名'}</strong></strong></p>
			</td>
		</tr><tr><td style="width:127.6pt;">
			<p style="margin-left:0pt;"><strong><strong>更新记录 </strong></strong></p>
			</td>
			<td style="width:365.4pt;">
			<p style="margin-left:0pt;"><strong><strong>就是重写一遍，进行覆盖，hbase没有修改，都是追加</strong></strong></p>
			</td>
		</tr></tbody></table><p style="margin-left:0pt;"><strong>3.4.1 建表</strong></p>

<blockquote>
<p style="margin-left:0pt;">create 't_user_info','base_info','extra_info'</p>

<p style="margin-left:0pt;">                      表名      列族名   列族名</p>
</blockquote>

<p style="margin-left:0pt;"><strong>3.4.2 插入数据</strong></p>

<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;">
			<blockquote>
			<p style="margin-left:0pt;">hbase(main):011:0&gt; put 't_user_info','001','base_info:username','zhangsan'</p>

			<p style="margin-left:0pt;">0 row(s) in 0.2420 seconds</p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;">hbase(main):012:0&gt; put 't_user_info','001','base_info:age','18'</p>

			<p style="margin-left:0pt;">0 row(s) in 0.0140 seconds</p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;">hbase(main):013:0&gt; put 't_user_info','001','base_info:sex','female'</p>

			<p style="margin-left:0pt;">0 row(s) in 0.0070 seconds</p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;">hbase(main):014:0&gt; put 't_user_info','001','extra_info:career','it'</p>

			<p style="margin-left:0pt;">0 row(s) in 0.0090 seconds</p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;">hbase(main):015:0&gt; put 't_user_info','002','extra_info:career','actoress'</p>

			<p style="margin-left:0pt;">0 row(s) in 0.0090 seconds</p>

			<p style="margin-left:0pt;"> </p>

			<p style="margin-left:0pt;">hbase(main):016:0&gt; put 't_user_info','002','base_info:username','liuyifei'</p>

			<p style="margin-left:0pt;">0 row(s) in 0.0060 seconds</p>
			</blockquote>
			</td>
		</tr></tbody></table><p><strong>3.4.3 查询方式一 scan扫描</strong></p>

<blockquote>
<p style="margin-left:0pt;">hbase(main):017:0&gt; scan 't_user_info'</p>

<p style="margin-left:0pt;">ROW                               COLUMN+CELL                                                                                     </p>

<p style="margin-left:0pt;"> 001                              column=base_info:age, timestamp=1496567924507, value=18                                         </p>

<p style="margin-left:0pt;"> 001                              column=base_info:sex, timestamp=1496567934669, value=female                                     </p>

<p style="margin-left:0pt;"> 001                              column=base_info:username, timestamp=1496567889554, value=zhangsan                              </p>

<p style="margin-left:0pt;"> 001                              column=extra_info:career, timestamp=1496567963992, value=it                                     </p>

<p style="margin-left:0pt;"> 002                              column=base_info:username, timestamp=1496568034187, value=liuyifei                              </p>

<p style="margin-left:0pt;"> 002                              column=extra_info:career, timestamp=1496568008631, value=actoress    </p>
</blockquote>

<p style="margin-left:0pt;"><strong>3.4.4 查询方式二 get单行数据</strong></p>

<blockquote>
<p style="margin-left:0pt;">hbase(main):020:0&gt; get 't_user_info','001'</p>

<p style="margin-left:0pt;">COLUMN                            CELL                                                                                            </p>

<p style="margin-left:0pt;"> base_info:age                    timestamp=1496568160192, value=19                                                               </p>

<p style="margin-left:0pt;"> base_info:sex                    timestamp=1496567934669, value=female                                                           </p>

<p style="margin-left:0pt;"> base_info:username               timestamp=1496567889554, value=zhangsan                                                         </p>

<p style="margin-left:0pt;"> extra_info:career                timestamp=1496567963992, value=it                                                               </p>

<p style="margin-left:0pt;">4 row(s) in 0.0770 seconds</p>
</blockquote>

<p style="margin-left:0pt;"><strong>3.4.5 删除一个kv数据</strong></p>

<blockquote>
<p style="margin-left:0pt;">hbase(main):021:0&gt; delete 't_user_info','001','base_info:sex'</p>

<p style="margin-left:0pt;">0 row(s) in 0.0390 seconds</p>
</blockquote>

<p style="margin-left:0pt;">删除整行数据</p>

<blockquote>
<p style="margin-left:0pt;">hbase(main):024:0&gt; deleteall 't_user_info','001'</p>

<p style="margin-left:0pt;">0 row(s) in 0.0090 seconds</p>

<p style="margin-left:0pt;">hbase(main):025:0&gt; get 't_user_info','001'</p>

<p style="margin-left:0pt;">COLUMN                            CELL                                                                                            </p>

<p style="margin-left:0pt;">0 row(s) in 0.0110 seconds</p>
</blockquote>

<p style="margin-left:0pt;"><strong>3.4.6 删除整个表</strong></p>

<blockquote>
<p style="margin-left:0pt;">hbase(main):028:0&gt; disable 't_user_info'</p>

<p style="margin-left:0pt;">0 row(s) in 2.3640 seconds</p>

<p style="margin-left:0pt;">hbase(main):029:0&gt; drop 't_user_info'</p>

<p style="margin-left:0pt;">0 row(s) in 1.2950 seconds</p>

<p style="margin-left:0pt;">hbase(main):030:0&gt; list</p>

<p style="margin-left:0pt;">TABLE                                                                                                                             </p>

<p style="margin-left:0pt;">0 row(s) in 0.0130 seconds</p>

<p style="margin-left:0pt;">=&gt; []</p>
</blockquote>

<p style="margin-left:0pt;"><strong>3.5 Hbase重要特性-排序特性(行键)</strong></p>

<blockquote>
<p style="margin-left:0pt;">与nosql数据库们一样,row key是用来检索记录的主键。访问HBASE table中的行，只有三种方式：</p>

<p style="margin-left:0pt;">1.通过单个row key访问</p>

<p style="margin-left:0pt;">2.通过row key的range（正则）</p>

<p style="margin-left:0pt;">3.全表扫描</p>

<p style="margin-left:0pt;">Row key行键 (Row key)可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为 10-100bytes)，在HBASE内部，row key保存为字节数组。存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p>
</blockquote>

<p style="margin-left:0pt;">插入到hbase中去的数据，hbase会自动排序存储：</p>

<p style="margin-left:0pt;"><span style="color:#c00000;">排序规则：  首先看行键，然后看列族名，然后看列（key）名； 按字典顺序</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">Hbase的这个特性跟查询效率有极大的关系</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">比如：一张用来存储用户信息的表，有名字，户籍，年龄，职业....等信息</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">然后，在业务系统中经常需要：</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">查询某个省的所有用户</span></p>

<p style="margin-left:0pt;"><span style="color:#000000;">经常需要查询某个省的指定姓的所有用户</span></p>

<p style="margin-left:0pt;">思路：如果能将相同省的用户在hbase的存储文件中连续存储，并且能将相同省中相同姓的用户连续存储，那么，上述两个查询需求的效率就会提高！！！</p>

<p style="margin-left:0pt;">做法：将查询条件拼到rowkey内</p>

<h1 id="%E5%9B%9B%20HBASE%E5%AE%A2%E6%88%B7%E7%AB%AFAPI%E6%93%8D%E4%BD%9C" style="margin-left:0pt;">四 HBASE客户端API操作</h1>

<p><strong>4.1 简洁版 </strong></p>

<p><strong>HbaseClientDDL </strong></p>

<pre class="has">
<code class="language-java">package cn.hbase.demo;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.regionserver.BloomType;
import org.junit.Before;
import org.junit.Test;


/**
 *  
 *  1、构建连接
 *  2、从连接中取到一个表DDL操作工具admin
 *  3、admin.createTable(表描述对象);
 *  4、admin.disableTable(表名);
	5、admin.deleteTable(表名);
	6、admin.modifyTable(表名，表描述对象);	
 *  
 * @author hunter.d
 *
 */
public class HbaseClientDDL {
	Connection conn = null;
	
	@Before
	public void getConn() throws Exception{
		// 构建一个连接对象
		Configuration conf = HBaseConfiguration.create(); // 会自动加载hbase-site.xml
		conf.set("hbase.zookeeper.quorum", "hdp-01:2181,hdp-02:2181,hdp-03:2181");
		
		conn = ConnectionFactory.createConnection(conf);
	}
	
	
	
	/**
	 * DDL
	 * @throws Exception 
	 */
	@Test
	public void testCreateTable() throws Exception{

		// 从连接中构造一个DDL操作器
		Admin admin = conn.getAdmin();
		
		// 创建一个表定义描述对象
		HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf("user_info"));
		
		// 创建列族定义描述对象
		HColumnDescriptor hColumnDescriptor_1 = new HColumnDescriptor("base_info");
		hColumnDescriptor_1.setMaxVersions(3); // 设置该列族中存储数据的最大版本数,默认是1
		
		HColumnDescriptor hColumnDescriptor_2 = new HColumnDescriptor("extra_info");
		
		// 将列族定义信息对象放入表定义对象中
		hTableDescriptor.addFamily(hColumnDescriptor_1);
		hTableDescriptor.addFamily(hColumnDescriptor_2);
		
		
		// 用ddl操作器对象：admin 来建表
		admin.createTable(hTableDescriptor);
		
		// 关闭连接
		admin.close();
		conn.close();
		
	}
	
	
	/**
	 * 删除表
	 * @throws Exception 
	 */
	@Test
	public void testDropTable() throws Exception{
		
		Admin admin = conn.getAdmin();
		
		// 停用表
		admin.disableTable(TableName.valueOf("user_info"));
		// 删除表
		admin.deleteTable(TableName.valueOf("user_info"));
		
		
		admin.close();
		conn.close();
	}
	
	// 修改表定义--添加一个列族
	@Test
	public void testAlterTable() throws Exception{
		
		Admin admin = conn.getAdmin();
		
		// 取出旧的表定义信息
		HTableDescriptor tableDescriptor = admin.getTableDescriptor(TableName.valueOf("user_info"));
		
		
		// 新构造一个列族定义
		HColumnDescriptor hColumnDescriptor = new HColumnDescriptor("other_info");
		hColumnDescriptor.setBloomFilterType(BloomType.ROWCOL); // 设置该列族的布隆过滤器类型
		
		// 将列族定义添加到表定义对象中
		tableDescriptor.addFamily(hColumnDescriptor);
		
		
		// 将修改过的表定义交给admin去提交
		admin.modifyTable(TableName.valueOf("user_info"), tableDescriptor);
		
		
		admin.close();
		conn.close();
		
	}
	
	
	/**
	 * DML -- 数据的增删改查
	 */
	
	

}
</code></pre>

<p><strong>HbaseClientDML</strong></p>

<pre class="has">
<code class="language-java">package cn.hbase.demo;


import java.util.ArrayList;
import java.util.Iterator;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellScanner;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Before;
import org.junit.Test;

public class HbaseClientDML {
	Connection conn = null;
	
	@Before
	public void getConn() throws Exception{
		// 构建一个连接对象
		Configuration conf = HBaseConfiguration.create(); // 会自动加载hbase-site.xml
		conf.set("hbase.zookeeper.quorum", "hdp-01:2181,hdp-02:2181,hdp-03:2181");
		
		conn = ConnectionFactory.createConnection(conf);
	}
	
	
	/**
	 * 增
	 * 改:put来覆盖
	 * @throws Exception 
	 */
	@Test
	public void testPut() throws Exception{
		
		// 获取一个操作指定表的table对象,进行DML操作
		Table table = conn.getTable(TableName.valueOf("user_info"));
		
		// 构造要插入的数据为一个Put类型(一个put对象只能对应一个rowkey)的对象
		Put put = new Put(Bytes.toBytes("001"));
		put.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("username"), Bytes.toBytes("张三"));
		put.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("age"), Bytes.toBytes("18"));
		put.addColumn(Bytes.toBytes("extra_info"), Bytes.toBytes("addr"), Bytes.toBytes("北京"));
		
		
		Put put2 = new Put(Bytes.toBytes("002"));
		put2.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("username"), Bytes.toBytes("李四"));
		put2.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("age"), Bytes.toBytes("28"));
		put2.addColumn(Bytes.toBytes("extra_info"), Bytes.toBytes("addr"), Bytes.toBytes("上海"));
	
		
		ArrayList&lt;Put&gt; puts = new ArrayList&lt;&gt;();
		puts.add(put);
		puts.add(put2);
		
		
		// 插进去
		table.put(puts);
		
		table.close();
		conn.close();
		
	}
	
	
	/**
	 * 循环插入大量数据
	 * @throws Exception 
	 */
	@Test
	public void testManyPuts() throws Exception{
		
		Table table = conn.getTable(TableName.valueOf("user_info"));
		ArrayList&lt;Put&gt; puts = new ArrayList&lt;&gt;();
		
		for(int i=0;i&lt;100000;i++){
			Put put = new Put(Bytes.toBytes(""+i));
			put.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("username"), Bytes.toBytes("张三"+i));
			put.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("age"), Bytes.toBytes((18+i)+""));
			put.addColumn(Bytes.toBytes("extra_info"), Bytes.toBytes("addr"), Bytes.toBytes("北京"));
			
			puts.add(put);
		}
		
		table.put(puts);
		
	}
	
	/**
	 * 删
	 * @throws Exception 
	 */
	@Test
	public void testDelete() throws Exception{
		Table table = conn.getTable(TableName.valueOf("user_info"));
		
		// 构造一个对象封装要删除的数据信息
		Delete delete1 = new Delete(Bytes.toBytes("001"));
		
		Delete delete2 = new Delete(Bytes.toBytes("002"));
		delete2.addColumn(Bytes.toBytes("extra_info"), Bytes.toBytes("addr"));
		
		ArrayList&lt;Delete&gt; dels = new ArrayList&lt;&gt;();
		dels.add(delete1);
		dels.add(delete2);
		
		table.delete(dels);
		
		
		table.close();
		conn.close();
	}
	
	/**
	 * 查
	 * @throws Exception 
	 */
	@Test
	public void testGet() throws Exception{
		
		Table table = conn.getTable(TableName.valueOf("user_info"));
		
		Get get = new Get("002".getBytes());
		
		Result result = table.get(get);
		
		// 从结果中取用户指定的某个key的value
		byte[] value = result.getValue("base_info".getBytes(), "age".getBytes());
		System.out.println(new String(value));
		
		System.out.println("-------------------------");
		
		// 遍历整行结果中的所有kv单元格
		CellScanner cellScanner = result.cellScanner();
		while(cellScanner.advance()){
			Cell cell = cellScanner.current();
			
			byte[] rowArray = cell.getRowArray();  //本kv所属的行键的字节数组
			byte[] familyArray = cell.getFamilyArray();  //列族名的字节数组
			byte[] qualifierArray = cell.getQualifierArray();  //列名的字节数据
			byte[] valueArray = cell.getValueArray(); // value的字节数组
			
			System.out.println("行键: "+new String(rowArray,cell.getRowOffset(),cell.getRowLength()));
			System.out.println("列族名: "+new String(familyArray,cell.getFamilyOffset(),cell.getFamilyLength()));
			System.out.println("列名: "+new String(qualifierArray,cell.getQualifierOffset(),cell.getQualifierLength()));
			System.out.println("value: "+new String(valueArray,cell.getValueOffset(),cell.getValueLength()));
			
		}
		
		table.close();
		conn.close();
		
	}
	
	
	/**
	 * 按行键范围查询数据
	 * @throws Exception 
	 */
	@Test
	public void testScan() throws Exception{
		
		Table table = conn.getTable(TableName.valueOf("user_info"));
		
		// 包含起始行键，不包含结束行键,但是如果真的想查询出末尾的那个行键，那么，可以在末尾行键上拼接一个不可见的字节（\000）
		Scan scan = new Scan("10".getBytes(), "10000\001".getBytes());
		
		ResultScanner scanner = table.getScanner(scan);
		
		Iterator&lt;Result&gt; iterator = scanner.iterator();
		
		while(iterator.hasNext()){
			
			Result result = iterator.next();
			// 遍历整行结果中的所有kv单元格
			CellScanner cellScanner = result.cellScanner();
			while(cellScanner.advance()){
				Cell cell = cellScanner.current();
				
				byte[] rowArray = cell.getRowArray();  //本kv所属的行键的字节数组
				byte[] familyArray = cell.getFamilyArray();  //列族名的字节数组
				byte[] qualifierArray = cell.getQualifierArray();  //列名的字节数据
				byte[] valueArray = cell.getValueArray(); // value的字节数组
				
				System.out.println("行键: "+new String(rowArray,cell.getRowOffset(),cell.getRowLength()));
				System.out.println("列族名: "+new String(familyArray,cell.getFamilyOffset(),cell.getFamilyLength()));
				System.out.println("列名: "+new String(qualifierArray,cell.getQualifierOffset(),cell.getQualifierLength()));
				System.out.println("value: "+new String(valueArray,cell.getValueOffset(),cell.getValueLength()));
			}
			System.out.println("----------------------");
		}
	}
	
	@Test
	public void test(){
		String a = "000";
		String b = "000\0";
		
		System.out.println(a);
		System.out.println(b);
		
		
		byte[] bytes = a.getBytes();
		byte[] bytes2 = b.getBytes();
		
		System.out.println("");
		
	}
	
	

}
</code></pre>

<p><strong>4.2 完整版</strong></p>

<pre class="has">
<code class="language-java">package com.zgcbank.hbase;

import java.util.ArrayList;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.MasterNotRunningException;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.ZooKeeperConnectionException;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HConnection;
import org.apache.hadoop.hbase.client.HConnectionManager;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.FilterList;
import org.apache.hadoop.hbase.filter.FilterList.Operator;
import org.apache.hadoop.hbase.filter.RegexStringComparator;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class HbaseTest {

	/**
	 * 配置ss
	 */
	static Configuration config = null;
	private Connection connection = null;
	private Table table = null;

	@Before
	public void init() throws Exception {
		config = HBaseConfiguration.create();// 配置
		config.set("hbase.zookeeper.quorum", "master,work1,work2");// zookeeper地址
		config.set("hbase.zookeeper.property.clientPort", "2181");// zookeeper端口
		connection = ConnectionFactory.createConnection(config);
		table = connection.getTable(TableName.valueOf("user"));
	}

	/**
	 * 创建一个表
	 * 
	 * @throws Exception
	 */
	@Test
	public void createTable() throws Exception {
		// 创建表管理类
		HBaseAdmin admin = new HBaseAdmin(config); // hbase表管理
		// 创建表描述类
		TableName tableName = TableName.valueOf("test3"); // 表名称
		HTableDescriptor desc = new HTableDescriptor(tableName);
		// 创建列族的描述类
		HColumnDescriptor family = new HColumnDescriptor("info"); // 列族
		// 将列族添加到表中
		desc.addFamily(family);
		HColumnDescriptor family2 = new HColumnDescriptor("info2"); // 列族
		// 将列族添加到表中
		desc.addFamily(family2);
		// 创建表
		admin.createTable(desc); // 创建表
	}

	@Test
	@SuppressWarnings("deprecation")
	public void deleteTable() throws MasterNotRunningException,
			ZooKeeperConnectionException, Exception {
		HBaseAdmin admin = new HBaseAdmin(config);
		admin.disableTable("test3");
		admin.deleteTable("test3");
		admin.close();
	}

	/**
	 * 向hbase中增加数据
	 * 
	 * @throws Exception
	 */
	@SuppressWarnings({ "deprecation", "resource" })
	@Test
	public void insertData() throws Exception {
		table.setAutoFlushTo(false);
		table.setWriteBufferSize(534534534);
		ArrayList&lt;Put&gt; arrayList = new ArrayList&lt;Put&gt;();
		for (int i = 21; i &lt; 50; i++) {
			Put put = new Put(Bytes.toBytes("1234"+i));
			put.add(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes("wangwu"+i));
			put.add(Bytes.toBytes("info"), Bytes.toBytes("password"), Bytes.toBytes(1234+i));
			arrayList.add(put);
		}
		
		//插入数据
		table.put(arrayList);
		//提交
		table.flushCommits();
	}

	/**
	 * 修改数据
	 * 
	 * @throws Exception
	 */
	@Test
	public void uodateData() throws Exception {
		Put put = new Put(Bytes.toBytes("1234"));
		put.add(Bytes.toBytes("info"), Bytes.toBytes("namessss"), Bytes.toBytes("lisi1234"));
		put.add(Bytes.toBytes("info"), Bytes.toBytes("password"), Bytes.toBytes(1234));
		//插入数据
		table.put(put);
		//提交
		table.flushCommits();
	}

	/**
	 * 删除数据
	 * 
	 * @throws Exception
	 */
	@Test
	public void deleteDate() throws Exception {
		Delete delete = new Delete(Bytes.toBytes("1234"));
		table.delete(delete);
		table.flushCommits();
	}

	/**
	 * 单条查询
	 * 
	 * @throws Exception
	 */
	@Test
	public void queryData() throws Exception {
		Get get = new Get(Bytes.toBytes("1234"));
		Result result = table.get(get);
		System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password"))));
		System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("namessss"))));
		System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex"))));
	}

	/**
	 * 全表扫描
	 * 
	 * @throws Exception
	 */
	@Test
	public void scanData() throws Exception {
		Scan scan = new Scan();
		//scan.addFamily(Bytes.toBytes("info"));
		//scan.addColumn(Bytes.toBytes("info"), Bytes.toBytes("password"));
		scan.setStartRow(Bytes.toBytes("wangsf_0"));
		scan.setStopRow(Bytes.toBytes("wangwu"));
		ResultScanner scanner = table.getScanner(scan);
		for (Result result : scanner) {
			System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password"))));
			System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name"))));
			//System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("password"))));
			//System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name"))));
		}
	}

	/**
	 * 全表扫描的过滤器
	 * 列值过滤器
	 * 
	 * @throws Exception
	 */
	@Test
	public void scanDataByFilter1() throws Exception {

		// 创建全表扫描的scan
		Scan scan = new Scan();
		//过滤器：列值过滤器
		SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes("info"),
				Bytes.toBytes("name"), CompareFilter.CompareOp.EQUAL,
				Bytes.toBytes("zhangsan2"));
		// 设置过滤器
		scan.setFilter(filter);

		// 打印结果集
		ResultScanner scanner = table.getScanner(scan);
		for (Result result : scanner) {
			System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password"))));
			System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name"))));
			//System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("password"))));
			//System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name"))));
		}

	}
	/**
	 * rowkey过滤器
	 * @throws Exception
	 */
	@Test
	public void scanDataByFilter2() throws Exception {
		
		// 创建全表扫描的scan
		Scan scan = new Scan();
		//匹配rowkey以wangsenfeng开头的
		RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator("^12341"));
		// 设置过滤器
		scan.setFilter(filter);
		// 打印结果集
		ResultScanner scanner = table.getScanner(scan);
		for (Result result : scanner) {
			System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password"))));
			System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name"))));
			//System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("password"))));
			//System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name"))));
		}

		
	}
	
	/**
	 * 匹配列名前缀
	 * @throws Exception
	 */
	@Test
	public void scanDataByFilter3() throws Exception {
		
		// 创建全表扫描的scan
		Scan scan = new Scan();
		//匹配rowkey以wangsenfeng开头的
		ColumnPrefixFilter filter = new ColumnPrefixFilter(Bytes.toBytes("na"));
		// 设置过滤器
		scan.setFilter(filter);
		// 打印结果集
		ResultScanner scanner = table.getScanner(scan);
		for (Result result : scanner) {
			System.out.println("rowkey：" + Bytes.toString(result.getRow()));
			System.out.println("info:name："
					+ Bytes.toString(result.getValue(Bytes.toBytes("info"),
							Bytes.toBytes("name"))));
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age")) != null) {
				System.out.println("info:age："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info"),
								Bytes.toBytes("age"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")) != null) {
				System.out.println("infi:sex："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info"),
								Bytes.toBytes("sex"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")) != null) {
				System.out
				.println("info2:name："
						+ Bytes.toString(result.getValue(
								Bytes.toBytes("info2"),
								Bytes.toBytes("name"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("age")) != null) {
				System.out.println("info2:age："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info2"),
								Bytes.toBytes("age"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("sex")) != null) {
				System.out.println("info2:sex："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info2"),
								Bytes.toBytes("sex"))));
			}
		}
		
	}
	/**
	 * 过滤器集合
	 * @throws Exception
	 */
	@Test
	public void scanDataByFilter4() throws Exception {
		
		// 创建全表扫描的scan
		Scan scan = new Scan();
		//过滤器集合：MUST_PASS_ALL（and）,MUST_PASS_ONE(or)
		FilterList filterList = new FilterList(Operator.MUST_PASS_ONE);
		//匹配rowkey以wangsenfeng开头的
		RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator("^wangsenfeng"));
		//匹配name的值等于wangsenfeng
		SingleColumnValueFilter filter2 = new SingleColumnValueFilter(Bytes.toBytes("info"),
				Bytes.toBytes("name"), CompareFilter.CompareOp.EQUAL,
				Bytes.toBytes("zhangsan"));
		filterList.addFilter(filter);
		filterList.addFilter(filter2);
		// 设置过滤器
		scan.setFilter(filterList);
		// 打印结果集
		ResultScanner scanner = table.getScanner(scan);
		for (Result result : scanner) {
			System.out.println("rowkey：" + Bytes.toString(result.getRow()));
			System.out.println("info:name："
					+ Bytes.toString(result.getValue(Bytes.toBytes("info"),
							Bytes.toBytes("name"))));
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age")) != null) {
				System.out.println("info:age："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info"),
								Bytes.toBytes("age"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")) != null) {
				System.out.println("infi:sex："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info"),
								Bytes.toBytes("sex"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")) != null) {
				System.out
				.println("info2:name："
						+ Bytes.toString(result.getValue(
								Bytes.toBytes("info2"),
								Bytes.toBytes("name"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("age")) != null) {
				System.out.println("info2:age："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info2"),
								Bytes.toBytes("age"))));
			}
			// 判断取出来的值是否为空
			if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("sex")) != null) {
				System.out.println("info2:sex："
						+ Bytes.toInt(result.getValue(Bytes.toBytes("info2"),
								Bytes.toBytes("sex"))));
			}
		}
		
	}

	@After
	public void close() throws Exception {
		table.close();
		connection.close();
	}

}</code></pre>

<p><strong>4.3 MapReduce操作Hbase</strong></p>

<p><strong>4.3.1 实现方法</strong></p>

<p style="margin-left:0pt;">Hbase对MapReduce提供支持，它实现了TableMapper类和TableReducer类，我们只需要继承这两个类即可。</p>

<p style="margin-left:0pt;">1、写个mapper继承TableMapper&lt;Text, IntWritable&gt;</p>

<p style="margin-left:0pt;">参数：Text：mapper的输出key类型； IntWritable：mapper的输出value类型。</p>

<p style="margin-left:0pt;">      其中的map方法如下：</p>

<p style="margin-left:0pt;">map(ImmutableBytesWritable key, Result value,Context context)</p>

<p style="margin-left:0pt;"> 参数：key：rowKey；value： Result ，一行数据； context上下文</p>

<p style="margin-left:0pt;">2、写个reduce继承TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt;</p>

<p style="margin-left:0pt;">参数：Text:reducer的输入key； IntWritable：reduce的输入value；</p>

<p style="margin-left:0pt;"> ImmutableBytesWritable：reduce输出到hbase中的rowKey类型。</p>

<p style="margin-left:0pt;">      其中的reduce方法如下：</p>

<p style="margin-left:0pt;">reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context)</p>

<p style="margin-left:0pt;">参数： key：reduce的输入key；values：reduce的输入value；</p>

<p style="margin-left:0pt;"><strong>4.3.2 准备表</strong></p>

<p style="margin-left:0pt;">1、建立数据来源表‘word’，包含一个列族‘content’</p>

<p style="margin-left:0pt;">向表中添加数据，在列族中放入列‘info’，并将短文数据放入该列中，如此插入多行，行键为不同的数据即可</p>

<p style="margin-left:0pt;">2、建立输出表‘stat’，包含一个列族‘content’</p>

<p style="margin-left:0pt;">3、通过Mr操作Hbase的‘word’表，对‘content：info’中的短文做词频统计，并将统计结果写入‘stat’表的‘content：info中’，行键为单词</p>

<p style="margin-left:0pt;"><strong>4.3.3 实现</strong></p>

<pre class="has">
<code class="language-java">package com.zgcbank.hbase;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
/**
 * mapreduce操作hbase
 * @author wilson
 *
 */
public class HBaseMr {
	/**
	 * 创建hbase配置
	 */
	static Configuration config = null;
	static {
		config = HBaseConfiguration.create();
		config.set("hbase.zookeeper.quorum", "slave1,slave2,slave3");
		config.set("hbase.zookeeper.property.clientPort", "2181");
	}
	/**
	 * 表信息
	 */
	public static final String tableName = "word";//表名1
	public static final String colf = "content";//列族
	public static final String col = "info";//列
	public static final String tableName2 = "stat";//表名2
	/**
	 * 初始化表结构，及其数据
	 */
	public static void initTB() {
		HTable table=null;
		HBaseAdmin admin=null;
		try {
			admin = new HBaseAdmin(config);//创建表管理
			/*删除表*/
			if (admin.tableExists(tableName)||admin.tableExists(tableName2)) {
				System.out.println("table is already exists!");
				admin.disableTable(tableName);
				admin.deleteTable(tableName);
				admin.disableTable(tableName2);
				admin.deleteTable(tableName2);
			}
			/*创建表*/
				HTableDescriptor desc = new HTableDescriptor(tableName);
				HColumnDescriptor family = new HColumnDescriptor(colf);
				desc.addFamily(family);
				admin.createTable(desc);
				HTableDescriptor desc2 = new HTableDescriptor(tableName2);
				HColumnDescriptor family2 = new HColumnDescriptor(colf);
				desc2.addFamily(family2);
				admin.createTable(desc2);
			/*插入数据*/
				table = new HTable(config,tableName);
				table.setAutoFlush(false);
				table.setWriteBufferSize(500);
				List&lt;Put&gt; lp = new ArrayList&lt;Put&gt;();
				Put p1 = new Put(Bytes.toBytes("1"));
				p1.add(colf.getBytes(), col.getBytes(),	("The Apache Hadoop software library is a framework").getBytes());
				lp.add(p1);
				Put p2 = new Put(Bytes.toBytes("2"));p2.add(colf.getBytes(),col.getBytes(),("The common utilities that support the other Hadoop modules").getBytes());
				lp.add(p2);
				Put p3 = new Put(Bytes.toBytes("3"));
				p3.add(colf.getBytes(), col.getBytes(),("Hadoop by reading the documentation").getBytes());
				lp.add(p3);
				Put p4 = new Put(Bytes.toBytes("4"));
				p4.add(colf.getBytes(), col.getBytes(),("Hadoop from the release page").getBytes());
				lp.add(p4);
				Put p5 = new Put(Bytes.toBytes("5"));
				p5.add(colf.getBytes(), col.getBytes(),("Hadoop on the mailing list").getBytes());
				lp.add(p5);
				table.put(lp);
				table.flushCommits();
				lp.clear();
		} catch (Exception e) {
			e.printStackTrace();
		} finally {
			try {
				if(table!=null){
					table.close();
				}
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
	}
	/**
	 * MyMapper 继承 TableMapper
	 * TableMapper&lt;Text,IntWritable&gt; 
	 * Text:输出的key类型，
	 * IntWritable：输出的value类型
	 */
	public static class MyMapper extends TableMapper&lt;Text, IntWritable&gt; {
		private static IntWritable one = new IntWritable(1);
		private static Text word = new Text();
		@Override
		//输入的类型为：key：rowKey； value：一行数据的结果集Result
		protected void map(ImmutableBytesWritable key, Result value,
				Context context) throws IOException, InterruptedException {
			//获取一行数据中的colf：col
			String words = Bytes.toString(value.getValue(Bytes.toBytes(colf), Bytes.toBytes(col)));// 表里面只有一个列族，所以我就直接获取每一行的值
			//按空格分割
			String itr[] = words.toString().split(" ");
			//循环输出word和1
			for (int i = 0; i &lt; itr.length; i++) {
				word.set(itr[i]);
				context.write(word, one);
			}
		}
	}
	/**
	 * MyReducer 继承 TableReducer
	 * TableReducer&lt;Text,IntWritable&gt; 
	 * Text:输入的key类型，
	 * IntWritable：输入的value类型，
	 * ImmutableBytesWritable：输出类型，表示rowkey的类型
	 */
	public static class MyReducer extends
			TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt; {
		@Override
		protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,
				Context context) throws IOException, InterruptedException {
			//对mapper的数据求和
			int sum = 0;
			for (IntWritable val : values) {//叠加
				sum += val.get();
			}
			// 创建put，设置rowkey为单词
			Put put = new Put(Bytes.toBytes(key.toString()));
			// 封装数据
			put.add(Bytes.toBytes(colf), Bytes.toBytes(col),Bytes.toBytes(String.valueOf(sum)));
			//写到hbase,需要指定rowkey、put
			context.write(new ImmutableBytesWritable(Bytes.toBytes(key.toString())),put);
		}
	}
	
	public static void main(String[] args) throws IOException,
			ClassNotFoundException, InterruptedException {
		config.set("df.default.name", "hdfs://master:9000/");//设置hdfs的默认路径
		config.set("hadoop.job.ugi", "hadoop,hadoop");//用户名，组
		config.set("mapred.job.tracker", "master:9001");//设置jobtracker在哪
		//初始化表
		initTB();//初始化表
		//创建job
		Job job = new Job(config, "HBaseMr");//job
		job.setJarByClass(HBaseMr.class);//主类
		//创建scan
		Scan scan = new Scan();
		//可以指定查询某一列
		scan.addColumn(Bytes.toBytes(colf), Bytes.toBytes(col));
		//创建查询hbase的mapper，设置表名、scan、mapper类、mapper的输出key、mapper的输出value
		TableMapReduceUtil.initTableMapperJob(tableName, scan, MyMapper.class,Text.class, IntWritable.class, job);
		//创建写入hbase的reducer，指定表名、reducer类、job
		TableMapReduceUtil.initTableReducerJob(tableName2, MyReducer.class, job);
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}</code></pre>

<h1 id="%E4%BA%94%20HBASE%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86" style="margin-left:0pt;"><strong>五 HBASE运行原理</strong></h1>

<h2 id="5.1%20master%E8%81%8C%E8%B4%A3"><strong>5.1 master职责</strong></h2>

<p>1.管理HRegionServer，实现其负载均衡。</p>

<p>2.管理和分配HRegion，比如在HRegion split时分配新的HRegion；在HRegionServer退出时迁移其负责的HRegion到其他                 HRegionServer上。</p>

<p>3.Admin职能</p>

<p style="margin-left:0pt;">         创建、删除、修改Table的定义。实现DDL操作（namespace和table的增删改，column familiy的增删改等）。</p>

<p>4.管理namespace和table的元数据（实际存储在HDFS上）。</p>

<p>5.权限控制（ACL）。</p>

<p>6.监控集群中所有HRegionServer的状态(通过Heartbeat和监听ZooKeeper中的状态)。</p>

<h2 id="5.2%20Region%20Server%20%E8%81%8C%E8%B4%A3">5.2 Region Server 职责</h2>

<ol><li>管理自己所负责的region数据的读写。</li>
	<li>读写HDFS，管理Table中的数据。</li>
	<li>Client直接通过HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的HRegion/HRegionServer后）。</li>
</ol><h2 id="5.3%20zookeeper%E9%9B%86%E7%BE%A4%E6%89%80%E8%B5%B7%E4%BD%9C%E7%94%A8" style="margin-left:0pt;">5.3 zookeeper集群所起作用</h2>

<ol><li>存放整个HBase集群的元数据以及集群的状态信息。</li>
	<li>实现HMaster主从节点的failover。</li>
</ol><blockquote>
<p style="margin-left:0pt;"><em><em>注： HMaster通过监听ZooKeeper中的Ephemeral节点(默认：/hbase/rs/*)来监控HRegionServer的加入和宕机。</em></em></p>

<p style="margin-left:0pt;"><em><em>在第一个HMaster连接到ZooKeeper时会创建Ephemeral节点(默认：/hbasae/master)来表示Active的HMaster，其后加进来的HMaster则监听该Ephemeral节点</em></em></p>

<p style="margin-left:0pt;"><em><em>如果当前Active的HMaster宕机，则该节点消失，因而其他HMaster得到通知，而将自身转换成Active的HMaster，在变为Active的HMaster之前，它会在/hbase/masters/下创建自己的Ephemeral节点。</em></em></p>
</blockquote>

<h2 id="5.4%20HBASE%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B" style="margin-left:0pt;">5.4 HBASE读写数据流程</h2>

<blockquote>
<ol><li>从ZooKeeper(/hbase/meta-region-server)中获取hbase:meta的位置（HRegionServer的位置），缓存该位置信息。</li>
	<li>从HRegionServer中查询用户Table对应请求的RowKey所在的HRegionServer，缓存该位置信息。</li>
	<li>从查询到HRegionServer中读取Row。</li>
</ol></blockquote>

<p style="margin-left:0pt;"><em><em>注：客户会缓存这些位置信息，然而第二步它只是缓存当前RowKey对应的HRegion的位置，因而如果下一个要查的RowKey不在同一个HRegion中，则需要继续查询hbase:meta所在的HRegion，然而随着时间的推移，客户端缓存的位置信息越来越多，以至于不需要再次查找hbase:meta Table的信息，除非某个HRegion因为宕机或Split被移动，此时需要重新查询并且更新缓存。</em></em></p>

<h2 id="5.5%20hbase%3Ameta%E8%A1%A8" style="margin-left:0pt;"><em><em>5.5 hbase:meta表</em></em></h2>

<p style="margin-left:0pt;">hbase:meta表存储了所有用户HRegion的位置信息：</p>

<p style="margin-left:0pt;"><strong><strong>Rowkey：</strong></strong>tableName,regionStartKey,regionId,replicaId等；</p>

<p style="margin-left:0pt;"><strong><strong>info列族</strong></strong>：这个列族包含三个列，他们分别是：</p>

<p style="margin-left:0pt;">info:regioninfo列：</p>

<p style="margin-left:0pt;">regionId,tableName,startKey,endKey,offline,split,replicaId；</p>

<p style="margin-left:0pt;">info:server列：HRegionServer对应的server:port；</p>

<p style="margin-left:0pt;">info:serverstartcode列：HRegionServer的启动时间戳。</p>

<p style="margin-left:0pt;"><img alt="" class="has" height="341" src="https://img-blog.csdn.net/20180806135240917?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p1b2NoYW5nX2xpdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="692"></p>

<h2 id="5.6%20Region%20Server%E5%86%85%E9%83%A8%E6%9C%BA%E5%88%B6" style="margin-left:0pt;">5.6 Region Server内部机制</h2>

<p style="margin-left:0pt;"><img alt="" class="has" height="376" src="https://img-blog.csdn.net/20180806135326123?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p1b2NoYW5nX2xpdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="692"></p>

<ul><li>WAL即Write Ahead Log，在早期版本中称为HLog，它是HDFS上的一个文件，如其名字所表示的，所有写操作都会先保证将数据写入这个Log文件后，才会真正更新MemStore，最后写入HFile中。WAL文件存储在/hbase/WALs/${HRegionServer_Name}的目录中</li>
	<li>BlockCache是一个读缓存，即“引用局部性”原理（也应用于CPU，分空间局部性和时间局部性，空间局部性是指CPU在某一时刻需要某个数据，那么有很大的概率在一下时刻它需要的数据在其附近；时间局部性是指某个数据在被访问过一次后，它有很大的概率在不久的将来会被再次的访问），将数据预读取到内存中，以提升读的性能。</li>
	<li>HRegion是一个Table中的一个Region在一个HRegionServer中的表达。一个Table可以有一个或多个Region，他们可以在一个相同的HRegionServer上，也可以分布在不同的HRegionServer上，一个HRegionServer可以有多个HRegion，他们分别属于不同的Table。HRegion由多个Store(HStore)构成，每个HStore对应了一个Table在这个HRegion中的一个Column Family，即每个Column Family就是一个集中的存储单元，因而最好将具有相近IO特性的Column存储在一个Column Family，以实现高效读取(数据局部性原理，可以提高缓存的命中率)。HStore是HBase中存储的核心，它实现了读写HDFS功能，一个HStore由一个MemStore 和0个或多个StoreFile组成。</li>
	<li>MemStore是一个写缓存(In Memory Sorted Buffer)，所有数据的写在完成WAL日志写后，会 写入MemStore中，由MemStore根据一定的算法将数据Flush到地层HDFS文件中(HFile)，通常每个HRegion中的每个 Column Family有一个自己的MemStore。</li>
	<li>HFile(StoreFile) 用于存储HBase的数据(Cell/KeyValue)。在HFile中的数据是按RowKey、Column Family、Column排序，对相同的Cell(即这三个值都一样)，则按timestamp倒序排列。</li>
	<li>FLUSH详述</li>
</ul><ol><li>每一次Put/Delete请求都是先写入到MemStore中，当MemStore满后会Flush成一个新的StoreFile(底层实现是HFile)，即一个HStore(Column Family)可以有0个或多个StoreFile(HFile)。</li>
	<li>当一个HRegion中的所有MemStore的大小总和超过了hbase.hregion.memstore.flush.size的大小，默认128MB。此时当前的HRegion中所有的MemStore会Flush到HDFS中。</li>
	<li>当全局MemStore的大小超过了hbase.regionserver.global.memstore.upperLimit的大小，默认40％的内存使用量。此时当前HRegionServer中所有HRegion中的MemStore都会Flush到HDFS中，Flush顺序是MemStore大小的倒序（一个HRegion中所有MemStore总和作为该HRegion的MemStore的大小还是选取最大的MemStore作为参考？有待考证），直到总体的MemStore使用量低于hbase.regionserver.global.memstore.lowerLimit，默认38%的内存使用量。</li>
	<li>当前HRegionServer中WAL的大小超过了<em><em>hbase.regionserver.hlog.blocksize * hbase.regionserver.max.logs</em></em>的数量，当前HRegionServer中所有HRegion中的MemStore都会Flush到HDFS中，Flush使用时间顺序，最早的MemStore先Flush直到WAL的数量少于<em><em>hbase.regionserver.hlog.blocksize * hbase.regionserver.max.logs</em></em>这里说这两个相乘的默认大小是2GB，查代码，hbase.regionserver.max.logs默认值是32，而hbase.regionserver.hlog.blocksize默认是32MB。但不管怎么样，因为这个大小超过限制引起的Flush不是一件好事，可能引起长时间的延迟</li>
</ol>            </div>
                </div>