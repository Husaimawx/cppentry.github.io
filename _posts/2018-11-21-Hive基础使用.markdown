---
layout:     post
title:      Hive基础使用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>Hive启动方式：</p>
<p>1.CLI界面：使用hive命令或hive --service cli即可进入hive cli。</p>
<p>2.远程服务：使用hive --service hiveserver2 &amp;方式启动远程服务，这样就可以使用jdbc或thrift客户端调用hive数据仓库。</p>
<p><br></p>
<p></p>
<p><strong>Hive数据类型</strong><br></p>
<p>我们可以在官网的hive wiki中查看到所有的数据类型。<a href="https://cwiki.apache.org/confluence/display/Hive/Home" rel="nofollow">Hive wiki</a><br></p>
<p>Hive数据类型大体可以分成两类，基础数据类型和复杂数据类型。这里主要讨论常用类型。<br></p>
<p>1.基础数据类型。</p>
<p>  1）数字类型</p>
<pre><code class="language-html">整型    tinyint/smallint/int/integer/bigint
浮点型  float/double/decimal
</code></pre>
<p>  2）日期类型</p>
<pre><code class="language-html">timestamp/date/interval</code></pre>
<p>  3）字符串类型</p>
<p></p>
<pre><code class="language-html">string/varchar/char</code></pre>
<p></p>
<p>  4）其他类型</p>
<p></p>
<pre><code class="language-html">boolean/binary</code></pre>
<p></p>
<p>2.复杂数据类型。<br></p>
<p></p>
<pre><code class="language-html">arrays: ARRAY&lt;data_type&gt; (Note: negative values and non-constant expressions are allowed as of Hive 0.14.)
maps: MAP&lt;primitive_type, data_type&gt; (Note: negative values and non-constant expressions are allowed as of Hive 0.14.)
structs: STRUCT&lt;col_name : data_type [COMMENT col_comment], ...&gt;
union: UNIONTYPE&lt;data_type, data_type, ...&gt; (Note: Only available starting with Hive 0.7.0.)</code></pre><br><p></p>
<p><strong>Hive表</strong></p>
<p>Hive中的表可以分为内部表，外部表，分区表和桶表。</p>
<p><br></p>
<p>内部表：</p>
<p>表中的数据受表定义影响，表删除后表中数据随之删除。使用一般的语句创建的就是内部表，例如create table test (id int);</p>
<p><br></p>
外部表：
<p>数据不受表定义影响，表删除后数据仍在。</p>
<p>外部表的数据指向已经在HDFS中存在的数据。它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。外部表只有一个过程，加载数据和创建表同时完成，并不会移动到数据库目录中，和外部数据建立一个连接。当删除一个外部表时，仅删除连接。有点类似视图。</p>
<p>语法：</p>
<p></p>
<pre><code class="language-html">create external table external_test (
id int,
name string
)
row format delimited fields terminated by ','
location '/input';</code></pre>hadoop的/input/input.txt下文件<br><pre><code class="language-html">1,Tom
2,Mary
</code></pre>
<p></p>
<p><img src="https://img-blog.csdn.net/20170718135547064?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTYwNzgyODg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br></p>
<p></p>
<pre><code class="language-html">select * from external_test;
OK
NULL	NULL
NULL	NULL
NULL	NULL
1	Tom
2	Mary
Time taken: 0.496 seconds, Fetched: 5 row(s)</code></pre>删除表不会对数据造成影响：
<p></p>
<p></p>
<pre><code class="language-html">hive&gt; drop table external_text;
OK
Time taken: 0.11 seconds</code></pre>发现数据依然存在。
<p></p>
<p><br></p>
<p>分区表：</p>
<p>在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。有时候只需要扫描表中关心的一部分数据，所以就有了分区表。</p>
<p>对表进行分区，很容易实现对数据部分查询的功能。</p>
<p>语法：</p>
<p></p>
<pre><code class="language-html">create table testPartition (
id int
)
partitioned by (
name string
)
row format delimited fields terminated by ',';</code></pre>描述表：
<p></p>
<p></p>
<pre><code class="language-html">hive&gt; desc testPartition;
OK
id                  	int                 	                    
name                	string              	                    
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
name                	string              	                    
Time taken: 0.089 seconds, Fetched: 7 row(s)</code></pre>可以看到，有两个字段，并且使用name作为分区。
<p></p>
<p>当然这是不够的，我们还需要定义使用确定的值作为分区，例如：</p>
<p></p>
<pre><code class="language-html">alter table testPartition add partition (name='Mary');
alter table testPartition add partition (name='Tom');
</code></pre>这里就使用了Mary和Tom的名字作为分区。
<p></p>
<p><img src="https://img-blog.csdn.net/20170718164912239?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTYwNzgyODg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p>这样，就可以在hdfs中看到，一个分区用一个文件，这样就实现了数据的部分查询快速的功能。</p>
<p>当然像上面那样对名字进行分区很愚蠢，这只是一个demo罢了。<br></p>
<p><br></p>
<p>桶表：</p>
<p>对数据做一个hash计算，经过hash运算后，  然后对hash进行取模计算，比如mod 10，那么取模计算后，划分的每份的数据量是差不多的。</p>
<p>即自动帮我们进行分区，分区是按hash运算进行的。</p>
<p>优点：</p>
<p>hadoop进行的map任务时，每个map任务消耗的事件相差很小，做到了均衡化。</p>
<p>缺点：</p>
<p>对于where之类的业务逻辑查询没什么帮助。</p>
<p>语法：</p>
<p></p>
<pre><code class="language-html">create table testBucket (
id int,
name string
)
clustered by (name) into 5 buckets;</code></pre><br>
除了表之外，像在传统数据库中一样，我们还能创建视图。
<p></p>
<p>语法：</p>
<p></p>
<pre><code class="language-html">create view testView
as
select *
from external_test
union
select *
from testBucket;</code></pre>将两个表的结果合起来得到视图。视图可以简化我们的操作。
<p></p>
<p><br></p>
<p>上面我们已经知道了hive如何创建表，视图。</p>
<p>可是我们的表，视图总需要数据吧，如何导入数据呢？</p>
<p><strong>导入数据</strong></p>
<p>方法1：</p>
<p>使用传统的insert into 语句插入数据。</p>
<p>语法：</p>
<p></p>
<pre><code class="language-html">insert into table test2 (id, name) values(3, "King");</code></pre>但是insert into语句并不是所有版本的hive都支持的，低版本的hive是不支持insert into语句的。并且插入的时候，会生成mapReduce任务，插入速度比较慢。
<p></p>
<p>方法2：</p>
<p>使用load语句导入数据。</p>
<p>语法：</p>
<p></p>
<pre><code class="language-html">LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></pre>Local指从本地导入，不使用local则从hdfs中导入数据。
<p></p>
<p>overwrite指示是否覆盖原表数据。</p>
<p>但需要注意的是，使用load导入的数据格式一定要会这个表能识别的数据。</p>
<p>意思是除了列数据之外，列与列之间的分隔符也要是正确的。</p>
<p>如果文件以‘，’作为分隔符，则需要在创建表的时候指定表的分隔符是什么。</p>
<p>使用</p>
<p></p>
<pre><code class="language-html">row format delimited fields terminated by ',';</code></pre>来设置表的分隔符为‘，’。
<p></p>
<p>默认表的分隔符为制表符。</p>
<p>使用load语句导入数据速度非常快。</p>
<p><br></p>
<p>Hive的很多其他查询，例如where，having过滤，order by排序，group by分组，count(),avg()等聚集函数，自连接，外连接等和SQL都大同小异，这里就不赘述了。</p>
<p>值得一说的是hive的条件函数：</p>
<p>hive条件表达式：</p>
<p></p>
<pre><code class="language-html">case...when...</code></pre>
<p>例如：</p>
<p></p>
<pre><code class="language-html">select *,
  case name
  when 'King' then concat(name," hi")
  else "what"
  end
from test2</code></pre>
<p><br></p>
<p><br></p>
<p>Hive提供远程服务</p>
<p>使用JDBC/Thrift客户端的方式来访问hive数据库。</p>
<p>使用JDBC的方式连接hive数据库：</p>
<p>要连接hive数据库，首先hive数据库需要开启远程服务。</p>
<p></p>
<pre><code class="language-html">hive --service hiveserver2</code></pre>在后面的版本，使用hiveserver2代替了原本的hivesever<br>
创建一个maven项目。
<p></p>
<p>增加以下依赖：</p>
<p></p>
<pre><code class="language-html">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
    &lt;version&gt;2.6.4&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
    &lt;exclusions&gt;
        &lt;exclusion&gt;
            &lt;artifactId&gt;
                pentaho-aggdesigner-algorithm
            &lt;/artifactId&gt;
            &lt;groupId&gt;org.pentaho&lt;/groupId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
    &lt;version&gt;1.2.0&lt;/version&gt;
&lt;/dependency&gt;</code></pre>
<p></p>
<p>代码查询示例：</p>
<p></p>
<pre><code class="language-html">import java.sql.*;

/**
 * Created by haoye on 17-7-18.
 */
public class JDBC {
    private static String driverName = "org.apache.hive.jdbc.HiveDriver";//jdbc驱动路径
    private static String url = "jdbc:hive2://127.0.0.1:10000/default";//hive库地址+库名
//    private static String user = "username";//用户名
//    private static String password = "pwd";//密码
    private static String sql = "";
    private static ResultSet res;

    public static void main(String[] args) {
        Connection conn = null;
        Statement stmt = null;
        try {
            conn = getConn();
            System.out.println(conn);
            stmt = conn.createStatement();
            String tableName="test2";//hive表名
            sql = "select * from " + tableName;
            System.out.println("Running:" + sql);
            res = stmt.executeQuery(sql);
            System.out.println("执行 select * query 运行结果:");
            while (res.next()) {
                System.out.println(res.getInt(1) + "\t" + res.getString(2));
            }

        } catch (ClassNotFoundException e) {
            e.printStackTrace();
            System.exit(1);
        } catch (SQLException e) {
            e.printStackTrace();
            System.exit(1);
        } finally {
            try {
                if (conn != null) {
                    conn.close();
                    conn = null;
                }
                if (stmt != null) {
                    stmt.close();
                    stmt = null;
                }
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }

    private static Connection getConn() throws ClassNotFoundException,
            SQLException {
        Class.forName(driverName);
        Connection conn = DriverManager.getConnection(url, "", "");
        return conn;
    }
}
</code></pre>只需要和平常使用jdbc一样的方式使用，只是驱动不同。<br>
可以得到结果。<br><p></p>
<p></p>
<pre><code class="language-html">Running:select * from test2
执行 select * query 运行结果:
3	King
0	null
0	null</code></pre><br><br><p></p>
<p>最后，我们来讨论一下hive的自定义函数。</p>
<p>hive本身有很多自带的函数，例如count()，concat()等，我们也可以自定义函数来完成复杂的功能。</p>
<p>hive自定义函数只需要编写函数继承UDF，并且实现evaluate方法即可。<br></p>
<p>例如：</p>
<p></p><pre><code class="language-java">public class ConcatString extends UDF {

    public Text evaluate(Text a, Text b) {
        return new Text(a.toString() + "------" + b.toString());
    }

}</code></pre>evaluate中的参数必须是hive的类型。<br>
实现了之后，直接使用maven进行打包。
<p></p><pre><code class="language-java">mvn package</code></pre>打包了之后可以在目录下的target/找到对应的jar包。
<p><br></p>
<p>之后进入hive cli。</p>
<p></p><pre><code class="language-java">hive&gt; add jar 你的工程目录/target/你的jar包.jar</code></pre>生成临时自定义函数：
<p></p><pre><code class="language-java">create temporary function myconcat as 'ConcatString';</code></pre>这样我们就可以调用我们的自定义函数了。
<p></p><pre><code class="language-java">hive&gt; select  myconcat('hello', 'world');
OK
hello------world
Time taken: 0.207 seconds, Fetched: 1 row(s)</code></pre><br>
上面的方式生成的是临时自定义函数，在下一个会话当中就会失效，并不是永久的。
<p>如果我们希望生成永久自定义函数怎么办呢？</p>
<p>首先我们要把jar上传到hdfs中。</p>
<p></p><pre><code class="language-java"> hadoop fs -put thriftTest-1.0-SNAPSHOT.jar 'hdfs:///input/'</code></pre><br>
然后进入hive cli。创建永久自定义函数。
<p></p><pre><code class="language-java">create function myconcat2 as 'ConcatString' using jar 'hdfs:///input/thriftTest-1.0-SNAPSHOT.jar';</code></pre>创建好了之后就可以调用了。
<p>可以验证，即使退出，下次也能再次调用。</p>
<p>删除永久自定义函数：</p>
<p></p><pre><code class="language-java">drop function myconcat2;</code></pre><br>
删除临时自定义函数：
<p></p><pre><code class="language-java">drop temporary function myconcat;</code></pre><br><br><p><br></p>
            </div>
                </div>