---
layout:     post
title:      Spark概述详解
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：本文为博主原创文章，未经博主允许不得转载。					https://blog.csdn.net/wtzhm/article/details/80722084				</div>
								            <div id="content_views" class="markdown_views prism-atom-one-dark">
							<!-- flowchart 箭头图标 勿删 -->
							<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path></svg>
							<h2><a id="Spark_0"></a>Spark概述详解</h2>
<h3><a id="1_spark_2"></a>1. spark概念</h3>
<p>官网：<a href="http://spark.apache.org" rel="nofollow">http://spark.apache.org</a>，Spark是一种快速、通用、可扩展的大数据分析引擎。<br>
Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。</p>
<h3><a id="2spark_6"></a>2.spark特点</h3>
<ol>
<li>
<p>快： 与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。</p>
<pre><code> A.spark是基于内存的运行框架，比hadoop基于磁盘对数据的读取或者处理要快。
 
 B.spark 提供了更高级的执行引擎（DAG），支持数据流的处理与内存计算
 
 C. mapreduce它的作业都是以进程的级别进行运行。启动和运行需要一定的开销的。
   spark是基于线程模型的，也是有线程池的。
</code></pre>
</li>
<li>
<p>易用：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。</p>
</li>
<li>
<p>通用：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。</p>
</li>
<li>
<p>兼容性：Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。</p>
</li>
</ol>
<h3><a id="3_24"></a>3.核心组件</h3>
<p>SparkCore: 核心部分  包含Spark基本功能（任务调度 内存管理 容错机制等）</p>
<p>SparkSQL: Spark中交互式处理模块</p>
<p>SparkStreaming: Spark中流式数据处理的模块</p>
<p>SparkMLib：Spark机器学习相关模块 =&gt; Mahout</p>
<p>SparkGraphX: Spark中图形计算的模块</p>
<p>SparkManagers：集群管理  （HadoopYARN、ApacheMesos、Spark自带的单独调度器）</p>
<h3><a id="4_sparkmapreduce_38"></a>4. spark与mapreduce的比较</h3>
<p>代码繁琐<br>
只能支持map和reduce方法<br>
执行效率低下<br>
不适合迭代多次、交互式、流式的处理<br>
Spark计算的核心思路就是将数据集缓存在内存中加快读取速度，Spark的中间结果放到内存中，一次创建数据集，可以多次迭代运算，减少IO开销。适合运算比较多的ML和DL。</p>
<p><img src="https://i.imgur.com/hG5E4dz.png" alt=""><br>
Hadoop MapReduce计算模型是磁盘级的计算，下图是Hadoop权威指南上很经典的一幅关于MapReduce计算过程的截图</p>
<p><img src="https://i.imgur.com/YHuvVvn.png" alt=""><br>
Hadoop MapReduce将每个计算任务都划分为Map、Shuffle和Reduce三个阶段，Map的输入和输出要读写磁盘，Reduce的输入和输出也要读写磁盘，这对于通过递归迭代算法来解决的问题，如机器学习和数据挖掘，无疑在性能上产生很大的影响。</p>
<p><strong>Spark内存级计算模型</strong><br>
Spark的DAG(有向无环图作业)，Spark实现了非常精致的作业调度系统，这是Spark的精髓所在.<br>
<img src="https://i.imgur.com/O11h2eQ.png" alt=""></p>
<h3><a id="5_spark_56"></a>5. 安装spark集群</h3>
<ol>
<li>
<p>下载spark.tgz安装包，解压到安装目录</p>
</li>
<li>
<p>进入到spark安装目录，进入conf目录并重命名并修改spark-env.sh.template文件<br>
修改文件：mv spark-env.sh.template <a href="http://spark-env.sh" rel="nofollow">spark-env.sh</a><br>
vi <a href="http://spark-evn.sh" rel="nofollow">spark-evn.sh</a><br>
在该配置文件中添加如下配置<br>
export JAVA_HOME=/usr/java/jdk1.7.0_45<br>
export SPARK_MASTER_IP=master  <br>
export SPARK_MASTER_PORT=7077<br>
保存退出</p>
</li>
<li>
<p>重命名并修改slaves.template文件 mv slaves.template slaves<br>
在该文件中添加子节点所在的位置（Worker节点）vi slaves<br>
slave1 <br>
slave2<br>
slave3<br>
保存退出,将配置好的Spark拷贝到其他节点上</p>
</li>
<li>
<p>Spark集群配置完毕，目前是1个Master，3个Work，在master上启动Spark集群<br>
启动：/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh<br>
启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，<br>
登录Spark管理界面查看集群状态（主节点）：<a href="http://master:8080/" rel="nofollow">http://master:8080/</a></p>
</li>
<li>
<p>spark 本地模式运行 :    bin/spark-shell --master  local[2]</p>
</li>
<li>
<p>编译spark源码的两种方式</p>
</li>
</ol>
<p><strong>maven编译：</strong><br>
./build/mvn   -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package</p>
<p><strong>make-distribution编译：</strong><br>
./dev/make-distribution.sh --name custom-spark --tgz -Psparkr -Phadoop-2.4 -Phive -Phive-thriftserver -Pmesos -Pyarn</p>
<h3><a id="6_spark_shell_89"></a>6. spark shell</h3>
<ol>
<li>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</li>
<li>启动spark shell<br>
/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell &lt;/br&gt;<br>
–master spark://master:7077 \指定Master的地址<br>
–executor-memory 2g \指定每个worker可用内存为2G<br>
–total-executor-cores 2 \指定整个集群使用的cup核数为2个</li>
<li>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</li>
<li>执行spark程序<br>
/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit &lt;/br&gt;<br>
–class org.apache.spark.examples.SparkPi &lt;/br&gt;<br>
–master spark://master:7077 &lt;/br&gt;<br>
–executor-memory 1G &lt;/br&gt;<br>
–total-executor-cores 2 &lt;/br&gt;<br>
/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \100</li>
</ol>
<h3><a id="7_105"></a>7.总结</h3>
<p><img src="https://i.imgur.com/8TH9zGi.png" alt=""></p>

            </div>
						<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-9e5741c4b9.css" rel="stylesheet">
                </div>