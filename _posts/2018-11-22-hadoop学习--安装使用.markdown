---
layout:     post
title:      hadoop学习--安装使用
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<p>原理普及：</p>
<p>关于hadoop的原理，可以去很多地方普及，这里不再啰嗦。</p>
<p>hadoop内部结构：</p>
<p><img src="http://hi.csdn.net/attachment/201108/8/0_1312783004Uh3h.gif" alt=""><br></p>
<p>hadoop执行map-reduce流程图：</p>
<p><img src="http://hi.csdn.net/attachment/201108/8/0_131278305431m6.gif" alt=""><br></p>
<p>使用教程：</p>
<p>1.下载hadoop core 源代码：http://apache.etoak.com//hadoop/common/hadoop-0.20.2/hadoop-0.20.2.tar.gz</p>
<p>2.安装jdk（pass），修改hadoop脚本中java home变量：songjings-macpro31:conf songjing$ sudo vim bin/hadoop-env.sh </p>
<p>3.跑一个hadoop自带的单机模式的例子，单词统计：</p>
<p>bin/hadoop jar hadoop-0.20.2-examples.jar  wordcount test-in test-out<br></p>
<p>其中test-in为一个目录，目录中是多个文件，文件内容即为待统计的字符串</p>
<p>test-out为该例子输出结果目录</p>
<p>4.查看计算过程和结果：</p>
<p></p>
songjing$ bin/hadoop jar hadoop-0.20.2-examples.jar  wordcount test-in test-out<br>
11/08/08 13:37:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=<br>
11/08/08 13:37:46 INFO input.FileInputFormat: Total input paths to process : 2    ##输入文件为两个<br>
11/08/08 13:37:47 INFO mapred.JobClient: Running job: job_local_0001  ##启动一个job<br>
11/08/08 13:37:48 INFO input.FileInputFormat: Total input paths to process : 2<br>
11/08/08 13:37:48 INFO mapred.MapTask: io.sort.mb = 100<br>
11/08/08 13:37:50 INFO mapred.MapTask: data buffer = 79691776/99614720<br>
11/08/08 13:37:50 INFO mapred.MapTask: record buffer = 262144/327680<br>
11/08/08 13:37:50 INFO mapred.JobClient:  map 0% reduce 0% ##map开始<br>
11/08/08 13:37:50 INFO mapred.MapTask: Starting flush of map output<br>
11/08/08 13:37:50 INFO mapred.MapTask: Finished spill 0<br>
11/08/08 13:37:50 INFO mapred.TaskRunner: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting<br>
11/08/08 13:37:50 INFO mapred.LocalJobRunner: <br>
11/08/08 13:37:50 INFO mapred.TaskRunner: Task 'attempt_local_0001_m_000000_0' done.<br>
11/08/08 13:37:50 INFO mapred.MapTask: io.sort.mb = 100<br>
11/08/08 13:37:51 INFO mapred.JobClient:  map 100% reduce 0%<br>
11/08/08 13:37:51 INFO mapred.MapTask: data buffer = 79691776/99614720<br>
11/08/08 13:37:51 INFO mapred.MapTask: record buffer = 262144/327680<br>
11/08/08 13:37:51 INFO mapred.MapTask: Starting flush of map output<br>
11/08/08 13:37:51 INFO mapred.MapTask: Finished spill 0<br>
11/08/08 13:37:51 INFO mapred.TaskRunner: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting<br>
11/08/08 13:37:51 INFO mapred.LocalJobRunner: <br>
11/08/08 13:37:51 INFO mapred.TaskRunner: Task 'attempt_local_0001_m_000001_0' done.<br>
11/08/08 13:37:51 INFO mapred.LocalJobRunner: <br>
11/08/08 13:37:51 INFO mapred.Merger: Merging 2 sorted segments  ##两个文件的结果进行合并<br>
11/08/08 13:37:51 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 76 bytes<br>
11/08/08 13:37:51 INFO mapred.LocalJobRunner: <br>
11/08/08 13:37:51 INFO mapred.TaskRunner: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting<br>
11/08/08 13:37:51 INFO mapred.LocalJobRunner: <br>
11/08/08 13:37:51 INFO mapred.TaskRunner: Task attempt_local_0001_r_000000_0 is allowed to commit now<br>
11/08/08 13:37:51 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to test-out ##map结束，reduce开始<br>
11/08/08 13:37:51 INFO mapred.LocalJobRunner: reduce &gt; reduce<br>
11/08/08 13:37:51 INFO mapred.TaskRunner: Task 'attempt_local_0001_r_000000_0' done.<br>
11/08/08 13:37:52 INFO mapred.JobClient:  map 100% reduce 100%<br>
11/08/08 13:37:52 INFO mapred.JobClient: Job complete: job_local_0001<br>
11/08/08 13:37:52 INFO mapred.JobClient: Counters: 12<br>
11/08/08 13:37:52 INFO mapred.JobClient:   FileSystemCounters<br>
11/08/08 13:37:52 INFO mapred.JobClient:     FILE_BYTES_READ=467984<br>
11/08/08 13:37:52 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=513615<br>
11/08/08 13:37:52 INFO mapred.JobClient:   Map-Reduce Framework<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Reduce input groups=5<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Combine output records=6<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Map input records=2<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Reduce shuffle bytes=0<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Reduce output records=5<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Spilled Records=12<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Map output bytes=81<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Combine input records=8<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Map output records=8<br>
11/08/08 13:37:52 INFO mapred.JobClient:     Reduce input records=6<br>
songjings-macpro31:hadoop-0.20.2 songjing$ cd test-out/<br>
songjings-macpro31:test-out songjing$ ls<br><p>part-r-00000</p>
<p>察看最后的结果：</p>
songjings-macpro31:test-out songjing$ more part-r-00000 <br>
by      1<br>
goodbye 1<br>
hadoop  2<br>
hello   2<br><p>world   2</p>
<p>5.如何写job：</p>
<p>实现mapper和reducer对应的方法map(),reduce():</p>
<p>map方法：</p>
<p></p>
<pre><code class="language-java"> public static class TokenizerMapper 
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
      
    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }</code></pre>
<p></p>
<p> 以上map方法扫描文件中的每一个字符并将起存储为key-value，其中key为单词，value为1</p>
reduce方法：
<p></p>
<pre><code class="language-java">public static class IntSumReducer 
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }</code></pre>
<p></p>
<p>reduce工作对每个key对应的list进行统计个数</p>
写完map和reduce方法后，需要写主程序来调用map和reduce方法执行job工作：
<p></p>
<pre><code class="language-java">public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length != 2) {
      System.err.println("Usage: wordcount &lt;in&gt; &lt;out&gt;");
      System.exit(2);
    }
    Job job = new Job(conf, "word count");
    job.setJarByClass(WordCount.class);//设置跑job的map-reduce jar包
    job.setMapperClass(TokenizerMapper.class);//指定map实现类
    job.setCombinerClass(IntSumReducer.class);//指定对map结果按照key合并的类
    job.setReducerClass(IntSumReducer.class);//指定reduce类
    job.setOutputKeyClass(Text.class);//设置输出字段的key为text
    job.setOutputValueClass(IntWritable.class);//设置输出字段的value为int类型
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));//设置要分析的文件路径
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));//设置输出的文件路径
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }</code></pre><br><p>注意：output路径不能是已存在目录，不然hadoop会报错：</p>
<p></p><pre><code class="language-java">// Ensure that the output directory is set and not already there
    Path outDir = getOutputPath(job);
    if (outDir == null &amp;&amp; job.getNumReduceTasks() != 0) {
      throw new InvalidJobConfException("Output directory not set in JobConf.");
    }
    if (outDir != null) {
      FileSystem fs = outDir.getFileSystem(job);
      // normalize the output directory
      outDir = fs.makeQualified(outDir);
      setOutputPath(job, outDir);
      // check its existence
      if (fs.exists(outDir)) {
        throw new FileAlreadyExistsException("Output directory " + outDir + 
                                             " already exists");
      }
    }</code></pre><br><br><p></p>
            </div>
                </div>