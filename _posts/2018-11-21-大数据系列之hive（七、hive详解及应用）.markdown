---
layout:     post
title:      大数据系列之hive（七、hive详解及应用）
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								<div class="article-copyright">
					版权声明：版权所有，转载、分享请说明转载地址。					https://blog.csdn.net/snail_bing/article/details/82840840				</div>
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                <p id="main-toc"><strong>目录</strong></p>

<p id="%E4%B8%80%E3%80%81HIVE%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81HIVE%E6%A6%82%E8%BF%B0" rel="nofollow">一、HIVE概述</a></p>

<p id="1.Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98-toc" style="margin-left:40px;"><a href="#1.Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98" rel="nofollow">1.Hadoop分布式计算遇到的问题</a></p>

<p id="2.HQL-toc" style="margin-left:40px;"><a href="#2.HQL" rel="nofollow">2.HQL</a></p>

<p id="3.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB-toc" style="margin-left:40px;"><a href="#3.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB" rel="nofollow">3.数据仓库与数据库的主要区别</a></p>

<p id="%E4%BA%8C%E3%80%81Hive%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81Hive%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE" rel="nofollow">二、Hive的安装配置</a></p>

<p id="%E4%B8%89%E3%80%81Hive%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81Hive%E5%8E%9F%E7%90%86" rel="nofollow">三、Hive原理</a></p>

<p id="%E5%9B%9B%E3%80%81Hive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81Hive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93" rel="nofollow">四、Hive的元数据库</a></p>

<p id="1.%E9%85%8D%E7%BD%AEmysql%E4%BD%9C%E4%B8%BAhive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93-toc" style="margin-left:40px;"><a href="#1.%E9%85%8D%E7%BD%AEmysql%E4%BD%9C%E4%B8%BAhive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93" rel="nofollow">1.配置mysql作为hive的元数据库</a></p>

<p id="2.%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;"><a href="#2.%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BB%8B%E7%BB%8D" rel="nofollow">2.元数据库介绍</a></p>

<p id="%E4%BA%94%E3%80%81%E5%86%85%E9%83%A8%E8%A1%A8%E3%80%81%E5%A4%96%E9%83%A8%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81%E5%86%85%E9%83%A8%E8%A1%A8%E3%80%81%E5%A4%96%E9%83%A8%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89" rel="nofollow">五、内部表、外部表（重要）</a></p>

<p id="%E5%85%AD%E3%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E5%85%AD%E3%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89" rel="nofollow">六、分区表（重要）</a></p>

<p id="%E4%B8%83%E3%80%81%E5%88%86%E6%A1%B6%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%B8%83%E3%80%81%E5%88%86%E6%A1%B6%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89" rel="nofollow">七、分桶表（重要）</a></p>

<p id="%E5%85%AB%E3%80%81Hive%E7%9A%84%E8%AF%AD%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E5%85%AB%E3%80%81Hive%E7%9A%84%E8%AF%AD%E6%B3%95" rel="nofollow">八、Hive的语法</a></p>

<p id="%E4%B9%9D%E3%80%81HIVE%E7%9A%84UDF-toc" style="margin-left:0px;"><a href="#%E4%B9%9D%E3%80%81HIVE%E7%9A%84UDF" rel="nofollow">九、HIVE的UDF</a></p>

<p id="%E5%8D%81%E3%80%81HIVE%E7%9A%84java%20api%E6%93%8D%E4%BD%9C-toc" style="margin-left:0px;"><a href="#%E5%8D%81%E3%80%81HIVE%E7%9A%84java%20api%E6%93%8D%E4%BD%9C" rel="nofollow">十、HIVE的java api操作</a></p>

<hr id="hr-toc"><h1 id="%E4%B8%80%E3%80%81HIVE%E6%A6%82%E8%BF%B0">一、HIVE概述</h1>

<h2 id="1.Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98">1.Hadoop分布式计算遇到的问题</h2>

<p style="text-indent:50px;">MapReduce只能用java开发(也支持其他语言，但是不是主流)需要对Hadoop的底层原理 api比较了解才能顺畅的开发出分布式的处理程序开发调试麻烦。</p>

<h2 id="2.HQL">2.HQL</h2>

<p style="text-indent:50px;">Hive通过类SQL的语法，来进行分布式的计算。HQL用起来和SQL非常的类似，Hive在执行的过程中会将HQL转换为MapReduce去执行，所以Hive其实是基于Hadoop的一种分布式计算框架，底层仍然是MapReduce，所以它本质上还是一种离线大数据分析工具。</p>

<p style="text-indent:50px;">Hive是基于Hadoop的一个数据仓库工具。可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>

<p style="text-indent:50px;">Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HiveQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。</p>

<p style="text-indent:50px;"><strong>Hive不支持在线事务处理，.也不支持行级的插入和更新和删除。</strong></p>

<h2 id="3.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB"><br><strong>3.数据仓库与数据库的主要区别</strong></h2>

<p style="text-indent:50px;">（1）数据库是面向事务的设计，数据仓库是面向主题设计的。</p>

<p style="text-indent:50px;">（2）数据库一般存储在线交易数据，数据仓库存储的一般是历史数据。 </p>

<p style="text-indent:50px;">（3）数据库设计是尽量避免冗余，数据仓库在设计是有意引入冗余。</p>

<p style="text-indent:50px;">（4）数据库是为捕获数据而设计，数据仓库是为分析数据而设计。</p>

<p style="text-indent:50px;">（5）数据库实现完整的增删改查，数据仓库只能一次写入多次查询 不支持行级别的增删改。</p>

<p style="text-indent:50px;">（6）数据库具有完整的事务保证，数据仓库不强调事务特性。</p>

<h1 id="%E4%BA%8C%E3%80%81Hive%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE">二、Hive的安装配置</h1>

<p style="text-indent:50px;">下载从apache官网下载新版本hive，要注意和hadoop版本的匹配。</p>

<p style="text-indent:0;"> <strong>安装</strong></p>

<p style="text-indent:50px;">需要提前安装好jdk和hadoop，并且配置过JAVA_HOME和HADOOP_HOME，解压即可安装，安装完成后可以进入bin下执行hive脚本，如果能够进入hive命令行说明hive安装成功。</p>

<h1 id="%E4%B8%89%E3%80%81Hive%E5%8E%9F%E7%90%86">三、Hive原理</h1>

<p style="text-indent:0px;">结论1:hive中的数据库对应hdfs中/user/hive/warehouse目录下以.db结尾的目录。<br>
结论2:hive中的表对应hdfs/user/hive/warehouse/[db目录]中的一个目录<br>
结论3:hive中的数据对应当前hive表对应的hdfs目录中的文件。<br>
结论4:hive会将命令转换为mapreduce执行。<br>
结论5:hive默认的default数据库直接对应/user/hive/warehouse目录，在default库中创建的表直接会在该目录下创建对应目录。</p>

<h1 id="%E5%9B%9B%E3%80%81Hive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93">四、Hive的元数据库</h1>

<p style="text-indent:50px;">Hive默认使用Derby作为元数据库，但是这种方式只能用于测试不应用于生产环境中，生产环境中应该使用其他数据库作为元数据储存的数据库使用。Hive目前支持Derby和MySql作为元数据库使用。</p>

<h2 id="1.%E9%85%8D%E7%BD%AEmysql%E4%BD%9C%E4%B8%BAhive%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93">1.配置mysql作为hive的元数据库</h2>

<p style="text-indent:50px;">mysql在linux中的安装过程这里不讲述。</p>

<p style="text-indent:0;"><strong>步骤：</strong></p>

<p style="text-indent:0;">（1）删除hdfs中的/user/hive</p>

<pre class="has">
<code>hadoop fs -rmr /user/hive   </code></pre>

<p style="text-indent:0;">（2）复制hive/conf/hive-default.xml.template为hive-site.xml</p>

<pre class="has">
<code>cp hive-default.xml.template hive-site.xml </code></pre>

<p style="text-indent:0;">在&lt;configuration&gt;中进行配置</p>

<pre class="has">
<code>#配置数据库连接地址
            &lt;property&gt;
              &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
              &lt;value&gt;jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
              &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
            &lt;/property&gt;

#配置数据库驱动
            &lt;property&gt;
              &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
              &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
              &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
            &lt;/property&gt;

#配置数据库用户名
            &lt;property&gt;
              &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
              &lt;value&gt;root&lt;/value&gt;
              &lt;description&gt;username to use against metastore database&lt;/description&gt;
            &lt;/property&gt;

#配置数据库登录密码
            &lt;property&gt;
              &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
              &lt;value&gt;root&lt;/value&gt;
              &lt;description&gt;password to use against metastore database&lt;/description&gt;
            &lt;/property&gt;</code></pre>

<p><strong>配置过mysql元数据库后可能遇到的问题：</strong></p>

<p style="text-indent:50px;">（1）hive无法启动，提示缺少驱动包：将mysql的数据库驱动包拷贝到hive/lib下</p>

<p style="text-indent:50px;">（2）hive无法启动，提示连接被拒绝：在mysql中启动权限：</p>

<p style="text-indent:50px;">#(执行下面的语句  *.*:所有库下的所有表   %：任何IP地址或主机都可以连接)<br>
            GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;<br>
            FLUSH PRIVILEGES;</p>

<p style="text-indent:50px;">（3）hive能够启动，但是在执行一些操作时报奇怪的错误：原因很可能是mysql中hive库不是latin编码，删除元数据库手动创建hive库，指定编码集为latin1。</p>

<p style="text-indent:50px;">连接mysql，发现多了一个hive库。其中保存有hive的元数据。</p>

<h2 id="2.%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BB%8B%E7%BB%8D"><strong>2.元数据库介绍</strong></h2>

<p>    DBS：数据库的元数据信息<br>
        DB_ID 数据库编号<br>
        DB_LOCATION_URI 数据库在hdfs中的位置<br>
    TBLS：表信息。<br>
        TBL_ID 表的编号<br>
        DB_ID 属于哪个数据库<br>
        TBL_NAME 表的名称<br>
        TBL_TYPE 表的类型 内部表MANAGED_TABLE/外部表<br>
        COLUMNS_V2 表中字段信息<br>
        CD_ID 所属表的编号<br>
        COLUMN_NAME 列明<br>
        TYPE_NAME 列的类型名<br>
    SDS：表对应hdfs目录<br>
        CD_ID 表的编号<br>
        LOCATION 表对应到hdfs中的位置</p>

<h1 id="%E4%BA%94%E3%80%81%E5%86%85%E9%83%A8%E8%A1%A8%E3%80%81%E5%A4%96%E9%83%A8%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89">五、内部表、外部表（重要）</h1>

<p><strong>内部表：</strong><br>
        如果表是通过hive来创建，其中的数据也是通过hive来导入，则先有表后有表中的数据，则称这种表为内部表。<br><strong>外部表：</strong><br>
        如果是先有的数据文件，再创建hive表来管理，则这样的表称为外部表。<br>
      <strong>  创建语法：</strong>create <span style="color:#f33b45;">external </span>table ext_student(id int ,name string) row format delimited fields terminated by '\t' location '/datax';</p>

<p><strong>注意：</strong></p>

<p style="text-indent:50px;"><strong>内部表被Drop的时候，表关联的hdfs中的文件夹和其中的文件都会被删除。</strong></p>

<p style="text-indent:50px;"><strong>外部表被Drop的时候，表关联的hdfs中的文件夹和其中的文件都不会被删除。</strong></p>

<p style="text-indent:0;">内部表/外部表可以在表文件夹下直接上传符合格式的文件，从而增加表中的记录。</p>

<h1 id="%E5%85%AD%E3%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89">六、分区表（重要）</h1>

<p style="text-indent:0px;">hive也支持分区表<br>
    对数据进行分区可以提高查询时的效率 。<br>
    普通表和分区表区别：有大量数据增加的需要建分区表<br>
    create table book (id bigint, name string) <span style="color:#f33b45;">partitioned </span>by (category string) row format delimited fields terminated by '\t'; <br>
    create table book (id bigint, name string) <span style="color:#f33b45;">partitioned </span>by (category string,gender string) row format delimited fields terminated by '\t'; </p>

<p style="text-indent:50px;">所谓的分区表是又在表文件夹下创建了子文件夹（[分区的名]=分区名字的值 ）分别存放同一张表中不同分区的数据，从而将数据分区存放，提高按照分区查询数据时的效率。所以当数据量比较大，而且数据经常要按照某个字段作为条件进行查询时，最好按照该条件进行分区存放。通过观察元数据库发现，分区表也会当作元数据存放在SDS表中。</p>

<p style="text-indent:50px;"><strong>注意：</strong>如果手动的创建分区目录，是无法被表识别到的，因为在元数据库中并没有该分区的信息，如果想让手动创建的分区能够被表识别需要在元数据库SDS表中增加分区的元数据信息。</p>

<pre class="has">
<code>ALTER TABLE book add  PARTITION (category = 'jp',gender='male') location '/user/hive/warehouse/testx.db/book/category=jp/gender=male';</code></pre>

<h1 id="%E4%B8%83%E3%80%81%E5%88%86%E6%A1%B6%E8%A1%A8%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89">七、分桶表（重要）</h1>

<p style="text-indent:50px;">hive也支持分桶表。分桶操作是更细粒度的分配方式。一张表可以同时分区和分桶。分桶的原理是根据指定的列的计算hash值模余分桶数量后将数据分开存放。分桶的主要作用是：对于一个庞大的数据集我们经常需要拿出来一小部分作为样例，然后在样例上验证我们的查询，优化我们的程序。</p>

<p style="text-indent:0;"><strong>创建带桶的 table ：</strong><br>
        create table teacher(id int,name string) clustered by (id) into 4 buckets row format delimited fields terminated by '\t';<br>
强制多个 reduce 进行输出：<br>
        set hive.enforce.bucketing=true; <br>
往表中插入数据：<br>
        insert overwrite table teacher select * from temp;//需要提前准备好temp，从temp查询数据写入到teacher<br>
查看表的结构，会发现当前表下有四个文件：<br>
        dfs -ls /user/hive/warehouse/teacher;<br>
读取数据，看一个文件的数据：<br>
        dfs -cat /user/hive/warehouse/teacher/000000_0;<br>
        //桶使用 hash 来实现，所以每个文件拥有的数据的个数都有可能不相等。<br>
对桶中的数据进行采样：<br>
        select * from teacher tablesample(bucket 1 out of 4 on id);<br>
        //桶的个数从1开始计数，前面的查询从4个桶中的第一个桶获取数据。其实就是四分之一。<br>
查询一半返回的桶数：<br>
        select * from bucketed_user tablesample(bucket 1 out of 2 on id);<br>
        //桶的个数从1开始计数，前2个桶中的第一个桶获取数据。其实就是二分之一。</p>

<h1 id="%E5%85%AB%E3%80%81Hive%E7%9A%84%E8%AF%AD%E6%B3%95">八、Hive的语法</h1>

<p style="text-indent:0;"><strong>1.数据类型</strong><br>
        TINYINT - byte<br>
        SMALLINT - short<br>
        INT    - int<br>
        BIGINT - long<br>
        BOOLEAN - boolean<br>
        FLOAT - float<br>
        DOUBLE - double<br>
        STRING - String<br>
        TIMESTAMP - TimeStamp<br>
        BINARY - byte[]</p>

<p style="text-indent:0;"><strong>2.create table</strong><br>
        create table book (id bigint, name string)  row format delimited fields terminated by '\t'; <br>
        create external table book (id bigint, name string)  row format delimited fields terminated by '\t' location 'xxxxx'; <br>
        create table book (id bigint, name string) partitioned by (category string) row format delimited fields terminated by '\t'; <br>
        <br><strong>3.修改表</strong><br>
        增加分区:ALTER TABLE book add  PARTITION (category = 'zazhi') location '/user/hive/warehouse/datax.db/book/category=zazhi';<br>
        删除分区:ALTER TABLE table_name DROP partition_spec, partition_spec,...<br>
        重命名表:ALTER TABLE table_name RENAME TO new_table_name<br>
     修改列:ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]<br>
       增加/替换列:ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</p>

<p style="text-indent:0;"><strong>4.Show</strong><br>
        查看数据库<br>
        SHOW DATABASES;<br>
        查看表名<br>
        SHOW TABLES;</p>

<p>        查看表名，部分匹配<br>
        SHOW TABLES 'page.*';<br>
        SHOW TABLES '.*view';</p>

<p>        查看某表的所有Partition，如果没有就报错：<br>
        SHOW PARTITIONS page_view;</p>

<p>        查看某表结构：<br>
        DESCRIBE invites;</p>

<p>        查看分区内容<br>
        SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';</p>

<p>        查看有限行内容，同Greenplum，用limit关键词<br>
        SELECT a.foo FROM invites a limit 3;</p>

<p>        查看表分区定义<br>
        DESCRIBE EXTENDED page_view PARTITION (ds='2008-08-08');</p>

<p><strong>5.Load</strong><br>
        LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]<br>
        Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。</p>

<p><strong>6.Insert</strong><br>
        (1)将查询的结果插入指定表中<br>
        INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement<br>
        (2)将查询结果写入文件系统中<br>
         INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...</p>

<p><strong>7.Drop</strong><br>
        删除一个内部表的同时会同时删除表的元数据和数据。删除一个外部表，只删除元数据而保留数据。</p>

<p><strong>8.Limit </strong><br>
        Limit 可以限制查询的记录数。查询的结果是随机选择的</p>

<p><strong>9.Select</strong><br>
        SELECT [ALL | DISTINCT] select_expr, select_expr, ...<br>
        FROM table_reference<br>
        [WHERE where_condition] <br>
        [GROUP BY col_list]<br>
        [   CLUSTER BY col_list<br>
          | [DISTRIBUTE BY col_list] [SORT BY col_list]<br>
        ]<br>
        [LIMIT number]</p>

<p><strong>10.JOIN</strong><br>
        join_table:<br>
            table_reference JOIN table_factor [join_condition]<br>
          | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition<br>
          | table_reference LEFT SEMI JOIN table_reference join_condition</p>

<p>        table_reference:<br>
            table_factor<br>
          | join_table</p>

<p>        table_factor:<br>
            tbl_name [alias]<br>
          | table_subquery alias<br>
          | ( table_references )</p>

<p>        join_condition:<br>
            ON equality_expression ( AND equality_expression )*</p>

<p>        equality_expression:<br>
            expression = expression</p>

<h1 id="%E4%B9%9D%E3%80%81HIVE%E7%9A%84UDF">九、HIVE的UDF</h1>

<p style="text-indent:50px;">如果hive的内置函数不够用，我们也可以自己定义函数来使用，这样的函数称为hive的用户自定义函数，简称UDF。</p>

<p>    新建java工程，导入hive相关包，导入hive相关的lib。<br>
    创建类继承UDF<br>
    自己编写一个evaluate方法，返回值和参数任意。<br>
    为了能让mapreduce处理，String要用Text处理。<br>
    将写好的类打成jar包，上传到linux中<br>
    在hive命令行下，向hive注册UDF：add jar /xxxx/xxxx.jar<br>
    为当前udf起一个名字：create temporary function fname as '类的全路径名';<br>
    之后就可以在hql中使用该自定义函数了。</p>

<h1 id="%E5%8D%81%E3%80%81HIVE%E7%9A%84java%20api%E6%93%8D%E4%BD%9C">十、HIVE的java api操作</h1>

<p style="text-indent:50px;">hive实现了jdbc接口，所以可以非常方便用jdbc技术通过java代码操作。</p>

<p style="text-indent:0;"><strong>1.在服务器端开启HiveServer服务</strong><br>
        ./hive --service hiveserver2</p>

<p style="text-indent:0;"><strong>2.创建本地工程，导入jar包</strong><br>
        导入hive\lib目录下的hive-jdbc-1.2.0-standalone.jar<br>
        导入hadoop-2.7.1\share\hadoop\common下的hadoop-common-2.7.1.jar</p>

<p><strong>3.编写jdbc代码执行</strong><br>
        Class.forName("org.apache.hive.jdbc.HiveDriver");<br>
        Connection conn = DriverManager.getConnection("jdbc:hive2://192.168.242.101:10000/park","root","root");<br>
        Statement statement = conn.createStatement();<br>
        //statement.executeUpdate("create table x2xx (id int,name string)");<br>
        //其他sql语句...<br>
        statement.close();<br>
        conn.close();</p>

<p>以上就是hive的全部内容，如有疑问和建议欢迎留言。</p>            </div>
                </div>